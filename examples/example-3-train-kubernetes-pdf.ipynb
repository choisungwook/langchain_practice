{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PDF파일 로드"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.document_loaders import PyPDFLoader\n",
    "loader = PyPDFLoader(\"./PDFS/Concepts.pdf\")\n",
    "pages = loader.load_and_split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(page_content='The Concepts section helps you learn about the parts of the Kubernetes system and the\\nabstractions Kubernetes uses to represent your cluster , and helps you obtain a deeper\\nunderstanding of how Kubernetes works.\\nOverview\\nKubernetes is a portable, extensible, open source platform for managing containerized\\nworkloads and services, that facilitates both declarative configuration and automation. It has a\\nlarge, rapidly growing ecosystem. Kubernetes services, support, and tools are widely available.\\nCluster Architecture\\nThe architectural concepts behind Kubernetes.\\nContainers\\nTechnology for packaging an application along with its runtime dependencies.\\nWorkloads\\nUnderstand Pods, the smallest deployable compute object in Kubernetes, and the higher-level\\nabstractions that help you to run them.\\nServices, Load Balancing, and Networking\\nConcepts and resources behind networking in Kubernetes.\\nStorage\\nWays to provide both long-term and temporary storage to Pods in your cluster.\\nConfiguration\\nResources that Kubernetes provides for configuring Pods.\\nSecurity\\nConcepts for keeping your cloud-native workload secure.\\nPolicies\\nManage security and best-practices with policies.\\nScheduling, Preemption and Eviction\\nIn Kubernetes, scheduling refers to making sure that Pods are matched to Nodes so that the\\nkubelet can run them. Preemption is the process of terminating Pods with lower Priority so that', metadata={'source': './PDFS/Concepts.pdf', 'page': 0}),\n",
       " Document(page_content='Pods with higher Priority can schedule on Nodes. Eviction is the process of proactively\\nterminating one or more Pods on resource-starved Nodes.\\nCluster Administration\\nLower-level detail relevant to creating or administering a Kubernetes cluster.\\nWindows in Kubernetes\\nKubernetes supports nodes that run Microsoft Windows.\\nExtending Kubernetes\\nDifferent ways to change the behavior of your Kubernetes cluster.\\nOverview\\nKubernetes is a portable, extensible, open source platform for managing containerized\\nworkloads and services, that facilitates both declarative configuration and automation. It has a\\nlarge, rapidly growing ecosystem. Kubernetes services, support, and tools are widely available.\\nThis page is an overview of Kubernetes.\\nKubernetes is a portable, extensible, open source platform for managing containerized\\nworkloads and services, that facilitates both declarative configuration and automation. It has a\\nlarge, rapidly growing ecosystem. Kubernetes services, support, and tools are widely available.\\nThe name Kubernetes originates from Greek, meaning helmsman or pilot. K8s as an\\nabbreviation results from counting the eight letters between the \"K\" and the \"s\". Google open-\\nsourced the Kubernetes project in 2014. Kubernetes combines over 15 years of Google\\'s\\nexperience  running production workloads at scale with best-of-breed ideas and practices from\\nthe community.\\nGoing back in time\\nLet\\'s take a look at why Kubernetes is so useful by going back in time.\\nDeployment evolution\\nTraditional deployment era:  Early on, organizations ran applications on physical servers.\\nThere was no way to define resource boundaries for applications in a physical server, and this\\ncaused resource allocation issues. For example, if multiple applications run on a physical server,\\nthere can be instances where one application would take up most of the resources, and as a\\nresult, the other applications would underperform. A solution for this would be to run each\\napplication on a different physical server. But this did not scale as resources were underutilized,\\nand it was expensive for organizations to maintain many physical servers.\\nVirtualized deployment era:  As a solution, virtualization was introduced. It allows you to\\nrun multiple Virtual Machines (VMs) on a single physical server\\'s CPU. Virtualization allows\\napplications to be isolated between VMs and provides a level of security as the information of\\none application cannot be freely accessed by another application.', metadata={'source': './PDFS/Concepts.pdf', 'page': 1}),\n",
       " Document(page_content=\"Virtualization allows better utilization of resources in a physical server and allows better\\nscalability because an application can be added or updated easily, reduces hardware costs, and\\nmuch more. With virtualization you can present a set of physical resources as a cluster of\\ndisposable virtual machines.\\nEach VM is a full machine running all the components, including its own operating system, on\\ntop of the virtualized hardware.\\nContainer deployment era:  Containers are similar to VMs, but they have relaxed isolation\\nproperties to share the Operating System (OS) among the applications. Therefore, containers\\nare considered lightweight. Similar to a VM, a container has its own filesystem, share of CPU,\\nmemory, process space, and more. As they are decoupled from the underlying infrastructure,\\nthey are portable across clouds and OS distributions.\\nContainers have become popular because they provide extra benefits, such as:\\nAgile application creation and deployment: increased ease and efficiency of container\\nimage creation compared to VM image use.\\nContinuous development, integration, and deployment: provides for reliable and frequent\\ncontainer image build and deployment with quick and efficient rollbacks (due to image\\nimmutability).\\nDev and Ops separation of concerns: create application container images at build/release\\ntime rather than deployment time, thereby decoupling applications from infrastructure.\\nObservability: not only surfaces OS-level information and metrics, but also application\\nhealth and other signals.\\nEnvironmental consistency across development, testing, and production: runs the same\\non a laptop as it does in the cloud.\\nCloud and OS distribution portability: runs on Ubuntu, RHEL, CoreOS, on-premises, on\\nmajor public clouds, and anywhere else.\\nApplication-centric management: raises the level of abstraction from running an OS on\\nvirtual hardware to running an application on an OS using logical resources.\\nLoosely coupled, distributed, elastic, liberated micro-services: applications are broken\\ninto smaller, independent pieces and can be deployed and managed dynamically – not a\\nmonolithic stack running on one big single-purpose machine.\\nResource isolation: predictable application performance.\\nResource utilization: high efficiency and density.\\nWhy you need Kubernetes and what it can do\\nContainers are a good way to bundle and run your applications. In a production environment,\\nyou need to manage the containers that run the applications and ensure that there is no\\ndowntime. For example, if a container goes down, another container needs to start. Wouldn't it\\nbe easier if this behavior was handled by a system?\\nThat's how Kubernetes comes to the rescue! Kubernetes provides you with a framework to run\\ndistributed systems resiliently. It takes care of scaling and failover for your application, provides\\ndeployment patterns, and more. For example: Kubernetes can easily manage a canary\\ndeployment for your system.• \\n• \\n• \\n• \\n• \\n• \\n• \\n• \\n• \\n•\", metadata={'source': './PDFS/Concepts.pdf', 'page': 2}),\n",
       " Document(page_content=\"Kubernetes provides you with:\\nService discovery and load balancing  Kubernetes can expose a container using the\\nDNS name or using their own IP address. If traffic to a container is high, Kubernetes is\\nable to load balance and distribute the network traffic so that the deployment is stable.\\nStorage orchestration  Kubernetes allows you to automatically mount a storage system\\nof your choice, such as local storages, public cloud providers, and more.\\nAutomated rollouts and rollbacks  You can describe the desired state for your deployed\\ncontainers using Kubernetes, and it can change the actual state to the desired state at a\\ncontrolled rate. For example, you can automate Kubernetes to create new containers for\\nyour deployment, remove existing containers and adopt all their resources to the new\\ncontainer.\\nAutomatic bin packing  You provide Kubernetes with a cluster of nodes that it can use\\nto run containerized tasks. You tell Kubernetes how much CPU and memory (RAM) each\\ncontainer needs. Kubernetes can fit containers onto your nodes to make the best use of\\nyour resources.\\nSelf-healing  Kubernetes restarts containers that fail, replaces containers, kills containers\\nthat don't respond to your user-defined health check, and doesn't advertise them to\\nclients until they are ready to serve.\\nSecret and configuration management  Kubernetes lets you store and manage\\nsensitive information, such as passwords, OAuth tokens, and SSH keys. You can deploy\\nand update secrets and application configuration without rebuilding your container\\nimages, and without exposing secrets in your stack configuration.\\nBatch execution  In addition to services, Kubernetes can manage your batch and CI\\nworkloads, replacing containers that fail, if desired.\\nHorizontal scaling  Scale your application up and down with a simple command, with a\\nUI, or automatically based on CPU usage.\\nIPv4/IPv6 dual-stack  Allocation of IPv4 and IPv6 addresses to Pods and Services\\nDesigned for extensibility  Add features to your Kubernetes cluster without changing\\nupstream source code.\\nWhat Kubernetes is not\\nKubernetes is not a traditional, all-inclusive PaaS (Platform as a Service) system. Since\\nKubernetes operates at the container level rather than at the hardware level, it provides some\\ngenerally applicable features common to PaaS offerings, such as deployment, scaling, load\\nbalancing, and lets users integrate their logging, monitoring, and alerting solutions. However,\\nKubernetes is not monolithic, and these default solutions are optional and pluggable.\\nKubernetes provides the building blocks for building developer platforms, but preserves user\\nchoice and flexibility where it is important.\\nKubernetes:\\nDoes not limit the types of applications supported. Kubernetes aims to support an\\nextremely diverse variety of workloads, including stateless, stateful, and data-processing\\nworkloads. If an application can run in a container, it should run great on Kubernetes.\\nDoes not deploy source code and does not build your application. Continuous Integration,\\nDelivery, and Deployment (CI/CD) workflows are determined by organization cultures\\nand preferences as well as technical requirements.\\nDoes not provide application-level services, such as middleware (for example, message\\nbuses), data-processing frameworks (for example, Spark), databases (for example,\\nMySQL), caches, nor cluster storage systems (for example, Ceph) as built-in services.• \\n• \\n• \\n• \\n• \\n• \\n• \\n• \\n• \\n• \\n• \\n• \\n•\", metadata={'source': './PDFS/Concepts.pdf', 'page': 3}),\n",
       " Document(page_content='Such components can run on Kubernetes, and/or can be accessed by applications running\\non Kubernetes through portable mechanisms, such as the Open Service Broker .\\nDoes not dictate logging, monitoring, or alerting solutions. It provides some integrations\\nas proof of concept, and mechanisms to collect and export metrics.\\nDoes not provide nor mandate a configuration language/system (for example, Jsonnet). It\\nprovides a declarative API that may be targeted by arbitrary forms of declarative\\nspecifications.\\nDoes not provide nor adopt any comprehensive machine configuration, maintenance,\\nmanagement, or self-healing systems.\\nAdditionally, Kubernetes is not a mere orchestration system. In fact, it eliminates the\\nneed for orchestration. The technical definition of orchestration is execution of a defined\\nworkflow: first do A, then B, then C. In contrast, Kubernetes comprises a set of\\nindependent, composable control processes that continuously drive the current state\\ntowards the provided desired state. It shouldn\\'t matter how you get from A to C.\\nCentralized control is also not required. This results in a system that is easier to use and\\nmore powerful, robust, resilient, and extensible.\\nWhat\\'s next\\nTake a look at the Kubernetes Components\\nTake a look at the The Kubernetes API\\nTake a look at the Cluster Architecture\\nReady to Get Started ?\\nObjects In Kubernetes\\nKubernetes objects are persistent entities in the Kubernetes system. Kubernetes uses these\\nentities to represent the state of your cluster. Learn about the Kubernetes object model and how\\nto work with these objects.\\nThis page explains how Kubernetes objects are represented in the Kubernetes API, and how you\\ncan express them in .yaml  format.\\nUnderstanding Kubernetes objects\\nKubernetes objects  are persistent entities in the Kubernetes system. Kubernetes uses these\\nentities to represent the state of your cluster. Specifically, they can describe:\\nWhat containerized applications are running (and on which nodes)\\nThe resources available to those applications\\nThe policies around how those applications behave, such as restart policies, upgrades, and\\nfault-tolerance\\nA Kubernetes object is a \"record of intent\"--once you create the object, the Kubernetes system\\nwill constantly work to ensure that object exists. By creating an object, you\\'re effectively telling\\nthe Kubernetes system what you want your cluster\\'s workload to look like; this is your cluster\\'s\\ndesired state .\\nTo work with Kubernetes objects—whether to create, modify, or delete them—you\\'ll need to use\\nthe Kubernetes API . When you use the kubectl  command-line interface, for example, the CLI• \\n• \\n• \\n• \\n• \\n• \\n• \\n• \\n• \\n• \\n•', metadata={'source': './PDFS/Concepts.pdf', 'page': 4}),\n",
       " Document(page_content=\"makes the necessary Kubernetes API calls for you. You can also use the Kubernetes API directly\\nin your own programs using one of the Client Libraries .\\nObject spec and status\\nAlmost every Kubernetes object includes two nested object fields that govern the object's\\nconfiguration: the object spec and the object status . For objects that have a spec, you have to set\\nthis when you create the object, providing a description of the characteristics you want the\\nresource to have: its desired state .\\nThe status  describes the current state  of the object, supplied and updated by the Kubernetes\\nsystem and its components. The Kubernetes control plane  continually and actively manages\\nevery object's actual state to match the desired state you supplied.\\nFor example: in Kubernetes, a Deployment is an object that can represent an application\\nrunning on your cluster. When you create the Deployment, you might set the Deployment spec\\nto specify that you want three replicas of the application to be running. The Kubernetes system\\nreads the Deployment spec and starts three instances of your desired application--updating the\\nstatus to match your spec. If any of those instances should fail (a status change), the Kubernetes\\nsystem responds to the difference between spec and status by making a correction--in this case,\\nstarting a replacement instance.\\nFor more information on the object spec, status, and metadata, see the Kubernetes API\\nConventions .\\nDescribing a Kubernetes object\\nWhen you create an object in Kubernetes, you must provide the object spec that describes its\\ndesired state, as well as some basic information about the object (such as a name). When you\\nuse the Kubernetes API to create the object (either directly or via kubectl ), that API request\\nmust include that information as JSON in the request body. Most often, you provide the\\ninformation to kubectl  in file known as a manifest . By convention, manifests are YAML (you\\ncould also use JSON format). Tools such as kubectl  convert the information from a manifest into\\nJSON or another supported serialization format when making the API request over HTTP.\\nHere's an example manifest that shows the required fields and object spec for a Kubernetes\\nDeployment:\\napplication/deployment.yaml  \\napiVersion : apps/v1\\nkind: Deployment\\nmetadata :\\n  name : nginx-deployment\\nspec:\\n  selector :\\n    matchLabels :\\n      app: nginx\\n  replicas : 2 # tells deployment to run 2 pods matching the template\\n  template :\\n    metadata :\\n      labels :\\n        app: nginx\", metadata={'source': './PDFS/Concepts.pdf', 'page': 5}),\n",
       " Document(page_content=\"spec:\\n      containers :\\n      - name : nginx\\n        image : nginx:1.14.2\\n        ports :\\n        - containerPort : 80\\nOne way to create a Deployment using a manifest file like the one above is to use the kubectl \\napply  command in the kubectl  command-line interface, passing the .yaml  file as an argument.\\nHere's an example:\\nkubectl apply -f https://k8s.io/examples/application/deployment.yaml\\nThe output is similar to this:\\ndeployment.apps/nginx-deployment created\\nRequired fields\\nIn the manifest (YAML or JSON file) for the Kubernetes object you want to create, you'll need to\\nset values for the following fields:\\napiVersion  - Which version of the Kubernetes API you're using to create this object\\nkind - What kind of object you want to create\\nmetadata  - Data that helps uniquely identify the object, including a name  string, UID, and\\noptional namespace\\nspec - What state you desire for the object\\nThe precise format of the object spec is different for every Kubernetes object, and contains\\nnested fields specific to that object. The Kubernetes API Reference  can help you find the spec\\nformat for all of the objects you can create using Kubernetes.\\nFor example, see the spec field  for the Pod API reference. For each Pod, the .spec  field specifies\\nthe pod and its desired state (such as the container image name for each container within that\\npod). Another example of an object specification is the spec field  for the StatefulSet API. For\\nStatefulSet, the .spec  field specifies the StatefulSet and its desired state. Within the .spec  of a\\nStatefulSet is a template  for Pod objects. That template describes Pods that the StatefulSet\\ncontroller will create in order to satisfy the StatefulSet specification. Different kinds of object\\ncan also have different .status ; again, the API reference pages detail the structure of that .status\\nfield, and its content for each different type of object.\\nNote:  See Configuration Best Practices  for additional information on writing YAML\\nconfiguration files.\\nServer side field validation\\nStarting with Kubernetes v1.25, the API server offers server side field validation  that detects\\nunrecognized or duplicate fields in an object. It provides all the functionality of kubectl --\\nvalidate  on the server side.\\nThe kubectl  tool uses the --validate  flag to set the level of field validation. It accepts the values \\nignore , warn , and strict  while also accepting the values true (equivalent to strict ) and false\\n(equivalent to ignore ). The default validation setting for kubectl  is --validate=true .• \\n• \\n• \\n•\", metadata={'source': './PDFS/Concepts.pdf', 'page': 6}),\n",
       " Document(page_content=\"Strict\\nStrict field validation, errors on validation failure\\nWarn\\nField validation is performed, but errors are exposed as warnings rather than failing the\\nrequest\\nIgnore\\nNo server side field validation is performed\\nWhen kubectl  cannot connect to an API server that supports field validation it will fall back to\\nusing client-side validation. Kubernetes 1.27 and later versions always offer field validation;\\nolder Kubernetes releases might not. If your cluster is older than v1.27, check the\\ndocumentation for your version of Kubernetes.\\nWhat's next\\nIf you're new to Kubernetes, read more about the following:\\nPods  which are the most important basic Kubernetes objects.\\nDeployment  objects.\\nControllers  in Kubernetes.\\nkubectl  and kubectl commands .\\nKubernetes Object Management  explains how to use kubectl  to manage objects. You might need\\nto install kubectl  if you don't already have it available.\\nTo learn about the Kubernetes API in general, visit:\\nKubernetes API overview\\nTo learn about objects in Kubernetes in more depth, read other pages in this section:\\nKubernetes Object Management\\nObject Names and IDs\\nLabels and Selectors\\nNamespaces\\nAnnotations\\nField Selectors\\nFinalizers\\nOwners and Dependents\\nRecommended Labels\\nKubernetes Object Management\\nThe kubectl  command-line tool supports several different ways to create and manage\\nKubernetes objects . This document provides an overview of the different approaches. Read the \\nKubectl book  for details of managing objects by Kubectl.\\nManagement techniques\\nWarning:  A Kubernetes object should be managed using only one technique. Mixing and\\nmatching techniques for the same object results in undefined behavior.• \\n• \\n• \\n• \\n• \\n• \\n• \\n• \\n• \\n• \\n• \\n• \\n• \\n•\", metadata={'source': './PDFS/Concepts.pdf', 'page': 7}),\n",
       " Document(page_content='Management\\ntechniqueOperates onRecommended\\nenvironmentSupported\\nwritersLearning\\ncurve\\nImperative commands Live objects Development projects 1+ Lowest\\nImperative object\\nconfigurationIndividual files Production projects 1 Moderate\\nDeclarative object\\nconfigurationDirectories of\\nfilesProduction projects 1+ Highest\\nImperative commands\\nWhen using imperative commands, a user operates directly on live objects in a cluster. The user\\nprovides operations to the kubectl  command as arguments or flags.\\nThis is the recommended way to get started or to run a one-off task in a cluster. Because this\\ntechnique operates directly on live objects, it provides no history of previous configurations.\\nExamples\\nRun an instance of the nginx container by creating a Deployment object:\\nkubectl create deployment nginx --image nginx\\nTrade-offs\\nAdvantages compared to object configuration:\\nCommands are expressed as a single action word.\\nCommands require only a single step to make changes to the cluster.\\nDisadvantages compared to object configuration:\\nCommands do not integrate with change review processes.\\nCommands do not provide an audit trail associated with changes.\\nCommands do not provide a source of records except for what is live.\\nCommands do not provide a template for creating new objects.\\nImperative object configuration\\nIn imperative object configuration, the kubectl command specifies the operation (create,\\nreplace, etc.), optional flags and at least one file name. The file specified must contain a full\\ndefinition of the object in YAML or JSON format.\\nSee the API reference  for more details on object definitions.\\nWarning:  The imperative replace  command replaces the existing spec with the newly provided\\none, dropping all changes to the object missing from the configuration file. This approach\\nshould not be used with resource types whose specs are updated independently of the\\nconfiguration file. Services of type LoadBalancer , for example, have their externalIPs  field\\nupdated independently from the configuration by the cluster.• \\n• \\n• \\n• \\n• \\n•', metadata={'source': './PDFS/Concepts.pdf', 'page': 8}),\n",
       " Document(page_content='Examples\\nCreate the objects defined in a configuration file:\\nkubectl create -f nginx.yaml\\nDelete the objects defined in two configuration files:\\nkubectl delete -f nginx.yaml -f redis.yaml\\nUpdate the objects defined in a configuration file by overwriting the live configuration:\\nkubectl replace -f nginx.yaml\\nTrade-offs\\nAdvantages compared to imperative commands:\\nObject configuration can be stored in a source control system such as Git.\\nObject configuration can integrate with processes such as reviewing changes before push\\nand audit trails.\\nObject configuration provides a template for creating new objects.\\nDisadvantages compared to imperative commands:\\nObject configuration requires basic understanding of the object schema.\\nObject configuration requires the additional step of writing a YAML file.\\nAdvantages compared to declarative object configuration:\\nImperative object configuration behavior is simpler and easier to understand.\\nAs of Kubernetes version 1.5, imperative object configuration is more mature.\\nDisadvantages compared to declarative object configuration:\\nImperative object configuration works best on files, not directories.\\nUpdates to live objects must be reflected in configuration files, or they will be lost during\\nthe next replacement.\\nDeclarative object configuration\\nWhen using declarative object configuration, a user operates on object configuration files\\nstored locally, however the user does not define the operations to be taken on the files. Create,\\nupdate, and delete operations are automatically detected per-object by kubectl . This enables\\nworking on directories, where different operations might be needed for different objects.\\nNote:  Declarative object configuration retains changes made by other writers, even if the\\nchanges are not merged back to the object configuration file. This is possible by using the patch\\nAPI operation to write only observed differences, instead of using the replace  API operation to\\nreplace the entire object configuration.• \\n• \\n• \\n• \\n• \\n• \\n• \\n• \\n•', metadata={'source': './PDFS/Concepts.pdf', 'page': 9}),\n",
       " Document(page_content=\"Examples\\nProcess all object configuration files in the configs  directory, and create or patch the live\\nobjects. You can first diff to see what changes are going to be made, and then apply:\\nkubectl diff -f configs/\\nkubectl apply -f configs/\\nRecursively process directories:\\nkubectl diff -R -f configs/\\nkubectl apply -R -f configs/\\nTrade-offs\\nAdvantages compared to imperative object configuration:\\nChanges made directly to live objects are retained, even if they are not merged back into\\nthe configuration files.\\nDeclarative object configuration has better support for operating on directories and\\nautomatically detecting operation types (create, patch, delete) per-object.\\nDisadvantages compared to imperative object configuration:\\nDeclarative object configuration is harder to debug and understand results when they are\\nunexpected.\\nPartial updates using diffs create complex merge and patch operations.\\nWhat's next\\nManaging Kubernetes Objects Using Imperative Commands\\nImperative Management of Kubernetes Objects Using Configuration Files\\nDeclarative Management of Kubernetes Objects Using Configuration Files\\nDeclarative Management of Kubernetes Objects Using Kustomize\\nKubectl Command Reference\\nKubectl Book\\nKubernetes API Reference\\nObject Names and IDs\\nEach object  in your cluster has a Name  that is unique for that type of resource. Every\\nKubernetes object also has a UID that is unique across your whole cluster.\\nFor example, you can only have one Pod named myapp-1234  within the same namespace , but\\nyou can have one Pod and one Deployment that are each named myapp-1234 .\\nFor non-unique user-provided attributes, Kubernetes provides labels  and annotations .• \\n• \\n• \\n• \\n• \\n• \\n• \\n• \\n• \\n• \\n•\", metadata={'source': './PDFS/Concepts.pdf', 'page': 10}),\n",
       " Document(page_content='Names\\nA client-provided string that refers to an object in a resource URL, such as /api/v1/pods/some-\\nname .\\nOnly one object of a given kind can have a given name at a time. However, if you delete the\\nobject, you can make a new object with the same name.\\nNames must be unique across all API versions  of the same resource. API resources are\\ndistinguished by their API group, resource type, namespace (for namespaced\\nresources), and name. In other words, API version is irrelevant in this context.\\nNote:  In cases when objects represent a physical entity, like a Node representing a physical\\nhost, when the host is re-created under the same name without deleting and re-creating the\\nNode, Kubernetes treats the new host as the old one, which may lead to inconsistencies.\\nBelow are four types of commonly used name constraints for resources.\\nDNS Subdomain Names\\nMost resource types require a name that can be used as a DNS subdomain name as defined in \\nRFC 1123 . This means the name must:\\ncontain no more than 253 characters\\ncontain only lowercase alphanumeric characters, \\'-\\' or \\'.\\'\\nstart with an alphanumeric character\\nend with an alphanumeric character\\nRFC 1123 Label Names\\nSome resource types require their names to follow the DNS label standard as defined in RFC\\n1123. This means the name must:\\ncontain at most 63 characters\\ncontain only lowercase alphanumeric characters or \\'-\\'\\nstart with an alphanumeric character\\nend with an alphanumeric character\\nRFC 1035 Label Names\\nSome resource types require their names to follow the DNS label standard as defined in RFC\\n1035. This means the name must:\\ncontain at most 63 characters\\ncontain only lowercase alphanumeric characters or \\'-\\'\\nstart with an alphabetic character\\nend with an alphanumeric character\\nPath Segment Names\\nSome resource types require their names to be able to be safely encoded as a path segment. In\\nother words, the name may not be \".\" or \"..\" and the name may not contain \"/\" or \"%\".• \\n• \\n• \\n• \\n• \\n• \\n• \\n• \\n• \\n• \\n• \\n•', metadata={'source': './PDFS/Concepts.pdf', 'page': 11}),\n",
       " Document(page_content='Here\\'s an example manifest for a Pod named nginx-demo .\\napiVersion : v1\\nkind: Pod\\nmetadata :\\n  name : nginx-demo\\nspec:\\n  containers :\\n  - name : nginx\\n    image : nginx:1.14.2\\n    ports :\\n    - containerPort : 80\\nNote:  Some resource types have additional restrictions on their names.\\nUIDs\\nA Kubernetes systems-generated string to uniquely identify objects.\\nEvery object created over the whole lifetime of a Kubernetes cluster has a distinct UID. It is\\nintended to distinguish between historical occurrences of similar entities.\\nKubernetes UIDs are universally unique identifiers (also known as UUIDs). UUIDs are\\nstandardized as ISO/IEC 9834-8 and as ITU-T X.667.\\nWhat\\'s next\\nRead about labels  and annotations  in Kubernetes.\\nSee the Identifiers and Names in Kubernetes  design document.\\nLabels and Selectors\\nLabels  are key/value pairs that are attached to objects  such as Pods. Labels are intended to be\\nused to specify identifying attributes of objects that are meaningful and relevant to users, but\\ndo not directly imply semantics to the core system. Labels can be used to organize and to select\\nsubsets of objects. Labels can be attached to objects at creation time and subsequently added\\nand modified at any time. Each object can have a set of key/value labels defined. Each Key must\\nbe unique for a given object.\\n\"metadata\" : {\\n  \"labels\" : {\\n    \"key1\"  : \"value1\" ,\\n    \"key2\"  : \"value2\"\\n  }\\n}\\nLabels allow for efficient queries and watches and are ideal for use in UIs and CLIs. Non-\\nidentifying information should be recorded using annotations .• \\n•', metadata={'source': './PDFS/Concepts.pdf', 'page': 12}),\n",
       " Document(page_content='Motivation\\nLabels enable users to map their own organizational structures onto system objects in a loosely\\ncoupled fashion, without requiring clients to store these mappings.\\nService deployments and batch processing pipelines are often multi-dimensional entities (e.g.,\\nmultiple partitions or deployments, multiple release tracks, multiple tiers, multiple micro-\\nservices per tier). Management often requires cross-cutting operations, which breaks\\nencapsulation of strictly hierarchical representations, especially rigid hierarchies determined by\\nthe infrastructure rather than by users.\\nExample labels:\\n\"release\" : \"stable\" , \"release\" : \"canary\"\\n\"environment\" : \"dev\" , \"environment\" : \"qa\" , \"environment\" : \"production\"\\n\"tier\" : \"frontend\" , \"tier\" : \"backend\" , \"tier\" : \"cache\"\\n\"partition\" : \"customerA\" , \"partition\" : \"customerB\"\\n\"track\" : \"daily\" , \"track\" : \"weekly\"\\nThese are examples of commonly used labels ; you are free to develop your own conventions.\\nKeep in mind that label Key must be unique for a given object.\\nSyntax and character set\\nLabels  are key/value pairs. Valid label keys have two segments: an optional prefix and name,\\nseparated by a slash ( /). The name segment is required and must be 63 characters or less,\\nbeginning and ending with an alphanumeric character ( [a-z0-9A-Z] ) with dashes ( -),\\nunderscores ( _), dots ( .), and alphanumerics between. The prefix is optional. If specified, the\\nprefix must be a DNS subdomain: a series of DNS labels separated by dots ( .), not longer than\\n253 characters in total, followed by a slash ( /).\\nIf the prefix is omitted, the label Key is presumed to be private to the user. Automated system\\ncomponents (e.g. kube-scheduler , kube-controller-manager , kube-apiserver , kubectl , or other\\nthird-party automation) which add labels to end-user objects must specify a prefix.\\nThe kubernetes.io/  and k8s.io/  prefixes are reserved  for Kubernetes core components.\\nValid label value:\\nmust be 63 characters or less (can be empty),\\nunless empty, must begin and end with an alphanumeric character ( [a-z0-9A-Z] ),\\ncould contain dashes ( -), underscores ( _), dots ( .), and alphanumerics between.\\nFor example, here\\'s a manifest for a Pod that has two labels environment: production  and app: \\nnginx :\\napiVersion : v1\\nkind: Pod\\nmetadata :\\n  name : label-demo\\n  labels :\\n    environment : production\\n    app: nginx• \\n• \\n• \\n• \\n• \\n• \\n• \\n•', metadata={'source': './PDFS/Concepts.pdf', 'page': 13}),\n",
       " Document(page_content='spec:\\n  containers :\\n  - name : nginx\\n    image : nginx:1.14.2\\n    ports :\\n    - containerPort : 80\\nLabel selectors\\nUnlike names and UIDs , labels do not provide uniqueness. In general, we expect many objects\\nto carry the same label(s).\\nVia a label selector , the client/user can identify a set of objects. The label selector is the core\\ngrouping primitive in Kubernetes.\\nThe API currently supports two types of selectors: equality-based  and set-based . A label selector\\ncan be made of multiple requirements  which are comma-separated. In the case of multiple\\nrequirements, all must be satisfied so the comma separator acts as a logical AND  (&&) operator.\\nThe semantics of empty or non-specified selectors are dependent on the context, and API types\\nthat use selectors should document the validity and meaning of them.\\nNote:  For some API types, such as ReplicaSets, the label selectors of two instances must not\\noverlap within a namespace, or the controller can see that as conflicting instructions and fail to\\ndetermine how many replicas should be present.\\nCaution:  For both equality-based and set-based conditions there is no logical OR (||) operator.\\nEnsure your filter statements are structured accordingly.\\nEquality-based  requirement\\nEquality-  or inequality-based  requirements allow filtering by label keys and values. Matching\\nobjects must satisfy all of the specified label constraints, though they may have additional labels\\nas well. Three kinds of operators are admitted =,==,!=. The first two represent equality  (and are\\nsynonyms), while the latter represents inequality . For example:\\nenvironment = production\\ntier != frontend\\nThe former selects all resources with key equal to environment  and value equal to production .\\nThe latter selects all resources with key equal to tier and value distinct from frontend , and all\\nresources with no labels with the tier key. One could filter for resources in production\\nexcluding frontend  using the comma operator: environment=production,tier!=frontend\\nOne usage scenario for equality-based label requirement is for Pods to specify node selection\\ncriteria. For example, the sample Pod below selects nodes with the label \" accelerator=nvidia-\\ntesla-p100 \".\\napiVersion : v1\\nkind: Pod\\nmetadata :\\n  name : cuda-test\\nspec:\\n  containers :', metadata={'source': './PDFS/Concepts.pdf', 'page': 14}),\n",
       " Document(page_content='- name : cuda-test\\n      image : \"registry.k8s.io/cuda-vector-add:v0.1\"\\n      resources :\\n        limits :\\n          nvidia.com/gpu : 1\\n  nodeSelector :\\n    accelerator : nvidia-tesla-p100\\nSet-based  requirement\\nSet-based  label requirements allow filtering keys according to a set of values. Three kinds of\\noperators are supported: in,notin  and exists  (only the key identifier). For example:\\nenvironment in (production, qa)\\ntier notin (frontend, backend)\\npartition\\n!partition\\nThe first example selects all resources with key equal to environment  and value equal to \\nproduction  or qa.\\nThe second example selects all resources with key equal to tier and values other than \\nfrontend  and backend , and all resources with no labels with the tier key.\\nThe third example selects all resources including a label with key partition ; no values are\\nchecked.\\nThe fourth example selects all resources without a label with key partition ; no values are\\nchecked.\\nSimilarly the comma separator acts as an AND  operator. So filtering resources with a partition\\nkey (no matter the value) and with environment  different than qa can be achieved using \\npartition,environment notin (qa) . The set-based  label selector is a general form of equality since \\nenvironment=production  is equivalent to environment in (production) ; similarly for != and \\nnotin .\\nSet-based  requirements can be mixed with equality-based  requirements. For example: partition \\nin (customerA, customerB),environment!=qa .\\nAPI\\nLIST and WATCH filtering\\nLIST and WATCH operations may specify label selectors to filter the sets of objects returned\\nusing a query parameter. Both requirements are permitted (presented here as they would\\nappear in a URL query string):\\nequality-based  requirements: ?labelSelector=environment%3Dproduction,tier%3Dfrontend\\nset-based  requirements: ?labelSelector=environment+in+\\n%28production%2Cqa%29%2Ctier+in+%28frontend%29\\nBoth label selector styles can be used to list or watch resources via a REST client. For example,\\ntargeting apiserver  with kubectl  and using equality-based  one may write:\\nkubectl get pods -l environment =production,tier =frontend• \\n• \\n• \\n• \\n• \\n•', metadata={'source': './PDFS/Concepts.pdf', 'page': 15}),\n",
       " Document(page_content='or using set-based  requirements:\\nkubectl get pods -l \\'environment in (production),tier in (frontend)\\'\\nAs already mentioned set-based  requirements are more expressive. For instance, they can\\nimplement the OR operator on values:\\nkubectl get pods -l \\'environment in (production, qa)\\'\\nor restricting negative matching via notin  operator:\\nkubectl get pods -l \\'environment,environment notin (frontend)\\'\\nSet references in API objects\\nSome Kubernetes objects, such as services  and replicationcontrollers , also use label selectors to\\nspecify sets of other resources, such as pods .\\nService and ReplicationController\\nThe set of pods that a service  targets is defined with a label selector. Similarly, the population of\\npods that a replicationcontroller  should manage is also defined with a label selector.\\nLabel selectors for both objects are defined in json or yaml  files using maps, and only equality-\\nbased  requirement selectors are supported:\\n\"selector\" : {\\n    \"component\"  : \"redis\" ,\\n}\\nor\\nselector :\\n  component : redis\\nThis selector (respectively in json or yaml  format) is equivalent to component=redis  or \\ncomponent in (redis) .\\nResources that support set-based requirements\\nNewer resources, such as Job, Deployment , ReplicaSet , and DaemonSet , support set-based\\nrequirements as well.\\nselector :\\n  matchLabels :\\n    component : redis\\n  matchExpressions :\\n    - { key: tier, operator: In, values : [cache] }\\n    - { key: environment, operator: NotIn, values : [dev] }\\nmatchLabels  is a map of {key,value}  pairs. A single {key,value}  in the matchLabels  map is\\nequivalent to an element of matchExpressions , whose key field is \"key\", the operator  is \"In\", and\\nthe values  array contains only \"value\". matchExpressions  is a list of pod selector requirements.', metadata={'source': './PDFS/Concepts.pdf', 'page': 16}),\n",
       " Document(page_content='Valid operators include In, NotIn, Exists, and DoesNotExist. The values set must be non-empty\\nin the case of In and NotIn. All of the requirements, from both matchLabels  and \\nmatchExpressions  are ANDed together -- they must all be satisfied in order to match.\\nSelecting sets of nodes\\nOne use case for selecting over labels is to constrain the set of nodes onto which a pod can\\nschedule. See the documentation on node selection  for more information.\\nUsing labels effectively\\nYou can apply a single label to any resources, but this is not always the best practice. There are\\nmany scenarios where multiple labels should be used to distinguish resource sets from one\\nanother.\\nFor instance, different applications would use different values for the app label, but a multi-tier\\napplication, such as the guestbook example , would additionally need to distinguish each tier.\\nThe frontend could carry the following labels:\\nlabels :\\n  app: guestbook\\n  tier: frontend\\nwhile the Redis master and replica would have different tier labels, and perhaps even an\\nadditional role label:\\nlabels :\\n  app: guestbook\\n  tier: backend\\n  role: master\\nand\\nlabels :\\n  app: guestbook\\n  tier: backend\\n  role: replica\\nThe labels allow for slicing and dicing the resources along any dimension specified by a label:\\nkubectl apply -f examples/guestbook/all-in-one/guestbook-all-in-one.yaml\\nkubectl get pods -Lapp -Ltier -Lrole\\nNAME                           READY  STATUS    RESTARTS   AGE   APP         TIER       ROLE\\nguestbook-fe-4nlpb             1/1    Running   0          1m    guestbook   frontend   <none>\\nguestbook-fe-ght6d             1/1    Running   0          1m    guestbook   frontend   <none>\\nguestbook-fe-jpy62             1/1    Running   0          1m    guestbook   frontend   <none>\\nguestbook-redis-master-5pg3b   1/1    Running   0          1m    guestbook   backend    master\\nguestbook-redis-replica-2q2yf  1/1    Running   0          1m    guestbook   backend    replica\\nguestbook-redis-replica-qgazl  1/1    Running   0          1m    guestbook   backend    replica\\nmy-nginx-divi2                 1/1    Running   0          29m   nginx       <none>     <none>\\nmy-nginx-o0ef1                 1/1    Running   0          29m   nginx       <none>     <none>', metadata={'source': './PDFS/Concepts.pdf', 'page': 17}),\n",
       " Document(page_content='kubectl get pods -lapp =guestbook,role =replica\\nNAME                           READY  STATUS   RESTARTS  AGE\\nguestbook-redis-replica-2q2yf  1/1    Running  0         3m\\nguestbook-redis-replica-qgazl  1/1    Running  0         3m\\nUpdating labels\\nSometimes you may want to relabel existing pods and other resources before creating new\\nresources. This can be done with kubectl label . For example, if you want to label all your\\nNGINX Pods as frontend tier, run:\\nkubectl label pods -l app=nginx tier=fe\\npod/my-nginx-2035384211-j5fhi labeled\\npod/my-nginx-2035384211-u2c7e labeled\\npod/my-nginx-2035384211-u3t6x labeled\\nThis first filters all pods with the label \"app=nginx\", and then labels them with the \"tier=fe\". To\\nsee the pods you labeled, run:\\nkubectl get pods -l app=nginx -L tier\\nNAME                        READY     STATUS    RESTARTS   AGE       TIER\\nmy-nginx-2035384211-j5fhi   1/1       Running   0          23m       fe\\nmy-nginx-2035384211-u2c7e   1/1       Running   0          23m       fe\\nmy-nginx-2035384211-u3t6x   1/1       Running   0          23m       fe\\nThis outputs all \"app=nginx\" pods, with an additional label column of pods\\' tier (specified with -\\nL or --label-columns ).\\nFor more information, please see kubectl label .\\nWhat\\'s next\\nLearn how to add a label to a node\\nFind Well-known labels, Annotations and Taints\\nSee Recommended labels\\nEnforce Pod Security Standards with Namespace Labels\\nRead a blog on Writing a Controller for Pod Labels\\nNamespaces\\nIn Kubernetes, namespaces  provides a mechanism for isolating groups of resources within a\\nsingle cluster. Names of resources need to be unique within a namespace, but not across\\nnamespaces. Namespace-based scoping is applicable only for namespaced objects  (e.g.\\nDeployments, Services, etc)  and not for cluster-wide objects (e.g. StorageClass, Nodes,\\nPersistentVolumes, etc) .• \\n• \\n• \\n• \\n•', metadata={'source': './PDFS/Concepts.pdf', 'page': 18}),\n",
       " Document(page_content='When to Use Multiple Namespaces\\nNamespaces are intended for use in environments with many users spread across multiple\\nteams, or projects. For clusters with a few to tens of users, you should not need to create or\\nthink about namespaces at all. Start using namespaces when you need the features they\\nprovide.\\nNamespaces provide a scope for names. Names of resources need to be unique within a\\nnamespace, but not across namespaces. Namespaces cannot be nested inside one another and\\neach Kubernetes resource can only be in one namespace.\\nNamespaces are a way to divide cluster resources between multiple users (via resource quota ).\\nIt is not necessary to use multiple namespaces to separate slightly different resources, such as\\ndifferent versions of the same software: use labels  to distinguish resources within the same\\nnamespace.\\nNote:  For a production cluster, consider not using the default  namespace. Instead, make other\\nnamespaces and use those.\\nInitial namespaces\\nKubernetes starts with four initial namespaces:\\ndefault\\nKubernetes includes this namespace so that you can start using your new cluster without\\nfirst creating a namespace.\\nkube-node-lease\\nThis namespace holds Lease  objects associated with each node. Node leases allow the\\nkubelet to send heartbeats  so that the control plane can detect node failure.\\nkube-public\\nThis namespace is readable by all clients (including those not authenticated). This\\nnamespace is mostly reserved for cluster usage, in case that some resources should be\\nvisible and readable publicly throughout the whole cluster. The public aspect of this\\nnamespace is only a convention, not a requirement.\\nkube-system\\nThe namespace for objects created by the Kubernetes system.\\nWorking with Namespaces\\nCreation and deletion of namespaces are described in the Admin Guide documentation for\\nnamespaces .\\nNote:  Avoid creating namespaces with the prefix kube- , since it is reserved for Kubernetes\\nsystem namespaces.\\nViewing namespaces\\nYou can list the current namespaces in a cluster using:\\nkubectl get namespace', metadata={'source': './PDFS/Concepts.pdf', 'page': 19}),\n",
       " Document(page_content=\"NAME              STATUS   AGE\\ndefault           Active   1d\\nkube-node-lease   Active   1d\\nkube-public       Active   1d\\nkube-system       Active   1d\\nSetting the namespace for a request\\nTo set the namespace for a current request, use the --namespace  flag.\\nFor example:\\nkubectl run nginx --image =nginx --namespace =<insert-namespace-name-here>\\nkubectl get pods --namespace =<insert-namespace-name-here>\\nSetting the namespace preference\\nYou can permanently save the namespace for all subsequent kubectl commands in that context.\\nkubectl config set-context --current --namespace =<insert-namespace-name-here>\\n# Validate it\\nkubectl config view --minify | grep namespace:\\nNamespaces and DNS\\nWhen you create a Service , it creates a corresponding DNS entry . This entry is of the form \\n<service-name>.<namespace-name>.svc.cluster.local , which means that if a container only uses \\n<service-name> , it will resolve to the service which is local to a namespace. This is useful for\\nusing the same configuration across multiple namespaces such as Development, Staging and\\nProduction. If you want to reach across namespaces, you need to use the fully qualified domain\\nname (FQDN).\\nAs a result, all namespace names must be valid RFC 1123 DNS labels .\\nWarning:\\nBy creating namespaces with the same name as public top-level domains , Services in these\\nnamespaces can have short DNS names that overlap with public DNS records. Workloads from\\nany namespace performing a DNS lookup without a trailing dot  will be redirected to those\\nservices, taking precedence over public DNS.\\nTo mitigate this, limit privileges for creating namespaces to trusted users. If required, you could\\nadditionally configure third-party security controls, such as admission webhooks , to block\\ncreating any namespace with the name of public TLDs .\\nNot all objects are in a namespace\\nMost Kubernetes resources (e.g. pods, services, replication controllers, and others) are in some\\nnamespaces. However namespace resources are not themselves in a namespace. And low-level\\nresources, such as nodes  and persistentVolumes , are not in any namespace.\\nTo see which Kubernetes resources are and aren't in a namespace:\", metadata={'source': './PDFS/Concepts.pdf', 'page': 20}),\n",
       " Document(page_content='# In a namespace\\nkubectl api-resources --namespaced =true\\n# Not in a namespace\\nkubectl api-resources --namespaced =false\\nAutomatic labelling\\nFEATURE STATE:  Kubernetes 1.22 [stable]\\nThe Kubernetes control plane sets an immutable label  kubernetes.io/metadata.name  on all\\nnamespaces. The value of the label is the namespace name.\\nWhat\\'s next\\nLearn more about creating a new namespace .\\nLearn more about deleting a namespace .\\nAnnotations\\nYou can use Kubernetes annotations to attach arbitrary non-identifying metadata to objects .\\nClients such as tools and libraries can retrieve this metadata.\\nAttaching metadata to objects\\nYou can use either labels or annotations to attach metadata to Kubernetes objects. Labels can be\\nused to select objects and to find collections of objects that satisfy certain conditions. In\\ncontrast, annotations are not used to identify and select objects. The metadata in an annotation\\ncan be small or large, structured or unstructured, and can include characters not permitted by\\nlabels. It is possible to use labels as well as annotations in the metadata of the same object.\\nAnnotations, like labels, are key/value maps:\\n\"metadata\" : {\\n  \"annotations\" : {\\n    \"key1\"  : \"value1\" ,\\n    \"key2\"  : \"value2\"\\n  }\\n}\\nNote:  The keys and the values in the map must be strings. In other words, you cannot use\\nnumeric, boolean, list or other types for either the keys or the values.\\nHere are some examples of information that could be recorded in annotations:\\nFields managed by a declarative configuration layer. Attaching these fields as annotations\\ndistinguishes them from default values set by clients or servers, and from auto-generated\\nfields and fields set by auto-sizing or auto-scaling systems.• \\n• \\n•', metadata={'source': './PDFS/Concepts.pdf', 'page': 21}),\n",
       " Document(page_content='Build, release, or image information like timestamps, release IDs, git branch, PR numbers,\\nimage hashes, and registry address.\\nPointers to logging, monitoring, analytics, or audit repositories.\\nClient library or tool information that can be used for debugging purposes: for example,\\nname, version, and build information.\\nUser or tool/system provenance information, such as URLs of related objects from other\\necosystem components.\\nLightweight rollout tool metadata: for example, config or checkpoints.\\nPhone or pager numbers of persons responsible, or directory entries that specify where\\nthat information can be found, such as a team web site.\\nDirectives from the end-user to the implementations to modify behavior or engage non-\\nstandard features.\\nInstead of using annotations, you could store this type of information in an external database or\\ndirectory, but that would make it much harder to produce shared client libraries and tools for\\ndeployment, management, introspection, and the like.\\nSyntax and character set\\nAnnotations  are key/value pairs. Valid annotation keys have two segments: an optional prefix\\nand name, separated by a slash ( /). The name segment is required and must be 63 characters or\\nless, beginning and ending with an alphanumeric character ( [a-z0-9A-Z] ) with dashes ( -),\\nunderscores ( _), dots ( .), and alphanumerics between. The prefix is optional. If specified, the\\nprefix must be a DNS subdomain: a series of DNS labels separated by dots ( .), not longer than\\n253 characters in total, followed by a slash ( /).\\nIf the prefix is omitted, the annotation Key is presumed to be private to the user. Automated\\nsystem components (e.g. kube-scheduler , kube-controller-manager , kube-apiserver , kubectl , or\\nother third-party automation) which add annotations to end-user objects must specify a prefix.\\nThe kubernetes.io/  and k8s.io/  prefixes are reserved for Kubernetes core components.\\nFor example, here\\'s a manifest for a Pod that has the annotation imageregistry: https://\\nhub.docker.com/  :\\napiVersion : v1\\nkind: Pod\\nmetadata :\\n  name : annotations-demo\\n  annotations :\\n    imageregistry : \"https://hub.docker.com/\"\\nspec:\\n  containers :\\n  - name : nginx\\n    image : nginx:1.14.2\\n    ports :\\n    - containerPort : 80• \\n• \\n• \\n• \\n• \\n• \\n•', metadata={'source': './PDFS/Concepts.pdf', 'page': 22}),\n",
       " Document(page_content='What\\'s next\\nLearn more about Labels and Selectors .\\nFind Well-known labels, Annotations and Taints\\nField Selectors\\nField selectors  let you select Kubernetes objects  based on the value of one or more resource\\nfields. Here are some examples of field selector queries:\\nmetadata.name=my-service\\nmetadata.namespace!=default\\nstatus.phase=Pending\\nThis kubectl  command selects all Pods for which the value of the status.phase  field is Running :\\nkubectl get pods --field-selector status.phase =Running\\nNote:  Field selectors are essentially resource filters . By default, no selectors/filters are applied,\\nmeaning that all resources of the specified type are selected. This makes the kubectl  queries \\nkubectl get pods  and kubectl get pods --field-selector \"\"  equivalent.\\nSupported fields\\nSupported field selectors vary by Kubernetes resource type. All resource types support the \\nmetadata.name  and metadata.namespace  fields. Using unsupported field selectors produces an\\nerror. For example:\\nkubectl get ingress --field-selector foo.bar =baz\\nError from server (BadRequest): Unable to find \"ingresses\" that match label selector \"\", field \\nselector \"foo.bar=baz\": \"foo.bar\" is not a known field selector: only \"metadata.name\", \\n\"metadata.namespace\"\\nSupported operators\\nYou can use the =, ==, and != operators with field selectors ( = and == mean the same thing).\\nThis kubectl  command, for example, selects all Kubernetes Services that aren\\'t in the default\\nnamespace:\\nkubectl get services  --all-namespaces --field-selector metadata.namespace! =default\\nNote:  Set-based operators  (in, notin , exists ) are not supported for field selectors.\\nChained selectors\\nAs with label  and other selectors, field selectors can be chained together as a comma-separated\\nlist. This kubectl  command selects all Pods for which the status.phase  does not equal Running\\nand the spec.restartPolicy  field equals Always :• \\n• \\n• \\n• \\n•', metadata={'source': './PDFS/Concepts.pdf', 'page': 23}),\n",
       " Document(page_content='kubectl get pods --field-selector =status.phase! =Running,spec.restartPolicy =Always\\nMultiple resource types\\nYou can use field selectors across multiple resource types. This kubectl  command selects all\\nStatefulsets and Services that are not in the default  namespace:\\nkubectl get statefulsets,services --all-namespaces --field-selector metadata.namespace! =default\\nFinalizers\\nFinalizers are namespaced keys that tell Kubernetes to wait until specific conditions are met\\nbefore it fully deletes resources marked for deletion. Finalizers alert controllers  to clean up\\nresources the deleted object owned.\\nWhen you tell Kubernetes to delete an object that has finalizers specified for it, the Kubernetes\\nAPI marks the object for deletion by populating .metadata.deletionTimestamp , and returns a \\n202 status code (HTTP \"Accepted\"). The target object remains in a terminating state while the\\ncontrol plane, or other components, take the actions defined by the finalizers. After these\\nactions are complete, the controller removes the relevant finalizers from the target object.\\nWhen the metadata.finalizers  field is empty, Kubernetes considers the deletion complete and\\ndeletes the object.\\nYou can use finalizers to control garbage collection  of resources. For example, you can define a\\nfinalizer to clean up related resources or infrastructure before the controller deletes the target\\nresource.\\nYou can use finalizers to control garbage collection  of objects  by alerting controllers  to perform\\nspecific cleanup tasks before deleting the target resource.\\nFinalizers don\\'t usually specify the code to execute. Instead, they are typically lists of keys on a\\nspecific resource similar to annotations. Kubernetes specifies some finalizers automatically, but\\nyou can also specify your own.\\nHow finalizers work\\nWhen you create a resource using a manifest file, you can specify finalizers in the \\nmetadata.finalizers  field. When you attempt to delete the resource, the API server handling the\\ndelete request notices the values in the finalizers  field and does the following:\\nModifies the object to add a metadata.deletionTimestamp  field with the time you started\\nthe deletion.\\nPrevents the object from being removed until all items are removed from its \\nmetadata.finalizers  field\\nReturns a 202 status code (HTTP \"Accepted\")\\nThe controller managing that finalizer notices the update to the object setting the \\nmetadata.deletionTimestamp , indicating deletion of the object has been requested. The\\ncontroller then attempts to satisfy the requirements of the finalizers specified for that resource.\\nEach time a finalizer condition is satisfied, the controller removes that key from the resource\\'s \\nfinalizers  field. When the finalizers  field is emptied, an object with a deletionTimestamp  field• \\n• \\n•', metadata={'source': './PDFS/Concepts.pdf', 'page': 24}),\n",
       " Document(page_content=\"set is automatically deleted. You can also use finalizers to prevent deletion of unmanaged\\nresources.\\nA common example of a finalizer is kubernetes.io/pv-protection , which prevents accidental\\ndeletion of PersistentVolume  objects. When a PersistentVolume  object is in use by a Pod,\\nKubernetes adds the pv-protection  finalizer. If you try to delete the PersistentVolume , it enters\\na Terminating  status, but the controller can't delete it because the finalizer exists. When the Pod\\nstops using the PersistentVolume , Kubernetes clears the pv-protection  finalizer, and the\\ncontroller deletes the volume.\\nNote:\\nWhen you DELETE  an object, Kubernetes adds the deletion timestamp for that object and\\nthen immediately starts to restrict changes to the .metadata.finalizers  field for the object\\nthat is now pending deletion. You can remove existing finalizers (deleting an entry from\\nthe finalizers  list) but you cannot add a new finalizer. You also cannot modify the \\ndeletionTimestamp  for an object once it is set.\\nAfter the deletion is requested, you can not resurrect this object. The only way is to delete\\nit and make a new similar object.\\nOwner references, labels, and finalizers\\nLike labels , owner references  describe the relationships between objects in Kubernetes, but are\\nused for a different purpose. When a controller  manages objects like Pods, it uses labels to track\\nchanges to groups of related objects. For example, when a Job creates one or more Pods, the Job\\ncontroller applies labels to those pods and tracks changes to any Pods in the cluster with the\\nsame label.\\nThe Job controller also adds owner references  to those Pods, pointing at the Job that created the\\nPods. If you delete the Job while these Pods are running, Kubernetes uses the owner references\\n(not labels) to determine which Pods in the cluster need cleanup.\\nKubernetes also processes finalizers when it identifies owner references on a resource targeted\\nfor deletion.\\nIn some situations, finalizers can block the deletion of dependent objects, which can cause the\\ntargeted owner object to remain for longer than expected without being fully deleted. In these\\nsituations, you should check finalizers and owner references on the target owner and\\ndependent objects to troubleshoot the cause.\\nNote:  In cases where objects are stuck in a deleting state, avoid manually removing finalizers to\\nallow deletion to continue. Finalizers are usually added to resources for a reason, so forcefully\\nremoving them can lead to issues in your cluster. This should only be done when the purpose of\\nthe finalizer is understood and is accomplished in another way (for example, manually cleaning\\nup some dependent object).\\nWhat's next\\nRead Using Finalizers to Control Deletion  on the Kubernetes blog.• \\n• \\n•\", metadata={'source': './PDFS/Concepts.pdf', 'page': 25}),\n",
       " Document(page_content=\"Owners and Dependents\\nIn Kubernetes, some objects  are owners  of other objects. For example, a ReplicaSet  is the owner\\nof a set of Pods. These owned objects are dependents  of their owner.\\nOwnership is different from the labels and selectors  mechanism that some resources also use.\\nFor example, consider a Service that creates EndpointSlice  objects. The Service uses labels  to\\nallow the control plane to determine which EndpointSlice  objects are used for that Service. In\\naddition to the labels, each EndpointSlice  that is managed on behalf of a Service has an owner\\nreference. Owner references help different parts of Kubernetes avoid interfering with objects\\nthey don’t control.\\nOwner references in object specifications\\nDependent objects have a metadata.ownerReferences  field that references their owner object. A\\nvalid owner reference consists of the object name and a UID within the same namespace  as the\\ndependent object. Kubernetes sets the value of this field automatically for objects that are\\ndependents of other objects like ReplicaSets, DaemonSets, Deployments, Jobs and CronJobs, and\\nReplicationControllers. You can also configure these relationships manually by changing the\\nvalue of this field. However, you usually don't need to and can allow Kubernetes to\\nautomatically manage the relationships.\\nDependent objects also have an ownerReferences.blockOwnerDeletion  field that takes a\\nboolean value and controls whether specific dependents can block garbage collection from\\ndeleting their owner object. Kubernetes automatically sets this field to true if a controller  (for\\nexample, the Deployment controller) sets the value of the metadata.ownerReferences  field. You\\ncan also set the value of the blockOwnerDeletion  field manually to control which dependents\\nblock garbage collection.\\nA Kubernetes admission controller controls user access to change this field for dependent\\nresources, based on the delete permissions of the owner. This control prevents unauthorized\\nusers from delaying owner object deletion.\\nNote:\\nCross-namespace owner references are disallowed by design. Namespaced dependents can\\nspecify cluster-scoped or namespaced owners. A namespaced owner must  exist in the same\\nnamespace as the dependent. If it does not, the owner reference is treated as absent, and the\\ndependent is subject to deletion once all owners are verified absent.\\nCluster-scoped dependents can only specify cluster-scoped owners. In v1.20+, if a cluster-\\nscoped dependent specifies a namespaced kind as an owner, it is treated as having an\\nunresolvable owner reference, and is not able to be garbage collected.\\nIn v1.20+, if the garbage collector detects an invalid cross-namespace ownerReference , or a\\ncluster-scoped dependent with an ownerReference  referencing a namespaced kind, a warning\\nEvent with a reason of OwnerRefInvalidNamespace  and an involvedObject  of the invalid\\ndependent is reported. You can check for that kind of Event by running kubectl get events -A --\\nfield-selector=reason=OwnerRefInvalidNamespace .\", metadata={'source': './PDFS/Concepts.pdf', 'page': 26}),\n",
       " Document(page_content=\"Ownership and finalizers\\nWhen you tell Kubernetes to delete a resource, the API server allows the managing controller\\nto process any finalizer rules  for the resource. Finalizers  prevent accidental deletion of\\nresources your cluster may still need to function correctly. For example, if you try to delete a \\nPersistentVolume  that is still in use by a Pod, the deletion does not happen immediately because\\nthe PersistentVolume  has the kubernetes.io/pv-protection  finalizer on it. Instead, the volume\\nremains in the Terminating  status until Kubernetes clears the finalizer, which only happens\\nafter the PersistentVolume  is no longer bound to a Pod.\\nKubernetes also adds finalizers to an owner resource when you use either foreground or orphan\\ncascading deletion . In foreground deletion, it adds the foreground  finalizer so that the controller\\nmust delete dependent resources that also have ownerReferences.blockOwnerDeletion=true\\nbefore it deletes the owner. If you specify an orphan deletion policy, Kubernetes adds the \\norphan  finalizer so that the controller ignores dependent resources after it deletes the owner\\nobject.\\nWhat's next\\nLearn more about Kubernetes finalizers .\\nLearn about garbage collection .\\nRead the API reference for object metadata .\\nRecommended Labels\\nYou can visualize and manage Kubernetes objects with more tools than kubectl and the\\ndashboard. A common set of labels allows tools to work interoperably, describing objects in a\\ncommon manner that all tools can understand.\\nIn addition to supporting tooling, the recommended labels describe applications in a way that\\ncan be queried.\\nThe metadata is organized around the concept of an application . Kubernetes is not a platform as\\na service (PaaS) and doesn't have or enforce a formal notion of an application. Instead,\\napplications are informal and described with metadata. The definition of what an application\\ncontains is loose.\\nNote:  These are recommended labels. They make it easier to manage applications but aren't\\nrequired for any core tooling.\\nShared labels and annotations share a common prefix: app.kubernetes.io . Labels without a\\nprefix are private to users. The shared prefix ensures that shared labels do not interfere with\\ncustom user labels.\\nLabels\\nIn order to take full advantage of using these labels, they should be applied on every resource\\nobject.• \\n• \\n•\", metadata={'source': './PDFS/Concepts.pdf', 'page': 27}),\n",
       " Document(page_content='Key Description Example Type\\napp.kubernetes.io/name The name of the application mysql string\\napp.kubernetes.io/\\ninstanceA unique name identifying the instance of an\\napplicationmysql-\\nabcxzystring\\napp.kubernetes.io/\\nversionThe current version of the application (e.g., a \\nSemVer 1.0 , revision hash, etc.)5.7.21 string\\napp.kubernetes.io/\\ncomponentThe component within the architecture database string\\napp.kubernetes.io/part-ofThe name of a higher level application this one is\\npart ofwordpress string\\napp.kubernetes.io/\\nmanaged-byThe tool being used to manage the operation of\\nan applicationhelm string\\nTo illustrate these labels in action, consider the following StatefulSet  object:\\n# This is an excerpt\\napiVersion : apps/v1\\nkind: StatefulSet\\nmetadata :\\n  labels :\\n    app.kubernetes.io/name : mysql\\n    app.kubernetes.io/instance : mysql-abcxzy\\n    app.kubernetes.io/version : \"5.7.21\"\\n    app.kubernetes.io/component : database\\n    app.kubernetes.io/part-of : wordpress\\n    app.kubernetes.io/managed-by : helm\\nApplications And Instances Of Applications\\nAn application can be installed one or more times into a Kubernetes cluster and, in some cases,\\nthe same namespace. For example, WordPress can be installed more than once where different\\nwebsites are different installations of WordPress.\\nThe name of an application and the instance name are recorded separately. For example,\\nWordPress has a app.kubernetes.io/name  of wordpress  while it has an instance name,\\nrepresented as app.kubernetes.io/instance  with a value of wordpress-abcxzy . This enables the\\napplication and instance of the application to be identifiable. Every instance of an application\\nmust have a unique name.\\nExamples\\nTo illustrate different ways to use these labels the following examples have varying complexity.\\nA Simple Stateless Service\\nConsider the case for a simple stateless service deployed using Deployment  and Service  objects.\\nThe following two snippets represent how the labels could be used in their simplest form.\\nThe Deployment  is used to oversee the pods running the application itself.', metadata={'source': './PDFS/Concepts.pdf', 'page': 28}),\n",
       " Document(page_content='apiVersion : apps/v1\\nkind: Deployment\\nmetadata :\\n  labels :\\n    app.kubernetes.io/name : myservice\\n    app.kubernetes.io/instance : myservice-abcxzy\\n...\\nThe Service  is used to expose the application.\\napiVersion : v1\\nkind: Service\\nmetadata :\\n  labels :\\n    app.kubernetes.io/name : myservice\\n    app.kubernetes.io/instance : myservice-abcxzy\\n...\\nWeb Application With A Database\\nConsider a slightly more complicated application: a web application (WordPress) using a\\ndatabase (MySQL), installed using Helm. The following snippets illustrate the start of objects\\nused to deploy this application.\\nThe start to the following Deployment  is used for WordPress:\\napiVersion : apps/v1\\nkind: Deployment\\nmetadata :\\n  labels :\\n    app.kubernetes.io/name : wordpress\\n    app.kubernetes.io/instance : wordpress-abcxzy\\n    app.kubernetes.io/version : \"4.9.4\"\\n    app.kubernetes.io/managed-by : helm\\n    app.kubernetes.io/component : server\\n    app.kubernetes.io/part-of : wordpress\\n...\\nThe Service  is used to expose WordPress:\\napiVersion : v1\\nkind: Service\\nmetadata :\\n  labels :\\n    app.kubernetes.io/name : wordpress\\n    app.kubernetes.io/instance : wordpress-abcxzy\\n    app.kubernetes.io/version : \"4.9.4\"\\n    app.kubernetes.io/managed-by : helm\\n    app.kubernetes.io/component : server\\n    app.kubernetes.io/part-of : wordpress\\n...', metadata={'source': './PDFS/Concepts.pdf', 'page': 29}),\n",
       " Document(page_content='MySQL is exposed as a StatefulSet  with metadata for both it and the larger application it\\nbelongs to:\\napiVersion : apps/v1\\nkind: StatefulSet\\nmetadata :\\n  labels :\\n    app.kubernetes.io/name : mysql\\n    app.kubernetes.io/instance : mysql-abcxzy\\n    app.kubernetes.io/version : \"5.7.21\"\\n    app.kubernetes.io/managed-by : helm\\n    app.kubernetes.io/component : database\\n    app.kubernetes.io/part-of : wordpress\\n...\\nThe Service  is used to expose MySQL as part of WordPress:\\napiVersion : v1\\nkind: Service\\nmetadata :\\n  labels :\\n    app.kubernetes.io/name : mysql\\n    app.kubernetes.io/instance : mysql-abcxzy\\n    app.kubernetes.io/version : \"5.7.21\"\\n    app.kubernetes.io/managed-by : helm\\n    app.kubernetes.io/component : database\\n    app.kubernetes.io/part-of : wordpress\\n...\\nWith the MySQL StatefulSet  and Service  you\\'ll notice information about both MySQL and\\nWordPress, the broader application, are included.\\nKubernetes Components\\nA Kubernetes cluster consists of the components that are a part of the control plane and a set of\\nmachines called nodes.\\nWhen you deploy Kubernetes, you get a cluster.\\nA Kubernetes cluster consists of a set of worker machines, called nodes , that run containerized\\napplications. Every cluster has at least one worker node.\\nThe worker node(s) host the Pods  that are the components of the application workload. The \\ncontrol plane  manages the worker nodes and the Pods in the cluster. In production\\nenvironments, the control plane usually runs across multiple computers and a cluster usually\\nruns multiple nodes, providing fault-tolerance and high availability.\\nThis document outlines the various components you need to have for a complete and working\\nKubernetes cluster.\\nComponents of Kubernetes\\nThe components of a Kubernetes cluster', metadata={'source': './PDFS/Concepts.pdf', 'page': 30}),\n",
       " Document(page_content=\"Control Plane Components\\nThe control plane's components make global decisions about the cluster (for example,\\nscheduling), as well as detecting and responding to cluster events (for example, starting up a\\nnew pod when a deployment's replicas  field is unsatisfied).\\nControl plane components can be run on any machine in the cluster. However, for simplicity,\\nset up scripts typically start all control plane components on the same machine, and do not run\\nuser containers on this machine. See Creating Highly Available clusters with kubeadm  for an\\nexample control plane setup that runs across multiple machines.\\nkube-apiserver\\nThe API server is a component of the Kubernetes control plane  that exposes the Kubernetes\\nAPI. The API server is the front end for the Kubernetes control plane.\\nThe main implementation of a Kubernetes API server is kube-apiserver . kube-apiserver is\\ndesigned to scale horizontally—that is, it scales by deploying more instances. You can run\\nseveral instances of kube-apiserver and balance traffic between those instances.\\netcd\\nConsistent and highly-available key value store used as Kubernetes' backing store for all cluster\\ndata.\\nIf your Kubernetes cluster uses etcd as its backing store, make sure you have a back up  plan for\\nthe data.\\nYou can find in-depth information about etcd in the official documentation .\\nkube-scheduler\\nControl plane component that watches for newly created Pods  with no assigned node , and\\nselects a node for them to run on.\\nFactors taken into account for scheduling decisions include: individual and collective resource\\nrequirements, hardware/software/policy constraints, affinity and anti-affinity specifications,\\ndata locality, inter-workload interference, and deadlines.\\nkube-controller-manager\\nControl plane component that runs controller  processes.\\nLogically, each controller  is a separate process, but to reduce complexity, they are all compiled\\ninto a single binary and run in a single process.\\nThere are many different types of controllers. Some examples of them are:\\nNode controller: Responsible for noticing and responding when nodes go down.\\nJob controller: Watches for Job objects that represent one-off tasks, then creates Pods to\\nrun those tasks to completion.• \\n•\", metadata={'source': './PDFS/Concepts.pdf', 'page': 31}),\n",
       " Document(page_content=\"EndpointSlice controller: Populates EndpointSlice objects (to provide a link between\\nServices and Pods).\\nServiceAccount controller: Create default ServiceAccounts for new namespaces.\\nThe above is not an exhaustive list.\\ncloud-controller-manager\\nA Kubernetes control plane  component that embeds cloud-specific control logic. The cloud\\ncontroller manager  lets you link your cluster into your cloud provider's API, and separates out\\nthe components that interact with that cloud platform from components that only interact with\\nyour cluster.\\nThe cloud-controller-manager only runs controllers that are specific to your cloud provider. If\\nyou are running Kubernetes on your own premises, or in a learning environment inside your\\nown PC, the cluster does not have a cloud controller manager.\\nAs with the kube-controller-manager, the cloud-controller-manager combines several logically\\nindependent control loops into a single binary that you run as a single process. You can scale\\nhorizontally (run more than one copy) to improve performance or to help tolerate failures.\\nThe following controllers can have cloud provider dependencies:\\nNode controller: For checking the cloud provider to determine if a node has been deleted\\nin the cloud after it stops responding\\nRoute controller: For setting up routes in the underlying cloud infrastructure\\nService controller: For creating, updating and deleting cloud provider load balancers\\nNode Components\\nNode components run on every node, maintaining running pods and providing the Kubernetes\\nruntime environment.\\nkubelet\\nAn agent that runs on each node  in the cluster. It makes sure that containers  are running in a \\nPod.\\nThe kubelet  takes a set of PodSpecs that are provided through various mechanisms and ensures\\nthat the containers described in those PodSpecs are running and healthy. The kubelet doesn't\\nmanage containers which were not created by Kubernetes.\\nkube-proxy\\nkube-proxy is a network proxy that runs on each node  in your cluster, implementing part of the\\nKubernetes Service  concept.\\nkube-proxy  maintains network rules on nodes. These network rules allow network\\ncommunication to your Pods from network sessions inside or outside of your cluster.\\nkube-proxy uses the operating system packet filtering layer if there is one and it's available.\\nOtherwise, kube-proxy forwards the traffic itself.• \\n• \\n• \\n• \\n•\", metadata={'source': './PDFS/Concepts.pdf', 'page': 32}),\n",
       " Document(page_content='Container runtime\\nA fundamental component that empowers Kubernetes to run containers effectively. It is\\nresponsible for managing the execution and lifecycle of containers within the Kubernetes\\nenvironment.\\nKubernetes supports container runtimes such as containerd , CRI-O , and any other\\nimplementation of the Kubernetes CRI (Container Runtime Interface) .\\nAddons\\nAddons use Kubernetes resources ( DaemonSet , Deployment , etc) to implement cluster features.\\nBecause these are providing cluster-level features, namespaced resources for addons belong\\nwithin the kube-system  namespace.\\nSelected addons are described below; for an extended list of available addons, please see \\nAddons .\\nDNS\\nWhile the other addons are not strictly required, all Kubernetes clusters should have cluster\\nDNS , as many examples rely on it.\\nCluster DNS is a DNS server, in addition to the other DNS server(s) in your environment, which\\nserves DNS records for Kubernetes services.\\nContainers started by Kubernetes automatically include this DNS server in their DNS searches.\\nWeb UI (Dashboard)\\nDashboard  is a general purpose, web-based UI for Kubernetes clusters. It allows users to\\nmanage and troubleshoot applications running in the cluster, as well as the cluster itself.\\nContainer Resource Monitoring\\nContainer Resource Monitoring  records generic time-series metrics about containers in a\\ncentral database, and provides a UI for browsing that data.\\nCluster-level Logging\\nA cluster-level logging  mechanism is responsible for saving container logs to a central log store\\nwith search/browsing interface.\\nNetwork Plugins\\nNetwork plugins  are software components that implement the container network interface\\n(CNI) specification. They are responsible for allocating IP addresses to pods and enabling them\\nto communicate with each other within the cluster.', metadata={'source': './PDFS/Concepts.pdf', 'page': 33}),\n",
       " Document(page_content=\"What's next\\nLearn more about the following:\\nNodes  and their communication  with the control plane.\\nKubernetes controllers .\\nkube-scheduler  which is the default scheduler for Kubernetes.\\nEtcd's official documentation .\\nSeveral container runtimes  in Kubernetes.\\nIntegrating with cloud providers using cloud-controller-manager .\\nkubectl  commands.\\nThe Kubernetes API\\nThe Kubernetes API lets you query and manipulate the state of objects in Kubernetes. The core\\nof Kubernetes' control plane is the API server and the HTTP API that it exposes. Users, the\\ndifferent parts of your cluster, and external components all communicate with one another\\nthrough the API server.\\nThe core of Kubernetes' control plane  is the API server . The API server exposes an HTTP API\\nthat lets end users, different parts of your cluster, and external components communicate with\\none another.\\nThe Kubernetes API lets you query and manipulate the state of API objects in Kubernetes (for\\nexample: Pods, Namespaces, ConfigMaps, and Events).\\nMost operations can be performed through the kubectl  command-line interface or other\\ncommand-line tools, such as kubeadm , which in turn use the API. However, you can also access\\nthe API directly using REST calls.\\nConsider using one of the client libraries  if you are writing an application using the Kubernetes\\nAPI.\\nOpenAPI specification\\nComplete API details are documented using OpenAPI .\\nOpenAPI V2\\nThe Kubernetes API server serves an aggregated OpenAPI v2 spec via the /openapi/v2\\nendpoint. You can request the response format using request headers as follows:\\nValid request header values for OpenAPI v2 queries\\nHeader Possible values Notes\\nAccept-\\nEncodinggzipnot supplying this header is also\\nacceptable\\nAcceptapplication/com.github.proto-\\nopenapi.spec.v2@v1.0+protobufmainly for intra-cluster use\\napplication/json default\\n* serves application/json• \\n• \\n• \\n• \\n• \\n• \\n•\", metadata={'source': './PDFS/Concepts.pdf', 'page': 34}),\n",
       " Document(page_content='Kubernetes implements an alternative Protobuf based serialization format that is primarily\\nintended for intra-cluster communication. For more information about this format, see the \\nKubernetes Protobuf serialization  design proposal and the Interface Definition Language (IDL)\\nfiles for each schema located in the Go packages that define the API objects.\\nOpenAPI V3\\nFEATURE STATE:  Kubernetes v1.27 [stable]\\nKubernetes supports publishing a description of its APIs as OpenAPI v3.\\nA discovery endpoint /openapi/v3  is provided to see a list of all group/versions available. This\\nendpoint only returns JSON. These group/versions are provided in the following format:\\n{\\n    \"paths\": {\\n        ...,\\n        \"api/v1\": {\\n            \"serverRelativeURL\": \"/openapi/v3/api/v1?\\nhash=CC0E9BFD992D8C59AEC98A1E2336F899E8318D3CF4C68944C3DEC640AF5AB52D864A\\nC50DAA8D145B3494F75FA3CFF939FCBDDA431DAD3CA79738B297795818CF\"\\n        },\\n        \"apis/admissionregistration.k8s.io/v1\": {\\n            \"serverRelativeURL\": \"/openapi/v3/apis/admissionregistration.k8s.io/v1?\\nhash=E19CC93A116982CE5422FC42B590A8AFAD92CDE9AE4D59B5CAAD568F083AD07946E6\\nCB5817531680BCE6E215C16973CD39003B0425F3477CFD854E89A9DB6597\"\\n        },\\n        ....\\n    }\\n}\\nThe relative URLs are pointing to immutable OpenAPI descriptions, in order to improve client-\\nside caching. The proper HTTP caching headers are also set by the API server for that purpose\\n(Expires  to 1 year in the future, and Cache-Control  to immutable ). When an obsolete URL is\\nused, the API server returns a redirect to the newest URL.\\nThe Kubernetes API server publishes an OpenAPI v3 spec per Kubernetes group version at the /\\nopenapi/v3/apis/<group>/<version>?hash=<hash>  endpoint.\\nRefer to the table below for accepted request headers.\\nValid request header values for OpenAPI v3 queries\\nHeader Possible values Notes\\nAccept-\\nEncodinggzipnot supplying this header is also\\nacceptable\\nAcceptapplication/com.github.proto-\\nopenapi.spec.v3@v1.0+protobufmainly for intra-cluster use\\napplication/json default\\n* serves application/json\\nA Golang implementation to fetch the OpenAPI V3 is provided in the package k8s.io/client-go/\\nopenapi3 .', metadata={'source': './PDFS/Concepts.pdf', 'page': 35}),\n",
       " Document(page_content='Persistence\\nKubernetes stores the serialized state of objects by writing them into etcd.\\nAPI Discovery\\nA list of all group versions supported by a cluster is published at the /api and /apis  endpoints.\\nEach group version also advertises the list of resources supported via /apis/<group>/<version>\\n(for example: /apis/rbac.authorization.k8s.io/v1alpha1 ). These endpoints are used by kubectl to\\nfetch the list of resources supported by a cluster.\\nAggregated Discovery\\nFEATURE STATE:  Kubernetes v1.27 [beta]\\nKubernetes offers beta support for aggregated discovery, publishing all resources supported by\\na cluster through two endpoints ( /api and /apis ) compared to one for every group version.\\nRequesting this endpoint drastically reduces the number of requests sent to fetch the discovery\\nfor the average Kubernetes cluster. This may be accessed by requesting the respective endpoints\\nwith an Accept header indicating the aggregated discovery resource: Accept: application/\\njson;v=v2beta1;g=apidiscovery.k8s.io;as=APIGroupDiscoveryList .\\nThe endpoint also supports ETag and protobuf encoding.\\nAPI groups and versioning\\nTo make it easier to eliminate fields or restructure resource representations, Kubernetes\\nsupports multiple API versions, each at a different API path, such as /api/v1  or /apis/\\nrbac.authorization.k8s.io/v1alpha1 .\\nVersioning is done at the API level rather than at the resource or field level to ensure that the\\nAPI presents a clear, consistent view of system resources and behavior, and to enable\\ncontrolling access to end-of-life and/or experimental APIs.\\nTo make it easier to evolve and to extend its API, Kubernetes implements API groups  that can\\nbe enabled or disabled .\\nAPI resources are distinguished by their API group, resource type, namespace (for namespaced\\nresources), and name. The API server handles the conversion between API versions\\ntransparently: all the different versions are actually representations of the same persisted data.\\nThe API server may serve the same underlying data through multiple API versions.\\nFor example, suppose there are two API versions, v1 and v1beta1 , for the same resource. If you\\noriginally created an object using the v1beta1  version of its API, you can later read, update, or\\ndelete that object using either the v1beta1  or the v1 API version, until the v1beta1  version is\\ndeprecated and removed. At that point you can continue accessing and modifying the object\\nusing the v1 API.', metadata={'source': './PDFS/Concepts.pdf', 'page': 36}),\n",
       " Document(page_content=\"API changes\\nAny system that is successful needs to grow and change as new use cases emerge or existing\\nones change. Therefore, Kubernetes has designed the Kubernetes API to continuously change\\nand grow. The Kubernetes project aims to not break compatibility with existing clients, and to\\nmaintain that compatibility for a length of time so that other projects have an opportunity to\\nadapt.\\nIn general, new API resources and new resource fields can be added often and frequently.\\nElimination of resources or fields requires following the API deprecation policy .\\nKubernetes makes a strong commitment to maintain compatibility for official Kubernetes APIs\\nonce they reach general availability (GA), typically at API version v1. Additionally, Kubernetes\\nmaintains compatibility with data persisted via beta API versions of official Kubernetes APIs,\\nand ensures that data can be converted and accessed via GA API versions when the feature\\ngoes stable.\\nIf you adopt a beta API version, you will need to transition to a subsequent beta or stable API\\nversion once the API graduates. The best time to do this is while the beta API is in its\\ndeprecation period, since objects are simultaneously accessible via both API versions. Once the\\nbeta API completes its deprecation period and is no longer served, the replacement API version\\nmust be used.\\nNote:  Although Kubernetes also aims to maintain compatibility for alpha  APIs versions, in\\nsome circumstances this is not possible. If you use any alpha API versions, check the release\\nnotes for Kubernetes when upgrading your cluster, in case the API did change in incompatible\\nways that require deleting all existing alpha objects prior to upgrade.\\nRefer to API versions reference  for more details on the API version level definitions.\\nAPI Extension\\nThe Kubernetes API can be extended in one of two ways:\\nCustom resources  let you declaratively define how the API server should provide your\\nchosen resource API.\\nYou can also extend the Kubernetes API by implementing an aggregation layer .\\nWhat's next\\nLearn how to extend the Kubernetes API by adding your own CustomResourceDefinition .\\nControlling Access To The Kubernetes API  describes how the cluster manages\\nauthentication and authorization for API access.\\nLearn about API endpoints, resource types and samples by reading API Reference .\\nLearn about what constitutes a compatible change, and how to change the API, from API\\nchanges .\\nCluster Architecture\\nThe architectural concepts behind Kubernetes.1. \\n2. \\n• \\n• \\n• \\n•\", metadata={'source': './PDFS/Concepts.pdf', 'page': 37}),\n",
       " Document(page_content='Components of Kubernetes\\nKubernetes cluster architecture\\nNodes\\nCommunication between Nodes and the Control Plane\\nControllers\\nLeases\\nCloud Controller Manager\\nAbout cgroup v2\\nContainer Runtime Interface (CRI)\\nGarbage Collection\\nMixed Version Proxy\\nNodes\\nKubernetes runs your workload  by placing containers into Pods to run on Nodes . A node may\\nbe a virtual or physical machine, depending on the cluster. Each node is managed by the control\\nplane  and contains the services necessary to run Pods .\\nTypically you have several nodes in a cluster; in a learning or resource-limited environment,\\nyou might have only one node.\\nThe components  on a node include the kubelet , a container runtime , and the kube-proxy .\\nManagement\\nThere are two main ways to have Nodes added to the API server :\\nThe kubelet on a node self-registers to the control plane\\nYou (or another human user) manually add a Node object\\nAfter you create a Node object , or the kubelet on a node self-registers, the control plane checks\\nwhether the new Node object is valid. For example, if you try to create a Node from the\\nfollowing JSON manifest:\\n{\\n  \"kind\" : \"Node\" ,\\n  \"apiVersion\" : \"v1\",\\n  \"metadata\" : {\\n    \"name\" : \"10.240.79.157\" ,\\n    \"labels\" : {1. \\n2.', metadata={'source': './PDFS/Concepts.pdf', 'page': 38}),\n",
       " Document(page_content='\"name\" : \"my-first-k8s-node\"\\n    }\\n  }\\n}\\nKubernetes creates a Node object internally (the representation). Kubernetes checks that a\\nkubelet has registered to the API server that matches the metadata.name  field of the Node. If\\nthe node is healthy (i.e. all necessary services are running), then it is eligible to run a Pod.\\nOtherwise, that node is ignored for any cluster activity until it becomes healthy.\\nNote:\\nKubernetes keeps the object for the invalid Node and continues checking to see whether it\\nbecomes healthy.\\nYou, or a controller , must explicitly delete the Node object to stop that health checking.\\nThe name of a Node object must be a valid DNS subdomain name .\\nNode name uniqueness\\nThe name  identifies a Node. Two Nodes cannot have the same name at the same time.\\nKubernetes also assumes that a resource with the same name is the same object. In case of a\\nNode, it is implicitly assumed that an instance using the same name will have the same state\\n(e.g. network settings, root disk contents) and attributes like node labels. This may lead to\\ninconsistencies if an instance was modified without changing its name. If the Node needs to be\\nreplaced or updated significantly, the existing Node object needs to be removed from API server\\nfirst and re-added after the update.\\nSelf-registration of Nodes\\nWhen the kubelet flag --register-node  is true (the default), the kubelet will attempt to register\\nitself with the API server. This is the preferred pattern, used by most distros.\\nFor self-registration, the kubelet is started with the following options:\\n--kubeconfig  - Path to credentials to authenticate itself to the API server.\\n--cloud-provider  - How to talk to a cloud provider  to read metadata about itself.\\n--register-node  - Automatically register with the API server.\\n--register-with-taints  - Register the node with the given list of taints  (comma separated \\n<key>=<value>:<effect> ).\\nNo-op if register-node  is false.\\n--node-ip  - Optional comma-separated list of the IP addresses for the node. You can only\\nspecify a single address for each address family. For example, in a single-stack IPv4\\ncluster, you set this value to be the IPv4 address that the kubelet should use for the node.\\nSee configure IPv4/IPv6 dual stack  for details of running a dual-stack cluster.• \\n• \\n• \\n• \\n•', metadata={'source': './PDFS/Concepts.pdf', 'page': 39}),\n",
       " Document(page_content=\"If you don't provide this argument, the kubelet uses the node's default IPv4 address, if\\nany; if the node has no IPv4 addresses then the kubelet uses the node's default IPv6\\naddress.\\n--node-labels  - Labels  to add when registering the node in the cluster (see label\\nrestrictions enforced by the NodeRestriction admission plugin ).\\n--node-status-update-frequency  - Specifies how often kubelet posts its node status to the\\nAPI server.\\nWhen the Node authorization mode  and NodeRestriction admission plugin  are enabled,\\nkubelets are only authorized to create/modify their own Node resource.\\nNote:\\nAs mentioned in the Node name uniqueness  section, when Node configuration needs to be\\nupdated, it is a good practice to re-register the node with the API server. For example, if the\\nkubelet being restarted with the new set of --node-labels , but the same Node name is used, the\\nchange will not take an effect, as labels are being set on the Node registration.\\nPods already scheduled on the Node may misbehave or cause issues if the Node configuration\\nwill be changed on kubelet restart. For example, already running Pod may be tainted against\\nthe new labels assigned to the Node, while other Pods, that are incompatible with that Pod will\\nbe scheduled based on this new label. Node re-registration ensures all Pods will be drained and\\nproperly re-scheduled.\\nManual Node administration\\nYou can create and modify Node objects using kubectl .\\nWhen you want to create Node objects manually, set the kubelet flag --register-node=false .\\nYou can modify Node objects regardless of the setting of --register-node . For example, you can\\nset labels on an existing Node or mark it unschedulable.\\nYou can use labels on Nodes in conjunction with node selectors on Pods to control scheduling.\\nFor example, you can constrain a Pod to only be eligible to run on a subset of the available\\nnodes.\\nMarking a node as unschedulable prevents the scheduler from placing new pods onto that Node\\nbut does not affect existing Pods on the Node. This is useful as a preparatory step before a node\\nreboot or other maintenance.\\nTo mark a Node unschedulable, run:\\nkubectl cordon $NODENAME\\nSee Safely Drain a Node  for more details.\\nNote:  Pods that are part of a DaemonSet  tolerate being run on an unschedulable Node.\\nDaemonSets typically provide node-local services that should run on the Node even if it is\\nbeing drained of workload applications.• \\n•\", metadata={'source': './PDFS/Concepts.pdf', 'page': 40}),\n",
       " Document(page_content=\"Node status\\nA Node's status contains the following information:\\nAddresses\\nConditions\\nCapacity and Allocatable\\nInfo\\nYou can use kubectl  to view a Node's status and other details:\\nkubectl describe node <insert-node-name-here>\\nSee Node Status  for more details.\\nNode heartbeats\\nHeartbeats, sent by Kubernetes nodes, help your cluster determine the availability of each node,\\nand to take action when failures are detected.\\nFor nodes there are two forms of heartbeats:\\nUpdates to the .status  of a Node.\\nLease  objects within the kube-node-lease  namespace . Each Node has an associated Lease\\nobject.\\nNode controller\\nThe node controller  is a Kubernetes control plane component that manages various aspects of\\nnodes.\\nThe node controller has multiple roles in a node's life. The first is assigning a CIDR block to the\\nnode when it is registered (if CIDR assignment is turned on).\\nThe second is keeping the node controller's internal list of nodes up to date with the cloud\\nprovider's list of available machines. When running in a cloud environment and whenever a\\nnode is unhealthy, the node controller asks the cloud provider if the VM for that node is still\\navailable. If not, the node controller deletes the node from its list of nodes.\\nThe third is monitoring the nodes' health. The node controller is responsible for:\\nIn the case that a node becomes unreachable, updating the Ready  condition in the\\nNode's .status  field. In this case the node controller sets the Ready  condition to Unknown .\\nIf a node remains unreachable: triggering API-initiated eviction  for all of the Pods on the\\nunreachable node. By default, the node controller waits 5 minutes between marking the\\nnode as Unknown  and submitting the first eviction request.\\nBy default, the node controller checks the state of each node every 5 seconds. This period can\\nbe configured using the --node-monitor-period  flag on the kube-controller-manager\\ncomponent.• \\n• \\n• \\n• \\n• \\n• \\n• \\n•\", metadata={'source': './PDFS/Concepts.pdf', 'page': 41}),\n",
       " Document(page_content=\"Rate limits on eviction\\nIn most cases, the node controller limits the eviction rate to --node-eviction-rate  (default 0.1)\\nper second, meaning it won't evict pods from more than 1 node per 10 seconds.\\nThe node eviction behavior changes when a node in a given availability zone becomes\\nunhealthy. The node controller checks what percentage of nodes in the zone are unhealthy (the \\nReady  condition is Unknown  or False ) at the same time:\\nIf the fraction of unhealthy nodes is at least --unhealthy-zone-threshold  (default 0.55),\\nthen the eviction rate is reduced.\\nIf the cluster is small (i.e. has less than or equal to --large-cluster-size-threshold  nodes -\\ndefault 50), then evictions are stopped.\\nOtherwise, the eviction rate is reduced to --secondary-node-eviction-rate  (default 0.01)\\nper second.\\nThe reason these policies are implemented per availability zone is because one availability zone\\nmight become partitioned from the control plane while the others remain connected. If your\\ncluster does not span multiple cloud provider availability zones, then the eviction mechanism\\ndoes not take per-zone unavailability into account.\\nA key reason for spreading your nodes across availability zones is so that the workload can be\\nshifted to healthy zones when one entire zone goes down. Therefore, if all nodes in a zone are\\nunhealthy, then the node controller evicts at the normal rate of --node-eviction-rate . The corner\\ncase is when all zones are completely unhealthy (none of the nodes in the cluster are healthy).\\nIn such a case, the node controller assumes that there is some problem with connectivity\\nbetween the control plane and the nodes, and doesn't perform any evictions. (If there has been\\nan outage and some nodes reappear, the node controller does evict pods from the remaining\\nnodes that are unhealthy or unreachable).\\nThe node controller is also responsible for evicting pods running on nodes with NoExecute\\ntaints, unless those pods tolerate that taint. The node controller also adds taints  corresponding\\nto node problems like node unreachable or not ready. This means that the scheduler won't place\\nPods onto unhealthy nodes.\\nResource capacity tracking\\nNode objects track information about the Node's resource capacity: for example, the amount of\\nmemory available and the number of CPUs. Nodes that self register  report their capacity during\\nregistration. If you manually  add a Node, then you need to set the node's capacity information\\nwhen you add it.\\nThe Kubernetes scheduler  ensures that there are enough resources for all the Pods on a Node.\\nThe scheduler checks that the sum of the requests of containers on the node is no greater than\\nthe node's capacity. That sum of requests includes all containers managed by the kubelet, but\\nexcludes any containers started directly by the container runtime, and also excludes any\\nprocesses running outside of the kubelet's control.\\nNote:  If you want to explicitly reserve resources for non-Pod processes, see reserve resources\\nfor system daemons .• \\n• \\n•\", metadata={'source': './PDFS/Concepts.pdf', 'page': 42}),\n",
       " Document(page_content='Node topology\\nFEATURE STATE:  Kubernetes v1.18 [beta]\\nIf you have enabled the TopologyManager  feature gate , then the kubelet can use topology hints\\nwhen making resource assignment decisions. See Control Topology Management Policies on a\\nNode  for more information.\\nGraceful node shutdown\\nFEATURE STATE:  Kubernetes v1.21 [beta]\\nThe kubelet attempts to detect node system shutdown and terminates pods running on the\\nnode.\\nKubelet ensures that pods follow the normal pod termination process  during the node\\nshutdown. During node shutdown, the kubelet does not accept new Pods (even if those Pods are\\nalready bound to the node).\\nThe Graceful node shutdown feature depends on systemd since it takes advantage of systemd\\ninhibitor locks  to delay the node shutdown with a given duration.\\nGraceful node shutdown is controlled with the GracefulNodeShutdown  feature gate  which is\\nenabled by default in 1.21.\\nNote that by default, both configuration options described below, shutdownGracePeriod  and \\nshutdownGracePeriodCriticalPods  are set to zero, thus not activating the graceful node\\nshutdown functionality. To activate the feature, the two kubelet config settings should be\\nconfigured appropriately and set to non-zero values.\\nOnce systemd detects or notifies node shutdown, the kubelet sets a NotReady  condition on the\\nNode, with the reason  set to \"node is shutting down\" . The kube-scheduler honors this condition\\nand does not schedule any Pods onto the affected node; other third-party schedulers are\\nexpected to follow the same logic. This means that new Pods won\\'t be scheduled onto that node\\nand therefore none will start.\\nThe kubelet also rejects Pods during the PodAdmission  phase if an ongoing node shutdown has\\nbeen detected, so that even Pods with a toleration  for node.kubernetes.io/not-ready:NoSchedule\\ndo not start there.\\nAt the same time when kubelet is setting that condition on its Node via the API, the kubelet\\nalso begins terminating any Pods that are running locally.\\nDuring a graceful shutdown, kubelet terminates pods in two phases:\\nTerminate regular pods running on the node.\\nTerminate critical pods  running on the node.\\nGraceful node shutdown feature is configured with two KubeletConfiguration  options:\\nshutdownGracePeriod :\\nSpecifies the total duration that the node should delay the shutdown by. This is the\\ntotal grace period for pod termination for both regular and critical pods .1. \\n2. \\n• \\n◦', metadata={'source': './PDFS/Concepts.pdf', 'page': 43}),\n",
       " Document(page_content='shutdownGracePeriodCriticalPods :\\nSpecifies the duration used to terminate critical pods  during a node shutdown. This\\nvalue should be less than shutdownGracePeriod .\\nNote:  There are cases when Node termination was cancelled by the system (or perhaps\\nmanually by an administrator). In either of those situations the Node will return to the Ready\\nstate. However, Pods which already started the process of termination will not be restored by\\nkubelet and will need to be re-scheduled.\\nFor example, if shutdownGracePeriod=30s , and shutdownGracePeriodCriticalPods=10s , kubelet\\nwill delay the node shutdown by 30 seconds. During the shutdown, the first 20 (30-10) seconds\\nwould be reserved for gracefully terminating normal pods, and the last 10 seconds would be\\nreserved for terminating critical pods .\\nNote:\\nWhen pods were evicted during the graceful node shutdown, they are marked as shutdown.\\nRunning kubectl get pods  shows the status of the evicted pods as Terminated . And kubectl \\ndescribe pod  indicates that the pod was evicted because of node shutdown:\\nReason:         Terminated\\nMessage:        Pod was terminated in response to imminent node shutdown.\\nPod Priority based graceful node shutdown\\nFEATURE STATE:  Kubernetes v1.24 [beta]\\nTo provide more flexibility during graceful node shutdown around the ordering of pods during\\nshutdown, graceful node shutdown honors the PriorityClass for Pods, provided that you\\nenabled this feature in your cluster. The feature allows cluster administers to explicitly define\\nthe ordering of pods during graceful node shutdown based on priority classes .\\nThe Graceful Node Shutdown  feature, as described above, shuts down pods in two phases, non-\\ncritical pods, followed by critical pods. If additional flexibility is needed to explicitly define the\\nordering of pods during shutdown in a more granular way, pod priority based graceful\\nshutdown can be used.\\nWhen graceful node shutdown honors pod priorities, this makes it possible to do graceful node\\nshutdown in multiple phases, each phase shutting down a particular priority class of pods. The\\nkubelet can be configured with the exact phases and shutdown time per phase.\\nAssuming the following custom pod priority classes  in a cluster,\\nPod priority class name Pod priority class value\\ncustom-class-a 100000\\ncustom-class-b 10000\\ncustom-class-c 1000\\nregular/unset 0\\nWithin the kubelet configuration  the settings for shutdownGracePeriodByPodPriority  could\\nlook like:• \\n◦', metadata={'source': './PDFS/Concepts.pdf', 'page': 44}),\n",
       " Document(page_content=\"Pod priority class value Shutdown period\\n100000 10 seconds\\n10000 180 seconds\\n1000 120 seconds\\n0 60 seconds\\nThe corresponding kubelet config YAML configuration would be:\\nshutdownGracePeriodByPodPriority :\\n  - priority : 100000\\n    shutdownGracePeriodSeconds : 10\\n  - priority : 10000\\n    shutdownGracePeriodSeconds : 180\\n  - priority : 1000\\n    shutdownGracePeriodSeconds : 120\\n  - priority : 0\\n    shutdownGracePeriodSeconds : 60\\nThe above table implies that any pod with priority  value >= 100000 will get just 10 seconds to\\nstop, any pod with value >= 10000 and < 100000 will get 180 seconds to stop, any pod with\\nvalue >= 1000 and < 10000 will get 120 seconds to stop. Finally, all other pods will get 60\\nseconds to stop.\\nOne doesn't have to specify values corresponding to all of the classes. For example, you could\\ninstead use these settings:\\nPod priority class value Shutdown period\\n100000 300 seconds\\n1000 120 seconds\\n0 60 seconds\\nIn the above case, the pods with custom-class-b  will go into the same bucket as custom-class-c\\nfor shutdown.\\nIf there are no pods in a particular range, then the kubelet does not wait for pods in that\\npriority range. Instead, the kubelet immediately skips to the next priority class value range.\\nIf this feature is enabled and no configuration is provided, then no ordering action will be\\ntaken.\\nUsing this feature requires enabling the GracefulNodeShutdownBasedOnPodPriority  feature\\ngate, and setting ShutdownGracePeriodByPodPriority  in the kubelet config  to the desired\\nconfiguration containing the pod priority class values and their respective shutdown periods.\\nNote:  The ability to take Pod priority into account during graceful node shutdown was\\nintroduced as an Alpha feature in Kubernetes v1.23. In Kubernetes 1.28 the feature is Beta and\\nis enabled by default.\\nMetrics graceful_shutdown_start_time_seconds  and graceful_shutdown_end_time_seconds  are\\nemitted under the kubelet subsystem to monitor node shutdowns.\", metadata={'source': './PDFS/Concepts.pdf', 'page': 45}),\n",
       " Document(page_content=\"Non-graceful node shutdown handling\\nFEATURE STATE:  Kubernetes v1.28 [stable]\\nA node shutdown action may not be detected by kubelet's Node Shutdown Manager, either\\nbecause the command does not trigger the inhibitor locks mechanism used by kubelet or\\nbecause of a user error, i.e., the ShutdownGracePeriod and ShutdownGracePeriodCriticalPods\\nare not configured properly. Please refer to above section Graceful Node Shutdown  for more\\ndetails.\\nWhen a node is shutdown but not detected by kubelet's Node Shutdown Manager, the pods that\\nare part of a StatefulSet  will be stuck in terminating status on the shutdown node and cannot\\nmove to a new running node. This is because kubelet on the shutdown node is not available to\\ndelete the pods so the StatefulSet cannot create a new pod with the same name. If there are\\nvolumes used by the pods, the VolumeAttachments will not be deleted from the original\\nshutdown node so the volumes used by these pods cannot be attached to a new running node.\\nAs a result, the application running on the StatefulSet cannot function properly. If the original\\nshutdown node comes up, the pods will be deleted by kubelet and new pods will be created on a\\ndifferent running node. If the original shutdown node does not come up, these pods will be\\nstuck in terminating status on the shutdown node forever.\\nTo mitigate the above situation, a user can manually add the taint node.kubernetes.io/out-of-\\nservice  with either NoExecute  or NoSchedule  effect to a Node marking it out-of-service. If the \\nNodeOutOfServiceVolumeDetach feature gate  is enabled on kube-controller-manager , and a\\nNode is marked out-of-service with this taint, the pods on the node will be forcefully deleted if\\nthere are no matching tolerations on it and volume detach operations for the pods terminating\\non the node will happen immediately. This allows the Pods on the out-of-service node to\\nrecover quickly on a different node.\\nDuring a non-graceful shutdown, Pods are terminated in the two phases:\\nForce delete the Pods that do not have matching out-of-service  tolerations.\\nImmediately perform detach volume operation for such pods.\\nNote:\\nBefore adding the taint node.kubernetes.io/out-of-service , it should be verified that the\\nnode is already in shutdown or power off state (not in the middle of restarting).\\nThe user is required to manually remove the out-of-service taint after the pods are moved\\nto a new node and the user has checked that the shutdown node has been recovered since\\nthe user was the one who originally added the taint.\\nSwap memory management\\nFEATURE STATE:  Kubernetes v1.28 [beta]\\nTo enable swap on a node, the NodeSwap  feature gate must be enabled on the kubelet, and the \\n--fail-swap-on  command line flag or failSwapOn  configuration setting  must be set to false.\\nWarning:  When the memory swap feature is turned on, Kubernetes data such as the content of\\nSecret objects that were written to tmpfs now could be swapped to disk.1. \\n2. \\n• \\n•\", metadata={'source': './PDFS/Concepts.pdf', 'page': 46}),\n",
       " Document(page_content=\"A user can also optionally configure memorySwap.swapBehavior  in order to specify how a\\nnode will use swap memory. For example,\\nmemorySwap :\\n  swapBehavior : UnlimitedSwap\\nUnlimitedSwap  (default): Kubernetes workloads can use as much swap memory as they\\nrequest, up to the system limit.\\nLimitedSwap : The utilization of swap memory by Kubernetes workloads is subject to\\nlimitations. Only Pods of Burstable QoS are permitted to employ swap.\\nIf configuration for memorySwap  is not specified and the feature gate is enabled, by default the\\nkubelet will apply the same behaviour as the UnlimitedSwap  setting.\\nWith LimitedSwap , Pods that do not fall under the Burstable QoS classification (i.e. BestEffort /\\nGuaranteed  Qos Pods) are prohibited from utilizing swap memory. To maintain the\\naforementioned security and node health guarantees, these Pods are not permitted to use swap\\nmemory when LimitedSwap  is in effect.\\nPrior to detailing the calculation of the swap limit, it is necessary to define the following terms:\\nnodeTotalMemory : The total amount of physical memory available on the node.\\ntotalPodsSwapAvailable : The total amount of swap memory on the node that is available\\nfor use by Pods (some swap memory may be reserved for system use).\\ncontainerMemoryRequest : The container's memory request.\\nSwap limitation is configured as: (containerMemoryRequest / nodeTotalMemory) * \\ntotalPodsSwapAvailable .\\nIt is important to note that, for containers within Burstable QoS Pods, it is possible to opt-out of\\nswap usage by specifying memory requests that are equal to memory limits. Containers\\nconfigured in this manner will not have access to swap memory.\\nSwap is supported only with cgroup v2 , cgroup v1 is not supported.\\nFor more information, and to assist with testing and provide feedback, please see the blog-post\\nabout Kubernetes 1.28: NodeSwap graduates to Beta1 , KEP-2400  and its design proposal .\\nWhat's next\\nLearn more about the following:\\nComponents  that make up a node.\\nAPI definition for Node .\\nNode  section of the architecture design document.\\nTaints and Tolerations .\\nNode Resource Managers .\\nResource Management for Windows nodes .• \\n• \\n• \\n• \\n• \\n• \\n• \\n• \\n• \\n• \\n•\", metadata={'source': './PDFS/Concepts.pdf', 'page': 47}),\n",
       " Document(page_content='Communication between Nodes and the\\nControl Plane\\nThis document catalogs the communication paths between the API server  and the Kubernetes \\ncluster . The intent is to allow users to customize their installation to harden the network\\nconfiguration such that the cluster can be run on an untrusted network (or on fully public IPs\\non a cloud provider).\\nNode to Control Plane\\nKubernetes has a \"hub-and-spoke\" API pattern. All API usage from nodes (or the pods they run)\\nterminates at the API server. None of the other control plane components are designed to\\nexpose remote services. The API server is configured to listen for remote connections on a\\nsecure HTTPS port (typically 443) with one or more forms of client authentication  enabled. One\\nor more forms of authorization  should be enabled, especially if anonymous requests  or service\\naccount tokens  are allowed.\\nNodes should be provisioned with the public root certificate  for the cluster such that they can\\nconnect securely to the API server along with valid client credentials. A good approach is that\\nthe client credentials provided to the kubelet are in the form of a client certificate. See kubelet\\nTLS bootstrapping  for automated provisioning of kubelet client certificates.\\nPods  that wish to connect to the API server can do so securely by leveraging a service account\\nso that Kubernetes will automatically inject the public root certificate and a valid bearer token\\ninto the pod when it is instantiated. The kubernetes  service (in default  namespace) is configured\\nwith a virtual IP address that is redirected (via kube-proxy ) to the HTTPS endpoint on the API\\nserver.\\nThe control plane components also communicate with the API server over the secure port.\\nAs a result, the default operating mode for connections from the nodes and pod running on the\\nnodes to the control plane is secured by default and can run over untrusted and/or public\\nnetworks.\\nControl plane to node\\nThere are two primary communication paths from the control plane (the API server) to the\\nnodes. The first is from the API server to the kubelet  process which runs on each node in the\\ncluster. The second is from the API server to any node, pod, or service through the API server\\'s \\nproxy  functionality.\\nAPI server to kubelet\\nThe connections from the API server to the kubelet are used for:\\nFetching logs for pods.\\nAttaching (usually through kubectl ) to running pods.\\nProviding the kubelet\\'s port-forwarding functionality.• \\n• \\n•', metadata={'source': './PDFS/Concepts.pdf', 'page': 48}),\n",
       " Document(page_content=\"These connections terminate at the kubelet's HTTPS endpoint. By default, the API server does\\nnot verify the kubelet's serving certificate, which makes the connection subject to man-in-the-\\nmiddle attacks and unsafe  to run over untrusted and/or public networks.\\nTo verify this connection, use the --kubelet-certificate-authority  flag to provide the API server\\nwith a root certificate bundle to use to verify the kubelet's serving certificate.\\nIf that is not possible, use SSH tunneling  between the API server and kubelet if required to\\navoid connecting over an untrusted or public network.\\nFinally, Kubelet authentication and/or authorization  should be enabled to secure the kubelet\\nAPI.\\nAPI server to nodes, pods, and services\\nThe connections from the API server to a node, pod, or service default to plain HTTP\\nconnections and are therefore neither authenticated nor encrypted. They can be run over a\\nsecure HTTPS connection by prefixing https:  to the node, pod, or service name in the API URL,\\nbut they will not validate the certificate provided by the HTTPS endpoint nor provide client\\ncredentials. So while the connection will be encrypted, it will not provide any guarantees of\\nintegrity. These connections are not currently safe  to run over untrusted or public networks.\\nSSH tunnels\\nKubernetes supports SSH tunnels  to protect the control plane to nodes communication paths. In\\nthis configuration, the API server initiates an SSH tunnel to each node in the cluster\\n(connecting to the SSH server listening on port 22) and passes all traffic destined for a kubelet,\\nnode, pod, or service through the tunnel. This tunnel ensures that the traffic is not exposed\\noutside of the network in which the nodes are running.\\nNote:  SSH tunnels are currently deprecated, so you shouldn't opt to use them unless you know\\nwhat you are doing. The Konnectivity service  is a replacement for this communication channel.\\nKonnectivity service\\nFEATURE STATE:  Kubernetes v1.18 [beta]\\nAs a replacement to the SSH tunnels, the Konnectivity service provides TCP level proxy for the\\ncontrol plane to cluster communication. The Konnectivity service consists of two parts: the\\nKonnectivity server in the control plane network and the Konnectivity agents in the nodes\\nnetwork. The Konnectivity agents initiate connections to the Konnectivity server and maintain\\nthe network connections. After enabling the Konnectivity service, all control plane to nodes\\ntraffic goes through these connections.\\nFollow the Konnectivity service task  to set up the Konnectivity service in your cluster.\\nWhat's next\\nRead about the Kubernetes control plane components\\nLearn more about Hubs and Spoke model\\nLearn how to Secure a Cluster\\nLearn more about the Kubernetes API• \\n• \\n• \\n•\", metadata={'source': './PDFS/Concepts.pdf', 'page': 49}),\n",
       " Document(page_content=\"Set up Konnectivity service\\nUse Port Forwarding to Access Applications in a Cluster\\nLearn how to Fetch logs for Pods , use kubectl port-forward\\nControllers\\nIn robotics and automation, a control loop  is a non-terminating loop that regulates the state of a\\nsystem.\\nHere is one example of a control loop: a thermostat in a room.\\nWhen you set the temperature, that's telling the thermostat about your desired state . The actual\\nroom temperature is the current state . The thermostat acts to bring the current state closer to\\nthe desired state, by turning equipment on or off.\\nIn Kubernetes, controllers are control loops that watch the state of your cluster , then make or\\nrequest changes where needed. Each controller tries to move the current cluster state closer to\\nthe desired state.\\nController pattern\\nA controller tracks at least one Kubernetes resource type. These objects  have a spec field that\\nrepresents the desired state. The controller(s) for that resource are responsible for making the\\ncurrent state come closer to that desired state.\\nThe controller might carry the action out itself; more commonly, in Kubernetes, a controller\\nwill send messages to the API server  that have useful side effects. You'll see examples of this\\nbelow.\\nControl via API server\\nThe Job controller is an example of a Kubernetes built-in controller. Built-in controllers manage\\nstate by interacting with the cluster API server.\\nJob is a Kubernetes resource that runs a Pod, or perhaps several Pods, to carry out a task and\\nthen stop.\\n(Once scheduled , Pod objects become part of the desired state for a kubelet).\\nWhen the Job controller sees a new task it makes sure that, somewhere in your cluster, the\\nkubelets on a set of Nodes are running the right number of Pods to get the work done. The Job\\ncontroller does not run any Pods or containers itself. Instead, the Job controller tells the API\\nserver to create or remove Pods. Other components in the control plane  act on the new\\ninformation (there are new Pods to schedule and run), and eventually the work is done.\\nAfter you create a new Job, the desired state is for that Job to be completed. The Job controller\\nmakes the current state for that Job be nearer to your desired state: creating Pods that do the\\nwork you wanted for that Job, so that the Job is closer to completion.\\nControllers also update the objects that configure them. For example: once the work is done for\\na Job, the Job controller updates that Job object to mark it Finished .• \\n• \\n•\", metadata={'source': './PDFS/Concepts.pdf', 'page': 50}),\n",
       " Document(page_content=\"(This is a bit like how some thermostats turn a light off to indicate that your room is now at the\\ntemperature you set).\\nDirect control\\nIn contrast with Job, some controllers need to make changes to things outside of your cluster.\\nFor example, if you use a control loop to make sure there are enough Nodes  in your cluster,\\nthen that controller needs something outside the current cluster to set up new Nodes when\\nneeded.\\nControllers that interact with external state find their desired state from the API server, then\\ncommunicate directly with an external system to bring the current state closer in line.\\n(There actually is a controller  that horizontally scales the nodes in your cluster.)\\nThe important point here is that the controller makes some changes to bring about your desired\\nstate, and then reports the current state back to your cluster's API server. Other control loops\\ncan observe that reported data and take their own actions.\\nIn the thermostat example, if the room is very cold then a different controller might also turn\\non a frost protection heater. With Kubernetes clusters, the control plane indirectly works with\\nIP address management tools, storage services, cloud provider APIs, and other services by \\nextending Kubernetes  to implement that.\\nDesired versus current state\\nKubernetes takes a cloud-native view of systems, and is able to handle constant change.\\nYour cluster could be changing at any point as work happens and control loops automatically\\nfix failures. This means that, potentially, your cluster never reaches a stable state.\\nAs long as the controllers for your cluster are running and able to make useful changes, it\\ndoesn't matter if the overall state is stable or not.\\nDesign\\nAs a tenet of its design, Kubernetes uses lots of controllers that each manage a particular aspect\\nof cluster state. Most commonly, a particular control loop (controller) uses one kind of resource\\nas its desired state, and has a different kind of resource that it manages to make that desired\\nstate happen. For example, a controller for Jobs tracks Job objects (to discover new work) and\\nPod objects (to run the Jobs, and then to see when the work is finished). In this case something\\nelse creates the Jobs, whereas the Job controller creates Pods.\\nIt's useful to have simple controllers rather than one, monolithic set of control loops that are\\ninterlinked. Controllers can fail, so Kubernetes is designed to allow for that.\\nNote:\\nThere can be several controllers that create or update the same kind of object. Behind the\\nscenes, Kubernetes controllers make sure that they only pay attention to the resources linked to\\ntheir controlling resource.\", metadata={'source': './PDFS/Concepts.pdf', 'page': 51}),\n",
       " Document(page_content='For example, you can have Deployments and Jobs; these both create Pods. The Job controller\\ndoes not delete the Pods that your Deployment created, because there is information ( labels ) the\\ncontrollers can use to tell those Pods apart.\\nWays of running controllers\\nKubernetes comes with a set of built-in controllers that run inside the kube-controller-manager .\\nThese built-in controllers provide important core behaviors.\\nThe Deployment controller and Job controller are examples of controllers that come as part of\\nKubernetes itself (\"built-in\" controllers). Kubernetes lets you run a resilient control plane, so\\nthat if any of the built-in controllers were to fail, another part of the control plane will take\\nover the work.\\nYou can find controllers that run outside the control plane, to extend Kubernetes. Or, if you\\nwant, you can write a new controller yourself. You can run your own controller as a set of Pods,\\nor externally to Kubernetes. What fits best will depend on what that particular controller does.\\nWhat\\'s next\\nRead about the Kubernetes control plane\\nDiscover some of the basic Kubernetes objects\\nLearn more about the Kubernetes API\\nIf you want to write your own controller, see Extension Patterns  in Extending\\nKubernetes.\\nLeases\\nDistributed systems often have a need for leases , which provide a mechanism to lock shared\\nresources and coordinate activity between members of a set. In Kubernetes, the lease concept is\\nrepresented by Lease  objects in the coordination.k8s.io  API Group , which are used for system-\\ncritical capabilities such as node heartbeats and component-level leader election.\\nNode heartbeats\\nKubernetes uses the Lease API to communicate kubelet node heartbeats to the Kubernetes API\\nserver. For every Node  , there is a Lease  object with a matching name in the kube-node-lease\\nnamespace. Under the hood, every kubelet heartbeat is an update  request to this Lease  object,\\nupdating the spec.renewTime  field for the Lease. The Kubernetes control plane uses the time\\nstamp of this field to determine the availability of this Node .\\nSee Node Lease objects  for more details.\\nLeader election\\nKubernetes also uses Leases to ensure only one instance of a component is running at any\\ngiven time. This is used by control plane components like kube-controller-manager  and kube-\\nscheduler  in HA configurations, where only one instance of the component should be actively\\nrunning while the other instances are on stand-by.• \\n• \\n• \\n•', metadata={'source': './PDFS/Concepts.pdf', 'page': 52}),\n",
       " Document(page_content='API server identity\\nFEATURE STATE:  Kubernetes v1.26 [beta]\\nStarting in Kubernetes v1.26, each kube-apiserver  uses the Lease API to publish its identity to\\nthe rest of the system. While not particularly useful on its own, this provides a mechanism for\\nclients to discover how many instances of kube-apiserver  are operating the Kubernetes control\\nplane. Existence of kube-apiserver leases enables future capabilities that may require\\ncoordination between each kube-apiserver.\\nYou can inspect Leases owned by each kube-apiserver by checking for lease objects in the kube-\\nsystem  namespace with the name kube-apiserver-<sha256-hash> . Alternatively you can use the\\nlabel selector apiserver.kubernetes.io/identity=kube-apiserver :\\nkubectl -n kube-system get lease -l apiserver.kubernetes.io/identity =kube-apiserver\\nNAME                                        HOLDER                                                                           AGE\\napiserver-07a5ea9b9b072c4a5f3d1c3702        \\napiserver-07a5ea9b9b072c4a5f3d1c3702_0c8914f7-0f35-440e-8676-7844977d3a05        5m33s\\napiserver-7be9e061c59d368b3ddaf1376e        \\napiserver-7be9e061c59d368b3ddaf1376e_84f2a85d-37c1-4b14-b6b9-603e62e4896f        4m23s\\napiserver-1dfef752bcb36637d2763d1868        \\napiserver-1dfef752bcb36637d2763d1868_c5ffa286-8a9a-45d4-91e7-61118ed58d2e        4m43s\\nThe SHA256 hash used in the lease name is based on the OS hostname as seen by that API\\nserver. Each kube-apiserver should be configured to use a hostname that is unique within the\\ncluster. New instances of kube-apiserver that use the same hostname will take over existing\\nLeases using a new holder identity, as opposed to instantiating new Lease objects. You can\\ncheck the hostname used by kube-apisever by checking the value of the kubernetes.io/\\nhostname  label:\\nkubectl -n kube-system get lease apiserver-07a5ea9b9b072c4a5f3d1c3702 -o yaml\\napiVersion : coordination.k8s.io/v1\\nkind: Lease\\nmetadata :\\n  creationTimestamp : \"2023-07-02T13:16:48Z\"\\n  labels :\\n    apiserver.kubernetes.io/identity : kube-apiserver\\n    kubernetes.io/hostname : master-1\\n  name : apiserver-07a5ea9b9b072c4a5f3d1c3702\\n  namespace : kube-system\\n  resourceVersion : \"334899\"\\n  uid: 90870ab5-1ba9-4523-b215-e4d4e662acb1\\nspec:\\n  holderIdentity : apiserver-07a5ea9b9b072c4a5f3d1c3702_0c8914f7-0f35-440e-8676-7844977d3a05\\n  leaseDurationSeconds : 3600\\n  renewTime : \"2023-07-04T21:58:48.065888Z\"\\nExpired leases from kube-apiservers that no longer exist are garbage collected by new kube-\\napiservers after 1 hour.\\nYou can disable API server identity leases by disabling the APIServerIdentity  feature gate .', metadata={'source': './PDFS/Concepts.pdf', 'page': 53}),\n",
       " Document(page_content=\"Workloads\\nYour own workload can define its own use of Leases. For example, you might run a custom \\ncontroller  where a primary or leader member performs operations that its peers do not. You\\ndefine a Lease so that the controller replicas can select or elect a leader, using the Kubernetes\\nAPI for coordination. If you do use a Lease, it's a good practice to define a name for the Lease\\nthat is obviously linked to the product or component. For example, if you have a component\\nnamed Example Foo, use a Lease named example-foo .\\nIf a cluster operator or another end user could deploy multiple instances of a component, select\\na name prefix and pick a mechanism (such as hash of the name of the Deployment) to avoid\\nname collisions for the Leases.\\nYou can use another approach so long as it achieves the same outcome: different software\\nproducts do not conflict with one another.\\nCloud Controller Manager\\nFEATURE STATE:  Kubernetes v1.11 [beta]\\nCloud infrastructure technologies let you run Kubernetes on public, private, and hybrid clouds.\\nKubernetes believes in automated, API-driven infrastructure without tight coupling between\\ncomponents.\\nThe cloud-controller-manager is a Kubernetes control plane  component that embeds cloud-\\nspecific control logic. The cloud controller manager  lets you link your cluster into your cloud\\nprovider's API, and separates out the components that interact with that cloud platform from\\ncomponents that only interact with your cluster.\\nBy decoupling the interoperability logic between Kubernetes and the underlying cloud\\ninfrastructure, the cloud-controller-manager component enables cloud providers to release\\nfeatures at a different pace compared to the main Kubernetes project.\\nThe cloud-controller-manager is structured using a plugin mechanism that allows different\\ncloud providers to integrate their platforms with Kubernetes.\\nDesign\\nKubernetes components\\nThe cloud controller manager runs in the control plane as a replicated set of processes (usually,\\nthese are containers in Pods). Each cloud-controller-manager implements multiple controllers\\nin a single process.\\nNote:  You can also run the cloud controller manager as a Kubernetes addon  rather than as part\\nof the control plane.\\nCloud controller manager functions\\nThe controllers inside the cloud controller manager include:\", metadata={'source': './PDFS/Concepts.pdf', 'page': 54}),\n",
       " Document(page_content=\"Node controller\\nThe node controller is responsible for updating Node  objects when new servers are created in\\nyour cloud infrastructure. The node controller obtains information about the hosts running\\ninside your tenancy with the cloud provider. The node controller performs the following\\nfunctions:\\nUpdate a Node object with the corresponding server's unique identifier obtained from the\\ncloud provider API.\\nAnnotating and labelling the Node object with cloud-specific information, such as the\\nregion the node is deployed into and the resources (CPU, memory, etc) that it has\\navailable.\\nObtain the node's hostname and network addresses.\\nVerifying the node's health. In case a node becomes unresponsive, this controller checks\\nwith your cloud provider's API to see if the server has been deactivated / deleted /\\nterminated. If the node has been deleted from the cloud, the controller deletes the Node\\nobject from your Kubernetes cluster.\\nSome cloud provider implementations split this into a node controller and a separate node\\nlifecycle controller.\\nRoute controller\\nThe route controller is responsible for configuring routes in the cloud appropriately so that\\ncontainers on different nodes in your Kubernetes cluster can communicate with each other.\\nDepending on the cloud provider, the route controller might also allocate blocks of IP addresses\\nfor the Pod network.\\nService controller\\nServices  integrate with cloud infrastructure components such as managed load balancers, IP\\naddresses, network packet filtering, and target health checking. The service controller interacts\\nwith your cloud provider's APIs to set up load balancers and other infrastructure components\\nwhen you declare a Service resource that requires them.\\nAuthorization\\nThis section breaks down the access that the cloud controller manager requires on various API\\nobjects, in order to perform its operations.\\nNode controller\\nThe Node controller only works with Node objects. It requires full access to read and modify\\nNode objects.\\nv1/Node :\\nget\\nlist\\ncreate\\nupdate1. \\n2. \\n3. \\n4. \\n• \\n• \\n• \\n•\", metadata={'source': './PDFS/Concepts.pdf', 'page': 55}),\n",
       " Document(page_content='patch\\nwatch\\ndelete\\nRoute controller\\nThe route controller listens to Node object creation and configures routes appropriately. It\\nrequires Get access to Node objects.\\nv1/Node :\\nget\\nService controller\\nThe service controller watches for Service object create , update  and delete  events and then\\nconfigures Endpoints for those Services appropriately (for EndpointSlices, the kube-controller-\\nmanager manages these on demand).\\nTo access Services, it requires list, and watch  access. To update Services, it requires patch  and \\nupdate  access.\\nTo set up Endpoints resources for the Services, it requires access to create , list, get, watch ,\\nand update .\\nv1/Service :\\nlist\\nget\\nwatch\\npatch\\nupdate\\nOthers\\nThe implementation of the core of the cloud controller manager requires access to create Event\\nobjects, and to ensure secure operation, it requires access to create ServiceAccounts.\\nv1/Event :\\ncreate\\npatch\\nupdate\\nv1/ServiceAccount :\\ncreate\\nThe RBAC  ClusterRole for the cloud controller manager looks like:\\napiVersion : rbac.authorization.k8s.io/v1\\nkind: ClusterRole\\nmetadata :\\n  name : cloud-controller-manager• \\n• \\n• \\n• \\n• \\n• \\n• \\n• \\n• \\n• \\n• \\n• \\n•', metadata={'source': './PDFS/Concepts.pdf', 'page': 56}),\n",
       " Document(page_content='rules :\\n- apiGroups :\\n  - \"\"\\n  resources :\\n  - events\\n  verbs :\\n  - create\\n  - patch\\n  - update\\n- apiGroups :\\n  - \"\"\\n  resources :\\n  - nodes\\n  verbs :\\n  - \\'*\\'\\n- apiGroups :\\n  - \"\"\\n  resources :\\n  - nodes/status\\n  verbs :\\n  - patch\\n- apiGroups :\\n  - \"\"\\n  resources :\\n  - services\\n  verbs :\\n  - list\\n  - patch\\n  - update\\n  - watch\\n- apiGroups :\\n  - \"\"\\n  resources :\\n  - serviceaccounts\\n  verbs :\\n  - create\\n- apiGroups :\\n  - \"\"\\n  resources :\\n  - persistentvolumes\\n  verbs :\\n  - get\\n  - list\\n  - update\\n  - watch\\n- apiGroups :\\n  - \"\"\\n  resources :\\n  - endpoints\\n  verbs :\\n  - create\\n  - get\\n  - list', metadata={'source': './PDFS/Concepts.pdf', 'page': 57}),\n",
       " Document(page_content=\"- watch\\n  - update\\nWhat's next\\nCloud Controller Manager Administration  has instructions on running and managing the\\ncloud controller manager.\\nTo upgrade a HA control plane to use the cloud controller manager, see Migrate\\nReplicated Control Plane To Use Cloud Controller Manager .\\nWant to know how to implement your own cloud controller manager, or extend an\\nexisting project?\\nThe cloud controller manager uses Go interfaces, specifically, CloudProvider\\ninterface defined in cloud.go  from kubernetes/cloud-provider  to allow\\nimplementations from any cloud to be plugged in.\\nThe implementation of the shared controllers highlighted in this document (Node,\\nRoute, and Service), and some scaffolding along with the shared cloudprovider\\ninterface, is part of the Kubernetes core. Implementations specific to cloud\\nproviders are outside the core of Kubernetes and implement the CloudProvider\\ninterface.\\nFor more information about developing plugins, see Developing Cloud Controller\\nManager .\\nAbout cgroup v2\\nOn Linux, control groups  constrain resources that are allocated to processes.\\nThe kubelet  and the underlying container runtime need to interface with cgroups to enforce \\nresource management for pods and containers  which includes cpu/memory requests and limits\\nfor containerized workloads.\\nThere are two versions of cgroups in Linux: cgroup v1 and cgroup v2. cgroup v2 is the new\\ngeneration of the cgroup  API.\\nWhat is cgroup v2?\\nFEATURE STATE:  Kubernetes v1.25 [stable]\\ncgroup v2 is the next version of the Linux cgroup  API. cgroup v2 provides a unified control\\nsystem with enhanced resource management capabilities.\\ncgroup v2 offers several improvements over cgroup v1, such as the following:\\nSingle unified hierarchy design in API\\nSafer sub-tree delegation to containers\\nNewer features like Pressure Stall Information\\nEnhanced resource allocation management and isolation across multiple resources\\nUnified accounting for different types of memory allocations (network memory,\\nkernel memory, etc)• \\n• \\n• \\n◦ \\n◦ \\n◦ \\n• \\n• \\n• \\n• \\n◦\", metadata={'source': './PDFS/Concepts.pdf', 'page': 58}),\n",
       " Document(page_content=\"Accounting for non-immediate resource changes such as page cache write backs\\nSome Kubernetes features exclusively use cgroup v2 for enhanced resource management and\\nisolation. For example, the MemoryQoS  feature improves memory QoS and relies on cgroup v2\\nprimitives.\\nUsing cgroup v2\\nThe recommended way to use cgroup v2 is to use a Linux distribution that enables and uses\\ncgroup v2 by default.\\nTo check if your distribution uses cgroup v2, refer to Identify cgroup version on Linux nodes .\\nRequirements\\ncgroup v2 has the following requirements:\\nOS distribution enables cgroup v2\\nLinux Kernel version is 5.8 or later\\nContainer runtime supports cgroup v2. For example:\\ncontainerd  v1.4 and later\\ncri-o  v1.20 and later\\nThe kubelet and the container runtime are configured to use the systemd cgroup driver\\nLinux Distribution cgroup v2 support\\nFor a list of Linux distributions that use cgroup v2, refer to the cgroup v2 documentation\\nContainer Optimized OS (since M97)\\nUbuntu (since 21.10, 22.04+ recommended)\\nDebian GNU/Linux (since Debian 11 bullseye)\\nFedora (since 31)\\nArch Linux (since April 2021)\\nRHEL and RHEL-like distributions (since 9)\\nTo check if your distribution is using cgroup v2, refer to your distribution's documentation or\\nfollow the instructions in Identify the cgroup version on Linux nodes .\\nYou can also enable cgroup v2 manually on your Linux distribution by modifying the kernel\\ncmdline boot arguments. If your distribution uses GRUB, systemd.unified_cgroup_hierarchy=1\\nshould be added in GRUB_CMDLINE_LINUX  under /etc/default/grub , followed by sudo update-\\ngrub . However, the recommended approach is to use a distribution that already enables cgroup\\nv2 by default.\\nMigrating to cgroup v2\\nTo migrate to cgroup v2, ensure that you meet the requirements , then upgrade to a kernel\\nversion that enables cgroup v2 by default.\\nThe kubelet automatically detects that the OS is running on cgroup v2 and performs\\naccordingly with no additional configuration required.◦ \\n• \\n• \\n• \\n◦ \\n◦ \\n• \\n• \\n• \\n• \\n• \\n• \\n•\", metadata={'source': './PDFS/Concepts.pdf', 'page': 59}),\n",
       " Document(page_content=\"There should not be any noticeable difference in the user experience when switching to cgroup\\nv2, unless users are accessing the cgroup file system directly, either on the node or from within\\nthe containers.\\ncgroup v2 uses a different API than cgroup v1, so if there are any applications that directly\\naccess the cgroup file system, they need to be updated to newer versions that support cgroup\\nv2. For example:\\nSome third-party monitoring and security agents may depend on the cgroup filesystem.\\nUpdate these agents to versions that support cgroup v2.\\nIf you run cAdvisor  as a stand-alone DaemonSet for monitoring pods and containers,\\nupdate it to v0.43.0 or later.\\nIf you deploy Java applications, prefer to use versions which fully support cgroup v2:\\nOpenJDK / HotSpot : jdk8u372, 11.0.16, 15 and later\\nIBM Semeru Runtimes : 8.0.382.0, 11.0.20.0, 17.0.8.0, and later\\nIBM Java : 8.0.8.6 and later\\nIf you are using the uber-go/automaxprocs  package, make sure the version you use is\\nv1.5.1 or higher.\\nIdentify the cgroup version on Linux Nodes\\nThe cgroup version depends on the Linux distribution being used and the default cgroup\\nversion configured on the OS. To check which cgroup version your distribution uses, run the \\nstat -fc %T /sys/fs/cgroup/  command on the node:\\nstat -fc %T /sys/fs/cgroup/\\nFor cgroup v2, the output is cgroup2fs .\\nFor cgroup v1, the output is tmpfs.\\nWhat's next\\nLearn more about cgroups\\nLearn more about container runtime\\nLearn more about cgroup drivers\\nContainer Runtime Interface (CRI)\\nThe CRI is a plugin interface which enables the kubelet to use a wide variety of container\\nruntimes, without having a need to recompile the cluster components.\\nYou need a working container runtime  on each Node in your cluster, so that the kubelet  can\\nlaunch Pods  and their containers.\\nThe Container Runtime Interface (CRI) is the main protocol for the communication between the\\nkubelet  and Container Runtime.\\nThe Kubernetes Container Runtime Interface (CRI) defines the main gRPC  protocol for the\\ncommunication between the node components  kubelet  and container runtime .• \\n• \\n• \\n◦ \\n◦ \\n◦ \\n• \\n• \\n• \\n•\", metadata={'source': './PDFS/Concepts.pdf', 'page': 60}),\n",
       " Document(page_content=\"The API\\nFEATURE STATE:  Kubernetes v1.23 [stable]\\nThe kubelet acts as a client when connecting to the container runtime via gRPC. The runtime\\nand image service endpoints have to be available in the container runtime, which can be\\nconfigured separately within the kubelet by using the --image-service-endpoint  command line\\nflags .\\nFor Kubernetes v1.28, the kubelet prefers to use CRI v1. If a container runtime does not support \\nv1 of the CRI, then the kubelet tries to negotiate any older supported version. The v1.28 kubelet\\ncan also negotiate CRI v1alpha2 , but this version is considered as deprecated. If the kubelet\\ncannot negotiate a supported CRI version, the kubelet gives up and doesn't register as a node.\\nUpgrading\\nWhen upgrading Kubernetes, the kubelet tries to automatically select the latest CRI version on\\nrestart of the component. If that fails, then the fallback will take place as mentioned above. If a\\ngRPC re-dial was required because the container runtime has been upgraded, then the\\ncontainer runtime must also support the initially selected version or the redial is expected to\\nfail. This requires a restart of the kubelet.\\nWhat's next\\nLearn more about the CRI protocol definition\\nGarbage Collection\\nGarbage collection is a collective term for the various mechanisms Kubernetes uses to clean up\\ncluster resources. This allows the clean up of resources like the following:\\nTerminated pods\\nCompleted Jobs\\nObjects without owner references\\nUnused containers and container images\\nDynamically provisioned PersistentVolumes with a StorageClass reclaim policy of Delete\\nStale or expired CertificateSigningRequests (CSRs)\\nNodes  deleted in the following scenarios:\\nOn a cloud when the cluster uses a cloud controller manager\\nOn-premises when the cluster uses an addon similar to a cloud controller manager\\nNode Lease objects\\nOwners and dependents\\nMany objects in Kubernetes link to each other through owner references . Owner references tell\\nthe control plane which objects are dependent on others. Kubernetes uses owner references to\\ngive the control plane, and other API clients, the opportunity to clean up related resources\\nbefore deleting an object. In most cases, Kubernetes manages owner references automatically.• \\n• \\n• \\n• \\n• \\n• \\n• \\n• \\n◦ \\n◦ \\n•\", metadata={'source': './PDFS/Concepts.pdf', 'page': 61}),\n",
       " Document(page_content=\"Ownership is different from the labels and selectors  mechanism that some resources also use.\\nFor example, consider a Service  that creates EndpointSlice  objects. The Service uses labels  to\\nallow the control plane to determine which EndpointSlice  objects are used for that Service. In\\naddition to the labels, each EndpointSlice  that is managed on behalf of a Service has an owner\\nreference. Owner references help different parts of Kubernetes avoid interfering with objects\\nthey don’t control.\\nNote:\\nCross-namespace owner references are disallowed by design. Namespaced dependents can\\nspecify cluster-scoped or namespaced owners. A namespaced owner must  exist in the same\\nnamespace as the dependent. If it does not, the owner reference is treated as absent, and the\\ndependent is subject to deletion once all owners are verified absent.\\nCluster-scoped dependents can only specify cluster-scoped owners. In v1.20+, if a cluster-\\nscoped dependent specifies a namespaced kind as an owner, it is treated as having an\\nunresolvable owner reference, and is not able to be garbage collected.\\nIn v1.20+, if the garbage collector detects an invalid cross-namespace ownerReference , or a\\ncluster-scoped dependent with an ownerReference  referencing a namespaced kind, a warning\\nEvent with a reason of OwnerRefInvalidNamespace  and an involvedObject  of the invalid\\ndependent is reported. You can check for that kind of Event by running kubectl get events -A --\\nfield-selector=reason=OwnerRefInvalidNamespace .\\nCascading deletion\\nKubernetes checks for and deletes objects that no longer have owner references, like the pods\\nleft behind when you delete a ReplicaSet. When you delete an object, you can control whether\\nKubernetes deletes the object's dependents automatically, in a process called cascading deletion .\\nThere are two types of cascading deletion, as follows:\\nForeground cascading deletion\\nBackground cascading deletion\\nYou can also control how and when garbage collection deletes resources that have owner\\nreferences using Kubernetes finalizers .\\nForeground cascading deletion\\nIn foreground cascading deletion, the owner object you're deleting first enters a deletion in\\nprogress  state. In this state, the following happens to the owner object:\\nThe Kubernetes API server sets the object's metadata.deletionTimestamp  field to the time\\nthe object was marked for deletion.\\nThe Kubernetes API server also sets the metadata.finalizers  field to foregroundDeletion .\\nThe object remains visible through the Kubernetes API until the deletion process is\\ncomplete.\\nAfter the owner object enters the deletion in progress state, the controller deletes the\\ndependents. After deleting all the dependent objects, the controller deletes the owner object. At\\nthis point, the object is no longer visible in the Kubernetes API.• \\n• \\n• \\n• \\n•\", metadata={'source': './PDFS/Concepts.pdf', 'page': 62}),\n",
       " Document(page_content='During foreground cascading deletion, the only dependents that block owner deletion are those\\nthat have the ownerReference.blockOwnerDeletion=true  field. See Use foreground cascading\\ndeletion  to learn more.\\nBackground cascading deletion\\nIn background cascading deletion, the Kubernetes API server deletes the owner object\\nimmediately and the controller cleans up the dependent objects in the background. By default,\\nKubernetes uses background cascading deletion unless you manually use foreground deletion\\nor choose to orphan the dependent objects.\\nSee Use background cascading deletion  to learn more.\\nOrphaned dependents\\nWhen Kubernetes deletes an owner object, the dependents left behind are called orphan  objects.\\nBy default, Kubernetes deletes dependent objects. To learn how to override this behaviour, see \\nDelete owner objects and orphan dependents .\\nGarbage collection of unused containers and images\\nThe kubelet  performs garbage collection on unused images every five minutes and on unused\\ncontainers every minute. You should avoid using external garbage collection tools, as these can\\nbreak the kubelet behavior and remove containers that should exist.\\nTo configure options for unused container and image garbage collection, tune the kubelet using\\na configuration file  and change the parameters related to garbage collection using the \\nKubeletConfiguration  resource type.\\nContainer image lifecycle\\nKubernetes manages the lifecycle of all images through its image manager , which is part of the\\nkubelet, with the cooperation of cadvisor . The kubelet considers the following disk usage limits\\nwhen making garbage collection decisions:\\nHighThresholdPercent\\nLowThresholdPercent\\nDisk usage above the configured HighThresholdPercent  value triggers garbage collection,\\nwhich deletes images in order based on the last time they were used, starting with the oldest\\nfirst. The kubelet deletes images until disk usage reaches the LowThresholdPercent  value.\\nContainer garbage collection\\nThe kubelet garbage collects unused containers based on the following variables, which you can\\ndefine:\\nMinAge : the minimum age at which the kubelet can garbage collect a container. Disable\\nby setting to 0.\\nMaxPerPodContainer : the maximum number of dead containers each Pod can have.\\nDisable by setting to less than 0.• \\n• \\n• \\n•', metadata={'source': './PDFS/Concepts.pdf', 'page': 63}),\n",
       " Document(page_content=\"MaxContainers : the maximum number of dead containers the cluster can have. Disable\\nby setting to less than 0.\\nIn addition to these variables, the kubelet garbage collects unidentified and deleted containers,\\ntypically starting with the oldest first.\\nMaxPerPodContainer  and MaxContainers  may potentially conflict with each other in situations\\nwhere retaining the maximum number of containers per Pod ( MaxPerPodContainer ) would go\\noutside the allowable total of global dead containers ( MaxContainers ). In this situation, the\\nkubelet adjusts MaxPerPodContainer  to address the conflict. A worst-case scenario would be to\\ndowngrade MaxPerPodContainer  to 1 and evict the oldest containers. Additionally, containers\\nowned by pods that have been deleted are removed once they are older than MinAge .\\nNote:  The kubelet only garbage collects the containers it manages.\\nConfiguring garbage collection\\nYou can tune garbage collection of resources by configuring options specific to the controllers\\nmanaging those resources. The following pages show you how to configure garbage collection:\\nConfiguring cascading deletion of Kubernetes objects\\nConfiguring cleanup of finished Jobs\\nWhat's next\\nLearn more about ownership of Kubernetes objects .\\nLearn more about Kubernetes finalizers .\\nLearn about the TTL controller  that cleans up finished Jobs.\\nMixed Version Proxy\\nFEATURE STATE:  Kubernetes v1.28 [alpha]\\nKubernetes 1.28 includes an alpha feature that lets an API Server  proxy a resource requests to\\nother peer API servers. This is useful when there are multiple API servers running different\\nversions of Kubernetes in one cluster (for example, during a long-lived rollout to a new release\\nof Kubernetes).\\nThis enables cluster administrators to configure highly available clusters that can be upgraded\\nmore safely, by directing resource requests (made during the upgrade) to the correct kube-\\napiserver. That proxying prevents users from seeing unexpected 404 Not Found errors that stem\\nfrom the upgrade process.\\nThis mechanism is called the Mixed Version Proxy .\\nEnabling the Mixed Version Proxy\\nEnsure that UnknownVersionInteroperabilityProxy  feature gate  is enabled when you start the \\nAPI Server :• \\n• \\n• \\n• \\n• \\n•\", metadata={'source': './PDFS/Concepts.pdf', 'page': 64}),\n",
       " Document(page_content='kube-apiserver \\\\\\n--feature-gates =UnknownVersionInteroperabilityProxy =true \\\\\\n# required command line arguments for this feature\\n--peer-ca-file =<path to kube-apiserver CA cert>\\n--proxy-client-cert-file =<path to aggregator proxy cert>,\\n--proxy-client-key-file =<path to aggregator proxy key>,\\n--requestheader-client-ca-file =<path to aggregator CA cert>,\\n# requestheader-allowed-names can be set to blank to allow any Common Name\\n--requestheader-allowed-names =<valid Common Names to verify proxy client cert against>,\\n# optional flags for this feature\\n--peer-advertise-ip =`IP of this kube-apiserver that should be used by peers to proxy requests `\\n--peer-advertise-port =`port of this kube-apiserver that should be used by peers to proxy \\nrequests `\\n# ...and other flags as usual\\nProxy transport and authentication between API servers\\nThe source kube-apiserver reuses the existing APIserver client authentication flags  --\\nproxy-client-cert-file  and --proxy-client-key-file  to present its identity that will be\\nverified by its peer (the destination kube-apiserver). The destination API server verifies\\nthat peer connection based on the configuration you specify using the --requestheader-\\nclient-ca-file  command line argument.\\nTo authenticate the destination server\\'s serving certs, you must configure a certificate\\nauthority bundle by specifying the --peer-ca-file  command line argument to the source\\nAPI server.\\nConfiguration for peer API server connectivity\\nTo set the network location of a kube-apiserver that peers will use to proxy requests, use the --\\npeer-advertise-ip  and --peer-advertise-port  command line arguments to kube-apiserver or\\nspecify these fields in the API server configuration file. If these flags are unspecified, peers will\\nuse the value from either --advertise-address  or --bind-address  command line argument to the\\nkube-apiserver. If those too, are unset, the host\\'s default interface is used.\\nMixed version proxying\\nWhen you enable mixed version proxying, the aggregation layer  loads a special filter that does\\nthe following:\\nWhen a resource request reaches an API server that cannot serve that API (either because\\nit is at a version pre-dating the introduction of the API or the API is turned off on the API\\nserver) the API server attempts to send the request to a peer API server that can serve the\\nrequested API. It does so by identifying API groups / versions / resources that the local\\nserver doesn\\'t recognise, and tries to proxy those requests to a peer API server that is\\ncapable of handling the request.\\nIf the peer API server fails to respond, the source  API server responds with 503 (\"Service\\nUnavailable\") error.• \\n• \\n• \\n•', metadata={'source': './PDFS/Concepts.pdf', 'page': 65}),\n",
       " Document(page_content='How it works under the hood\\nWhen an API Server receives a resource request, it first checks which API servers can serve the\\nrequested resource. This check happens using the internal StorageVersion  API.\\nIf the resource is known to the API server that received the request (for example, GET /\\napi/v1/pods/some-pod ), the request is handled locally.\\nIf there is no internal StorageVersion  object found for the requested resource (for\\nexample, GET /my-api/v1/my-resource ) and the configured APIService specifies proxying\\nto an extension API server, that proxying happens following the usual flow for extension\\nAPIs.\\nIf a valid internal StorageVersion  object is found for the requested resource (for example, \\nGET /batch/v1/jobs ) and the API server trying to handle the request (the handling API\\nserver ) has the batch  API disabled, then the handling API server  fetches the peer API\\nservers that do serve the relevant API group / version / resource ( api/v1/batch  in this\\ncase) using the information in the fetched StorageVersion  object. The handling API server\\nthen proxies the request to one of the matching peer kube-apiservers that are aware of\\nthe requested resource.\\nIf there is no peer known for that API group / version / resource, the handling API\\nserver passes the request to its own handler chain which should eventually return a\\n404 (\"Not Found\") response.\\nIf the handling API server has identified and selected a peer API server, but that\\npeer fails to respond (for reasons such as network connectivity issues, or a data\\nrace between the request being received and a controller registering the peer\\'s info\\ninto the control plane), then the handling API server responds with a 503 (\"Service\\nUnavailable\") error.\\nContainers\\nTechnology for packaging an application along with its runtime dependencies.\\nEach container that you run is repeatable; the standardization from having dependencies\\nincluded means that you get the same behavior wherever you run it.\\nContainers decouple applications from the underlying host infrastructure. This makes\\ndeployment easier in different cloud or OS environments.\\nEach node  in a Kubernetes cluster runs the containers that form the Pods  assigned to that node.\\nContainers in a Pod are co-located and co-scheduled to run on the same node.\\nContainer images\\nA container image  is a ready-to-run software package containing everything needed to run an\\napplication: the code and any runtime it requires, application and system libraries, and default\\nvalues for any essential settings.\\nContainers are intended to be stateless and immutable : you should not change the code of a\\ncontainer that is already running. If you have a containerized application and want to make• \\n• \\n• \\n◦ \\n◦', metadata={'source': './PDFS/Concepts.pdf', 'page': 66}),\n",
       " Document(page_content=\"changes, the correct process is to build a new image that includes the change, then recreate the\\ncontainer to start from the updated image.\\nContainer runtimes\\nA fundamental component that empowers Kubernetes to run containers effectively. It is\\nresponsible for managing the execution and lifecycle of containers within the Kubernetes\\nenvironment.\\nKubernetes supports container runtimes such as containerd , CRI-O , and any other\\nimplementation of the Kubernetes CRI (Container Runtime Interface) .\\nUsually, you can allow your cluster to pick the default container runtime for a Pod. If you need\\nto use more than one container runtime in your cluster, you can specify the RuntimeClass  for a\\nPod to make sure that Kubernetes runs those containers using a particular container runtime.\\nYou can also use RuntimeClass to run different Pods with the same container runtime but with\\ndifferent settings.\\nContainer Environment\\nContainer Lifecycle Hooks\\nImages\\nA container image represents binary data that encapsulates an application and all its software\\ndependencies. Container images are executable software bundles that can run standalone and\\nthat make very well defined assumptions about their runtime environment.\\nYou typically create a container image of your application and push it to a registry before\\nreferring to it in a Pod.\\nThis page provides an outline of the container image concept.\\nNote:  If you are looking for the container images for a Kubernetes release (such as v1.28, the\\nlatest minor release), visit Download Kubernetes .\\nImage names\\nContainer images are usually given a name such as pause , example/mycontainer , or kube-\\napiserver . Images can also include a registry hostname; for example: fictional.registry.example/\\nimagename , and possibly a port number as well; for example: fictional.registry.example:10443/\\nimagename .\\nIf you don't specify a registry hostname, Kubernetes assumes that you mean the Docker public\\nregistry.\\nAfter the image name part you can add a tag (in the same way you would when using with\\ncommands like docker  or podman ). Tags let you identify different versions of the same series of\\nimages.\", metadata={'source': './PDFS/Concepts.pdf', 'page': 67}),\n",
       " Document(page_content=\"Image tags consist of lowercase and uppercase letters, digits, underscores ( _), periods ( .), and\\ndashes ( -).\\nThere are additional rules about where you can place the separator characters ( _, -, and .) inside\\nan image tag.\\nIf you don't specify a tag, Kubernetes assumes you mean the tag latest .\\nUpdating images\\nWhen you first create a Deployment , StatefulSet , Pod, or other object that includes a Pod\\ntemplate, then by default the pull policy of all containers in that pod will be set to IfNotPresent\\nif it is not explicitly specified. This policy causes the kubelet  to skip pulling an image if it\\nalready exists.\\nImage pull policy\\nThe imagePullPolicy  for a container and the tag of the image affect when the kubelet  attempts\\nto pull (download) the specified image.\\nHere's a list of the values you can set for imagePullPolicy  and the effects these values have:\\nIfNotPresent\\nthe image is pulled only if it is not already present locally.\\nAlways\\nevery time the kubelet launches a container, the kubelet queries the container image\\nregistry to resolve the name to an image digest . If the kubelet has a container image with\\nthat exact digest cached locally, the kubelet uses its cached image; otherwise, the kubelet\\npulls the image with the resolved digest, and uses that image to launch the container.\\nNever\\nthe kubelet does not try fetching the image. If the image is somehow already present\\nlocally, the kubelet attempts to start the container; otherwise, startup fails. See pre-pulled\\nimages  for more details.\\nThe caching semantics of the underlying image provider make even imagePullPolicy: Always\\nefficient, as long as the registry is reliably accessible. Your container runtime can notice that the\\nimage layers already exist on the node so that they don't need to be downloaded again.\\nNote:\\nYou should avoid using the :latest  tag when deploying containers in production as it is harder to\\ntrack which version of the image is running and more difficult to roll back properly.\\nInstead, specify a meaningful tag such as v1.42.0  and/or a digest.\\nTo make sure the Pod always uses the same version of a container image, you can specify the\\nimage's digest; replace <image-name>:<tag>  with <image-name>@<digest>  (for example, \\nimage@sha256:45b23dee08af5e43a7fea6c4cf9c25ccf269ee113168c19722f87876677c5cb2 ).\\nWhen using image tags, if the image registry were to change the code that the tag on that\\nimage represents, you might end up with a mix of Pods running the old and new code. An\\nimage digest uniquely identifies a specific version of the image, so Kubernetes runs the same\\ncode every time it starts a container with that image name and digest specified. Specifying an\\nimage by digest fixes the code that you run so that a change at the registry cannot lead to that\\nmix of versions.\", metadata={'source': './PDFS/Concepts.pdf', 'page': 68}),\n",
       " Document(page_content=\"There are third-party admission controllers  that mutate Pods (and pod templates) when they are\\ncreated, so that the running workload is defined based on an image digest rather than a tag.\\nThat might be useful if you want to make sure that all your workload is running the same code\\nno matter what tag changes happen at the registry.\\nDefault image pull policy\\nWhen you (or a controller) submit a new Pod to the API server, your cluster sets the \\nimagePullPolicy  field when specific conditions are met:\\nif you omit the imagePullPolicy  field, and you specify the digest for the container image,\\nthe imagePullPolicy  is automatically set to IfNotPresent .\\nif you omit the imagePullPolicy  field, and the tag for the container image is :latest , \\nimagePullPolicy  is automatically set to Always ;\\nif you omit the imagePullPolicy  field, and you don't specify the tag for the container\\nimage, imagePullPolicy  is automatically set to Always ;\\nif you omit the imagePullPolicy  field, and you specify the tag for the container image that\\nisn't :latest , the imagePullPolicy  is automatically set to IfNotPresent .\\nNote:\\nThe value of imagePullPolicy  of the container is always set when the object is first created , and\\nis not updated if the image's tag or digest later changes.\\nFor example, if you create a Deployment with an image whose tag is not :latest , and later\\nupdate that Deployment's image to a :latest  tag, the imagePullPolicy  field will not change to \\nAlways . You must manually change the pull policy of any object after its initial creation.\\nRequired image pull\\nIf you would like to always force a pull, you can do one of the following:\\nSet the imagePullPolicy  of the container to Always .\\nOmit the imagePullPolicy  and use :latest  as the tag for the image to use; Kubernetes will\\nset the policy to Always  when you submit the Pod.\\nOmit the imagePullPolicy  and the tag for the image to use; Kubernetes will set the policy\\nto Always  when you submit the Pod.\\nEnable the AlwaysPullImages  admission controller.\\nImagePullBackOff\\nWhen a kubelet starts creating containers for a Pod using a container runtime, it might be\\npossible the container is in Waiting  state because of ImagePullBackOff .\\nThe status ImagePullBackOff  means that a container could not start because Kubernetes could\\nnot pull a container image (for reasons such as invalid image name, or pulling from a private\\nregistry without imagePullSecret ). The BackOff  part indicates that Kubernetes will keep trying\\nto pull the image, with an increasing back-off delay.\\nKubernetes raises the delay between each attempt until it reaches a compiled-in limit, which is\\n300 seconds (5 minutes).• \\n• \\n• \\n• \\n• \\n• \\n• \\n•\", metadata={'source': './PDFS/Concepts.pdf', 'page': 69}),\n",
       " Document(page_content='Serial and parallel image pulls\\nBy default, kubelet pulls images serially. In other words, kubelet sends only one image pull\\nrequest to the image service at a time. Other image pull requests have to wait until the one\\nbeing processed is complete.\\nNodes make image pull decisions in isolation. Even when you use serialized image pulls, two\\ndifferent nodes can pull the same image in parallel.\\nIf you would like to enable parallel image pulls, you can set the field serializeImagePulls  to false\\nin the kubelet configuration . With serializeImagePulls  set to false, image pull requests will be\\nsent to the image service immediately, and multiple images will be pulled at the same time.\\nWhen enabling parallel image pulls, please make sure the image service of your container\\nruntime can handle parallel image pulls.\\nThe kubelet never pulls multiple images in parallel on behalf of one Pod. For example, if you\\nhave a Pod that has an init container and an application container, the image pulls for the two\\ncontainers will not be parallelized. However, if you have two Pods that use different images, the\\nkubelet pulls the images in parallel on behalf of the two different Pods, when parallel image\\npulls is enabled.\\nMaximum parallel image pulls\\nFEATURE STATE:  Kubernetes v1.27 [alpha]\\nWhen serializeImagePulls  is set to false, the kubelet defaults to no limit on the maximum\\nnumber of images being pulled at the same time. If you would like to limit the number of\\nparallel image pulls, you can set the field maxParallelImagePulls  in kubelet configuration. With \\nmaxParallelImagePulls  set to n, only n images can be pulled at the same time, and any image\\npull beyond n will have to wait until at least one ongoing image pull is complete.\\nLimiting the number parallel image pulls would prevent image pulling from consuming too\\nmuch network bandwidth or disk I/O, when parallel image pulling is enabled.\\nYou can set maxParallelImagePulls  to a positive number that is greater than or equal to 1. If you\\nset maxParallelImagePulls  to be greater than or equal to 2, you must set the serializeImagePulls\\nto false. The kubelet will fail to start with invalid maxParallelImagePulls  settings.\\nMulti-architecture images with image indexes\\nAs well as providing binary images, a container registry can also serve a container image index .\\nAn image index can point to multiple image manifests  for architecture-specific versions of a\\ncontainer. The idea is that you can have a name for an image (for example: pause , example/\\nmycontainer , kube-apiserver ) and allow different systems to fetch the right binary image for\\nthe machine architecture they are using.\\nKubernetes itself typically names container images with a suffix -$(ARCH) . For backward\\ncompatibility, please generate the older images with suffixes. The idea is to generate say pause\\nimage which has the manifest for all the arch(es) and say pause-amd64  which is backwards\\ncompatible for older configurations or YAML files which may have hard coded the images with\\nsuffixes.', metadata={'source': './PDFS/Concepts.pdf', 'page': 70}),\n",
       " Document(page_content=\"Using a private registry\\nPrivate registries may require keys to read images from them.\\nCredentials can be provided in several ways:\\nConfiguring Nodes to Authenticate to a Private Registry\\nall pods can read any configured private registries\\nrequires node configuration by cluster administrator\\nKubelet Credential Provider to dynamically fetch credentials for private registries\\nkubelet can be configured to use credential provider exec plugin for the respective\\nprivate registry.\\nPre-pulled Images\\nall pods can use any images cached on a node\\nrequires root access to all nodes to set up\\nSpecifying ImagePullSecrets on a Pod\\nonly pods which provide own keys can access the private registry\\nVendor-specific or local extensions\\nif you're using a custom node configuration, you (or your cloud provider) can\\nimplement your mechanism for authenticating the node to the container registry.\\nThese options are explained in more detail below.\\nConfiguring nodes to authenticate to a private registry\\nSpecific instructions for setting credentials depends on the container runtime and registry you\\nchose to use. You should refer to your solution's documentation for the most accurate\\ninformation.\\nFor an example of configuring a private container image registry, see the Pull an Image from a\\nPrivate Registry  task. That example uses a private registry in Docker Hub.\\nKubelet credential provider for authenticated image pulls\\nNote:  This approach is especially suitable when kubelet needs to fetch registry credentials\\ndynamically. Most commonly used for registries provided by cloud providers where auth tokens\\nare short-lived.\\nYou can configure the kubelet to invoke a plugin binary to dynamically fetch registry\\ncredentials for a container image. This is the most robust and versatile way to fetch credentials\\nfor private registries, but also requires kubelet-level configuration to enable.\\nSee Configure a kubelet image credential provider  for more details.\\nInterpretation of config.json\\nThe interpretation of config.json  varies between the original Docker implementation and the\\nKubernetes interpretation. In Docker, the auths  keys can only specify root URLs, whereas\\nKubernetes allows glob URLs as well as prefix-matched paths. The only limitation is that glob\\npatterns ( *) have to include the dot ( .) for each subdomain. The amount of matched subdomains\\nhas to be equal to the amount of glob patterns ( *.), for example:\\n*.kubernetes.io  will not match kubernetes.io , but abc.kubernetes.io• \\n◦ \\n◦ \\n• \\n◦ \\n• \\n◦ \\n◦ \\n• \\n◦ \\n• \\n◦ \\n•\", metadata={'source': './PDFS/Concepts.pdf', 'page': 71}),\n",
       " Document(page_content='*.*.kubernetes.io  will not match abc.kubernetes.io , but abc.def.kubernetes.io\\nprefix.*.io  will match prefix.kubernetes.io\\n*-good.kubernetes.io  will match prefix-good.kubernetes.io\\nThis means that a config.json  like this is valid:\\n{\\n    \"auths\" : {\\n        \"my-registry.io/images\" : { \"auth\" : \"...\" },\\n        \"*.my-registry.io/images\" : { \"auth\" : \"...\" }\\n    }\\n}\\nImage pull operations would now pass the credentials to the CRI container runtime for every\\nvalid pattern. For example the following container image names would match successfully:\\nmy-registry.io/images\\nmy-registry.io/images/my-image\\nmy-registry.io/images/another-image\\nsub.my-registry.io/images/my-image\\nBut not:\\na.sub.my-registry.io/images/my-image\\na.b.sub.my-registry.io/images/my-image\\nThe kubelet performs image pulls sequentially for every found credential. This means, that\\nmultiple entries in config.json  for different paths are possible, too:\\n{\\n    \"auths\" : {\\n        \"my-registry.io/images\" : {\\n            \"auth\" : \"...\"\\n        },\\n        \"my-registry.io/images/subpath\" : {\\n            \"auth\" : \"...\"\\n        }\\n    }\\n}\\nIf now a container specifies an image my-registry.io/images/subpath/my-image  to be pulled,\\nthen the kubelet will try to download them from both authentication sources if one of them\\nfails.\\nPre-pulled images\\nNote:  This approach is suitable if you can control node configuration. It will not work reliably if\\nyour cloud provider manages nodes and replaces them automatically.\\nBy default, the kubelet tries to pull each image from the specified registry. However, if the \\nimagePullPolicy  property of the container is set to IfNotPresent  or Never , then a local image is\\nused (preferentially or exclusively, respectively).• \\n• \\n• \\n• \\n• \\n• \\n• \\n• \\n•', metadata={'source': './PDFS/Concepts.pdf', 'page': 72}),\n",
       " Document(page_content='If you want to rely on pre-pulled images as a substitute for registry authentication, you must\\nensure all nodes in the cluster have the same pre-pulled images.\\nThis can be used to preload certain images for speed or as an alternative to authenticating to a\\nprivate registry.\\nAll pods will have read access to any pre-pulled images.\\nSpecifying imagePullSecrets on a Pod\\nNote:  This is the recommended approach to run containers based on images in private\\nregistries.\\nKubernetes supports specifying container image registry keys on a Pod. imagePullSecrets  must\\nall be in the same namespace as the Pod. The referenced Secrets must be of type kubernetes.io/\\ndockercfg  or kubernetes.io/dockerconfigjson .\\nCreating a Secret with a Docker config\\nYou need to know the username, registry password and client email address for authenticating\\nto the registry, as well as its hostname. Run the following command, substituting the\\nappropriate uppercase values:\\nkubectl create secret docker-registry <name> \\\\\\n  --docker-server =DOCKER_REGISTRY_SERVER \\\\\\n  --docker-username =DOCKER_USER \\\\\\n  --docker-password =DOCKER_PASSWORD \\\\\\n  --docker-email =DOCKER_EMAIL\\nIf you already have a Docker credentials file then, rather than using the above command, you\\ncan import the credentials file as a Kubernetes Secrets .\\nCreate a Secret based on existing Docker credentials  explains how to set this up.\\nThis is particularly useful if you are using multiple private container registries, as \\nkubectl create secret docker-registry  creates a Secret that only works with a single private\\nregistry.\\nNote:  Pods can only reference image pull secrets in their own namespace, so this process needs\\nto be done one time per namespace.\\nReferring to an imagePullSecrets on a Pod\\nNow, you can create pods which reference that secret by adding an imagePullSecrets  section to\\na Pod definition. Each item in the imagePullSecrets  array can only reference a Secret in the\\nsame namespace.\\nFor example:\\ncat <<EOF > pod.yaml\\napiVersion: v1\\nkind: Pod\\nmetadata:\\n  name: foo', metadata={'source': './PDFS/Concepts.pdf', 'page': 73}),\n",
       " Document(page_content='namespace: awesomeapps\\nspec:\\n  containers:\\n    - name: foo\\n      image: janedoe/awesomeapp:v1\\n  imagePullSecrets:\\n    - name: myregistrykey\\nEOF\\ncat <<EOF >> ./kustomization.yaml\\nresources:\\n- pod.yaml\\nEOF\\nThis needs to be done for each pod that is using a private registry.\\nHowever, setting of this field can be automated by setting the imagePullSecrets in a \\nServiceAccount  resource.\\nCheck Add ImagePullSecrets to a Service Account  for detailed instructions.\\nYou can use this in conjunction with a per-node .docker/config.json . The credentials will be\\nmerged.\\nUse cases\\nThere are a number of solutions for configuring private registries. Here are some common use\\ncases and suggested solutions.\\nCluster running only non-proprietary (e.g. open-source) images. No need to hide images.\\nUse public images from a public registry\\nNo configuration required.\\nSome cloud providers automatically cache or mirror public images, which\\nimproves availability and reduces the time to pull images.\\nCluster running some proprietary images which should be hidden to those outside the\\ncompany, but visible to all cluster users.\\nUse a hosted private registry\\nManual configuration may be required on the nodes that need to access to\\nprivate registry\\nOr, run an internal private registry behind your firewall with open read access.\\nNo Kubernetes configuration is required.\\nUse a hosted container image registry service that controls image access\\nIt will work better with cluster autoscaling than manual node configuration.\\nOr, on a cluster where changing the node configuration is inconvenient, use \\nimagePullSecrets .\\nCluster with proprietary images, a few of which require stricter access control.\\nEnsure AlwaysPullImages admission controller  is active. Otherwise, all Pods\\npotentially have access to all images.\\nMove sensitive data into a \"Secret\" resource, instead of packaging it in an image.\\nA multi-tenant cluster where each tenant needs own private registry.\\nEnsure AlwaysPullImages admission controller  is active. Otherwise, all Pods of all\\ntenants potentially have access to all images.\\nRun a private registry with authorization required.1. \\n◦ \\n▪ \\n▪ \\n2. \\n◦ \\n▪ \\n◦ \\n▪ \\n◦ \\n▪ \\n◦ \\n3. \\n◦ \\n◦ \\n4. \\n◦ \\n◦', metadata={'source': './PDFS/Concepts.pdf', 'page': 74}),\n",
       " Document(page_content=\"Generate registry credential for each tenant, put into secret, and populate secret to\\neach tenant namespace.\\nThe tenant adds that secret to imagePullSecrets of each namespace.\\nIf you need access to multiple registries, you can create one secret for each registry.\\nLegacy built-in kubelet credential provider\\nIn older versions of Kubernetes, the kubelet had a direct integration with cloud provider\\ncredentials. This gave it the ability to dynamically fetch credentials for image registries.\\nThere were three built-in implementations of the kubelet credential provider integration: ACR\\n(Azure Container Registry), ECR (Elastic Container Registry), and GCR (Google Container\\nRegistry).\\nFor more information on the legacy mechanism, read the documentation for the version of\\nKubernetes that you are using. Kubernetes v1.26 through to v1.28 do not include the legacy\\nmechanism, so you would need to either:\\nconfigure a kubelet image credential provider on each node\\nspecify image pull credentials using imagePullSecrets  and at least one Secret\\nWhat's next\\nRead the OCI Image Manifest Specification .\\nLearn about container image garbage collection .\\nLearn more about pulling an Image from a Private Registry .\\nContainer Environment\\nThis page describes the resources available to Containers in the Container environment.\\nContainer environment\\nThe Kubernetes Container environment provides several important resources to Containers:\\nA filesystem, which is a combination of an image  and one or more volumes .\\nInformation about the Container itself.\\nInformation about other objects in the cluster.\\nContainer information\\nThe hostname  of a Container is the name of the Pod in which the Container is running. It is\\navailable through the hostname  command or the gethostname  function call in libc.\\nThe Pod name and namespace are available as environment variables through the downward\\nAPI.\\nUser defined environment variables from the Pod definition are also available to the Container,\\nas are any environment variables specified statically in the container image.◦ \\n◦ \\n• \\n• \\n• \\n• \\n• \\n• \\n• \\n•\", metadata={'source': './PDFS/Concepts.pdf', 'page': 75}),\n",
       " Document(page_content=\"Cluster information\\nA list of all services that were running when a Container was created is available to that\\nContainer as environment variables. This list is limited to services within the same namespace\\nas the new Container's Pod and Kubernetes control plane services.\\nFor a service named foo that maps to a Container named bar, the following variables are\\ndefined:\\nFOO_SERVICE_HOST =<the host the service is running on>\\nFOO_SERVICE_PORT =<the port the service is running on>\\nServices have dedicated IP addresses and are available to the Container via DNS, if DNS addon\\nis enabled.\\xa0\\nWhat's next\\nLearn more about Container lifecycle hooks .\\nGet hands-on experience attaching handlers to Container lifecycle events .\\nRuntime Class\\nFEATURE STATE:  Kubernetes v1.20 [stable]\\nThis page describes the RuntimeClass resource and runtime selection mechanism.\\nRuntimeClass is a feature for selecting the container runtime configuration. The container\\nruntime configuration is used to run a Pod's containers.\\nMotivation\\nYou can set a different RuntimeClass between different Pods to provide a balance of\\nperformance versus security. For example, if part of your workload deserves a high level of\\ninformation security assurance, you might choose to schedule those Pods so that they run in a\\ncontainer runtime that uses hardware virtualization. You'd then benefit from the extra isolation\\nof the alternative runtime, at the expense of some additional overhead.\\nYou can also use RuntimeClass to run different Pods with the same container runtime but with\\ndifferent settings.\\nSetup\\nConfigure the CRI implementation on nodes (runtime dependent)\\nCreate the corresponding RuntimeClass resources\\n1. Configure the CRI implementation on nodes\\nThe configurations available through RuntimeClass are Container Runtime Interface (CRI)\\nimplementation dependent. See the corresponding documentation ( below ) for your CRI\\nimplementation for how to configure.• \\n• \\n1. \\n2.\", metadata={'source': './PDFS/Concepts.pdf', 'page': 76}),\n",
       " Document(page_content='Note:  RuntimeClass assumes a homogeneous node configuration across the cluster by default\\n(which means that all nodes are configured the same way with respect to container runtimes).\\nTo support heterogeneous node configurations, see Scheduling  below.\\nThe configurations have a corresponding handler  name, referenced by the RuntimeClass. The\\nhandler must be a valid DNS label name .\\n2. Create the corresponding RuntimeClass resources\\nThe configurations setup in step 1 should each have an associated handler  name, which\\nidentifies the configuration. For each handler, create a corresponding RuntimeClass object.\\nThe RuntimeClass resource currently only has 2 significant fields: the RuntimeClass name\\n(metadata.name ) and the handler ( handler ). The object definition looks like this:\\n# RuntimeClass is defined in the node.k8s.io API group\\napiVersion : node.k8s.io/v1\\nkind: RuntimeClass\\nmetadata :\\n  # The name the RuntimeClass will be referenced by.\\n  # RuntimeClass is a non-namespaced resource.\\n  name : myclass \\n# The name of the corresponding CRI configuration\\nhandler : myconfiguration \\nThe name of a RuntimeClass object must be a valid DNS subdomain name .\\nNote:  It is recommended that RuntimeClass write operations (create/update/patch/delete) be\\nrestricted to the cluster administrator. This is typically the default. See Authorization Overview\\nfor more details.\\nUsage\\nOnce RuntimeClasses are configured for the cluster, you can specify a runtimeClassName  in the\\nPod spec to use it. For example:\\napiVersion : v1\\nkind: Pod\\nmetadata :\\n  name : mypod\\nspec:\\n  runtimeClassName : myclass\\n  # ...\\nThis will instruct the kubelet to use the named RuntimeClass to run this pod. If the named\\nRuntimeClass does not exist, or the CRI cannot run the corresponding handler, the pod will\\nenter the Failed  terminal phase . Look for a corresponding event  for an error message.\\nIf no runtimeClassName  is specified, the default RuntimeHandler will be used, which is\\nequivalent to the behavior when the RuntimeClass feature is disabled.', metadata={'source': './PDFS/Concepts.pdf', 'page': 77}),\n",
       " Document(page_content='CRI Configuration\\nFor more details on setting up CRI runtimes, see CRI installation .\\ncontainerd\\nRuntime handlers are configured through containerd\\'s configuration at /etc/containerd/\\nconfig.toml . Valid handlers are configured under the runtimes section:\\n[plugins.\"io.containerd.grpc.v1.cri\".containerd.runtimes.${HANDLER_NAME}]\\nSee containerd\\'s config documentation  for more details:\\nCRI-O\\nRuntime handlers are configured through CRI-O\\'s configuration at /etc/crio/crio.conf . Valid\\nhandlers are configured under the crio.runtime table :\\n[crio.runtime.runtimes.${HANDLER_NAME}]\\n  runtime_path = \"${PATH_TO_BINARY}\"\\nSee CRI-O\\'s config documentation  for more details.\\nScheduling\\nFEATURE STATE:  Kubernetes v1.16 [beta]\\nBy specifying the scheduling  field for a RuntimeClass, you can set constraints to ensure that\\nPods running with this RuntimeClass are scheduled to nodes that support it. If scheduling  is not\\nset, this RuntimeClass is assumed to be supported by all nodes.\\nTo ensure pods land on nodes supporting a specific RuntimeClass, that set of nodes should have\\na common label which is then selected by the runtimeclass.scheduling.nodeSelector  field. The\\nRuntimeClass\\'s nodeSelector is merged with the pod\\'s nodeSelector in admission, effectively\\ntaking the intersection of the set of nodes selected by each. If there is a conflict, the pod will be\\nrejected.\\nIf the supported nodes are tainted to prevent other RuntimeClass pods from running on the\\nnode, you can add tolerations  to the RuntimeClass. As with the nodeSelector , the tolerations are\\nmerged with the pod\\'s tolerations in admission, effectively taking the union of the set of nodes\\ntolerated by each.\\nTo learn more about configuring the node selector and tolerations, see Assigning Pods to\\nNodes .\\nPod Overhead\\nFEATURE STATE:  Kubernetes v1.24 [stable]\\nYou can specify overhead  resources that are associated with running a Pod. Declaring overhead\\nallows the cluster (including the scheduler) to account for it when making decisions about Pods\\nand resources.', metadata={'source': './PDFS/Concepts.pdf', 'page': 78}),\n",
       " Document(page_content=\"Pod overhead is defined in RuntimeClass through the overhead  field. Through the use of this\\nfield, you can specify the overhead of running pods utilizing this RuntimeClass and ensure\\nthese overheads are accounted for in Kubernetes.\\nWhat's next\\nRuntimeClass Design\\nRuntimeClass Scheduling Design\\nRead about the Pod Overhead  concept\\nPodOverhead Feature Design\\nContainer Lifecycle Hooks\\nThis page describes how kubelet managed Containers can use the Container lifecycle hook\\nframework to run code triggered by events during their management lifecycle.\\nOverview\\nAnalogous to many programming language frameworks that have component lifecycle hooks,\\nsuch as Angular, Kubernetes provides Containers with lifecycle hooks. The hooks enable\\nContainers to be aware of events in their management lifecycle and run code implemented in a\\nhandler when the corresponding lifecycle hook is executed.\\nContainer hooks\\nThere are two hooks that are exposed to Containers:\\nPostStart\\nThis hook is executed immediately after a container is created. However, there is no guarantee\\nthat the hook will execute before the container ENTRYPOINT. No parameters are passed to the\\nhandler.\\nPreStop\\nThis hook is called immediately before a container is terminated due to an API request or\\nmanagement event such as a liveness/startup probe failure, preemption, resource contention\\nand others. A call to the PreStop  hook fails if the container is already in a terminated or\\ncompleted state and the hook must complete before the TERM signal to stop the container can\\nbe sent. The Pod's termination grace period countdown begins before the PreStop  hook is\\nexecuted, so regardless of the outcome of the handler, the container will eventually terminate\\nwithin the Pod's termination grace period. No parameters are passed to the handler.\\nA more detailed description of the termination behavior can be found in Termination of Pods .• \\n• \\n• \\n•\", metadata={'source': './PDFS/Concepts.pdf', 'page': 79}),\n",
       " Document(page_content=\"Hook handler implementations\\nContainers can access a hook by implementing and registering a handler for that hook. There\\nare two types of hook handlers that can be implemented for Containers:\\nExec - Executes a specific command, such as pre-stop.sh , inside the cgroups and\\nnamespaces of the Container. Resources consumed by the command are counted against\\nthe Container.\\nHTTP - Executes an HTTP request against a specific endpoint on the Container.\\nHook handler execution\\nWhen a Container lifecycle management hook is called, the Kubernetes management system\\nexecutes the handler according to the hook action, httpGet  and tcpSocket  are executed by the\\nkubelet process, and exec is executed in the container.\\nHook handler calls are synchronous within the context of the Pod containing the Container.\\nThis means that for a PostStart  hook, the Container ENTRYPOINT and hook fire\\nasynchronously. However, if the hook takes too long to run or hangs, the Container cannot\\nreach a running  state.\\nPreStop  hooks are not executed asynchronously from the signal to stop the Container; the hook\\nmust complete its execution before the TERM signal can be sent. If a PreStop  hook hangs\\nduring execution, the Pod's phase will be Terminating  and remain there until the Pod is killed\\nafter its terminationGracePeriodSeconds  expires. This grace period applies to the total time it\\ntakes for both the PreStop  hook to execute and for the Container to stop normally. If, for\\nexample, terminationGracePeriodSeconds  is 60, and the hook takes 55 seconds to complete, and\\nthe Container takes 10 seconds to stop normally after receiving the signal, then the Container\\nwill be killed before it can stop normally, since terminationGracePeriodSeconds  is less than the\\ntotal time (55+10) it takes for these two things to happen.\\nIf either a PostStart  or PreStop  hook fails, it kills the Container.\\nUsers should make their hook handlers as lightweight as possible. There are cases, however,\\nwhen long running commands make sense, such as when saving state prior to stopping a\\nContainer.\\nHook delivery guarantees\\nHook delivery is intended to be at least once , which means that a hook may be called multiple\\ntimes for any given event, such as for PostStart  or PreStop . It is up to the hook implementation\\nto handle this correctly.\\nGenerally, only single deliveries are made. If, for example, an HTTP hook receiver is down and\\nis unable to take traffic, there is no attempt to resend. In some rare cases, however, double\\ndelivery may occur. For instance, if a kubelet restarts in the middle of sending a hook, the hook\\nmight be resent after the kubelet comes back up.\\nDebugging Hook handlers\\nThe logs for a Hook handler are not exposed in Pod events. If a handler fails for some reason, it\\nbroadcasts an event. For PostStart , this is the FailedPostStartHook  event, and for PreStop , this is\\nthe FailedPreStopHook  event. To generate a failed FailedPostStartHook  event yourself, modify• \\n•\", metadata={'source': './PDFS/Concepts.pdf', 'page': 80}),\n",
       " Document(page_content='the lifecycle-events.yaml  file to change the postStart command to \"badcommand\" and apply it.\\nHere is some example output of the resulting events you see from running \\nkubectl describe pod lifecycle-demo :\\nEvents:\\n  Type     Reason               Age              From               Message\\n  ----     ------               ----             ----               -------\\n  Normal   Scheduled            7s               default-scheduler  Successfully assigned default/\\nlifecycle-demo to ip-XXX-XXX-XX-XX.us-east-2...\\n  Normal   Pulled               6s               kubelet            Successfully pulled image \"nginx\" in \\n229.604315ms\\n  Normal   Pulling              4s (x2 over 6s)  kubelet            Pulling image \"nginx\"\\n  Normal   Created              4s (x2 over 5s)  kubelet            Created container lifecycle-demo-\\ncontainer\\n  Normal   Started              4s (x2 over 5s)  kubelet            Started container lifecycle-demo-\\ncontainer\\n  Warning  FailedPostStartHook  4s (x2 over 5s)  kubelet            Exec lifecycle hook \\n([badcommand]) for Container \"lifecycle-demo-container\" in Pod \"lifecycle-\\ndemo_default(30229739-9651-4e5a-9a32-a8f1688862db)\" failed - error: command \\'badcommand\\' \\nexited with 126: , message: \"OCI runtime exec failed: exec failed: container_linux.go:380: \\nstarting container process caused: exec: \\\\\"badcommand\\\\\": executable file not found in $PATH: \\nunknown\\\\r\\\\n\"\\n  Normal   Killing              4s (x2 over 5s)  kubelet            FailedPostStartHook\\n  Normal   Pulled               4s               kubelet            Successfully pulled image \"nginx\" in \\n215.66395ms\\n  Warning  BackOff              2s (x2 over 3s)  kubelet            Back-off restarting failed container\\nWhat\\'s next\\nLearn more about the Container environment .\\nGet hands-on experience attaching handlers to Container lifecycle events .\\nWorkloads\\nUnderstand Pods, the smallest deployable compute object in Kubernetes, and the higher-level\\nabstractions that help you to run them.\\nA workload is an application running on Kubernetes. Whether your workload is a single\\ncomponent or several that work together, on Kubernetes you run it inside a set of pods. In\\nKubernetes, a Pod represents a set of running containers  on your cluster.\\nKubernetes pods have a defined lifecycle . For example, once a pod is running in your cluster\\nthen a critical fault on the node  where that pod is running means that all the pods on that node\\nfail. Kubernetes treats that level of failure as final: you would need to create a new Pod to\\nrecover, even if the node later becomes healthy.\\nHowever, to make life considerably easier, you don\\'t need to manage each Pod directly. Instead,\\nyou can use workload resources  that manage a set of pods on your behalf. These resources\\nconfigure controllers  that make sure the right number of the right kind of pod are running, to\\nmatch the state you specified.• \\n•', metadata={'source': './PDFS/Concepts.pdf', 'page': 81}),\n",
       " Document(page_content=\"Kubernetes provides several built-in workload resources:\\nDeployment  and ReplicaSet  (replacing the legacy resource ReplicationController ).\\nDeployment is a good fit for managing a stateless application workload on your cluster,\\nwhere any Pod in the Deployment is interchangeable and can be replaced if needed.\\nStatefulSet  lets you run one or more related Pods that do track state somehow. For\\nexample, if your workload records data persistently, you can run a StatefulSet that\\nmatches each Pod with a PersistentVolume . Your code, running in the Pods for that\\nStatefulSet, can replicate data to other Pods in the same StatefulSet to improve overall\\nresilience.\\nDaemonSet  defines Pods that provide facilities that are local to nodes. Every time you add\\na node to your cluster that matches the specification in a DaemonSet, the control plane\\nschedules a Pod for that DaemonSet onto the new node. Each pod in a DaemonSet\\nperforms a job similar to a system daemon on a classic Unix / POSIX server. A\\nDaemonSet might be fundamental to the operation of your cluster, such as a plugin to run\\ncluster networking , it might help you to manage the node, or it could provide optional\\nbehavior that enhances the container platform you are running.\\nJob and CronJob  provide different ways to define tasks that run to completion and then\\nstop. You can use a Job to define a task that runs to completion, just once. You can use a \\nCronJob  to run the same Job multiple times according a schedule.\\nIn the wider Kubernetes ecosystem, you can find third-party workload resources that provide\\nadditional behaviors. Using a custom resource definition , you can add in a third-party workload\\nresource if you want a specific behavior that's not part of Kubernetes' core. For example, if you\\nwanted to run a group of Pods for your application but stop work unless all the Pods are\\navailable (perhaps for some high-throughput distributed task), then you can implement or\\ninstall an extension that does provide that feature.\\nWhat's next\\nAs well as reading about each API kind for workload management, you can read how to do\\nspecific tasks:\\nRun a stateless application using a Deployment\\nRun a stateful application either as a single instance  or as a replicated set\\nRun automated tasks with a CronJob\\nTo learn about Kubernetes' mechanisms for separating code from configuration, visit \\nConfiguration .\\nThere are two supporting concepts that provide backgrounds about how Kubernetes manages\\npods for applications:\\nGarbage collection  tidies up objects from your cluster after their owning resource  has been\\nremoved.\\nThe time-to-live after finished  controller  removes Jobs once a defined time has passed\\nsince they completed.\\nOnce your application is running, you might want to make it available on the internet as a \\nService  or, for web application only, using an Ingress .• \\n• \\n• \\n• \\n• \\n• \\n• \\n• \\n•\", metadata={'source': './PDFS/Concepts.pdf', 'page': 82}),\n",
       " Document(page_content='Pods\\nPods are the smallest deployable units of computing that you can create and manage in\\nKubernetes.\\nA Pod (as in a pod of whales or pea pod) is a group of one or more containers , with shared\\nstorage and network resources, and a specification for how to run the containers. A Pod\\'s\\ncontents are always co-located and co-scheduled, and run in a shared context. A Pod models an\\napplication-specific \"logical host\": it contains one or more application containers which are\\nrelatively tightly coupled. In non-cloud contexts, applications executed on the same physical or\\nvirtual machine are analogous to cloud applications executed on the same logical host.\\nAs well as application containers, a Pod can contain init containers  that run during Pod startup.\\nYou can also inject ephemeral containers  for debugging if your cluster offers this.\\nWhat is a Pod?\\nNote:  You need to install a container runtime  into each node in the cluster so that Pods can run\\nthere.\\nThe shared context of a Pod is a set of Linux namespaces, cgroups, and potentially other facets\\nof isolation - the same things that isolate a container . Within a Pod\\'s context, the individual\\napplications may have further sub-isolations applied.\\nA Pod is similar to a set of containers with shared namespaces and shared filesystem volumes.\\nUsing Pods\\nThe following is an example of a Pod which consists of a container running the image nginx:\\n1.14.2 .\\npods/simple-pod.yaml  \\napiVersion : v1\\nkind: Pod\\nmetadata :\\n  name : nginx\\nspec:\\n  containers :\\n  - name : nginx\\n    image : nginx:1.14.2\\n    ports :\\n    - containerPort : 80\\nTo create the Pod shown above, run the following command:\\nkubectl apply -f https://k8s.io/examples/pods/simple-pod.yaml\\nPods are generally not created directly and are created using workload resources. See Working\\nwith Pods  for more information on how Pods are used with workload resources.', metadata={'source': './PDFS/Concepts.pdf', 'page': 83}),\n",
       " Document(page_content='Workload resources for managing pods\\nUsually you don\\'t need to create Pods directly, even singleton Pods. Instead, create them using\\nworkload resources such as Deployment  or Job. If your Pods need to track state, consider the \\nStatefulSet  resource.\\nPods in a Kubernetes cluster are used in two main ways:\\nPods that run a single container . The \"one-container-per-Pod\" model is the most\\ncommon Kubernetes use case; in this case, you can think of a Pod as a wrapper around a\\nsingle container; Kubernetes manages Pods rather than managing the containers directly.\\nPods that run multiple containers that need to work together . A Pod can\\nencapsulate an application composed of multiple co-located containers that are tightly\\ncoupled and need to share resources. These co-located containers form a single cohesive\\nunit of service—for example, one container serving data stored in a shared volume to the\\npublic, while a separate sidecar  container refreshes or updates those files. The Pod wraps\\nthese containers, storage resources, and an ephemeral network identity together as a\\nsingle unit.\\nNote:  Grouping multiple co-located and co-managed containers in a single Pod is a\\nrelatively advanced use case. You should use this pattern only in specific instances in\\nwhich your containers are tightly coupled.\\nEach Pod is meant to run a single instance of a given application. If you want to scale your\\napplication horizontally (to provide more overall resources by running more instances), you\\nshould use multiple Pods, one for each instance. In Kubernetes, this is typically referred to as \\nreplication . Replicated Pods are usually created and managed as a group by a workload resource\\nand its controller .\\nSee Pods and controllers  for more information on how Kubernetes uses workload resources,\\nand their controllers, to implement application scaling and auto-healing.\\nHow Pods manage multiple containers\\nPods are designed to support multiple cooperating processes (as containers) that form a\\ncohesive unit of service. The containers in a Pod are automatically co-located and co-scheduled\\non the same physical or virtual machine in the cluster. The containers can share resources and\\ndependencies, communicate with one another, and coordinate when and how they are\\nterminated.\\nFor example, you might have a container that acts as a web server for files in a shared volume,\\nand a separate \"sidecar\" container that updates those files from a remote source, as in the\\nfollowing diagram:\\nPod creation diagram\\nSome Pods have init containers  as well as app containers . By default, init containers run and\\ncomplete before the app containers are started.\\nFEATURE STATE:  Kubernetes v1.28 [alpha]\\nEnabling the SidecarContainers  feature gate  allows you to specify restartPolicy: Always  for init\\ncontainers. Setting the Always  restart policy ensures that the init containers where you set it• \\n•', metadata={'source': './PDFS/Concepts.pdf', 'page': 84}),\n",
       " Document(page_content=\"are kept running during the entire lifetime of the Pod. See Sidecar containers and restartPolicy\\nfor more details.\\nPods natively provide two kinds of shared resources for their constituent containers: \\nnetworking  and storage .\\nWorking with Pods\\nYou'll rarely create individual Pods directly in Kubernetes—even singleton Pods. This is because\\nPods are designed as relatively ephemeral, disposable entities. When a Pod gets created (directly\\nby you, or indirectly by a controller ), the new Pod is scheduled to run on a Node  in your cluster.\\nThe Pod remains on that node until the Pod finishes execution, the Pod object is deleted, the\\nPod is evicted  for lack of resources, or the node fails.\\nNote:  Restarting a container in a Pod should not be confused with restarting a Pod. A Pod is\\nnot a process, but an environment for running container(s). A Pod persists until it is deleted.\\nThe name of a Pod must be a valid DNS subdomain  value, but this can produce unexpected\\nresults for the Pod hostname. For best compatibility, the name should follow the more\\nrestrictive rules for a DNS label .\\nPod OS\\nFEATURE STATE:  Kubernetes v1.25 [stable]\\nYou should set the .spec.os.name  field to either windows  or linux  to indicate the OS on which\\nyou want the pod to run. These two are the only operating systems supported for now by\\nKubernetes. In future, this list may be expanded.\\nIn Kubernetes v1.28, the value you set for this field has no effect on scheduling  of the pods.\\nSetting the .spec.os.name  helps to identify the pod OS authoritatively and is used for validation.\\nThe kubelet refuses to run a Pod where you have specified a Pod OS, if this isn't the same as the\\noperating system for the node where that kubelet is running. The Pod security standards  also\\nuse this field to avoid enforcing policies that aren't relevant to that operating system.\\nPods and controllers\\nYou can use workload resources to create and manage multiple Pods for you. A controller for\\nthe resource handles replication and rollout and automatic healing in case of Pod failure. For\\nexample, if a Node fails, a controller notices that Pods on that Node have stopped working and\\ncreates a replacement Pod. The scheduler places the replacement Pod onto a healthy Node.\\nHere are some examples of workload resources that manage one or more Pods:\\nDeployment\\nStatefulSet\\nDaemonSet\\nPod templates\\nControllers for workload  resources create Pods from a pod template  and manage those Pods on\\nyour behalf.• \\n• \\n•\", metadata={'source': './PDFS/Concepts.pdf', 'page': 85}),\n",
       " Document(page_content='PodTemplates are specifications for creating Pods, and are included in workload resources such\\nas Deployments , Jobs, and DaemonSets .\\nEach controller for a workload resource uses the PodTemplate  inside the workload object to\\nmake actual Pods. The PodTemplate  is part of the desired state of whatever workload resource\\nyou used to run your app.\\nThe sample below is a manifest for a simple Job with a template  that starts one container. The\\ncontainer in that Pod prints a message then pauses.\\napiVersion : batch/v1\\nkind: Job\\nmetadata :\\n  name : hello\\nspec:\\n  template :\\n    # This is the pod template\\n    spec:\\n      containers :\\n      - name : hello\\n        image : busybox:1.28\\n        command : [\\'sh\\', \\'-c\\', \\'echo \"Hello, Kubernetes!\" && sleep 3600\\' ]\\n      restartPolicy : OnFailure\\n    # The pod template ends here\\nModifying the pod template or switching to a new pod template has no direct effect on the Pods\\nthat already exist. If you change the pod template for a workload resource, that resource needs\\nto create replacement Pods that use the updated template.\\nFor example, the StatefulSet controller ensures that the running Pods match the current pod\\ntemplate for each StatefulSet object. If you edit the StatefulSet to change its pod template, the\\nStatefulSet starts to create new Pods based on the updated template. Eventually, all of the old\\nPods are replaced with new Pods, and the update is complete.\\nEach workload resource implements its own rules for handling changes to the Pod template. If\\nyou want to read more about StatefulSet specifically, read Update strategy  in the StatefulSet\\nBasics tutorial.\\nOn Nodes, the kubelet  does not directly observe or manage any of the details around pod\\ntemplates and updates; those details are abstracted away. That abstraction and separation of\\nconcerns simplifies system semantics, and makes it feasible to extend the cluster\\'s behavior\\nwithout changing existing code.\\nPod update and replacement\\nAs mentioned in the previous section, when the Pod template for a workload resource is\\nchanged, the controller creates new Pods based on the updated template instead of updating or\\npatching the existing Pods.', metadata={'source': './PDFS/Concepts.pdf', 'page': 86}),\n",
       " Document(page_content=\"Kubernetes doesn't prevent you from managing Pods directly. It is possible to update some\\nfields of a running Pod, in place. However, Pod update operations like patch , and replace  have\\nsome limitations:\\nMost of the metadata about a Pod is immutable. For example, you cannot change the \\nnamespace , name , uid, or creationTimestamp  fields; the generation  field is unique. It only\\naccepts updates that increment the field's current value.\\nIf the metadata.deletionTimestamp  is set, no new entry can be added to the \\nmetadata.finalizers  list.\\nPod updates may not change fields other than spec.containers[*].image , \\nspec.initContainers[*].image , spec.activeDeadlineSeconds  or spec.tolerations . For \\nspec.tolerations , you can only add new entries.\\nWhen updating the spec.activeDeadlineSeconds  field, two types of updates are allowed:\\nsetting the unassigned field to a positive number;\\nupdating the field from a positive number to a smaller, non-negative number.\\nResource sharing and communication\\nPods enable data sharing and communication among their constituent containers.\\nStorage in Pods\\nA Pod can specify a set of shared storage volumes . All containers in the Pod can access the\\nshared volumes, allowing those containers to share data. Volumes also allow persistent data in a\\nPod to survive in case one of the containers within needs to be restarted. See Storage  for more\\ninformation on how Kubernetes implements shared storage and makes it available to Pods.\\nPod networking\\nEach Pod is assigned a unique IP address for each address family. Every container in a Pod\\nshares the network namespace, including the IP address and network ports. Inside a Pod (and \\nonly  then), the containers that belong to the Pod can communicate with one another using \\nlocalhost . When containers in a Pod communicate with entities outside the Pod , they must\\ncoordinate how they use the shared network resources (such as ports). Within a Pod, containers\\nshare an IP address and port space, and can find each other via localhost . The containers in a\\nPod can also communicate with each other using standard inter-process communications like\\nSystemV semaphores or POSIX shared memory. Containers in different Pods have distinct IP\\naddresses and can not communicate by OS-level IPC without special configuration. Containers\\nthat want to interact with a container running in a different Pod can use IP networking to\\ncommunicate.\\nContainers within the Pod see the system hostname as being the same as the configured name\\nfor the Pod. There's more about this in the networking  section.\\nPrivileged mode for containers\\nNote:  Your container runtime  must support the concept of a privileged container for this\\nsetting to be relevant.• \\n• \\n• \\n• \\n1. \\n2.\", metadata={'source': './PDFS/Concepts.pdf', 'page': 87}),\n",
       " Document(page_content='Any container in a pod can run in privileged mode to use operating system administrative\\ncapabilities that would otherwise be inaccessible. This is available for both Windows and Linux.\\nLinux privileged containers\\nIn Linux, any container in a Pod can enable privileged mode using the privileged  (Linux) flag on\\nthe security context  of the container spec. This is useful for containers that want to use\\noperating system administrative capabilities such as manipulating the network stack or\\naccessing hardware devices.\\nWindows privileged containers\\nFEATURE STATE:  Kubernetes v1.26 [stable]\\nIn Windows, you can create a Windows HostProcess pod  by setting the \\nwindowsOptions.hostProcess  flag on the security context of the pod spec. All containers in\\nthese pods must run as Windows HostProcess containers. HostProcess pods run directly on the\\nhost and can also be used to perform administrative tasks as is done with Linux privileged\\ncontainers.\\nStatic Pods\\nStatic Pods  are managed directly by the kubelet daemon on a specific node, without the API\\nserver  observing them. Whereas most Pods are managed by the control plane (for example, a \\nDeployment ), for static Pods, the kubelet directly supervises each static Pod (and restarts it if it\\nfails).\\nStatic Pods are always bound to one Kubelet  on a specific node. The main use for static Pods is\\nto run a self-hosted control plane: in other words, using the kubelet to supervise the individual \\ncontrol plane components .\\nThe kubelet automatically tries to create a mirror Pod  on the Kubernetes API server for each\\nstatic Pod. This means that the Pods running on a node are visible on the API server, but cannot\\nbe controlled from there. See the guide Create static Pods  for more information.\\nNote:  The spec of a static Pod cannot refer to other API objects (e.g., ServiceAccount , \\nConfigMap , Secret , etc).\\nContainer probes\\nA probe  is a diagnostic performed periodically by the kubelet on a container. To perform a\\ndiagnostic, the kubelet can invoke different actions:\\nExecAction  (performed with the help of the container runtime)\\nTCPSocketAction  (checked directly by the kubelet)\\nHTTPGetAction  (checked directly by the kubelet)\\nYou can read more about probes  in the Pod Lifecycle documentation.• \\n• \\n•', metadata={'source': './PDFS/Concepts.pdf', 'page': 88}),\n",
       " Document(page_content=\"What's next\\nLearn about the lifecycle of a Pod .\\nLearn about RuntimeClass  and how you can use it to configure different Pods with\\ndifferent container runtime configurations.\\nRead about PodDisruptionBudget  and how you can use it to manage application\\navailability during disruptions.\\nPod is a top-level resource in the Kubernetes REST API. The Pod object definition\\ndescribes the object in detail.\\nThe Distributed System Toolkit: Patterns for Composite Containers  explains common\\nlayouts for Pods with more than one container.\\nRead about Pod topology spread constraints\\nTo understand the context for why Kubernetes wraps a common Pod API in other resources\\n(such as StatefulSets  or Deployments ), you can read about the prior art, including:\\nAurora\\nBorg\\nMarathon\\nOmega\\nTupperware .\\nPod Lifecycle\\nThis page describes the lifecycle of a Pod. Pods follow a defined lifecycle, starting in the \\nPending  phase , moving through Running  if at least one of its primary containers starts OK, and\\nthen through either the Succeeded  or Failed  phases depending on whether any container in the\\nPod terminated in failure.\\nWhilst a Pod is running, the kubelet is able to restart containers to handle some kind of faults.\\nWithin a Pod, Kubernetes tracks different container states  and determines what action to take\\nto make the Pod healthy again.\\nIn the Kubernetes API, Pods have both a specification and an actual status. The status for a Pod\\nobject consists of a set of Pod conditions . You can also inject custom readiness information  into\\nthe condition data for a Pod, if that is useful to your application.\\nPods are only scheduled  once in their lifetime. Once a Pod is scheduled (assigned) to a Node,\\nthe Pod runs on that Node until it stops or is terminated .\\nPod lifetime\\nLike individual application containers, Pods are considered to be relatively ephemeral (rather\\nthan durable) entities. Pods are created, assigned a unique ID ( UID), and scheduled to nodes\\nwhere they remain until termination (according to restart policy) or deletion. If a Node  dies, the\\nPods scheduled to that node are scheduled for deletion  after a timeout period.\\nPods do not, by themselves, self-heal. If a Pod is scheduled to a node  that then fails, the Pod is\\ndeleted; likewise, a Pod won't survive an eviction due to a lack of resources or Node\\nmaintenance. Kubernetes uses a higher-level abstraction, called a controller , that handles the\\nwork of managing the relatively disposable Pod instances.• \\n• \\n• \\n• \\n• \\n• \\n• \\n• \\n• \\n• \\n•\", metadata={'source': './PDFS/Concepts.pdf', 'page': 89}),\n",
       " Document(page_content='A given Pod (as defined by a UID) is never \"rescheduled\" to a different node; instead, that Pod\\ncan be replaced by a new, near-identical Pod, with even the same name if desired, but with a\\ndifferent UID.\\nWhen something is said to have the same lifetime as a Pod, such as a volume , that means that\\nthe thing exists as long as that specific Pod (with that exact UID) exists. If that Pod is deleted for\\nany reason, and even if an identical replacement is created, the related thing (a volume, in this\\nexample) is also destroyed and created anew.\\nPod diagram\\nA multi-container Pod that contains a file puller and a web server that uses a persistent volume\\nfor shared storage between the containers.\\nPod phase\\nA Pod\\'s status  field is a PodStatus  object, which has a phase  field.\\nThe phase of a Pod is a simple, high-level summary of where the Pod is in its lifecycle. The\\nphase is not intended to be a comprehensive rollup of observations of container or Pod state,\\nnor is it intended to be a comprehensive state machine.\\nThe number and meanings of Pod phase values are tightly guarded. Other than what is\\ndocumented here, nothing should be assumed about Pods that have a given phase  value.\\nHere are the possible values for phase :\\nValue Description\\nPendingThe Pod has been accepted by the Kubernetes cluster, but one or more of the\\ncontainers has not been set up and made ready to run. This includes time a Pod\\nspends waiting to be scheduled as well as the time spent downloading container\\nimages over the network.\\nRunningThe Pod has been bound to a node, and all of the containers have been created. At\\nleast one container is still running, or is in the process of starting or restarting.\\nSucceeded All containers in the Pod have terminated in success, and will not be restarted.\\nFailedAll containers in the Pod have terminated, and at least one container has terminated\\nin failure. That is, the container either exited with non-zero status or was terminated\\nby the system.\\nUnknownFor some reason the state of the Pod could not be obtained. This phase typically\\noccurs due to an error in communicating with the node where the Pod should be\\nrunning.\\nNote:  When a Pod is being deleted, it is shown as Terminating  by some kubectl commands.\\nThis Terminating  status is not one of the Pod phases. A Pod is granted a term to terminate\\ngracefully, which defaults to 30 seconds. You can use the flag --force  to terminate a Pod by\\nforce .\\nSince Kubernetes 1.27, the kubelet transitions deleted Pods, except for static Pods  and force-\\ndeleted Pods  without a finalizer, to a terminal phase ( Failed  or Succeeded  depending on the exit\\nstatuses of the pod containers) before their deletion from the API server.\\nIf a node dies or is disconnected from the rest of the cluster, Kubernetes applies a policy for\\nsetting the phase  of all Pods on the lost node to Failed.', metadata={'source': './PDFS/Concepts.pdf', 'page': 90}),\n",
       " Document(page_content=\"Container states\\nAs well as the phase  of the Pod overall, Kubernetes tracks the state of each container inside a\\nPod. You can use container lifecycle hooks  to trigger events to run at certain points in a\\ncontainer's lifecycle.\\nOnce the scheduler  assigns a Pod to a Node, the kubelet starts creating containers for that Pod\\nusing a container runtime . There are three possible container states: Waiting , Running , and \\nTerminated .\\nTo check the state of a Pod's containers, you can use kubectl describe pod <name-of-pod> . The\\noutput shows the state for each container within that Pod.\\nEach state has a specific meaning:\\nWaiting\\nIf a container is not in either the Running  or Terminated  state, it is Waiting . A container in the \\nWaiting  state is still running the operations it requires in order to complete start up: for\\nexample, pulling the container image from a container image registry, or applying Secret  data.\\nWhen you use kubectl  to query a Pod with a container that is Waiting , you also see a Reason\\nfield to summarize why the container is in that state.\\nRunning\\nThe Running  status indicates that a container is executing without issues. If there was a \\npostStart  hook configured, it has already executed and finished. When you use kubectl  to query\\na Pod with a container that is Running , you also see information about when the container\\nentered the Running  state.\\nTerminated\\nA container in the Terminated  state began execution and then either ran to completion or failed\\nfor some reason. When you use kubectl  to query a Pod with a container that is Terminated , you\\nsee a reason, an exit code, and the start and finish time for that container's period of execution.\\nIf a container has a preStop  hook configured, this hook runs before the container enters the \\nTerminated  state.\\nContainer restart policy\\nThe spec of a Pod has a restartPolicy  field with possible values Always, OnFailure, and Never.\\nThe default value is Always.\\nThe restartPolicy  applies to all containers in the Pod. restartPolicy  only refers to restarts of the\\ncontainers by the kubelet on the same node. After containers in a Pod exit, the kubelet restarts\\nthem with an exponential back-off delay (10s, 20s, 40s, ...), that is capped at five minutes. Once a\\ncontainer has executed for 10 minutes without any problems, the kubelet resets the restart\\nbackoff timer for that container.\", metadata={'source': './PDFS/Concepts.pdf', 'page': 91}),\n",
       " Document(page_content='Pod conditions\\nA Pod has a PodStatus, which has an array of PodConditions  through which the Pod has or has\\nnot passed. Kubelet manages the following PodConditions:\\nPodScheduled : the Pod has been scheduled to a node.\\nPodReadyToStartContainers : (alpha feature; must be enabled explicitly ) the Pod sandbox\\nhas been successfully created and networking configured.\\nContainersReady : all containers in the Pod are ready.\\nInitialized : all init containers  have completed successfully.\\nReady : the Pod is able to serve requests and should be added to the load balancing pools\\nof all matching Services.\\nField name Description\\ntype Name of this Pod condition.\\nstatusIndicates whether that condition is applicable, with possible values \" True \",\\n\"False \", or \" Unknown \".\\nlastProbeTime Timestamp of when the Pod condition was last probed.\\nlastTransitionTime Timestamp for when the Pod last transitioned from one status to another.\\nreasonMachine-readable, UpperCamelCase text indicating the reason for the\\ncondition\\'s last transition.\\nmessage Human-readable message indicating details about the last status transition.\\nPod readiness\\nFEATURE STATE:  Kubernetes v1.14 [stable]\\nYour application can inject extra feedback or signals into PodStatus: Pod readiness . To use this,\\nset readinessGates  in the Pod\\'s spec to specify a list of additional conditions that the kubelet\\nevaluates for Pod readiness.\\nReadiness gates are determined by the current state of status.condition  fields for the Pod. If\\nKubernetes cannot find such a condition in the status.conditions  field of a Pod, the status of the\\ncondition is defaulted to \" False \".\\nHere is an example:\\nkind: Pod\\n...\\nspec:\\n  readinessGates :\\n    - conditionType : \"www.example.com/feature-1\"\\nstatus :\\n  conditions :\\n    - type: Ready                              # a built in PodCondition\\n      status : \"False\"\\n      lastProbeTime : null\\n      lastTransitionTime : 2018-01-01T00:00:00Z\\n    - type: \"www.example.com/feature-1\"         # an extra PodCondition\\n      status : \"False\"\\n      lastProbeTime : null\\n      lastTransitionTime : 2018-01-01T00:00:00Z• \\n• \\n• \\n• \\n•', metadata={'source': './PDFS/Concepts.pdf', 'page': 92}),\n",
       " Document(page_content=\"containerStatuses :\\n    - containerID : docker://abcd...\\n      ready : true\\n...\\nThe Pod conditions you add must have names that meet the Kubernetes label key format .\\nStatus for Pod readiness\\nThe kubectl patch  command does not support patching object status. To set these \\nstatus.conditions  for the Pod, applications and operators  should use the PATCH  action. You can\\nuse a Kubernetes client library  to write code that sets custom Pod conditions for Pod readiness.\\nFor a Pod that uses custom conditions, that Pod is evaluated to be ready only  when both the\\nfollowing statements apply:\\nAll containers in the Pod are ready.\\nAll conditions specified in readinessGates  are True .\\nWhen a Pod's containers are Ready but at least one custom condition is missing or False , the\\nkubelet sets the Pod's condition  to ContainersReady .\\nPod network readiness\\nFEATURE STATE:  Kubernetes v1.25 [alpha]\\nNote:  This condition was renamed from PodHasNetwork to PodReadyToStartContainers.\\nAfter a Pod gets scheduled on a node, it needs to be admitted by the Kubelet and have any\\nvolumes mounted. Once these phases are complete, the Kubelet works with a container runtime\\n(using Container runtime interface (CRI) ) to set up a runtime sandbox and configure\\nnetworking for the Pod. If the PodReadyToStartContainersCondition  feature gate  is enabled,\\nKubelet reports whether a pod has reached this initialization milestone through the \\nPodReadyToStartContainers  condition in the status.conditions  field of a Pod.\\nThe PodReadyToStartContainers  condition is set to False  by the Kubelet when it detects a Pod\\ndoes not have a runtime sandbox with networking configured. This occurs in the following\\nscenarios:\\nEarly in the lifecycle of the Pod, when the kubelet has not yet begun to set up a sandbox\\nfor the Pod using the container runtime.\\nLater in the lifecycle of the Pod, when the Pod sandbox has been destroyed due to either:\\nthe node rebooting, without the Pod getting evicted\\nfor container runtimes that use virtual machines for isolation, the Pod sandbox\\nvirtual machine rebooting, which then requires creating a new sandbox and fresh\\ncontainer network configuration.\\nThe PodReadyToStartContainers  condition is set to True  by the kubelet after the successful\\ncompletion of sandbox creation and network configuration for the Pod by the runtime plugin.\\nThe kubelet can start pulling container images and create containers after \\nPodReadyToStartContainers  condition has been set to True .\\nFor a Pod with init containers, the kubelet sets the Initialized  condition to True  after the init\\ncontainers have successfully completed (which happens after successful sandbox creation and• \\n• \\n• \\n• \\n◦ \\n◦\", metadata={'source': './PDFS/Concepts.pdf', 'page': 93}),\n",
       " Document(page_content=\"network configuration by the runtime plugin). For a Pod without init containers, the kubelet\\nsets the Initialized  condition to True  before sandbox creation and network configuration starts.\\nPod scheduling readiness\\nFEATURE STATE:  Kubernetes v1.26 [alpha]\\nSee Pod Scheduling Readiness  for more information.\\nContainer probes\\nA probe  is a diagnostic performed periodically by the kubelet  on a container. To perform a\\ndiagnostic, the kubelet either executes code within the container, or makes a network request.\\nCheck mechanisms\\nThere are four different ways to check a container using a probe. Each probe must define\\nexactly one of these four mechanisms:\\nexec\\nExecutes a specified command inside the container. The diagnostic is considered\\nsuccessful if the command exits with a status code of 0.\\ngrpc\\nPerforms a remote procedure call using gRPC . The target should implement gRPC health\\nchecks . The diagnostic is considered successful if the status  of the response is SERVING .\\nhttpGet\\nPerforms an HTTP GET request against the Pod's IP address on a specified port and path.\\nThe diagnostic is considered successful if the response has a status code greater than or\\nequal to 200 and less than 400.\\ntcpSocket\\nPerforms a TCP check against the Pod's IP address on a specified port. The diagnostic is\\nconsidered successful if the port is open. If the remote system (the container) closes the\\nconnection immediately after it opens, this counts as healthy.\\nCaution:  Unlike the other mechanisms, exec probe's implementation involves the creation/\\nforking of multiple processes each time when executed. As a result, in case of the clusters\\nhaving higher pod densities, lower intervals of initialDelaySeconds , periodSeconds , configuring\\nany probe with exec mechanism might introduce an overhead on the cpu usage of the node. In\\nsuch scenarios, consider using the alternative probe mechanisms to avoid the overhead.\\nProbe outcome\\nEach probe has one of three results:\\nSuccess\\nThe container passed the diagnostic.\\nFailure\\nThe container failed the diagnostic.\\nUnknown\\nThe diagnostic failed (no action should be taken, and the kubelet will make further\\nchecks).\", metadata={'source': './PDFS/Concepts.pdf', 'page': 94}),\n",
       " Document(page_content=\"Types of probe\\nThe kubelet can optionally perform and react to three kinds of probes on running containers:\\nlivenessProbe\\nIndicates whether the container is running. If the liveness probe fails, the kubelet kills the\\ncontainer, and the container is subjected to its restart policy . If a container does not\\nprovide a liveness probe, the default state is Success .\\nreadinessProbe\\nIndicates whether the container is ready to respond to requests. If the readiness probe\\nfails, the endpoints controller removes the Pod's IP address from the endpoints of all\\nServices that match the Pod. The default state of readiness before the initial delay is \\nFailure . If a container does not provide a readiness probe, the default state is Success .\\nstartupProbe\\nIndicates whether the application within the container is started. All other probes are\\ndisabled if a startup probe is provided, until it succeeds. If the startup probe fails, the\\nkubelet kills the container, and the container is subjected to its restart policy . If a\\ncontainer does not provide a startup probe, the default state is Success .\\nFor more information about how to set up a liveness, readiness, or startup probe, see Configure\\nLiveness, Readiness and Startup Probes .\\nWhen should you use a liveness probe?\\nIf the process in your container is able to crash on its own whenever it encounters an issue or\\nbecomes unhealthy, you do not necessarily need a liveness probe; the kubelet will automatically\\nperform the correct action in accordance with the Pod's restartPolicy .\\nIf you'd like your container to be killed and restarted if a probe fails, then specify a liveness\\nprobe, and specify a restartPolicy  of Always or OnFailure.\\nWhen should you use a readiness probe?\\nIf you'd like to start sending traffic to a Pod only when a probe succeeds, specify a readiness\\nprobe. In this case, the readiness probe might be the same as the liveness probe, but the\\nexistence of the readiness probe in the spec means that the Pod will start without receiving any\\ntraffic and only start receiving traffic after the probe starts succeeding.\\nIf you want your container to be able to take itself down for maintenance, you can specify a\\nreadiness probe that checks an endpoint specific to readiness that is different from the liveness\\nprobe.\\nIf your app has a strict dependency on back-end services, you can implement both a liveness\\nand a readiness probe. The liveness probe passes when the app itself is healthy, but the\\nreadiness probe additionally checks that each required back-end service is available. This helps\\nyou avoid directing traffic to Pods that can only respond with error messages.\\nIf your container needs to work on loading large data, configuration files, or migrations during\\nstartup, you can use a startup probe . However, if you want to detect the difference between an\\napp that has failed and an app that is still processing its startup data, you might prefer a\\nreadiness probe.\", metadata={'source': './PDFS/Concepts.pdf', 'page': 95}),\n",
       " Document(page_content='Note:  If you want to be able to drain requests when the Pod is deleted, you do not necessarily\\nneed a readiness probe; on deletion, the Pod automatically puts itself into an unready state\\nregardless of whether the readiness probe exists. The Pod remains in the unready state while it\\nwaits for the containers in the Pod to stop.\\nWhen should you use a startup probe?\\nStartup probes are useful for Pods that have containers that take a long time to come into\\nservice. Rather than set a long liveness interval, you can configure a separate configuration for\\nprobing the container as it starts up, allowing a time longer than the liveness interval would\\nallow.\\nIf your container usually starts in more than initialDelaySeconds + failureThreshold × \\nperiodSeconds , you should specify a startup probe that checks the same endpoint as the\\nliveness probe. The default for periodSeconds  is 10s. You should then set its failureThreshold\\nhigh enough to allow the container to start, without changing the default values of the liveness\\nprobe. This helps to protect against deadlocks.\\nTermination of Pods\\nBecause Pods represent processes running on nodes in the cluster, it is important to allow those\\nprocesses to gracefully terminate when they are no longer needed (rather than being abruptly\\nstopped with a KILL  signal and having no chance to clean up).\\nThe design aim is for you to be able to request deletion and know when processes terminate,\\nbut also be able to ensure that deletes eventually complete. When you request deletion of a Pod,\\nthe cluster records and tracks the intended grace period before the Pod is allowed to be\\nforcefully killed. With that forceful shutdown tracking in place, the kubelet  attempts graceful\\nshutdown.\\nTypically, with this graceful termination of the pod, kubelet makes requests to the container\\nruntime to attempt to stop the containers in the pod by first sending a TERM (aka. SIGTERM)\\nsignal, with a grace period timeout, to the main process in each container. The requests to stop\\nthe containers are processed by the container runtime asynchronously. There is no guarantee to\\nthe order of processing for these requests. Many container runtimes respect the STOPSIGNAL\\nvalue defined in the container image and, if different, send the container image configured\\nSTOPSIGNAL instead of TERM. Once the grace period has expired, the KILL signal is sent to\\nany remaining processes, and the Pod is then deleted from the API Server . If the kubelet or the\\ncontainer runtime\\'s management service is restarted while waiting for processes to terminate,\\nthe cluster retries from the start including the full original grace period.\\nAn example flow:\\nYou use the kubectl  tool to manually delete a specific Pod, with the default grace period\\n(30 seconds).\\nThe Pod in the API server is updated with the time beyond which the Pod is considered\\n\"dead\" along with the grace period. If you use kubectl describe  to check the Pod you\\'re\\ndeleting, that Pod shows up as \"Terminating\". On the node where the Pod is running: as1. \\n2.', metadata={'source': './PDFS/Concepts.pdf', 'page': 96}),\n",
       " Document(page_content=\"soon as the kubelet sees that a Pod has been marked as terminating (a graceful shutdown\\nduration has been set), the kubelet begins the local Pod shutdown process.\\nIf one of the Pod's containers has defined a preStop  hook  and the \\nterminationGracePeriodSeconds  in the Pod spec is not set to 0, the kubelet runs\\nthat hook inside of the container. The default terminationGracePeriodSeconds\\nsetting is 30 seconds.\\nIf the preStop  hook is still running after the grace period expires, the kubelet\\nrequests a small, one-off grace period extension of 2 seconds.\\nNote:  If the preStop  hook needs longer to complete than the default grace period\\nallows, you must modify terminationGracePeriodSeconds  to suit this.\\nThe kubelet triggers the container runtime to send a TERM signal to process 1\\ninside each container.\\nNote:  The containers in the Pod receive the TERM signal at different times and in\\nan arbitrary order. If the order of shutdowns matters, consider using a preStop\\nhook to synchronize.\\nAt the same time as the kubelet is starting graceful shutdown of the Pod, the control\\nplane evaluates whether to remove that shutting-down Pod from EndpointSlice (and\\nEndpoints) objects, where those objects represent a Service  with a configured selector . \\nReplicaSets  and other workload resources no longer treat the shutting-down Pod as a\\nvalid, in-service replica.\\nPods that shut down slowly should not continue to serve regular traffic and should start\\nterminating and finish processing open connections. Some applications need to go\\nbeyond finishing open connections and need more graceful termination, for example,\\nsession draining and completion.\\nAny endpoints that represent the terminating Pods are not immediately removed from\\nEndpointSlices, and a status indicating terminating state  is exposed from the\\nEndpointSlice API (and the legacy Endpoints API). Terminating endpoints always have\\ntheir ready  status as false (for backward compatibility with versions before 1.26), so load\\nbalancers will not use it for regular traffic.\\nIf traffic draining on terminating Pod is needed, the actual readiness can be checked as a\\ncondition serving . You can find more details on how to implement connections draining\\nin the tutorial Pods And Endpoints Termination Flow\\nNote:  If you don't have the EndpointSliceTerminatingCondition  feature gate enabled in your\\ncluster (the gate is on by default from Kubernetes 1.22, and locked to default in 1.26), then the\\nKubernetes control plane removes a Pod from any relevant EndpointSlices as soon as the Pod's\\ntermination grace period begins . The behavior above is described when the feature gate \\nEndpointSliceTerminatingCondition  is enabled.\\nWhen the grace period expires, the kubelet triggers forcible shutdown. The container\\nruntime sends SIGKILL  to any processes still running in any container in the Pod. The\\nkubelet also cleans up a hidden pause  container if that container runtime uses one.\\nThe kubelet transitions the Pod into a terminal phase ( Failed  or Succeeded  depending on\\nthe end state of its containers). This step is guaranteed since version 1.27.1. \\n2. \\n3. \\n1. \\n2.\", metadata={'source': './PDFS/Concepts.pdf', 'page': 97}),\n",
       " Document(page_content=\"The kubelet triggers forcible removal of Pod object from the API server, by setting grace\\nperiod to 0 (immediate deletion).\\nThe API server deletes the Pod's API object, which is then no longer visible from any\\nclient.\\nForced Pod termination\\nCaution:  Forced deletions can be potentially disruptive for some workloads and their Pods.\\nBy default, all deletes are graceful within 30 seconds. The kubectl delete  command supports the \\n--grace-period=<seconds>  option which allows you to override the default and specify your\\nown value.\\nSetting the grace period to 0 forcibly and immediately deletes the Pod from the API server. If\\nthe Pod was still running on a node, that forcible deletion triggers the kubelet to begin\\nimmediate cleanup.\\nNote:  You must specify an additional flag --force  along with --grace-period=0  in order to\\nperform force deletions.\\nWhen a force deletion is performed, the API server does not wait for confirmation from the\\nkubelet that the Pod has been terminated on the node it was running on. It removes the Pod in\\nthe API immediately so a new Pod can be created with the same name. On the node, Pods that\\nare set to terminate immediately will still be given a small grace period before being force\\nkilled.\\nCaution:  Immediate deletion does not wait for confirmation that the running resource has\\nbeen terminated. The resource may continue to run on the cluster indefinitely.\\nIf you need to force-delete Pods that are part of a StatefulSet, refer to the task documentation\\nfor deleting Pods from a StatefulSet .\\nGarbage collection of Pods\\nFor failed Pods, the API objects remain in the cluster's API until a human or controller  process\\nexplicitly removes them.\\nThe Pod garbage collector (PodGC), which is a controller in the control plane, cleans up\\nterminated Pods (with a phase of Succeeded  or Failed ), when the number of Pods exceeds the\\nconfigured threshold (determined by terminated-pod-gc-threshold  in the kube-controller-\\nmanager). This avoids a resource leak as Pods are created and terminated over time.\\nAdditionally, PodGC cleans up any Pods which satisfy any of the following conditions:\\nare orphan Pods - bound to a node which no longer exists,\\nare unscheduled terminating Pods,\\nare terminating Pods, bound to a non-ready node tainted with node.kubernetes.io/out-of-\\nservice , when the NodeOutOfServiceVolumeDetach  feature gate is enabled.\\nWhen the PodDisruptionConditions  feature gate is enabled, along with cleaning up the Pods,\\nPodGC will also mark them as failed if they are in a non-terminal phase. Also, PodGC adds a\\nPod disruption condition when cleaning up an orphan Pod. See Pod disruption conditions  for\\nmore details.3. \\n4. \\n1. \\n2. \\n3.\", metadata={'source': './PDFS/Concepts.pdf', 'page': 98}),\n",
       " Document(page_content=\"What's next\\nGet hands-on experience attaching handlers to container lifecycle events .\\nGet hands-on experience configuring Liveness, Readiness and Startup Probes .\\nLearn more about container lifecycle hooks .\\nFor detailed information about Pod and container status in the API, see the API reference\\ndocumentation covering status  for Pod.\\nInit Containers\\nThis page provides an overview of init containers: specialized containers that run before app\\ncontainers in a Pod. Init containers can contain utilities or setup scripts not present in an app\\nimage.\\nYou can specify init containers in the Pod specification alongside the containers  array (which\\ndescribes app containers).\\nUnderstanding init containers\\nA Pod can have multiple containers running apps within it, but it can also have one or more init\\ncontainers, which are run before the app containers are started.\\nInit containers are exactly like regular containers, except:\\nInit containers always run to completion.\\nEach init container must complete successfully before the next one starts.\\nIf a Pod's init container fails, the kubelet repeatedly restarts that init container until it succeeds.\\nHowever, if the Pod has a restartPolicy  of Never, and an init container fails during startup of\\nthat Pod, Kubernetes treats the overall Pod as failed.\\nTo specify an init container for a Pod, add the initContainers  field into the Pod specification , as\\nan array of container  items (similar to the app containers  field and its contents). See Container\\nin the API reference for more details.\\nThe status of the init containers is returned in .status.initContainerStatuses  field as an array of\\nthe container statuses (similar to the .status.containerStatuses  field).\\nDifferences from regular containers\\nInit containers support all the fields and features of app containers, including resource limits, \\nvolumes , and security settings. However, the resource requests and limits for an init container\\nare handled differently, as documented in Resource sharing within containers .\\nAlso, init containers do not support lifecycle , livenessProbe , readinessProbe , or startupProbe\\nbecause they must run to completion before the Pod can be ready.• \\n• \\n• \\n• \\n• \\n•\", metadata={'source': './PDFS/Concepts.pdf', 'page': 99}),\n",
       " Document(page_content=\"If you specify multiple init containers for a Pod, kubelet runs each init container sequentially.\\nEach init container must succeed before the next can run. When all of the init containers have\\nrun to completion, kubelet initializes the application containers for the Pod and runs them as\\nusual.\\nUsing init containers\\nBecause init containers have separate images from app containers, they have some advantages\\nfor start-up related code:\\nInit containers can contain utilities or custom code for setup that are not present in an\\napp image. For example, there is no need to make an image FROM  another image just to\\nuse a tool like sed, awk, python , or dig during setup.\\nThe application image builder and deployer roles can work independently without the\\nneed to jointly build a single app image.\\nInit containers can run with a different view of the filesystem than app containers in the\\nsame Pod. Consequently, they can be given access to Secrets  that app containers cannot\\naccess.\\nBecause init containers run to completion before any app containers start, init containers\\noffer a mechanism to block or delay app container startup until a set of preconditions are\\nmet. Once preconditions are met, all of the app containers in a Pod can start in parallel.\\nInit containers can securely run utilities or custom code that would otherwise make an\\napp container image less secure. By keeping unnecessary tools separate you can limit the\\nattack surface of your app container image.\\nExamples\\nHere are some ideas for how to use init containers:\\nWait for a Service  to be created, using a shell one-line command like:\\nfor i in {1..100 }; do sleep 1; if nslookup myservice; then exit 0; fi; done ; exit 1\\nRegister this Pod with a remote server from the downward API with a command like:\\ncurl -X POST http:// $MANAGEMENT_SERVICE_HOST :$MANAGEMENT_SERVICE_PO\\nRT/register -d 'instance=$(<POD_NAME>)&ip=$(<POD_IP>)'\\nWait for some time before starting the app container with a command like\\nsleep 60\\nClone a Git repository into a Volume\\nPlace values into a configuration file and run a template tool to dynamically generate a\\nconfiguration file for the main app container. For example, place the POD_IP  value in a\\nconfiguration and generate the main app configuration file using Jinja.\\nInit containers in use\\nThis example defines a simple Pod that has two init containers. The first waits for myservice ,\\nand the second waits for mydb . Once both init containers complete, the Pod runs the app\\ncontainer from its spec section.• \\n• \\n• \\n• \\n• \\n• \\n• \\n• \\n• \\n•\", metadata={'source': './PDFS/Concepts.pdf', 'page': 100}),\n",
       " Document(page_content='apiVersion : v1\\nkind: Pod\\nmetadata :\\n  name : myapp-pod\\n  labels :\\n    app.kubernetes.io/name : MyApp\\nspec:\\n  containers :\\n  - name : myapp-container\\n    image : busybox:1.28\\n    command : [\\'sh\\', \\'-c\\', \\'echo The app is running! && sleep 3600\\' ]\\n  initContainers :\\n  - name : init-myservice\\n    image : busybox:1.28\\n    command : [\\'sh\\', \\'-c\\', \"until nslookup myservice.$(cat /var/run/secrets/kubernetes.io/\\nserviceaccount/namespace).svc.cluster.local; do echo waiting for myservice; sleep 2; done\" ]\\n  - name : init-mydb\\n    image : busybox:1.28\\n    command : [\\'sh\\', \\'-c\\', \"until nslookup mydb.$(cat /var/run/secrets/kubernetes.io/\\nserviceaccount/namespace).svc.cluster.local; do echo waiting for mydb; sleep 2; done\" ]\\nYou can start this Pod by running:\\nkubectl apply -f myapp.yaml\\nThe output is similar to this:\\npod/myapp-pod created\\nAnd check on its status with:\\nkubectl get -f myapp.yaml\\nThe output is similar to this:\\nNAME        READY     STATUS     RESTARTS   AGE\\nmyapp-pod   0/1       Init:0/2   0          6m\\nor for more details:\\nkubectl describe -f myapp.yaml\\nThe output is similar to this:\\nName:          myapp-pod\\nNamespace:     default\\n[...]\\nLabels:        app.kubernetes.io/name=MyApp\\nStatus:        Pending\\n[...]\\nInit Containers:\\n  init-myservice:\\n[...]\\n    State:         Running', metadata={'source': './PDFS/Concepts.pdf', 'page': 101}),\n",
       " Document(page_content='[...]\\n  init-mydb:\\n[...]\\n    State:         Waiting\\n      Reason:      PodInitializing\\n    Ready:         False\\n[...]\\nContainers:\\n  myapp-container:\\n[...]\\n    State:         Waiting\\n      Reason:      PodInitializing\\n    Ready:         False\\n[...]\\nEvents:\\n  FirstSeen    LastSeen    Count    From                      SubObjectPath                           Type          \\nReason        Message\\n  ---------    --------    -----    ----                      -------------                           --------      ------        \\n-------\\n  16s          16s         1        {default-scheduler }                                              Normal        \\nScheduled     Successfully assigned myapp-pod to 172.17.4.201\\n  16s          16s         1        {kubelet 172.17.4.201}    spec.initContainers{init-myservice}     \\nNormal        Pulling       pulling image \"busybox\"\\n  13s          13s         1        {kubelet 172.17.4.201}    spec.initContainers{init-myservice}     \\nNormal        Pulled        Successfully pulled image \"busybox\"\\n  13s          13s         1        {kubelet 172.17.4.201}    spec.initContainers{init-myservice}     \\nNormal        Created       Created container init-myservice\\n  13s          13s         1        {kubelet 172.17.4.201}    spec.initContainers{init-myservice}     \\nNormal        Started       Started container init-myservice\\nTo see logs for the init containers in this Pod, run:\\nkubectl logs myapp-pod -c init-myservice # Inspect the first init container\\nkubectl logs myapp-pod -c init-mydb      # Inspect the second init container\\nAt this point, those init containers will be waiting to discover Services  named mydb  and \\nmyservice .\\nHere\\'s a configuration you can use to make those Services appear:\\n---\\napiVersion : v1\\nkind: Service\\nmetadata :\\n  name : myservice\\nspec:\\n  ports :\\n  - protocol : TCP\\n    port: 80\\n    targetPort : 9376\\n---\\napiVersion : v1\\nkind: Service', metadata={'source': './PDFS/Concepts.pdf', 'page': 102}),\n",
       " Document(page_content=\"metadata :\\n  name : mydb\\nspec:\\n  ports :\\n  - protocol : TCP\\n    port: 80\\n    targetPort : 9377\\nTo create the mydb  and myservice  services:\\nkubectl apply -f services.yaml\\nThe output is similar to this:\\nservice/myservice created\\nservice/mydb created\\nYou'll then see that those init containers complete, and that the myapp-pod  Pod moves into the\\nRunning state:\\nkubectl get -f myapp.yaml\\nThe output is similar to this:\\nNAME        READY     STATUS    RESTARTS   AGE\\nmyapp-pod   1/1       Running   0          9m\\nThis simple example should provide some inspiration for you to create your own init\\ncontainers. What's next  contains a link to a more detailed example.\\nDetailed behavior\\nDuring Pod startup, the kubelet delays running init containers until the networking and storage\\nare ready. Then the kubelet runs the Pod's init containers in the order they appear in the Pod's\\nspec.\\nEach init container must exit successfully before the next container starts. If a container fails to\\nstart due to the runtime or exits with failure, it is retried according to the Pod restartPolicy .\\nHowever, if the Pod restartPolicy  is set to Always, the init containers use restartPolicy\\nOnFailure.\\nA Pod cannot be Ready  until all init containers have succeeded. The ports on an init container\\nare not aggregated under a Service. A Pod that is initializing is in the Pending  state but should\\nhave a condition Initialized  set to false.\\nIf the Pod restarts , or is restarted, all init containers must execute again.\\nChanges to the init container spec are limited to the container image field. Altering an init\\ncontainer image field is equivalent to restarting the Pod.\\nBecause init containers can be restarted, retried, or re-executed, init container code should be\\nidempotent. In particular, code that writes to files on EmptyDirs  should be prepared for the\\npossibility that an output file already exists.\", metadata={'source': './PDFS/Concepts.pdf', 'page': 103}),\n",
       " Document(page_content=\"Init containers have all of the fields of an app container. However, Kubernetes prohibits \\nreadinessProbe  from being used because init containers cannot define readiness distinct from\\ncompletion. This is enforced during validation.\\nUse activeDeadlineSeconds  on the Pod to prevent init containers from failing forever. The active\\ndeadline includes init containers. However it is recommended to use activeDeadlineSeconds\\nonly if teams deploy their application as a Job, because activeDeadlineSeconds  has an effect\\neven after initContainer finished. The Pod which is already running correctly would be killed by\\nactiveDeadlineSeconds  if you set.\\nThe name of each app and init container in a Pod must be unique; a validation error is thrown\\nfor any container sharing a name with another.\\nAPI for sidecar containers\\nFEATURE STATE:  Kubernetes v1.28 [alpha]\\nStarting with Kubernetes 1.28 in alpha, a feature gate named SidecarContainers  allows you to\\nspecify a restartPolicy  for init containers which is independent of the Pod and other init\\ncontainers. Container probes  can also be added to control their lifecycle.\\nIf an init container is created with its restartPolicy  set to Always , it will start and remain\\nrunning during the entire life of the Pod, which is useful for running supporting services\\nseparated from the main application containers.\\nIf a readinessProbe  is specified for this init container, its result will be used to determine the \\nready  state of the Pod.\\nSince these containers are defined as init containers, they benefit from the same ordering and\\nsequential guarantees as other init containers, allowing them to be mixed with other init\\ncontainers into complex Pod initialization flows.\\nCompared to regular init containers, sidecar-style init containers continue to run and the next\\ninit container can begin starting once the kubelet has set the started  container status for the\\nsidecar-style init container to true. That status either becomes true because there is a process\\nrunning in the container and no startup probe defined, or as a result of its startupProbe\\nsucceeding.\\nThis feature can be used to implement the sidecar container pattern in a more robust way, as\\nthe kubelet always restarts a sidecar container if it fails.\\nHere's an example of a Deployment with two containers, one of which is a sidecar:\\napplication/deployment-sidecar.yaml  \\napiVersion : apps/v1\\nkind: Deployment\\nmetadata :\\n  name : myapp\\n  labels :\\n    app: myapp\\nspec:\\n  replicas : 1\\n  selector :\", metadata={'source': './PDFS/Concepts.pdf', 'page': 104}),\n",
       " Document(page_content='matchLabels :\\n      app: myapp\\n  template :\\n    metadata :\\n      labels :\\n        app: myapp\\n    spec:\\n      containers :\\n        - name : myapp\\n          image : alpine:latest\\n          command : [\\'sh\\', \\'-c\\', \\'while true; do echo \"logging\" >> /opt/logs.txt; sleep 1; done\\' ]\\n          volumeMounts :\\n            - name : data\\n              mountPath : /opt\\n      initContainers :\\n        - name : logshipper\\n          image : alpine:latest\\n          restartPolicy : Always\\n          command : [\\'sh\\', \\'-c\\', \\'tail -F /opt/logs.txt\\' ]\\n          volumeMounts :\\n            - name : data\\n              mountPath : /opt\\n      volumes :\\n        - name : data\\n          emptyDir : {}\\nThis feature is also useful for running Jobs with sidecars, as the sidecar container will not\\nprevent the Job from completing after the main container has finished.\\nHere\\'s an example of a Job with two containers, one of which is a sidecar:\\napplication/job/job-sidecar.yaml  \\napiVersion : batch/v1\\nkind: Job\\nmetadata :\\n  name : myjob\\nspec:\\n  template :\\n    spec:\\n      containers :\\n        - name : myjob\\n          image : alpine:latest\\n          command : [\\'sh\\', \\'-c\\', \\'echo \"logging\" > /opt/logs.txt\\' ]\\n          volumeMounts :\\n            - name : data\\n              mountPath : /opt\\n      initContainers :\\n        - name : logshipper\\n          image : alpine:latest\\n          restartPolicy : Always\\n          command : [\\'sh\\', \\'-c\\', \\'tail -F /opt/logs.txt\\' ]\\n          volumeMounts :', metadata={'source': './PDFS/Concepts.pdf', 'page': 105}),\n",
       " Document(page_content=\"- name : data\\n              mountPath : /opt\\n      restartPolicy : Never\\n      volumes :\\n        - name : data\\n          emptyDir : {}\\nResource sharing within containers\\nGiven the ordering and execution for init containers, the following rules for resource usage\\napply:\\nThe highest of any particular resource request or limit defined on all init containers is the\\neffective init request/limit . If any resource has no resource limit specified this is considered\\nas the highest limit.\\nThe Pod's effective request/limit  for a resource is the higher of:\\nthe sum of all app containers request/limit for a resource\\nthe effective init request/limit for a resource\\nScheduling is done based on effective requests/limits, which means init containers can\\nreserve resources for initialization that are not used during the life of the Pod.\\nThe QoS (quality of service) tier of the Pod's effective QoS tier  is the QoS tier for init\\ncontainers and app containers alike.\\nQuota and limits are applied based on the effective Pod request and limit.\\nPod level control groups (cgroups) are based on the effective Pod request and limit, the same as\\nthe scheduler.\\nPod restart reasons\\nA Pod can restart, causing re-execution of init containers, for the following reasons:\\nThe Pod infrastructure container is restarted. This is uncommon and would have to be\\ndone by someone with root access to nodes.\\nAll containers in a Pod are terminated while restartPolicy  is set to Always, forcing a\\nrestart, and the init container completion record has been lost due to garbage collection .\\nThe Pod will not be restarted when the init container image is changed, or the init container\\ncompletion record has been lost due to garbage collection. This applies for Kubernetes v1.20 and\\nlater. If you are using an earlier version of Kubernetes, consult the documentation for the\\nversion you are using.\\nWhat's next\\nRead about creating a Pod that has an init container\\nLearn how to debug init containers\\nRead about an overview of kubelet  and kubectl\\nLearn about the types of probes : liveness, readiness, startup probe.• \\n• \\n◦ \\n◦ \\n• \\n• \\n• \\n• \\n• \\n• \\n• \\n•\", metadata={'source': './PDFS/Concepts.pdf', 'page': 106}),\n",
       " Document(page_content=\"Disruptions\\nThis guide is for application owners who want to build highly available applications, and thus\\nneed to understand what types of disruptions can happen to Pods.\\nIt is also for cluster administrators who want to perform automated cluster actions, like\\nupgrading and autoscaling clusters.\\nVoluntary and involuntary disruptions\\nPods do not disappear until someone (a person or a controller) destroys them, or there is an\\nunavoidable hardware or system software error.\\nWe call these unavoidable cases involuntary disruptions  to an application. Examples are:\\na hardware failure of the physical machine backing the node\\ncluster administrator deletes VM (instance) by mistake\\ncloud provider or hypervisor failure makes VM disappear\\na kernel panic\\nthe node disappears from the cluster due to cluster network partition\\neviction of a pod due to the node being out-of-resources .\\nExcept for the out-of-resources condition, all these conditions should be familiar to most users;\\nthey are not specific to Kubernetes.\\nWe call other cases voluntary disruptions . These include both actions initiated by the application\\nowner and those initiated by a Cluster Administrator. Typical application owner actions\\ninclude:\\ndeleting the deployment or other controller that manages the pod\\nupdating a deployment's pod template causing a restart\\ndirectly deleting a pod (e.g. by accident)\\nCluster administrator actions include:\\nDraining a node  for repair or upgrade.\\nDraining a node from a cluster to scale the cluster down (learn about Cluster\\nAutoscaling  ).\\nRemoving a pod from a node to permit something else to fit on that node.\\nThese actions might be taken directly by the cluster administrator, or by automation run by the\\ncluster administrator, or by your cluster hosting provider.\\nAsk your cluster administrator or consult your cloud provider or distribution documentation to\\ndetermine if any sources of voluntary disruptions are enabled for your cluster. If none are\\nenabled, you can skip creating Pod Disruption Budgets.\\nCaution:  Not all voluntary disruptions are constrained by Pod Disruption Budgets. For\\nexample, deleting deployments or pods bypasses Pod Disruption Budgets.• \\n• \\n• \\n• \\n• \\n• \\n• \\n• \\n• \\n• \\n• \\n•\", metadata={'source': './PDFS/Concepts.pdf', 'page': 107}),\n",
       " Document(page_content=\"Dealing with disruptions\\nHere are some ways to mitigate involuntary disruptions:\\nEnsure your pod requests the resources  it needs.\\nReplicate your application if you need higher availability. (Learn about running replicated\\nstateless  and stateful  applications.)\\nFor even higher availability when running replicated applications, spread applications\\nacross racks (using anti-affinity ) or across zones (if using a multi-zone cluster .)\\nThe frequency of voluntary disruptions varies. On a basic Kubernetes cluster, there are no\\nautomated voluntary disruptions (only user-triggered ones). However, your cluster\\nadministrator or hosting provider may run some additional services which cause voluntary\\ndisruptions. For example, rolling out node software updates can cause voluntary disruptions.\\nAlso, some implementations of cluster (node) autoscaling may cause voluntary disruptions to\\ndefragment and compact nodes. Your cluster administrator or hosting provider should have\\ndocumented what level of voluntary disruptions, if any, to expect. Certain configuration\\noptions, such as using PriorityClasses  in your pod spec can also cause voluntary (and\\ninvoluntary) disruptions.\\nPod disruption budgets\\nFEATURE STATE:  Kubernetes v1.21 [stable]\\nKubernetes offers features to help you run highly available applications even when you\\nintroduce frequent voluntary disruptions.\\nAs an application owner, you can create a PodDisruptionBudget (PDB) for each application. A\\nPDB limits the number of Pods of a replicated application that are down simultaneously from\\nvoluntary disruptions. For example, a quorum-based application would like to ensure that the\\nnumber of replicas running is never brought below the number needed for a quorum. A web\\nfront end might want to ensure that the number of replicas serving load never falls below a\\ncertain percentage of the total.\\nCluster managers and hosting providers should use tools which respect PodDisruptionBudgets\\nby calling the Eviction API  instead of directly deleting pods or deployments.\\nFor example, the kubectl drain  subcommand lets you mark a node as going out of service.\\nWhen you run kubectl drain , the tool tries to evict all of the Pods on the Node you're taking out\\nof service. The eviction request that kubectl  submits on your behalf may be temporarily\\nrejected, so the tool periodically retries all failed requests until all Pods on the target node are\\nterminated, or until a configurable timeout is reached.\\nA PDB specifies the number of replicas that an application can tolerate having, relative to how\\nmany it is intended to have. For example, a Deployment which has a .spec.replicas: 5  is\\nsupposed to have 5 pods at any given time. If its PDB allows for there to be 4 at a time, then the\\nEviction API will allow voluntary disruption of one (but not two) pods at a time.\\nThe group of pods that comprise the application is specified using a label selector, the same as\\nthe one used by the application's controller (deployment, stateful-set, etc).• \\n• \\n•\", metadata={'source': './PDFS/Concepts.pdf', 'page': 108}),\n",
       " Document(page_content='The \"intended\" number of pods is computed from the .spec.replicas  of the workload resource\\nthat is managing those pods. The control plane discovers the owning workload resource by\\nexamining the .metadata.ownerReferences  of the Pod.\\nInvoluntary disruptions  cannot be prevented by PDBs; however they do count against the\\nbudget.\\nPods which are deleted or unavailable due to a rolling upgrade to an application do count\\nagainst the disruption budget, but workload resources (such as Deployment and StatefulSet) are\\nnot limited by PDBs when doing rolling upgrades. Instead, the handling of failures during\\napplication updates is configured in the spec for the specific workload resource.\\nIt is recommended to set AlwaysAllow  Unhealthy Pod Eviction Policy  to your\\nPodDisruptionBudgets to support eviction of misbehaving applications during a node drain.\\nThe default behavior is to wait for the application pods to become healthy  before the drain can\\nproceed.\\nWhen a pod is evicted using the eviction API, it is gracefully terminated , honoring the \\nterminationGracePeriodSeconds  setting in its PodSpec .\\nPodDisruptionBudget example\\nConsider a cluster with 3 nodes, node-1  through node-3 . The cluster is running several\\napplications. One of them has 3 replicas initially called pod-a , pod-b , and pod-c . Another,\\nunrelated pod without a PDB, called pod-x , is also shown. Initially, the pods are laid out as\\nfollows:\\nnode-1 node-2 node-3\\npod-a available pod-b available pod-c available\\npod-x available\\nAll 3 pods are part of a deployment, and they collectively have a PDB which requires there be at\\nleast 2 of the 3 pods to be available at all times.\\nFor example, assume the cluster administrator wants to reboot into a new kernel version to fix\\na bug in the kernel. The cluster administrator first tries to drain node-1  using the kubectl drain\\ncommand. That tool tries to evict pod-a  and pod-x . This succeeds immediately. Both pods go\\ninto the terminating  state at the same time. This puts the cluster in this state:\\nnode-1 draining node-2 node-3\\npod-a terminating pod-b available pod-c available\\npod-x terminating\\nThe deployment notices that one of the pods is terminating, so it creates a replacement called \\npod-d . Since node-1  is cordoned, it lands on another node. Something has also created pod-y  as\\na replacement for pod-x .\\n(Note: for a StatefulSet, pod-a , which would be called something like pod-0 , would need to\\nterminate completely before its replacement, which is also called pod-0  but has a different UID,\\ncould be created. Otherwise, the example applies to a StatefulSet as well.)\\nNow the cluster is in this state:', metadata={'source': './PDFS/Concepts.pdf', 'page': 109}),\n",
       " Document(page_content=\"node-1 draining node-2 node-3\\npod-a terminating pod-b available pod-c available\\npod-x terminating pod-d starting pod-y\\nAt some point, the pods terminate, and the cluster looks like this:\\nnode-1 drained node-2 node-3\\npod-b available pod-c available\\npod-d starting pod-y\\nAt this point, if an impatient cluster administrator tries to drain node-2  or node-3 , the drain\\ncommand will block, because there are only 2 available pods for the deployment, and its PDB\\nrequires at least 2. After some time passes, pod-d  becomes available.\\nThe cluster state now looks like this:\\nnode-1 drained node-2 node-3\\npod-b available pod-c available\\npod-d available pod-y\\nNow, the cluster administrator tries to drain node-2 . The drain command will try to evict the\\ntwo pods in some order, say pod-b  first and then pod-d . It will succeed at evicting pod-b . But,\\nwhen it tries to evict pod-d , it will be refused because that would leave only one pod available\\nfor the deployment.\\nThe deployment creates a replacement for pod-b  called pod-e . Because there are not enough\\nresources in the cluster to schedule pod-e  the drain will again block. The cluster may end up in\\nthis state:\\nnode-1 drained node-2 node-3 no node\\npod-b terminating pod-c available pod-e pending\\npod-d available pod-y\\nAt this point, the cluster administrator needs to add a node back to the cluster to proceed with\\nthe upgrade.\\nYou can see how Kubernetes varies the rate at which disruptions can happen, according to:\\nhow many replicas an application needs\\nhow long it takes to gracefully shutdown an instance\\nhow long it takes a new instance to start up\\nthe type of controller\\nthe cluster's resource capacity\\nPod disruption conditions\\nFEATURE STATE:  Kubernetes v1.26 [beta]\\nNote:  In order to use this behavior, you must have the PodDisruptionConditions  feature gate\\nenabled in your cluster.• \\n• \\n• \\n• \\n•\", metadata={'source': './PDFS/Concepts.pdf', 'page': 110}),\n",
       " Document(page_content=\"When enabled, a dedicated Pod DisruptionTarget  condition  is added to indicate that the Pod is\\nabout to be deleted due to a disruption . The reason  field of the condition additionally indicates\\none of the following reasons for the Pod termination:\\nPreemptionByScheduler\\nPod is due to be preempted  by a scheduler in order to accommodate a new Pod with a\\nhigher priority. For more information, see Pod priority preemption .\\nDeletionByTaintManager\\nPod is due to be deleted by Taint Manager (which is part of the node lifecycle controller\\nwithin kube-controller-manager ) due to a NoExecute  taint that the Pod does not tolerate;\\nsee taint -based evictions.\\nEvictionByEvictionAPI\\nPod has been marked for eviction using the Kubernetes API  .\\nDeletionByPodGC\\nPod, that is bound to a no longer existing Node, is due to be deleted by Pod garbage\\ncollection .\\nTerminationByKubelet\\nPod has been terminated by the kubelet, because of either node pressure eviction  or the \\ngraceful node shutdown .\\nNote:  A Pod disruption might be interrupted. The control plane might re-attempt to continue\\nthe disruption of the same Pod, but it is not guaranteed. As a result, the DisruptionTarget\\ncondition might be added to a Pod, but that Pod might then not actually be deleted. In such a\\nsituation, after some time, the Pod disruption condition will be cleared.\\nWhen the PodDisruptionConditions  feature gate is enabled, along with cleaning up the pods,\\nthe Pod garbage collector (PodGC) will also mark them as failed if they are in a non-terminal\\nphase (see also Pod garbage collection ).\\nWhen using a Job (or CronJob), you may want to use these Pod disruption conditions as part of\\nyour Job's Pod failure policy .\\nSeparating Cluster Owner and Application Owner Roles\\nOften, it is useful to think of the Cluster Manager and Application Owner as separate roles with\\nlimited knowledge of each other. This separation of responsibilities may make sense in these\\nscenarios:\\nwhen there are many application teams sharing a Kubernetes cluster, and there is natural\\nspecialization of roles\\nwhen third-party tools or services are used to automate cluster management\\nPod Disruption Budgets support this separation of roles by providing an interface between the\\nroles.\\nIf you do not have such a separation of responsibilities in your organization, you may not need\\nto use Pod Disruption Budgets.• \\n•\", metadata={'source': './PDFS/Concepts.pdf', 'page': 111}),\n",
       " Document(page_content=\"How to perform Disruptive Actions on your Cluster\\nIf you are a Cluster Administrator, and you need to perform a disruptive action on all the nodes\\nin your cluster, such as a node or system software upgrade, here are some options:\\nAccept downtime during the upgrade.\\nFailover to another complete replica cluster.\\nNo downtime, but may be costly both for the duplicated nodes and for human\\neffort to orchestrate the switchover.\\nWrite disruption tolerant applications and use PDBs.\\nNo downtime.\\nMinimal resource duplication.\\nAllows more automation of cluster administration.\\nWriting disruption-tolerant applications is tricky, but the work to tolerate\\nvoluntary disruptions largely overlaps with work to support autoscaling and\\ntolerating involuntary disruptions.\\nWhat's next\\nFollow steps to protect your application by configuring a Pod Disruption Budget .\\nLearn more about draining nodes\\nLearn about updating a deployment  including steps to maintain its availability during the\\nrollout.\\nEphemeral Containers\\nFEATURE STATE:  Kubernetes v1.25 [stable]\\nThis page provides an overview of ephemeral containers: a special type of container that runs\\ntemporarily in an existing Pod to accomplish user-initiated actions such as troubleshooting. You\\nuse ephemeral containers to inspect services rather than to build applications.\\nUnderstanding ephemeral containers\\nPods  are the fundamental building block of Kubernetes applications. Since Pods are intended to\\nbe disposable and replaceable, you cannot add a container to a Pod once it has been created.\\nInstead, you usually delete and replace Pods in a controlled fashion using deployments .\\nSometimes it's necessary to inspect the state of an existing Pod, however, for example to\\ntroubleshoot a hard-to-reproduce bug. In these cases you can run an ephemeral container in an\\nexisting Pod to inspect its state and run arbitrary commands.\\nWhat is an ephemeral container?\\nEphemeral containers differ from other containers in that they lack guarantees for resources or\\nexecution, and they will never be automatically restarted, so they are not appropriate for• \\n• \\n◦ \\n• \\n◦ \\n◦ \\n◦ \\n◦ \\n• \\n• \\n•\", metadata={'source': './PDFS/Concepts.pdf', 'page': 112}),\n",
       " Document(page_content=\"building applications. Ephemeral containers are described using the same ContainerSpec  as\\nregular containers, but many fields are incompatible and disallowed for ephemeral containers.\\nEphemeral containers may not have ports, so fields such as ports , livenessProbe , \\nreadinessProbe  are disallowed.\\nPod resource allocations are immutable, so setting resources  is disallowed.\\nFor a complete list of allowed fields, see the EphemeralContainer reference\\ndocumentation .\\nEphemeral containers are created using a special ephemeralcontainers  handler in the API rather\\nthan by adding them directly to pod.spec , so it's not possible to add an ephemeral container\\nusing kubectl edit .\\nLike regular containers, you may not change or remove an ephemeral container after you have\\nadded it to a Pod.\\nNote:  Ephemeral containers are not supported by static pods .\\nUses for ephemeral containers\\nEphemeral containers are useful for interactive troubleshooting when kubectl exec  is\\ninsufficient because a container has crashed or a container image doesn't include debugging\\nutilities.\\nIn particular, distroless images  enable you to deploy minimal container images that reduce\\nattack surface and exposure to bugs and vulnerabilities. Since distroless images do not include a\\nshell or any debugging utilities, it's difficult to troubleshoot distroless images using kubectl exec\\nalone.\\nWhen using ephemeral containers, it's helpful to enable process namespace sharing  so you can\\nview processes in other containers.\\nWhat's next\\nLearn how to debug pods using ephemeral containers .\\nPod Quality of Service Classes\\nThis page introduces Quality of Service (QoS) classes  in Kubernetes, and explains how\\nKubernetes assigns a QoS class to each Pod as a consequence of the resource constraints that\\nyou specify for the containers in that Pod. Kubernetes relies on this classification to make\\ndecisions about which Pods to evict when there are not enough available resources on a Node.\\nQuality of Service classes\\nKubernetes classifies the Pods that you run and allocates each Pod into a specific quality of\\nservice (QoS) class . Kubernetes uses that classification to influence how different pods are\\nhandled. Kubernetes does this classification based on the resource requests  of the Containers  in\\nthat Pod, along with how those requests relate to resource limits. This is known as Quality of\\nService  (QoS) class. Kubernetes assigns every Pod a QoS class based on the resource requests• \\n• \\n• \\n•\", metadata={'source': './PDFS/Concepts.pdf', 'page': 113}),\n",
       " Document(page_content=\"and limits of its component Containers. QoS classes are used by Kubernetes to decide which\\nPods to evict from a Node experiencing Node Pressure . The possible QoS classes are \\nGuaranteed , Burstable , and BestEffort . When a Node runs out of resources, Kubernetes will first\\nevict BestEffort  Pods running on that Node, followed by Burstable  and finally Guaranteed  Pods.\\nWhen this eviction is due to resource pressure, only Pods exceeding resource requests are\\ncandidates for eviction.\\nGuaranteed\\nPods that are Guaranteed  have the strictest resource limits and are least likely to face eviction.\\nThey are guaranteed not to be killed until they exceed their limits or there are no lower-priority\\nPods that can be preempted from the Node. They may not acquire resources beyond their\\nspecified limits. These Pods can also make use of exclusive CPUs using the static  CPU\\nmanagement policy.\\nCriteria\\nFor a Pod to be given a QoS class of Guaranteed :\\nEvery Container in the Pod must have a memory limit and a memory request.\\nFor every Container in the Pod, the memory limit must equal the memory request.\\nEvery Container in the Pod must have a CPU limit and a CPU request.\\nFor every Container in the Pod, the CPU limit must equal the CPU request.\\nBurstable\\nPods that are Burstable  have some lower-bound resource guarantees based on the request, but\\ndo not require a specific limit. If a limit is not specified, it defaults to a limit equivalent to the\\ncapacity of the Node, which allows the Pods to flexibly increase their resources if resources are\\navailable. In the event of Pod eviction due to Node resource pressure, these Pods are evicted\\nonly after all BestEffort  Pods are evicted. Because a Burstable  Pod can include a Container that\\nhas no resource limits or requests, a Pod that is Burstable  can try to use any amount of node\\nresources.\\nCriteria\\nA Pod is given a QoS class of Burstable  if:\\nThe Pod does not meet the criteria for QoS class Guaranteed .\\nAt least one Container in the Pod has a memory or CPU request or limit.\\nBestEffort\\nPods in the BestEffort  QoS class can use node resources that aren't specifically assigned to Pods\\nin other QoS classes. For example, if you have a node with 16 CPU cores available to the\\nkubelet, and you assign 4 CPU cores to a Guaranteed  Pod, then a Pod in the BestEffort  QoS\\nclass can try to use any amount of the remaining 12 CPU cores.\\nThe kubelet prefers to evict BestEffort  Pods if the node comes under resource pressure.• \\n• \\n• \\n• \\n• \\n•\", metadata={'source': './PDFS/Concepts.pdf', 'page': 114}),\n",
       " Document(page_content=\"Criteria\\nA Pod has a QoS class of BestEffort  if it doesn't meet the criteria for either Guaranteed  or \\nBurstable . In other words, a Pod is BestEffort  only if none of the Containers in the Pod have a\\nmemory limit or a memory request, and none of the Containers in the Pod have a CPU limit or\\na CPU request. Containers in a Pod can request other resources (not CPU or memory) and still\\nbe classified as BestEffort .\\nMemory QoS with cgroup v2\\nFEATURE STATE:  Kubernetes v1.22 [alpha]\\nMemory QoS uses the memory controller of cgroup v2 to guarantee memory resources in\\nKubernetes. Memory requests and limits of containers in pod are used to set specific interfaces \\nmemory.min  and memory.high  provided by the memory controller. When memory.min  is set to\\nmemory requests, memory resources are reserved and never reclaimed by the kernel; this is\\nhow Memory QoS ensures memory availability for Kubernetes pods. And if memory limits are\\nset in the container, this means that the system needs to limit container memory usage;\\nMemory QoS uses memory.high  to throttle workload approaching its memory limit, ensuring\\nthat the system is not overwhelmed by instantaneous memory allocation.\\nMemory QoS relies on QoS class to determine which settings to apply; however, these are\\ndifferent mechanisms that both provide controls over quality of service.\\nSome behavior is independent of QoS class\\nCertain behavior is independent of the QoS class assigned by Kubernetes. For example:\\nAny Container exceeding a resource limit will be killed and restarted by the kubelet\\nwithout affecting other Containers in that Pod.\\nIf a Container exceeds its resource request and the node it runs on faces resource\\npressure, the Pod it is in becomes a candidate for eviction . If this occurs, all Containers in\\nthe Pod will be terminated. Kubernetes may create a replacement Pod, usually on a\\ndifferent node.\\nThe resource request of a Pod is equal to the sum of the resource requests of its\\ncomponent Containers, and the resource limit of a Pod is equal to the sum of the resource\\nlimits of its component Containers.\\nThe kube-scheduler does not consider QoS class when selecting which Pods to preempt .\\nPreemption can occur when a cluster does not have enough resources to run all the Pods\\nyou defined.\\nWhat's next\\nLearn about resource management for Pods and Containers .\\nLearn about Node-pressure eviction .\\nLearn about Pod priority and preemption .\\nLearn about Pod disruptions .\\nLearn how to assign memory resources to containers and pods .• \\n• \\n• \\n• \\n• \\n• \\n• \\n• \\n•\", metadata={'source': './PDFS/Concepts.pdf', 'page': 115}),\n",
       " Document(page_content=\"Learn how to assign CPU resources to containers and pods .\\nLearn how to configure Quality of Service for Pods .\\nUser Namespaces\\nFEATURE STATE:  Kubernetes v1.25 [alpha]\\nThis page explains how user namespaces are used in Kubernetes pods. A user namespace\\nisolates the user running inside the container from the one in the host.\\nA process running as root in a container can run as a different (non-root) user in the host; in\\nother words, the process has full privileges for operations inside the user namespace, but is\\nunprivileged for operations outside the namespace.\\nYou can use this feature to reduce the damage a compromised container can do to the host or\\nother pods in the same node. There are several security vulnerabilities  rated either HIGH  or \\nCRITICAL  that were not exploitable when user namespaces is active. It is expected user\\nnamespace will mitigate some future vulnerabilities too.\\nBefore you begin\\nNote:  This section links to third party projects that provide functionality required by\\nKubernetes. The Kubernetes project authors aren't responsible for these projects, which are\\nlisted alphabetically. To add a project to this list, read the content guide  before submitting a\\nchange. More information.\\nThis is a Linux-only feature and support is needed in Linux for idmap mounts on the\\nfilesystems used. This means:\\nOn the node, the filesystem you use for /var/lib/kubelet/pods/ , or the custom directory\\nyou configure for this, needs idmap mount support.\\nAll the filesystems used in the pod's volumes must support idmap mounts.\\nIn practice this means you need at least Linux 6.3, as tmpfs started supporting idmap mounts in\\nthat version. This is usually needed as several Kubernetes features use tmpfs (the service\\naccount token that is mounted by default uses a tmpfs, Secrets use a tmpfs, etc.)\\nSome popular filesystems that support idmap mounts in Linux 6.3 are: btrfs, ext4, xfs, fat, tmpfs,\\noverlayfs.\\nIn addition, support is needed in the container runtime  to use this feature with Kubernetes\\npods:\\nCRI-O: version 1.25 (and later) supports user namespaces for containers.\\ncontainerd v1.7 is not compatible with the userns support in Kubernetes v1.27 to v1.28.\\nKubernetes v1.25 and v1.26 used an earlier implementation that is compatible with containerd\\nv1.7, in terms of userns support. If you are using a version of Kubernetes other than 1.28, check\\nthe documentation for that version of Kubernetes for the most relevant information. If there is a\\nnewer release of containerd than v1.7 available for use, also check the containerd\\ndocumentation for compatibility information.• \\n• \\n• \\n• \\n•\", metadata={'source': './PDFS/Concepts.pdf', 'page': 116}),\n",
       " Document(page_content=\"You can see the status of user namespaces support in cri-dockerd tracked in an issue  on GitHub.\\nIntroduction\\nUser namespaces is a Linux feature that allows to map users in the container to different users\\nin the host. Furthermore, the capabilities granted to a pod in a user namespace are valid only in\\nthe namespace and void outside of it.\\nA pod can opt-in to use user namespaces by setting the pod.spec.hostUsers  field to false.\\nThe kubelet will pick host UIDs/GIDs a pod is mapped to, and will do so in a way to guarantee\\nthat no two pods on the same node use the same mapping.\\nThe runAsUser , runAsGroup , fsGroup , etc. fields in the pod.spec  always refer to the user inside\\nthe container.\\nThe valid UIDs/GIDs when this feature is enabled is the range 0-65535. This applies to files and\\nprocesses ( runAsUser , runAsGroup , etc.).\\nFiles using a UID/GID outside this range will be seen as belonging to the overflow ID, usually\\n65534 (configured in /proc/sys/kernel/overflowuid  and /proc/sys/kernel/overflowgid ). However,\\nit is not possible to modify those files, even by running as the 65534 user/group.\\nMost applications that need to run as root but don't access other host namespaces or resources,\\nshould continue to run fine without any changes needed if user namespaces is activated.\\nUnderstanding user namespaces for pods\\nSeveral container runtimes with their default configuration (like Docker Engine, containerd,\\nCRI-O) use Linux namespaces for isolation. Other technologies exist and can be used with those\\nruntimes too (e.g. Kata Containers uses VMs instead of Linux namespaces). This page is\\napplicable for container runtimes using Linux namespaces for isolation.\\nWhen creating a pod, by default, several new namespaces are used for isolation: a network\\nnamespace to isolate the network of the container, a PID namespace to isolate the view of\\nprocesses, etc. If a user namespace is used, this will isolate the users in the container from the\\nusers in the node.\\nThis means containers can run as root and be mapped to a non-root user on the host. Inside the\\ncontainer the process will think it is running as root (and therefore tools like apt, yum, etc.\\nwork fine), while in reality the process doesn't have privileges on the host. You can verify this,\\nfor example, if you check which user the container process is running by executing ps aux  from\\nthe host. The user ps shows is not the same as the user you see if you execute inside the\\ncontainer the command id.\\nThis abstraction limits what can happen, for example, if the container manages to escape to the\\nhost. Given that the container is running as a non-privileged user on the host, it is limited what\\nit can do to the host.\\nFurthermore, as users on each pod will be mapped to different non-overlapping users in the\\nhost, it is limited what they can do to other pods too.\", metadata={'source': './PDFS/Concepts.pdf', 'page': 117}),\n",
       " Document(page_content=\"Capabilities granted to a pod are also limited to the pod user namespace and mostly invalid out\\nof it, some are even completely void. Here are two examples:\\nCAP_SYS_MODULE  does not have any effect if granted to a pod using user namespaces,\\nthe pod isn't able to load kernel modules.\\nCAP_SYS_ADMIN  is limited to the pod's user namespace and invalid outside of it.\\nWithout using a user namespace a container running as root, in the case of a container\\nbreakout, has root privileges on the node. And if some capability were granted to the container,\\nthe capabilities are valid on the host too. None of this is true when we use user namespaces.\\nIf you want to know more details about what changes when user namespaces are in use, see \\nman 7 user_namespaces .\\nSet up a node to support user namespaces\\nIt is recommended that the host's files and host's processes use UIDs/GIDs in the range of\\n0-65535.\\nThe kubelet will assign UIDs/GIDs higher than that to pods. Therefore, to guarantee as much\\nisolation as possible, the UIDs/GIDs used by the host's files and host's processes should be in\\nthe range 0-65535.\\nNote that this recommendation is important to mitigate the impact of CVEs like \\nCVE-2021-25741 , where a pod can potentially read arbitrary files in the hosts. If the UIDs/GIDs\\nof the pod and the host don't overlap, it is limited what a pod would be able to do: the pod UID/\\nGID won't match the host's file owner/group.\\nLimitations\\nWhen using a user namespace for the pod, it is disallowed to use other host namespaces. In\\nparticular, if you set hostUsers: false  then you are not allowed to set any of:\\nhostNetwork: true\\nhostIPC: true\\nhostPID: true\\nWhat's next\\nTake a look at Use a User Namespace With a Pod\\nDownward API\\nThere are two ways to expose Pod and container fields to a running container: environment\\nvariables, and as files that are populated by a special volume type. Together, these two ways of\\nexposing Pod and container fields are called the downward API.\\nIt is sometimes useful for a container to have information about itself, without being overly\\ncoupled to Kubernetes. The downward API  allows containers to consume information about\\nthemselves or the cluster without using the Kubernetes client or API server.• \\n• \\n• \\n• \\n• \\n•\", metadata={'source': './PDFS/Concepts.pdf', 'page': 118}),\n",
       " Document(page_content=\"An example is an existing application that assumes a particular well-known environment\\nvariable holds a unique identifier. One possibility is to wrap the application, but that is tedious\\nand error-prone, and it violates the goal of low coupling. A better option would be to use the\\nPod's name as an identifier, and inject the Pod's name into the well-known environment\\nvariable.\\nIn Kubernetes, there are two ways to expose Pod and container fields to a running container:\\nas environment variables\\nas files in a downwardAPI  volume\\nTogether, these two ways of exposing Pod and container fields are called the downward API .\\nAvailable fields\\nOnly some Kubernetes API fields are available through the downward API. This section lists\\nwhich fields you can make available.\\nYou can pass information from available Pod-level fields using fieldRef . At the API level, the \\nspec for a Pod always defines at least one Container . You can pass information from available\\nContainer-level fields using resourceFieldRef .\\nInformation available via fieldRef\\nFor some Pod-level fields, you can provide them to a container either as an environment\\nvariable or using a downwardAPI  volume. The fields available via either mechanism are:\\nmetadata.name\\nthe pod's name\\nmetadata.namespace\\nthe pod's namespace\\nmetadata.uid\\nthe pod's unique ID\\nmetadata.annotations['<KEY>']\\nthe value of the pod's annotation  named <KEY>  (for example, \\nmetadata.annotations['myannotation'] )\\nmetadata.labels['<KEY>']\\nthe text value of the pod's label  named <KEY>  (for example, metadata.labels['mylabel'] )\\nThe following information is available through environment variables but not as a\\ndownwardAPI volume fieldRef :\\nspec.serviceAccountName\\nthe name of the pod's service account\\nspec.nodeName\\nthe name of the node  where the Pod is executing\\nstatus.hostIP\\nthe primary IP address of the node to which the Pod is assigned\\nstatus.hostIPs\\nthe IP addresses is a dual-stack version of status.hostIP , the first is always the same as \\nstatus.hostIP . The field is available if you enable the PodHostIPs  feature gate .\\nstatus.podIP\\nthe pod's primary IP address (usually, its IPv4 address)• \\n•\", metadata={'source': './PDFS/Concepts.pdf', 'page': 119}),\n",
       " Document(page_content='status.podIPs\\nthe IP addresses is a dual-stack version of status.podIP , the first is always the same as \\nstatus.podIP\\nThe following information is available through a downwardAPI  volume fieldRef , but not as\\nenvironment variables :\\nmetadata.labels\\nall of the pod\\'s labels, formatted as label-key=\"escaped-label-value\"  with one label per line\\nmetadata.annotations\\nall of the pod\\'s annotations, formatted as annotation-key=\"escaped-annotation-value\"\\nwith one annotation per line\\nInformation available via resourceFieldRef\\nThese container-level fields allow you to provide information about requests and limits  for\\nresources such as CPU and memory.\\nresource: limits.cpu\\nA container\\'s CPU limit\\nresource: requests.cpu\\nA container\\'s CPU request\\nresource: limits.memory\\nA container\\'s memory limit\\nresource: requests.memory\\nA container\\'s memory request\\nresource: limits.hugepages-*\\nA container\\'s hugepages limit\\nresource: requests.hugepages-*\\nA container\\'s hugepages request\\nresource: limits.ephemeral-storage\\nA container\\'s ephemeral-storage limit\\nresource: requests.ephemeral-storage\\nA container\\'s ephemeral-storage request\\nFallback information for resource limits\\nIf CPU and memory limits are not specified for a container, and you use the downward API to\\ntry to expose that information, then the kubelet defaults to exposing the maximum allocatable\\nvalue for CPU and memory based on the node allocatable  calculation.\\nWhat\\'s next\\nYou can read about downwardAPI  volumes .\\nYou can try using the downward API to expose container- or Pod-level information:\\nas environment variables\\nas files in downwardAPI  volume• \\n•', metadata={'source': './PDFS/Concepts.pdf', 'page': 120}),\n",
       " Document(page_content=\"Workload Resources\\nKubernetes provides several built-in APIs for declarative management of your workloads  and\\nthe components of those workloads.\\nUltimately, your applications run as containers inside Pods ; however, managing individual Pods\\nwould be a lot of effort. For example, if a Pod fails, you probably want to run a new Pod to\\nreplace it. Kubernetes can do that for you.\\nYou use the Kubernetes API to create a workload object  that represents a higher abstraction\\nlevel than a Pod, and then the Kubernetes control plane  automatically manages Pod objects on\\nyour behalf, based on the specification for the workload object you defined.\\nThe built-in APIs for managing workloads are:\\nDeployment  (and, indirectly, ReplicaSet ), the most common way to run an application on your\\ncluster. Deployment is a good fit for managing a stateless application workload on your cluster,\\nwhere any Pod in the Deployment is interchangeable and can be replaced if needed.\\n(Deployments are a replacement for the legacy ReplicationController  API).\\nA StatefulSet  lets you manage one or more Pods – all running the same application code –\\nwhere the Pods rely on having a distinct identity. This is different from a Deployment where the\\nPods are expected to be interchangeable. The most common use for a StatefulSet is to be able to\\nmake a link between its Pods and their persistent storage. For example, you can run a\\nStatefulSet that associates each Pod with a PersistentVolume . If one of the Pods in the\\nStatefulSet fails, Kubernetes makes a replacement Pod that is connected to the same\\nPersistentVolume.\\nA DaemonSet  defines Pods that provide facilities that are local to a specific node ; for example, a\\ndriver that lets containers on that node access a storage system. You use a DaemonSet when the\\ndriver, or other node-level service, has to run on the node where it's useful. Each Pod in a\\nDaemonSet performs a role similar to a system daemon on a classic Unix / POSIX server. A\\nDaemonSet might be fundamental to the operation of your cluster, such as a plugin to let that\\nnode access cluster networking , it might help you to manage the node, or it could provide less\\nessential facilities that enhance the container platform you are running. You can run\\nDaemonSets (and their pods) across every node in your cluster, or across just a subset (for\\nexample, only install the GPU accelerator driver on nodes that have a GPU installed).\\nYou can use a Job and / or a CronJob  to define tasks that run to completion and then stop. A Job\\nrepresents a one-off task, whereas each CronJob repeats according to a schedule.\\nOther topics in this section:\\nAutomatic Cleanup for Finished Jobs\\nReplicationController\\nDeployments\\nA Deployment manages a set of Pods to run an application workload, usually one that doesn't\\nmaintain state.\\nA Deployment  provides declarative updates for Pods  and ReplicaSets .• \\n•\", metadata={'source': './PDFS/Concepts.pdf', 'page': 121}),\n",
       " Document(page_content=\"You describe a desired state  in a Deployment, and the Deployment Controller  changes the actual\\nstate to the desired state at a controlled rate. You can define Deployments to create new\\nReplicaSets, or to remove existing Deployments and adopt all their resources with new\\nDeployments.\\nNote:  Do not manage ReplicaSets owned by a Deployment. Consider opening an issue in the\\nmain Kubernetes repository if your use case is not covered below.\\nUse Case\\nThe following are typical use cases for Deployments:\\nCreate a Deployment to rollout a ReplicaSet . The ReplicaSet creates Pods in the\\nbackground. Check the status of the rollout to see if it succeeds or not.\\nDeclare the new state of the Pods  by updating the PodTemplateSpec of the Deployment.\\nA new ReplicaSet is created and the Deployment manages moving the Pods from the old\\nReplicaSet to the new one at a controlled rate. Each new ReplicaSet updates the revision\\nof the Deployment.\\nRollback to an earlier Deployment revision  if the current state of the Deployment is not\\nstable. Each rollback updates the revision of the Deployment.\\nScale up the Deployment to facilitate more load .\\nPause the rollout of a Deployment  to apply multiple fixes to its PodTemplateSpec and\\nthen resume it to start a new rollout.\\nUse the status of the Deployment  as an indicator that a rollout has stuck.\\nClean up older ReplicaSets  that you don't need anymore.\\nCreating a Deployment\\nThe following is an example of a Deployment. It creates a ReplicaSet to bring up three nginx\\nPods:\\ncontrollers/nginx-deployment.yaml  \\napiVersion : apps/v1\\nkind: Deployment\\nmetadata :\\n  name : nginx-deployment\\n  labels :\\n    app: nginx\\nspec:\\n  replicas : 3\\n  selector :\\n    matchLabels :\\n      app: nginx\\n  template :\\n    metadata :\\n      labels :\\n        app: nginx\\n    spec:\\n      containers :\\n      - name : nginx\\n        image : nginx:1.14.2• \\n• \\n• \\n• \\n• \\n• \\n•\", metadata={'source': './PDFS/Concepts.pdf', 'page': 122}),\n",
       " Document(page_content='ports :\\n        - containerPort : 80\\nIn this example:\\nA Deployment named nginx-deployment  is created, indicated by the .metadata.name\\nfield. This name will become the basis for the ReplicaSets and Pods which are created\\nlater. See Writing a Deployment Spec  for more details.\\nThe Deployment creates a ReplicaSet that creates three replicated Pods, indicated by the\\n.spec.replicas  field.\\nThe .spec.selector  field defines how the created ReplicaSet finds which Pods to manage. In\\nthis case, you select a label that is defined in the Pod template ( app: nginx ). However,\\nmore sophisticated selection rules are possible, as long as the Pod template itself satisfies\\nthe rule.\\nNote:  The .spec.selector.matchLabels  field is a map of {key,value} pairs. A single\\n{key,value} in the matchLabels  map is equivalent to an element of matchExpressions ,\\nwhose key field is \"key\", the operator  is \"In\", and the values  array contains only \"value\".\\nAll of the requirements, from both matchLabels  and matchExpressions , must be satisfied\\nin order to match.\\nThe template  field contains the following sub-fields:\\nThe Pods are labeled app: nginx using the .metadata.labels  field.\\nThe Pod template\\'s specification, or .template.spec  field, indicates that the Pods run\\none container, nginx , which runs the nginx  Docker Hub  image at version 1.14.2.\\nCreate one container and name it nginx  using\\nthe .spec.template.spec.containers[0].name  field.\\nBefore you begin, make sure your Kubernetes cluster is up and running. Follow the steps given\\nbelow to create the above Deployment:\\nCreate the Deployment by running the following command:\\nkubectl apply -f https://k8s.io/examples/controllers/nginx-deployment.yaml\\nRun kubectl get deployments  to check if the Deployment was created.\\nIf the Deployment is still being created, the output is similar to the following:\\nNAME               READY   UP-TO-DATE   AVAILABLE   AGE\\nnginx-deployment   0/3     0            0           1s\\nWhen you inspect the Deployments in your cluster, the following fields are displayed:\\nNAME  lists the names of the Deployments in the namespace.\\nREADY  displays how many replicas of the application are available to your users. It\\nfollows the pattern ready/desired.\\nUP-TO-DATE  displays the number of replicas that have been updated to achieve\\nthe desired state.\\nAVAILABLE  displays how many replicas of the application are available to your\\nusers.\\nAGE  displays the amount of time that the application has been running.• \\n• \\n• \\n• \\n◦ \\n◦ \\n◦ \\n1. \\n2. \\n◦ \\n◦ \\n◦ \\n◦ \\n◦', metadata={'source': './PDFS/Concepts.pdf', 'page': 123}),\n",
       " Document(page_content='Notice how the number of desired replicas is 3 according to .spec.replicas  field.\\nTo see the Deployment rollout status, run kubectl rollout status deployment/nginx-\\ndeployment .\\nThe output is similar to:\\nWaiting for rollout to finish: 2 out of 3 new replicas have been updated...\\ndeployment \"nginx-deployment\" successfully rolled out\\nRun the kubectl get deployments  again a few seconds later. The output is similar to this:\\nNAME               READY   UP-TO-DATE   AVAILABLE   AGE\\nnginx-deployment   3/3     3            3           18s\\nNotice that the Deployment has created all three replicas, and all replicas are up-to-date\\n(they contain the latest Pod template) and available.\\nTo see the ReplicaSet ( rs) created by the Deployment, run kubectl get rs . The output is\\nsimilar to this:\\nNAME                          DESIRED   CURRENT   READY   AGE\\nnginx-deployment-75675f5897   3         3         3       18s\\nReplicaSet output shows the following fields:\\nNAME  lists the names of the ReplicaSets in the namespace.\\nDESIRED  displays the desired number of replicas  of the application, which you\\ndefine when you create the Deployment. This is the desired state .\\nCURRENT  displays how many replicas are currently running.\\nREADY  displays how many replicas of the application are available to your users.\\nAGE  displays the amount of time that the application has been running.\\nNotice that the name of the ReplicaSet is always formatted as [DEPLOYMENT-NAME]-\\n[HASH] . This name will become the basis for the Pods which are created.\\nThe HASH  string is the same as the pod-template-hash  label on the ReplicaSet.\\nTo see the labels automatically generated for each Pod, run kubectl get pods --show-\\nlabels . The output is similar to:\\nNAME                                READY     STATUS    RESTARTS   AGE       LABELS\\nnginx-deployment-75675f5897-7ci7o   1/1       Running   0          18s       app=nginx,pod-\\ntemplate-hash=75675f5897\\nnginx-deployment-75675f5897-kzszj   1/1       Running   0          18s       app=nginx,pod-\\ntemplate-hash=75675f5897\\nnginx-deployment-75675f5897-qqcnn   1/1       Running   0          18s       app=nginx,pod-\\ntemplate-hash=75675f5897\\nThe created ReplicaSet ensures that there are three nginx  Pods.\\nNote:\\nYou must specify an appropriate selector and Pod template labels in a Deployment (in this case, \\napp: nginx ).3. \\n4. \\n5. \\n◦ \\n◦ \\n◦ \\n◦ \\n◦ \\n6.', metadata={'source': './PDFS/Concepts.pdf', 'page': 124}),\n",
       " Document(page_content=\"Do not overlap labels or selectors with other controllers (including other Deployments and\\nStatefulSets). Kubernetes doesn't stop you from overlapping, and if multiple controllers have\\noverlapping selectors those controllers might conflict and behave unexpectedly.\\nPod-template-hash label\\nCaution:  Do not change this label.\\nThe pod-template-hash  label is added by the Deployment controller to every ReplicaSet that a\\nDeployment creates or adopts.\\nThis label ensures that child ReplicaSets of a Deployment do not overlap. It is generated by\\nhashing the PodTemplate  of the ReplicaSet and using the resulting hash as the label value that\\nis added to the ReplicaSet selector, Pod template labels, and in any existing Pods that the\\nReplicaSet might have.\\nUpdating a Deployment\\nNote:  A Deployment's rollout is triggered if and only if the Deployment's Pod template (that\\nis, .spec.template ) is changed, for example if the labels or container images of the template are\\nupdated. Other updates, such as scaling the Deployment, do not trigger a rollout.\\nFollow the steps given below to update your Deployment:\\nLet's update the nginx Pods to use the nginx:1.16.1  image instead of the nginx:1.14.2\\nimage.\\nkubectl set image deployment.v1.apps/nginx-deployment nginx =nginx:1.16.1\\nor use the following command:\\nkubectl set image deployment/nginx-deployment nginx =nginx:1.16.1\\nwhere deployment/nginx-deployment  indicates the Deployment, nginx  indicates the\\nContainer the update will take place and nginx:1.16.1  indicates the new image and its tag.\\nThe output is similar to:\\ndeployment.apps/nginx-deployment image updated\\nAlternatively, you can edit the Deployment and\\nchange .spec.template.spec.containers[0].image  from nginx:1.14.2  to nginx:1.16.1 :\\nkubectl edit deployment/nginx-deployment\\nThe output is similar to:\\ndeployment.apps/nginx-deployment edited\\nTo see the rollout status, run:\\nkubectl rollout status deployment/nginx-deployment\\nThe output is similar to this:1. \\n2.\", metadata={'source': './PDFS/Concepts.pdf', 'page': 125}),\n",
       " Document(page_content='Waiting for rollout to finish: 2 out of 3 new replicas have been updated...\\nor\\ndeployment \"nginx-deployment\" successfully rolled out\\nGet more details on your updated Deployment:\\nAfter the rollout succeeds, you can view the Deployment by running kubectl get \\ndeployments . The output is similar to this:\\nNAME               READY   UP-TO-DATE   AVAILABLE   AGE\\nnginx-deployment   3/3     3            3           36s\\nRun kubectl get rs  to see that the Deployment updated the Pods by creating a new\\nReplicaSet and scaling it up to 3 replicas, as well as scaling down the old ReplicaSet to 0\\nreplicas.\\nkubectl get rs\\nThe output is similar to this:\\nNAME                          DESIRED   CURRENT   READY   AGE\\nnginx-deployment-1564180365   3         3         3       6s\\nnginx-deployment-2035384211   0         0         0       36s\\nRunning get pods  should now show only the new Pods:\\nkubectl get pods\\nThe output is similar to this:\\nNAME                                READY     STATUS    RESTARTS   AGE\\nnginx-deployment-1564180365-khku8   1/1       Running   0          14s\\nnginx-deployment-1564180365-nacti   1/1       Running   0          14s\\nnginx-deployment-1564180365-z9gth   1/1       Running   0          14s\\nNext time you want to update these Pods, you only need to update the Deployment\\'s Pod\\ntemplate again.\\nDeployment ensures that only a certain number of Pods are down while they are being\\nupdated. By default, it ensures that at least 75% of the desired number of Pods are up (25%\\nmax unavailable).\\nDeployment also ensures that only a certain number of Pods are created above the\\ndesired number of Pods. By default, it ensures that at most 125% of the desired number of\\nPods are up (25% max surge).\\nFor example, if you look at the above Deployment closely, you will see that it first creates\\na new Pod, then deletes an old Pod, and creates another new one. It does not kill old Pods\\nuntil a sufficient number of new Pods have come up, and does not create new Pods until a\\nsufficient number of old Pods have been killed. It makes sure that at least 3 Pods are\\navailable and that at max 4 Pods in total are available. In case of a Deployment with 4\\nreplicas, the number of Pods would be between 3 and 5.• \\n• \\n•', metadata={'source': './PDFS/Concepts.pdf', 'page': 126}),\n",
       " Document(page_content='Get details of your Deployment:\\nkubectl describe deployments\\nThe output is similar to this:\\nName:                   nginx-deployment\\nNamespace:              default\\nCreationTimestamp:      Thu, 30 Nov 2017 10:56:25 +0000\\nLabels:                 app=nginx\\nAnnotations:            deployment.kubernetes.io/revision=2\\nSelector:               app=nginx\\nReplicas:               3 desired | 3 updated | 3 total | 3 available | 0 unavailable\\nStrategyType:           RollingUpdate\\nMinReadySeconds:        0\\nRollingUpdateStrategy:  25% max unavailable, 25% max surge\\nPod Template:\\n  Labels:  app=nginx\\n   Containers:\\n    nginx:\\n      Image:        nginx:1.16.1\\n      Port:         80/TCP\\n      Environment:  <none>\\n      Mounts:       <none>\\n    Volumes:        <none>\\n  Conditions:\\n    Type           Status  Reason\\n    ----           ------  ------\\n    Available      True    MinimumReplicasAvailable\\n    Progressing    True    NewReplicaSetAvailable\\n  OldReplicaSets:  <none>\\n  NewReplicaSet:   nginx-deployment-1564180365 (3/3 replicas created)\\n  Events:\\n    Type    Reason             Age   From                   Message\\n    ----    ------             ----  ----                   -------\\n    Normal  ScalingReplicaSet  2m    deployment-controller  Scaled up replica set nginx-\\ndeployment-2035384211 to 3\\n    Normal  ScalingReplicaSet  24s   deployment-controller  Scaled up replica set nginx-\\ndeployment-1564180365 to 1\\n    Normal  ScalingReplicaSet  22s   deployment-controller  Scaled down replica set nginx-\\ndeployment-2035384211 to 2\\n    Normal  ScalingReplicaSet  22s   deployment-controller  Scaled up replica set nginx-\\ndeployment-1564180365 to 2\\n    Normal  ScalingReplicaSet  19s   deployment-controller  Scaled down replica set nginx-\\ndeployment-2035384211 to 1\\n    Normal  ScalingReplicaSet  19s   deployment-controller  Scaled up replica set nginx-\\ndeployment-1564180365 to 3\\n    Normal  ScalingReplicaSet  14s   deployment-controller  Scaled down replica set nginx-\\ndeployment-2035384211 to 0\\nHere you see that when you first created the Deployment, it created a ReplicaSet (nginx-\\ndeployment-2035384211) and scaled it up to 3 replicas directly. When you updated the\\nDeployment, it created a new ReplicaSet (nginx-deployment-1564180365) and scaled it up•', metadata={'source': './PDFS/Concepts.pdf', 'page': 127}),\n",
       " Document(page_content=\"to 1 and waited for it to come up. Then it scaled down the old ReplicaSet to 2 and scaled\\nup the new ReplicaSet to 2 so that at least 3 Pods were available and at most 4 Pods were\\ncreated at all times. It then continued scaling up and down the new and the old\\nReplicaSet, with the same rolling update strategy. Finally, you'll have 3 available replicas\\nin the new ReplicaSet, and the old ReplicaSet is scaled down to 0.\\nNote:  Kubernetes doesn't count terminating Pods when calculating the number of \\navailableReplicas , which must be between replicas - maxUnavailable  and replicas + maxSurge .\\nAs a result, you might notice that there are more Pods than expected during a rollout, and that\\nthe total resources consumed by the Deployment is more than replicas + maxSurge  until the \\nterminationGracePeriodSeconds  of the terminating Pods expires.\\nRollover (aka multiple updates in-flight)\\nEach time a new Deployment is observed by the Deployment controller, a ReplicaSet is created\\nto bring up the desired Pods. If the Deployment is updated, the existing ReplicaSet that controls\\nPods whose labels match .spec.selector  but whose template does not match .spec.template  are\\nscaled down. Eventually, the new ReplicaSet is scaled to .spec.replicas  and all old ReplicaSets is\\nscaled to 0.\\nIf you update a Deployment while an existing rollout is in progress, the Deployment creates a\\nnew ReplicaSet as per the update and start scaling that up, and rolls over the ReplicaSet that it\\nwas scaling up previously -- it will add it to its list of old ReplicaSets and start scaling it down.\\nFor example, suppose you create a Deployment to create 5 replicas of nginx:1.14.2 , but then\\nupdate the Deployment to create 5 replicas of nginx:1.16.1 , when only 3 replicas of nginx:1.14.2\\nhad been created. In that case, the Deployment immediately starts killing the 3 nginx:1.14.2\\nPods that it had created, and starts creating nginx:1.16.1  Pods. It does not wait for the 5 replicas\\nof nginx:1.14.2  to be created before changing course.\\nLabel selector updates\\nIt is generally discouraged to make label selector updates and it is suggested to plan your\\nselectors up front. In any case, if you need to perform a label selector update, exercise great\\ncaution and make sure you have grasped all of the implications.\\nNote:  In API version apps/v1 , a Deployment's label selector is immutable after it gets created.\\nSelector additions require the Pod template labels in the Deployment spec to be updated\\nwith the new label too, otherwise a validation error is returned. This change is a non-\\noverlapping one, meaning that the new selector does not select ReplicaSets and Pods\\ncreated with the old selector, resulting in orphaning all old ReplicaSets and creating a\\nnew ReplicaSet.\\nSelector updates changes the existing value in a selector key -- result in the same\\nbehavior as additions.\\nSelector removals removes an existing key from the Deployment selector -- do not\\nrequire any changes in the Pod template labels. Existing ReplicaSets are not orphaned,\\nand a new ReplicaSet is not created, but note that the removed label still exists in any\\nexisting Pods and ReplicaSets.• \\n• \\n•\", metadata={'source': './PDFS/Concepts.pdf', 'page': 128}),\n",
       " Document(page_content=\"Rolling Back a Deployment\\nSometimes, you may want to rollback a Deployment; for example, when the Deployment is not\\nstable, such as crash looping. By default, all of the Deployment's rollout history is kept in the\\nsystem so that you can rollback anytime you want (you can change that by modifying revision\\nhistory limit).\\nNote:  A Deployment's revision is created when a Deployment's rollout is triggered. This means\\nthat the new revision is created if and only if the Deployment's Pod template ( .spec.template ) is\\nchanged, for example if you update the labels or container images of the template. Other\\nupdates, such as scaling the Deployment, do not create a Deployment revision, so that you can\\nfacilitate simultaneous manual- or auto-scaling. This means that when you roll back to an\\nearlier revision, only the Deployment's Pod template part is rolled back.\\nSuppose that you made a typo while updating the Deployment, by putting the image\\nname as nginx:1.161  instead of nginx:1.16.1 :\\nkubectl set image deployment/nginx-deployment nginx =nginx:1.161\\nThe output is similar to this:\\ndeployment.apps/nginx-deployment image updated\\nThe rollout gets stuck. You can verify it by checking the rollout status:\\nkubectl rollout status deployment/nginx-deployment\\nThe output is similar to this:\\nWaiting for rollout to finish: 1 out of 3 new replicas have been updated...\\nPress Ctrl-C to stop the above rollout status watch. For more information on stuck\\nrollouts, read more here .\\nYou see that the number of old replicas (adding the replica count from nginx-\\ndeployment-1564180365  and nginx-deployment-2035384211 ) is 3, and the number of new\\nreplicas (from nginx-deployment-3066724191 ) is 1.\\nkubectl get rs\\nThe output is similar to this:\\nNAME                          DESIRED   CURRENT   READY   AGE\\nnginx-deployment-1564180365   3         3         3       25s\\nnginx-deployment-2035384211   0         0         0       36s\\nnginx-deployment-3066724191   1         1         0       6s\\nLooking at the Pods created, you see that 1 Pod created by new ReplicaSet is stuck in an\\nimage pull loop.\\nkubectl get pods\\nThe output is similar to this:• \\n• \\n• \\n• \\n•\", metadata={'source': './PDFS/Concepts.pdf', 'page': 129}),\n",
       " Document(page_content='NAME                                READY     STATUS             RESTARTS   AGE\\nnginx-deployment-1564180365-70iae   1/1       Running            0          25s\\nnginx-deployment-1564180365-jbqqo   1/1       Running            0          25s\\nnginx-deployment-1564180365-hysrc   1/1       Running            0          25s\\nnginx-deployment-3066724191-08mng   0/1       ImagePullBackOff   0          6s\\nNote:  The Deployment controller stops the bad rollout automatically, and stops scaling\\nup the new ReplicaSet. This depends on the rollingUpdate parameters ( maxUnavailable\\nspecifically) that you have specified. Kubernetes by default sets the value to 25%.\\nGet the description of the Deployment:\\nkubectl describe deployment\\nThe output is similar to this:\\nName:           nginx-deployment\\nNamespace:      default\\nCreationTimestamp:  Tue, 15 Mar 2016 14:48:04 -0700\\nLabels:         app=nginx\\nSelector:       app=nginx\\nReplicas:       3 desired | 1 updated | 4 total | 3 available | 1 unavailable\\nStrategyType:       RollingUpdate\\nMinReadySeconds:    0\\nRollingUpdateStrategy:  25% max unavailable, 25% max surge\\nPod Template:\\n  Labels:  app=nginx\\n  Containers:\\n   nginx:\\n    Image:        nginx:1.161\\n    Port:         80/TCP\\n    Host Port:    0/TCP\\n    Environment:  <none>\\n    Mounts:       <none>\\n  Volumes:        <none>\\nConditions:\\n  Type           Status  Reason\\n  ----           ------  ------\\n  Available      True    MinimumReplicasAvailable\\n  Progressing    True    ReplicaSetUpdated\\nOldReplicaSets:     nginx-deployment-1564180365 (3/3 replicas created)\\nNewReplicaSet:      nginx-deployment-3066724191 (1/1 replicas created)\\nEvents:\\n  FirstSeen LastSeen    Count   From                    SubObjectPath   Type        Reason              \\nMessage\\n  --------- --------    -----   ----                    -------------   --------    ------              -------\\n  1m        1m          1       {deployment-controller }                Normal      ScalingReplicaSet   \\nScaled up replica set nginx-deployment-2035384211 to 3\\n  22s       22s         1       {deployment-controller }                Normal      ScalingReplicaSet   \\nScaled up replica set nginx-deployment-1564180365 to 1\\n  22s       22s         1       {deployment-controller }                Normal      ScalingReplicaSet   \\nScaled down replica set nginx-deployment-2035384211 to 2\\n  22s       22s         1       {deployment-controller }                Normal      ScalingReplicaSet   •', metadata={'source': './PDFS/Concepts.pdf', 'page': 130}),\n",
       " Document(page_content='Scaled up replica set nginx-deployment-1564180365 to 2\\n  21s       21s         1       {deployment-controller }                Normal      ScalingReplicaSet   \\nScaled down replica set nginx-deployment-2035384211 to 1\\n  21s       21s         1       {deployment-controller }                Normal      ScalingReplicaSet   \\nScaled up replica set nginx-deployment-1564180365 to 3\\n  13s       13s         1       {deployment-controller }                Normal      ScalingReplicaSet   \\nScaled down replica set nginx-deployment-2035384211 to 0\\n  13s       13s         1       {deployment-controller }                Normal      ScalingReplicaSet   \\nScaled up replica set nginx-deployment-3066724191 to 1\\nTo fix this, you need to rollback to a previous revision of Deployment that is stable.\\nChecking Rollout History of a Deployment\\nFollow the steps given below to check the rollout history:\\nFirst, check the revisions of this Deployment:\\nkubectl rollout history  deployment/nginx-deployment\\nThe output is similar to this:\\ndeployments \"nginx-deployment\"\\nREVISION    CHANGE-CAUSE\\n1           kubectl apply --filename=https://k8s.io/examples/controllers/nginx-\\ndeployment.yaml\\n2           kubectl set image deployment/nginx-deployment nginx=nginx:1.16.1\\n3           kubectl set image deployment/nginx-deployment nginx=nginx:1.161\\nCHANGE-CAUSE  is copied from the Deployment annotation kubernetes.io/change-cause\\nto its revisions upon creation. You can specify the CHANGE-CAUSE  message by:\\nAnnotating the Deployment with kubectl annotate deployment/nginx-deployment \\nkubernetes.io/change-cause=\"image updated to 1.16.1\"\\nManually editing the manifest of the resource.\\nTo see the details of each revision, run:\\nkubectl rollout history  deployment/nginx-deployment --revision =2\\nThe output is similar to this:\\ndeployments \"nginx-deployment\" revision 2\\n  Labels:       app=nginx\\n          pod-template-hash=1159050644\\n  Annotations:  kubernetes.io/change-cause=kubectl set image deployment/nginx-\\ndeployment nginx=nginx:1.16.1\\n  Containers:\\n   nginx:\\n    Image:      nginx:1.16.1\\n    Port:       80/TCP\\n     QoS Tier:\\n        cpu:      BestEffort\\n        memory:   BestEffort1. \\n◦ \\n◦ \\n2.', metadata={'source': './PDFS/Concepts.pdf', 'page': 131}),\n",
       " Document(page_content=\"Environment Variables:      <none>\\n  No volumes.\\nRolling Back to a Previous Revision\\nFollow the steps given below to rollback the Deployment from the current version to the\\nprevious version, which is version 2.\\nNow you've decided to undo the current rollout and rollback to the previous revision:\\nkubectl rollout undo deployment/nginx-deployment\\nThe output is similar to this:\\ndeployment.apps/nginx-deployment rolled back\\nAlternatively, you can rollback to a specific revision by specifying it with --to-revision :\\nkubectl rollout undo deployment/nginx-deployment --to-revision =2\\nThe output is similar to this:\\ndeployment.apps/nginx-deployment rolled back\\nFor more details about rollout related commands, read kubectl rollout .\\nThe Deployment is now rolled back to a previous stable revision. As you can see, a \\nDeploymentRollback  event for rolling back to revision 2 is generated from Deployment\\ncontroller.\\nCheck if the rollback was successful and the Deployment is running as expected, run:\\nkubectl get deployment nginx-deployment\\nThe output is similar to this:\\nNAME               READY   UP-TO-DATE   AVAILABLE   AGE\\nnginx-deployment   3/3     3            3           30m\\nGet the description of the Deployment:\\nkubectl describe deployment nginx-deployment\\nThe output is similar to this:\\nName:                   nginx-deployment\\nNamespace:              default\\nCreationTimestamp:      Sun, 02 Sep 2018 18:17:55 -0500\\nLabels:                 app=nginx\\nAnnotations:            deployment.kubernetes.io/revision=4\\n                        kubernetes.io/change-cause=kubectl set image deployment/nginx-\\ndeployment nginx=nginx:1.16.1\\nSelector:               app=nginx\\nReplicas:               3 desired | 3 updated | 3 total | 3 available | 0 unavailable\\nStrategyType:           RollingUpdate1. \\n2. \\n3.\", metadata={'source': './PDFS/Concepts.pdf', 'page': 132}),\n",
       " Document(page_content='MinReadySeconds:        0\\nRollingUpdateStrategy:  25% max unavailable, 25% max surge\\nPod Template:\\n  Labels:  app=nginx\\n  Containers:\\n   nginx:\\n    Image:        nginx:1.16.1\\n    Port:         80/TCP\\n    Host Port:    0/TCP\\n    Environment:  <none>\\n    Mounts:       <none>\\n  Volumes:        <none>\\nConditions:\\n  Type           Status  Reason\\n  ----           ------  ------\\n  Available      True    MinimumReplicasAvailable\\n  Progressing    True    NewReplicaSetAvailable\\nOldReplicaSets:  <none>\\nNewReplicaSet:   nginx-deployment-c4747d96c (3/3 replicas created)\\nEvents:\\n  Type    Reason              Age   From                   Message\\n  ----    ------              ----  ----                   -------\\n  Normal  ScalingReplicaSet   12m   deployment-controller  Scaled up replica set nginx-\\ndeployment-75675f5897 to 3\\n  Normal  ScalingReplicaSet   11m   deployment-controller  Scaled up replica set nginx-\\ndeployment-c4747d96c to 1\\n  Normal  ScalingReplicaSet   11m   deployment-controller  Scaled down replica set nginx-\\ndeployment-75675f5897 to 2\\n  Normal  ScalingReplicaSet   11m   deployment-controller  Scaled up replica set nginx-\\ndeployment-c4747d96c to 2\\n  Normal  ScalingReplicaSet   11m   deployment-controller  Scaled down replica set nginx-\\ndeployment-75675f5897 to 1\\n  Normal  ScalingReplicaSet   11m   deployment-controller  Scaled up replica set nginx-\\ndeployment-c4747d96c to 3\\n  Normal  ScalingReplicaSet   11m   deployment-controller  Scaled down replica set nginx-\\ndeployment-75675f5897 to 0\\n  Normal  ScalingReplicaSet   11m   deployment-controller  Scaled up replica set nginx-\\ndeployment-595696685f to 1\\n  Normal  DeploymentRollback  15s   deployment-controller  Rolled back deployment \\n\"nginx-deployment\" to revision 2\\n  Normal  ScalingReplicaSet   15s   deployment-controller  Scaled down replica set nginx-\\ndeployment-595696685f to 0\\nScaling a Deployment\\nYou can scale a Deployment by using the following command:\\nkubectl scale deployment/nginx-deployment --replicas =10\\nThe output is similar to this:\\ndeployment.apps/nginx-deployment scaled', metadata={'source': './PDFS/Concepts.pdf', 'page': 133}),\n",
       " Document(page_content=\"Assuming horizontal Pod autoscaling  is enabled in your cluster, you can set up an autoscaler for\\nyour Deployment and choose the minimum and maximum number of Pods you want to run\\nbased on the CPU utilization of your existing Pods.\\nkubectl autoscale deployment/nginx-deployment --min =10 --max =15 --cpu-percent =80\\nThe output is similar to this:\\ndeployment.apps/nginx-deployment scaled\\nProportional scaling\\nRollingUpdate Deployments support running multiple versions of an application at the same\\ntime. When you or an autoscaler scales a RollingUpdate Deployment that is in the middle of a\\nrollout (either in progress or paused), the Deployment controller balances the additional\\nreplicas in the existing active ReplicaSets (ReplicaSets with Pods) in order to mitigate risk. This\\nis called proportional scaling .\\nFor example, you are running a Deployment with 10 replicas, maxSurge =3, and \\nmaxUnavailable =2.\\nEnsure that the 10 replicas in your Deployment are running.\\nkubectl get deploy\\nThe output is similar to this:\\nNAME                 DESIRED   CURRENT   UP-TO-DATE   AVAILABLE   AGE\\nnginx-deployment     10        10        10           10          50s\\nYou update to a new image which happens to be unresolvable from inside the cluster.\\nkubectl set image deployment/nginx-deployment nginx =nginx:sometag\\nThe output is similar to this:\\ndeployment.apps/nginx-deployment image updated\\nThe image update starts a new rollout with ReplicaSet nginx-deployment-1989198191, but\\nit's blocked due to the maxUnavailable  requirement that you mentioned above. Check out\\nthe rollout status:\\nkubectl get rs\\nThe output is similar to this:\\nNAME                          DESIRED   CURRENT   READY     AGE\\nnginx-deployment-1989198191   5         5         0         9s\\nnginx-deployment-618515232    8         8         8         1m\\nThen a new scaling request for the Deployment comes along. The autoscaler increments\\nthe Deployment replicas to 15. The Deployment controller needs to decide where to add\\nthese new 5 replicas. If you weren't using proportional scaling, all 5 of them would be\\nadded in the new ReplicaSet. With proportional scaling, you spread the additional\\nreplicas across all ReplicaSets. Bigger proportions go to the ReplicaSets with the most• \\n• \\n• \\n•\", metadata={'source': './PDFS/Concepts.pdf', 'page': 134}),\n",
       " Document(page_content=\"replicas and lower proportions go to ReplicaSets with less replicas. Any leftovers are\\nadded to the ReplicaSet with the most replicas. ReplicaSets with zero replicas are not\\nscaled up.\\nIn our example above, 3 replicas are added to the old ReplicaSet and 2 replicas are added to the\\nnew ReplicaSet. The rollout process should eventually move all replicas to the new ReplicaSet,\\nassuming the new replicas become healthy. To confirm this, run:\\nkubectl get deploy\\nThe output is similar to this:\\nNAME                 DESIRED   CURRENT   UP-TO-DATE   AVAILABLE   AGE\\nnginx-deployment     15        18        7            8           7m\\nThe rollout status confirms how the replicas were added to each ReplicaSet.\\nkubectl get rs\\nThe output is similar to this:\\nNAME                          DESIRED   CURRENT   READY     AGE\\nnginx-deployment-1989198191   7         7         0         7m\\nnginx-deployment-618515232    11        11        11        7m\\nPausing and Resuming a rollout of a Deployment\\nWhen you update a Deployment, or plan to, you can pause rollouts for that Deployment before\\nyou trigger one or more updates. When you're ready to apply those changes, you resume\\nrollouts for the Deployment. This approach allows you to apply multiple fixes in between\\npausing and resuming without triggering unnecessary rollouts.\\nFor example, with a Deployment that was created:\\nGet the Deployment details:\\nkubectl get deploy\\nThe output is similar to this:\\nNAME      DESIRED   CURRENT   UP-TO-DATE   AVAILABLE   AGE\\nnginx     3         3         3            3           1m\\nGet the rollout status:\\nkubectl get rs\\nThe output is similar to this:\\nNAME               DESIRED   CURRENT   READY     AGE\\nnginx-2142116321   3         3         3         1m\\nPause by running the following command:• \\n•\", metadata={'source': './PDFS/Concepts.pdf', 'page': 135}),\n",
       " Document(page_content='kubectl rollout pause deployment/nginx-deployment\\nThe output is similar to this:\\ndeployment.apps/nginx-deployment paused\\nThen update the image of the Deployment:\\nkubectl set image deployment/nginx-deployment nginx =nginx:1.16.1\\nThe output is similar to this:\\ndeployment.apps/nginx-deployment image updated\\nNotice that no new rollout started:\\nkubectl rollout history  deployment/nginx-deployment\\nThe output is similar to this:\\ndeployments \"nginx\"\\nREVISION  CHANGE-CAUSE\\n1   <none>\\nGet the rollout status to verify that the existing ReplicaSet has not changed:\\nkubectl get rs\\nThe output is similar to this:\\nNAME               DESIRED   CURRENT   READY     AGE\\nnginx-2142116321   3         3         3         2m\\nYou can make as many updates as you wish, for example, update the resources that will\\nbe used:\\nkubectl set resources deployment/nginx-deployment -c =nginx --limits =cpu=200m,memor\\ny=512Mi\\nThe output is similar to this:\\ndeployment.apps/nginx-deployment resource requirements updated\\nThe initial state of the Deployment prior to pausing its rollout will continue its function,\\nbut new updates to the Deployment will not have any effect as long as the Deployment\\nrollout is paused.\\nEventually, resume the Deployment rollout and observe a new ReplicaSet coming up with\\nall the new updates:\\nkubectl rollout resume deployment/nginx-deployment\\nThe output is similar to this:\\ndeployment.apps/nginx-deployment resumed• \\n• \\n• \\n• \\n•', metadata={'source': './PDFS/Concepts.pdf', 'page': 136}),\n",
       " Document(page_content='Watch the status of the rollout until it\\'s done.\\nkubectl get rs -w\\nThe output is similar to this:\\nNAME               DESIRED   CURRENT   READY     AGE\\nnginx-2142116321   2         2         2         2m\\nnginx-3926361531   2         2         0         6s\\nnginx-3926361531   2         2         1         18s\\nnginx-2142116321   1         2         2         2m\\nnginx-2142116321   1         2         2         2m\\nnginx-3926361531   3         2         1         18s\\nnginx-3926361531   3         2         1         18s\\nnginx-2142116321   1         1         1         2m\\nnginx-3926361531   3         3         1         18s\\nnginx-3926361531   3         3         2         19s\\nnginx-2142116321   0         1         1         2m\\nnginx-2142116321   0         1         1         2m\\nnginx-2142116321   0         0         0         2m\\nnginx-3926361531   3         3         3         20s\\nGet the status of the latest rollout:\\nkubectl get rs\\nThe output is similar to this:\\nNAME               DESIRED   CURRENT   READY     AGE\\nnginx-2142116321   0         0         0         2m\\nnginx-3926361531   3         3         3         28s\\nNote:  You cannot rollback a paused Deployment until you resume it.\\nDeployment status\\nA Deployment enters various states during its lifecycle. It can be progressing  while rolling out a\\nnew ReplicaSet, it can be complete , or it can fail to progress .\\nProgressing Deployment\\nKubernetes marks a Deployment as progressing  when one of the following tasks is performed:\\nThe Deployment creates a new ReplicaSet.\\nThe Deployment is scaling up its newest ReplicaSet.\\nThe Deployment is scaling down its older ReplicaSet(s).\\nNew Pods become ready or available (ready for at least MinReadySeconds ).\\nWhen the rollout becomes “progressing”, the Deployment controller adds a condition with the\\nfollowing attributes to the Deployment\\'s .status.conditions :\\ntype: Progressing\\nstatus: \"True\"• \\n• \\n• \\n• \\n• \\n• \\n• \\n•', metadata={'source': './PDFS/Concepts.pdf', 'page': 137}),\n",
       " Document(page_content='reason: NewReplicaSetCreated  | reason: FoundNewReplicaSet  | reason: ReplicaSetUpdated\\nYou can monitor the progress for a Deployment by using kubectl rollout status .\\nComplete Deployment\\nKubernetes marks a Deployment as complete  when it has the following characteristics:\\nAll of the replicas associated with the Deployment have been updated to the latest\\nversion you\\'ve specified, meaning any updates you\\'ve requested have been completed.\\nAll of the replicas associated with the Deployment are available.\\nNo old replicas for the Deployment are running.\\nWhen the rollout becomes “complete”, the Deployment controller sets a condition with the\\nfollowing attributes to the Deployment\\'s .status.conditions :\\ntype: Progressing\\nstatus: \"True\"\\nreason: NewReplicaSetAvailable\\nThis Progressing  condition will retain a status value of \"True\"  until a new rollout is initiated.\\nThe condition holds even when availability of replicas changes (which does instead affect the \\nAvailable  condition).\\nYou can check if a Deployment has completed by using kubectl rollout status . If the rollout\\ncompleted successfully, kubectl rollout status  returns a zero exit code.\\nkubectl rollout status deployment/nginx-deployment\\nThe output is similar to this:\\nWaiting for rollout to finish: 2 of 3 updated replicas are available...\\ndeployment \"nginx-deployment\" successfully rolled out\\nand the exit status from kubectl rollout  is 0 (success):\\necho  $?\\n0\\nFailed Deployment\\nYour Deployment may get stuck trying to deploy its newest ReplicaSet without ever\\ncompleting. This can occur due to some of the following factors:\\nInsufficient quota\\nReadiness probe failures\\nImage pull errors\\nInsufficient permissions\\nLimit ranges\\nApplication runtime misconfiguration\\nOne way you can detect this condition is to specify a deadline parameter in your Deployment\\nspec: ( .spec.progressDeadlineSeconds ). .spec.progressDeadlineSeconds  denotes the number of• \\n• \\n• \\n• \\n• \\n• \\n• \\n• \\n• \\n• \\n• \\n• \\n•', metadata={'source': './PDFS/Concepts.pdf', 'page': 138}),\n",
       " Document(page_content='seconds the Deployment controller waits before indicating (in the Deployment status) that the\\nDeployment progress has stalled.\\nThe following kubectl  command sets the spec with progressDeadlineSeconds  to make the\\ncontroller report lack of progress of a rollout for a Deployment after 10 minutes:\\nkubectl patch deployment/nginx-deployment -p \\'{\"spec\":{\"progressDeadlineSeconds\":600}}\\'\\nThe output is similar to this:\\ndeployment.apps/nginx-deployment patched\\nOnce the deadline has been exceeded, the Deployment controller adds a DeploymentCondition\\nwith the following attributes to the Deployment\\'s .status.conditions :\\ntype: Progressing\\nstatus: \"False\"\\nreason: ProgressDeadlineExceeded\\nThis condition can also fail early and is then set to status value of \"False\"  due to reasons as \\nReplicaSetCreateError . Also, the deadline is not taken into account anymore once the\\nDeployment rollout completes.\\nSee the Kubernetes API conventions  for more information on status conditions.\\nNote:  Kubernetes takes no action on a stalled Deployment other than to report a status\\ncondition with reason: ProgressDeadlineExceeded . Higher level orchestrators can take\\nadvantage of it and act accordingly, for example, rollback the Deployment to its previous\\nversion.\\nNote:  If you pause a Deployment rollout, Kubernetes does not check progress against your\\nspecified deadline. You can safely pause a Deployment rollout in the middle of a rollout and\\nresume without triggering the condition for exceeding the deadline.\\nYou may experience transient errors with your Deployments, either due to a low timeout that\\nyou have set or due to any other kind of error that can be treated as transient. For example, let\\'s\\nsuppose you have insufficient quota. If you describe the Deployment you will notice the\\nfollowing section:\\nkubectl describe deployment nginx-deployment\\nThe output is similar to this:\\n<...>\\nConditions:\\n  Type            Status  Reason\\n  ----            ------  ------\\n  Available       True    MinimumReplicasAvailable\\n  Progressing     True    ReplicaSetUpdated\\n  ReplicaFailure  True    FailedCreate\\n<...>\\nIf you run kubectl get deployment nginx-deployment -o yaml , the Deployment status is similar\\nto this:• \\n• \\n•', metadata={'source': './PDFS/Concepts.pdf', 'page': 139}),\n",
       " Document(page_content='status:\\n  availableReplicas: 2\\n  conditions:\\n  - lastTransitionTime: 2016-10-04T12:25:39Z\\n    lastUpdateTime: 2016-10-04T12:25:39Z\\n    message: Replica set \"nginx-deployment-4262182780\" is progressing.\\n    reason: ReplicaSetUpdated\\n    status: \"True\"\\n    type: Progressing\\n  - lastTransitionTime: 2016-10-04T12:25:42Z\\n    lastUpdateTime: 2016-10-04T12:25:42Z\\n    message: Deployment has minimum availability.\\n    reason: MinimumReplicasAvailable\\n    status: \"True\"\\n    type: Available\\n  - lastTransitionTime: 2016-10-04T12:25:39Z\\n    lastUpdateTime: 2016-10-04T12:25:39Z\\n    message: \\'Error creating: pods \"nginx-deployment-4262182780-\" is forbidden: exceeded quota:\\n      object-counts, requested: pods=1, used: pods=3, limited: pods=2\\'\\n    reason: FailedCreate\\n    status: \"True\"\\n    type: ReplicaFailure\\n  observedGeneration: 3\\n  replicas: 2\\n  unavailableReplicas: 2\\nEventually, once the Deployment progress deadline is exceeded, Kubernetes updates the status\\nand the reason for the Progressing condition:\\nConditions:\\n  Type            Status  Reason\\n  ----            ------  ------\\n  Available       True    MinimumReplicasAvailable\\n  Progressing     False   ProgressDeadlineExceeded\\n  ReplicaFailure  True    FailedCreate\\nYou can address an issue of insufficient quota by scaling down your Deployment, by scaling\\ndown other controllers you may be running, or by increasing quota in your namespace. If you\\nsatisfy the quota conditions and the Deployment controller then completes the Deployment\\nrollout, you\\'ll see the Deployment\\'s status update with a successful condition ( status: \"True\"  and \\nreason: NewReplicaSetAvailable ).\\nConditions:\\n  Type          Status  Reason\\n  ----          ------  ------\\n  Available     True    MinimumReplicasAvailable\\n  Progressing   True    NewReplicaSetAvailable\\ntype: Available  with status: \"True\"  means that your Deployment has minimum availability.\\nMinimum availability is dictated by the parameters specified in the deployment strategy. type: \\nProgressing  with status: \"True\"  means that your Deployment is either in the middle of a rollout\\nand it is progressing or that it has successfully completed its progress and the minimum', metadata={'source': './PDFS/Concepts.pdf', 'page': 140}),\n",
       " Document(page_content='required new replicas are available (see the Reason of the condition for the particulars - in our\\ncase reason: NewReplicaSetAvailable  means that the Deployment is complete).\\nYou can check if a Deployment has failed to progress by using kubectl rollout status . kubectl \\nrollout status  returns a non-zero exit code if the Deployment has exceeded the progression\\ndeadline.\\nkubectl rollout status deployment/nginx-deployment\\nThe output is similar to this:\\nWaiting for rollout to finish: 2 out of 3 new replicas have been updated...\\nerror: deployment \"nginx\" exceeded its progress deadline\\nand the exit status from kubectl rollout  is 1 (indicating an error):\\necho  $?\\n1\\nOperating on a failed deployment\\nAll actions that apply to a complete Deployment also apply to a failed Deployment. You can\\nscale it up/down, roll back to a previous revision, or even pause it if you need to apply multiple\\ntweaks in the Deployment Pod template.\\nClean up Policy\\nYou can set .spec.revisionHistoryLimit  field in a Deployment to specify how many old\\nReplicaSets for this Deployment you want to retain. The rest will be garbage-collected in the\\nbackground. By default, it is 10.\\nNote:  Explicitly setting this field to 0, will result in cleaning up all the history of your\\nDeployment thus that Deployment will not be able to roll back.\\nCanary Deployment\\nIf you want to roll out releases to a subset of users or servers using the Deployment, you can\\ncreate multiple Deployments, one for each release, following the canary pattern described in \\nmanaging resources .\\nWriting a Deployment Spec\\nAs with all other Kubernetes configs, a Deployment needs .apiVersion , .kind , and .metadata\\nfields. For general information about working with config files, see deploying applications ,\\nconfiguring containers, and using kubectl to manage resources  documents.\\nWhen the control plane creates new Pods for a Deployment, the .metadata.name  of the\\nDeployment is part of the basis for naming those Pods. The name of a Deployment must be a\\nvalid DNS subdomain  value, but this can produce unexpected results for the Pod hostnames.\\nFor best compatibility, the name should follow the more restrictive rules for a DNS label .', metadata={'source': './PDFS/Concepts.pdf', 'page': 141}),\n",
       " Document(page_content=\"A Deployment also needs a .spec  section .\\nPod Template\\nThe .spec.template  and .spec.selector  are the only required fields of the .spec .\\nThe .spec.template  is a Pod template . It has exactly the same schema as a Pod, except it is nested\\nand does not have an apiVersion  or kind.\\nIn addition to required fields for a Pod, a Pod template in a Deployment must specify\\nappropriate labels and an appropriate restart policy. For labels, make sure not to overlap with\\nother controllers. See selector .\\nOnly a .spec.template.spec.restartPolicy  equal to Always  is allowed, which is the default if not\\nspecified.\\nReplicas\\n.spec.replicas  is an optional field that specifies the number of desired Pods. It defaults to 1.\\nShould you manually scale a Deployment, example via kubectl scale deployment deployment --\\nreplicas=X , and then you update that Deployment based on a manifest (for example: by\\nrunning kubectl apply -f deployment.yaml ), then applying that manifest overwrites the manual\\nscaling that you previously did.\\nIf a HorizontalPodAutoscaler  (or any similar API for horizontal scaling) is managing scaling for\\na Deployment, don't set .spec.replicas .\\nInstead, allow the Kubernetes control plane  to manage the .spec.replicas  field automatically.\\nSelector\\n.spec.selector  is a required field that specifies a label selector  for the Pods targeted by this\\nDeployment.\\n.spec.selector  must match .spec.template.metadata.labels , or it will be rejected by the API.\\nIn API version apps/v1 , .spec.selector  and .metadata.labels  do not default\\nto .spec.template.metadata.labels  if not set. So they must be set explicitly. Also note\\nthat .spec.selector  is immutable after creation of the Deployment in apps/v1 .\\nA Deployment may terminate Pods whose labels match the selector if their template is different\\nfrom .spec.template  or if the total number of such Pods exceeds .spec.replicas . It brings up new\\nPods with .spec.template  if the number of Pods is less than the desired number.\\nNote:  You should not create other Pods whose labels match this selector, either directly, by\\ncreating another Deployment, or by creating another controller such as a ReplicaSet or a\\nReplicationController. If you do so, the first Deployment thinks that it created these other Pods.\\nKubernetes does not stop you from doing this.\\nIf you have multiple controllers that have overlapping selectors, the controllers will fight with\\neach other and won't behave correctly.\", metadata={'source': './PDFS/Concepts.pdf', 'page': 142}),\n",
       " Document(page_content='Strategy\\n.spec.strategy  specifies the strategy used to replace old Pods by new ones. .spec.strategy.type\\ncan be \"Recreate\" or \"RollingUpdate\". \"RollingUpdate\" is the default value.\\nRecreate Deployment\\nAll existing Pods are killed before new ones are created when .spec.strategy.type==Recreate .\\nNote:  This will only guarantee Pod termination previous to creation for upgrades. If you\\nupgrade a Deployment, all Pods of the old revision will be terminated immediately. Successful\\nremoval is awaited before any Pod of the new revision is created. If you manually delete a Pod,\\nthe lifecycle is controlled by the ReplicaSet and the replacement will be created immediately\\n(even if the old Pod is still in a Terminating state). If you need an \"at most\" guarantee for your\\nPods, you should consider using a StatefulSet .\\nRolling Update Deployment\\nThe Deployment updates Pods in a rolling update fashion when\\n.spec.strategy.type==RollingUpdate . You can specify maxUnavailable  and maxSurge  to control\\nthe rolling update process.\\nMax Unavailable\\n.spec.strategy.rollingUpdate.maxUnavailable  is an optional field that specifies the maximum\\nnumber of Pods that can be unavailable during the update process. The value can be an absolute\\nnumber (for example, 5) or a percentage of desired Pods (for example, 10%). The absolute\\nnumber is calculated from percentage by rounding down. The value cannot be 0 if\\n.spec.strategy.rollingUpdate.maxSurge  is 0. The default value is 25%.\\nFor example, when this value is set to 30%, the old ReplicaSet can be scaled down to 70% of\\ndesired Pods immediately when the rolling update starts. Once new Pods are ready, old\\nReplicaSet can be scaled down further, followed by scaling up the new ReplicaSet, ensuring that\\nthe total number of Pods available at all times during the update is at least 70% of the desired\\nPods.\\nMax Surge\\n.spec.strategy.rollingUpdate.maxSurge  is an optional field that specifies the maximum number\\nof Pods that can be created over the desired number of Pods. The value can be an absolute\\nnumber (for example, 5) or a percentage of desired Pods (for example, 10%). The value cannot be\\n0 if MaxUnavailable  is 0. The absolute number is calculated from the percentage by rounding\\nup. The default value is 25%.\\nFor example, when this value is set to 30%, the new ReplicaSet can be scaled up immediately\\nwhen the rolling update starts, such that the total number of old and new Pods does not exceed\\n130% of desired Pods. Once old Pods have been killed, the new ReplicaSet can be scaled up\\nfurther, ensuring that the total number of Pods running at any time during the update is at\\nmost 130% of desired Pods.', metadata={'source': './PDFS/Concepts.pdf', 'page': 143}),\n",
       " Document(page_content='Here are some Rolling Update Deployment examples that use the maxUnavailable  and \\nmaxSurge :\\nMax Unavailable\\nMax Surge\\nHybrid\\napiVersion : apps/v1\\nkind: Deployment\\nmetadata :\\n name : nginx-deployment\\n labels :\\n   app: nginx\\nspec:\\n replicas : 3\\n selector :\\n   matchLabels :\\n     app: nginx\\n template :\\n   metadata :\\n     labels :\\n       app: nginx\\n   spec:\\n     containers :\\n     - name : nginx\\n       image : nginx:1.14.2\\n       ports :\\n       - containerPort : 80\\n strategy :\\n   type: RollingUpdate\\n   rollingUpdate :\\n     maxUnavailable : 1\\napiVersion : apps/v1\\nkind: Deployment\\nmetadata :\\n name : nginx-deployment\\n labels :\\n   app: nginx\\nspec:\\n replicas : 3\\n selector :\\n   matchLabels :\\n     app: nginx\\n template :\\n   metadata :\\n     labels :\\n       app: nginx\\n   spec:\\n     containers :\\n     - name : nginx\\n       image : nginx:1.14.2\\n       ports :• \\n• \\n•', metadata={'source': './PDFS/Concepts.pdf', 'page': 144}),\n",
       " Document(page_content='- containerPort : 80\\n strategy :\\n   type: RollingUpdate\\n   rollingUpdate :\\n     maxSurge : 1\\napiVersion : apps/v1\\nkind: Deployment\\nmetadata :\\n name : nginx-deployment\\n labels :\\n   app: nginx\\nspec:\\n replicas : 3\\n selector :\\n   matchLabels :\\n     app: nginx\\n template :\\n   metadata :\\n     labels :\\n       app: nginx\\n   spec:\\n     containers :\\n     - name : nginx\\n       image : nginx:1.14.2\\n       ports :\\n       - containerPort : 80\\n strategy :\\n   type: RollingUpdate\\n   rollingUpdate :\\n     maxSurge : 1\\n     maxUnavailable : 1\\nProgress Deadline Seconds\\n.spec.progressDeadlineSeconds  is an optional field that specifies the number of seconds you\\nwant to wait for your Deployment to progress before the system reports back that the\\nDeployment has failed progressing  - surfaced as a condition with type: Progressing , status: \\n\"False\" . and reason: ProgressDeadlineExceeded  in the status of the resource. The Deployment\\ncontroller will keep retrying the Deployment. This defaults to 600. In the future, once automatic\\nrollback will be implemented, the Deployment controller will roll back a Deployment as soon as\\nit observes such a condition.\\nIf specified, this field needs to be greater than .spec.minReadySeconds .\\nMin Ready Seconds\\n.spec.minReadySeconds  is an optional field that specifies the minimum number of seconds for\\nwhich a newly created Pod should be ready without any of its containers crashing, for it to be\\nconsidered available. This defaults to 0 (the Pod will be considered available as soon as it is\\nready). To learn more about when a Pod is considered ready, see Container Probes .', metadata={'source': './PDFS/Concepts.pdf', 'page': 145}),\n",
       " Document(page_content=\"Revision History Limit\\nA Deployment's revision history is stored in the ReplicaSets it controls.\\n.spec.revisionHistoryLimit  is an optional field that specifies the number of old ReplicaSets to\\nretain to allow rollback. These old ReplicaSets consume resources in etcd and crowd the output\\nof kubectl get rs . The configuration of each Deployment revision is stored in its ReplicaSets;\\ntherefore, once an old ReplicaSet is deleted, you lose the ability to rollback to that revision of\\nDeployment. By default, 10 old ReplicaSets will be kept, however its ideal value depends on the\\nfrequency and stability of new Deployments.\\nMore specifically, setting this field to zero means that all old ReplicaSets with 0 replicas will be\\ncleaned up. In this case, a new Deployment rollout cannot be undone, since its revision history\\nis cleaned up.\\nPaused\\n.spec.paused  is an optional boolean field for pausing and resuming a Deployment. The only\\ndifference between a paused Deployment and one that is not paused, is that any changes into\\nthe PodTemplateSpec of the paused Deployment will not trigger new rollouts as long as it is\\npaused. A Deployment is not paused by default when it is created.\\nWhat's next\\nLearn more about Pods .\\nRun a stateless application using a Deployment .\\nRead the Deployment  to understand the Deployment API.\\nRead about PodDisruptionBudget  and how you can use it to manage application\\navailability during disruptions.\\nUse kubectl to create a Deployment .\\nReplicaSet\\nA ReplicaSet's purpose is to maintain a stable set of replica Pods running at any given time.\\nUsually, you define a Deployment and let that Deployment manage ReplicaSets automatically.\\nA ReplicaSet's purpose is to maintain a stable set of replica Pods running at any given time. As\\nsuch, it is often used to guarantee the availability of a specified number of identical Pods.\\nHow a ReplicaSet works\\nA ReplicaSet is defined with fields, including a selector that specifies how to identify Pods it can\\nacquire, a number of replicas indicating how many Pods it should be maintaining, and a pod\\ntemplate specifying the data of new Pods it should create to meet the number of replicas\\ncriteria. A ReplicaSet then fulfills its purpose by creating and deleting Pods as needed to reach\\nthe desired number. When a ReplicaSet needs to create new Pods, it uses its Pod template.\\nA ReplicaSet is linked to its Pods via the Pods' metadata.ownerReferences  field, which specifies\\nwhat resource the current object is owned by. All Pods acquired by a ReplicaSet have their\\nowning ReplicaSet's identifying information within their ownerReferences field. It's through• \\n• \\n• \\n• \\n•\", metadata={'source': './PDFS/Concepts.pdf', 'page': 146}),\n",
       " Document(page_content=\"this link that the ReplicaSet knows of the state of the Pods it is maintaining and plans\\naccordingly.\\nA ReplicaSet identifies new Pods to acquire by using its selector. If there is a Pod that has no\\nOwnerReference or the OwnerReference is not a Controller  and it matches a ReplicaSet's\\nselector, it will be immediately acquired by said ReplicaSet.\\nWhen to use a ReplicaSet\\nA ReplicaSet ensures that a specified number of pod replicas are running at any given time.\\nHowever, a Deployment is a higher-level concept that manages ReplicaSets and provides\\ndeclarative updates to Pods along with a lot of other useful features. Therefore, we recommend\\nusing Deployments instead of directly using ReplicaSets, unless you require custom update\\norchestration or don't require updates at all.\\nThis actually means that you may never need to manipulate ReplicaSet objects: use a\\nDeployment instead, and define your application in the spec section.\\nExample\\ncontrollers/frontend.yaml  \\napiVersion : apps/v1\\nkind: ReplicaSet\\nmetadata :\\n  name : frontend\\n  labels :\\n    app: guestbook\\n    tier: frontend\\nspec:\\n  # modify replicas according to your case\\n  replicas : 3\\n  selector :\\n    matchLabels :\\n      tier: frontend\\n  template :\\n    metadata :\\n      labels :\\n        tier: frontend\\n    spec:\\n      containers :\\n      - name : php-redis\\n        image : gcr.io/google_samples/gb-frontend:v3\\nSaving this manifest into frontend.yaml  and submitting it to a Kubernetes cluster will create the\\ndefined ReplicaSet and the Pods that it manages.\\nkubectl apply -f https://kubernetes.io/examples/controllers/frontend.yaml\\nYou can then get the current ReplicaSets deployed:\\nkubectl get rs\", metadata={'source': './PDFS/Concepts.pdf', 'page': 147}),\n",
       " Document(page_content='And see the frontend one you created:\\nNAME       DESIRED   CURRENT   READY   AGE\\nfrontend   3         3         3       6s\\nYou can also check on the state of the ReplicaSet:\\nkubectl describe rs/frontend\\nAnd you will see output similar to:\\nName:         frontend\\nNamespace:    default\\nSelector:     tier=frontend\\nLabels:       app=guestbook\\n              tier=frontend\\nAnnotations:  kubectl.kubernetes.io/last-applied-configuration:\\n                {\"apiVersion\":\"apps/v1\",\"kind\":\"ReplicaSet\",\"metadata\":{\"annotations\":{},\"labels\":\\n{\"app\":\"guestbook\",\"tier\":\"frontend\"},\"name\":\"frontend\",...\\nReplicas:     3 current / 3 desired\\nPods Status:  3 Running / 0 Waiting / 0 Succeeded / 0 Failed\\nPod Template:\\n  Labels:  tier=frontend\\n  Containers:\\n   php-redis:\\n    Image:        gcr.io/google_samples/gb-frontend:v3\\n    Port:         <none>\\n    Host Port:    <none>\\n    Environment:  <none>\\n    Mounts:       <none>\\n  Volumes:        <none>\\nEvents:\\n  Type    Reason            Age   From                   Message\\n  ----    ------            ----  ----                   -------\\n  Normal  SuccessfulCreate  117s  replicaset-controller  Created pod: frontend-wtsmm\\n  Normal  SuccessfulCreate  116s  replicaset-controller  Created pod: frontend-b2zdv\\n  Normal  SuccessfulCreate  116s  replicaset-controller  Created pod: frontend-vcmts\\nAnd lastly you can check for the Pods brought up:\\nkubectl get pods\\nYou should see Pod information similar to:\\nNAME             READY   STATUS    RESTARTS   AGE\\nfrontend-b2zdv   1/1     Running   0          6m36s\\nfrontend-vcmts   1/1     Running   0          6m36s\\nfrontend-wtsmm   1/1     Running   0          6m36s\\nYou can also verify that the owner reference of these pods is set to the frontend ReplicaSet. To\\ndo this, get the yaml of one of the Pods running:\\nkubectl get pods frontend-b2zdv -o yaml', metadata={'source': './PDFS/Concepts.pdf', 'page': 148}),\n",
       " Document(page_content='The output will look similar to this, with the frontend ReplicaSet\\'s info set in the metadata\\'s\\nownerReferences field:\\napiVersion : v1\\nkind: Pod\\nmetadata :\\n  creationTimestamp : \"2020-02-12T07:06:16Z\"\\n  generateName : frontend-\\n  labels :\\n    tier: frontend\\n  name : frontend-b2zdv\\n  namespace : default\\n  ownerReferences :\\n  - apiVersion : apps/v1\\n    blockOwnerDeletion : true\\n    controller : true\\n    kind: ReplicaSet\\n    name : frontend\\n    uid: f391f6db-bb9b-4c09-ae74-6a1f77f3d5cf\\n...\\nNon-Template Pod acquisitions\\nWhile you can create bare Pods with no problems, it is strongly recommended to make sure\\nthat the bare Pods do not have labels which match the selector of one of your ReplicaSets. The\\nreason for this is because a ReplicaSet is not limited to owning Pods specified by its template--\\nit can acquire other Pods in the manner specified in the previous sections.\\nTake the previous frontend ReplicaSet example, and the Pods specified in the following\\nmanifest:\\npods/pod-rs.yaml  \\napiVersion : v1\\nkind: Pod\\nmetadata :\\n  name : pod1\\n  labels :\\n    tier: frontend\\nspec:\\n  containers :\\n  - name : hello1\\n    image : gcr.io/google-samples/hello-app:2.0\\n---\\napiVersion : v1\\nkind: Pod\\nmetadata :\\n  name : pod2\\n  labels :\\n    tier: frontend\\nspec:', metadata={'source': './PDFS/Concepts.pdf', 'page': 149}),\n",
       " Document(page_content='containers :\\n  - name : hello2\\n    image : gcr.io/google-samples/hello-app:1.0\\nAs those Pods do not have a Controller (or any object) as their owner reference and match the\\nselector of the frontend ReplicaSet, they will immediately be acquired by it.\\nSuppose you create the Pods after the frontend ReplicaSet has been deployed and has set up its\\ninitial Pod replicas to fulfill its replica count requirement:\\nkubectl apply -f https://kubernetes.io/examples/pods/pod-rs.yaml\\nThe new Pods will be acquired by the ReplicaSet, and then immediately terminated as the\\nReplicaSet would be over its desired count.\\nFetching the Pods:\\nkubectl get pods\\nThe output shows that the new Pods are either already terminated, or in the process of being\\nterminated:\\nNAME             READY   STATUS        RESTARTS   AGE\\nfrontend-b2zdv   1/1     Running       0          10m\\nfrontend-vcmts   1/1     Running       0          10m\\nfrontend-wtsmm   1/1     Running       0          10m\\npod1             0/1     Terminating   0          1s\\npod2             0/1     Terminating   0          1s\\nIf you create the Pods first:\\nkubectl apply -f https://kubernetes.io/examples/pods/pod-rs.yaml\\nAnd then create the ReplicaSet however:\\nkubectl apply -f https://kubernetes.io/examples/controllers/frontend.yaml\\nYou shall see that the ReplicaSet has acquired the Pods and has only created new ones\\naccording to its spec until the number of its new Pods and the original matches its desired\\ncount. As fetching the Pods:\\nkubectl get pods\\nWill reveal in its output:\\nNAME             READY   STATUS    RESTARTS   AGE\\nfrontend-hmmj2   1/1     Running   0          9s\\npod1             1/1     Running   0          36s\\npod2             1/1     Running   0          36s\\nIn this manner, a ReplicaSet can own a non-homogenous set of Pods', metadata={'source': './PDFS/Concepts.pdf', 'page': 150}),\n",
       " Document(page_content=\"Writing a ReplicaSet manifest\\nAs with all other Kubernetes API objects, a ReplicaSet needs the apiVersion , kind, and metadata\\nfields. For ReplicaSets, the kind is always a ReplicaSet.\\nWhen the control plane creates new Pods for a ReplicaSet, the .metadata.name  of the ReplicaSet\\nis part of the basis for naming those Pods. The name of a ReplicaSet must be a valid DNS\\nsubdomain  value, but this can produce unexpected results for the Pod hostnames. For best\\ncompatibility, the name should follow the more restrictive rules for a DNS label .\\nA ReplicaSet also needs a .spec  section .\\nPod Template\\nThe .spec.template  is a pod template  which is also required to have labels in place. In our \\nfrontend.yaml  example we had one label: tier: frontend . Be careful not to overlap with the\\nselectors of other controllers, lest they try to adopt this Pod.\\nFor the template's restart policy  field, .spec.template.spec.restartPolicy , the only allowed value\\nis Always , which is the default.\\nPod Selector\\nThe .spec.selector  field is a label selector . As discussed earlier  these are the labels used to\\nidentify potential Pods to acquire. In our frontend.yaml  example, the selector was:\\nmatchLabels :\\n  tier: frontend\\nIn the ReplicaSet, .spec.template.metadata.labels  must match spec.selector , or it will be rejected\\nby the API.\\nNote:  For 2 ReplicaSets specifying the same .spec.selector  but\\ndifferent .spec.template.metadata.labels  and .spec.template.spec  fields, each ReplicaSet ignores\\nthe Pods created by the other ReplicaSet.\\nReplicas\\nYou can specify how many Pods should run concurrently by setting .spec.replicas . The\\nReplicaSet will create/delete its Pods to match this number.\\nIf you do not specify .spec.replicas , then it defaults to 1.\\nWorking with ReplicaSets\\nDeleting a ReplicaSet and its Pods\\nTo delete a ReplicaSet and all of its Pods, use kubectl delete . The Garbage collector\\nautomatically deletes all of the dependent Pods by default.\\nWhen using the REST API or the client-go  library, you must set propagationPolicy  to \\nBackground  or Foreground  in the -d option. For example:\", metadata={'source': './PDFS/Concepts.pdf', 'page': 151}),\n",
       " Document(page_content='kubectl proxy --port =8080\\ncurl -X DELETE  \\'localhost:8080/apis/apps/v1/namespaces/default/replicasets/frontend\\'  \\\\\\n  -d \\'{\"kind\":\"DeleteOptions\",\"apiVersion\":\"v1\",\"propagationPolicy\":\"Foreground\"}\\'  \\\\\\n  -H \"Content-Type: application/json\"\\nDeleting just a ReplicaSet\\nYou can delete a ReplicaSet without affecting any of its Pods using kubectl delete  with the --\\ncascade=orphan  option. When using the REST API or the client-go  library, you must set \\npropagationPolicy  to Orphan . For example:\\nkubectl proxy --port =8080\\ncurl -X DELETE  \\'localhost:8080/apis/apps/v1/namespaces/default/replicasets/frontend\\'  \\\\\\n  -d \\'{\"kind\":\"DeleteOptions\",\"apiVersion\":\"v1\",\"propagationPolicy\":\"Orphan\"}\\'  \\\\\\n  -H \"Content-Type: application/json\"\\nOnce the original is deleted, you can create a new ReplicaSet to replace it. As long as the old\\nand new .spec.selector  are the same, then the new one will adopt the old Pods. However, it will\\nnot make any effort to make existing Pods match a new, different pod template. To update Pods\\nto a new spec in a controlled way, use a Deployment , as ReplicaSets do not support a rolling\\nupdate directly.\\nIsolating Pods from a ReplicaSet\\nYou can remove Pods from a ReplicaSet by changing their labels. This technique may be used to\\nremove Pods from service for debugging, data recovery, etc. Pods that are removed in this way\\nwill be replaced automatically ( assuming that the number of replicas is not also changed).\\nScaling a ReplicaSet\\nA ReplicaSet can be easily scaled up or down by simply updating the .spec.replicas  field. The\\nReplicaSet controller ensures that a desired number of Pods with a matching label selector are\\navailable and operational.\\nWhen scaling down, the ReplicaSet controller chooses which pods to delete by sorting the\\navailable pods to prioritize scaling down pods based on the following general algorithm:\\nPending (and unschedulable) pods are scaled down first\\nIf controller.kubernetes.io/pod-deletion-cost  annotation is set, then the pod with the\\nlower value will come first.\\nPods on nodes with more replicas come before pods on nodes with fewer replicas.\\nIf the pods\\' creation times differ, the pod that was created more recently comes before the\\nolder pod (the creation times are bucketed on an integer log scale when the \\nLogarithmicScaleDown  feature gate  is enabled)\\nIf all of the above match, then selection is random.\\nPod deletion cost\\nFEATURE STATE:  Kubernetes v1.22 [beta]\\nUsing the controller.kubernetes.io/pod-deletion-cost  annotation, users can set a preference\\nregarding which pods to remove first when downscaling a ReplicaSet.1. \\n2. \\n3. \\n4.', metadata={'source': './PDFS/Concepts.pdf', 'page': 152}),\n",
       " Document(page_content=\"The annotation should be set on the pod, the range is [-2147483648, 2147483647]. It represents\\nthe cost of deleting a pod compared to other pods belonging to the same ReplicaSet. Pods with\\nlower deletion cost are preferred to be deleted before pods with higher deletion cost.\\nThe implicit value for this annotation for pods that don't set it is 0; negative values are\\npermitted. Invalid values will be rejected by the API server.\\nThis feature is beta and enabled by default. You can disable it using the feature gate  \\nPodDeletionCost  in both kube-apiserver and kube-controller-manager.\\nNote:\\nThis is honored on a best-effort basis, so it does not offer any guarantees on pod deletion\\norder.\\nUsers should avoid updating the annotation frequently, such as updating it based on a\\nmetric value, because doing so will generate a significant number of pod updates on the\\napiserver.\\nExample Use Case\\nThe different pods of an application could have different utilization levels. On scale down, the\\napplication may prefer to remove the pods with lower utilization. To avoid frequently updating\\nthe pods, the application should update controller.kubernetes.io/pod-deletion-cost  once before\\nissuing a scale down (setting the annotation to a value proportional to pod utilization level).\\nThis works if the application itself controls the down scaling; for example, the driver pod of a\\nSpark deployment.\\nReplicaSet as a Horizontal Pod Autoscaler Target\\nA ReplicaSet can also be a target for Horizontal Pod Autoscalers (HPA) . That is, a ReplicaSet can\\nbe auto-scaled by an HPA. Here is an example HPA targeting the ReplicaSet we created in the\\nprevious example.\\ncontrollers/hpa-rs.yaml  \\napiVersion : autoscaling/v1\\nkind: HorizontalPodAutoscaler\\nmetadata :\\n  name : frontend-scaler\\nspec:\\n  scaleTargetRef :\\n    kind: ReplicaSet\\n    name : frontend\\n  minReplicas : 3\\n  maxReplicas : 10\\n  targetCPUUtilizationPercentage : 50\\nSaving this manifest into hpa-rs.yaml  and submitting it to a Kubernetes cluster should create\\nthe defined HPA that autoscales the target ReplicaSet depending on the CPU usage of the\\nreplicated Pods.\\nkubectl apply -f https://k8s.io/examples/controllers/hpa-rs.yaml• \\n•\", metadata={'source': './PDFS/Concepts.pdf', 'page': 153}),\n",
       " Document(page_content=\"Alternatively, you can use the kubectl autoscale  command to accomplish the same (and it's\\neasier!)\\nkubectl autoscale rs frontend --max =10 --min =3 --cpu-percent =50\\nAlternatives to ReplicaSet\\nDeployment (recommended)\\nDeployment  is an object which can own ReplicaSets and update them and their Pods via\\ndeclarative, server-side rolling updates. While ReplicaSets can be used independently, today\\nthey're mainly used by Deployments as a mechanism to orchestrate Pod creation, deletion and\\nupdates. When you use Deployments you don't have to worry about managing the ReplicaSets\\nthat they create. Deployments own and manage their ReplicaSets. As such, it is recommended\\nto use Deployments when you want ReplicaSets.\\nBare Pods\\nUnlike the case where a user directly created Pods, a ReplicaSet replaces Pods that are deleted\\nor terminated for any reason, such as in the case of node failure or disruptive node\\nmaintenance, such as a kernel upgrade. For this reason, we recommend that you use a\\nReplicaSet even if your application requires only a single Pod. Think of it similarly to a process\\nsupervisor, only it supervises multiple Pods across multiple nodes instead of individual\\nprocesses on a single node. A ReplicaSet delegates local container restarts to some agent on the\\nnode such as Kubelet.\\nJob\\nUse a Job instead of a ReplicaSet for Pods that are expected to terminate on their own (that is,\\nbatch jobs).\\nDaemonSet\\nUse a DaemonSet  instead of a ReplicaSet for Pods that provide a machine-level function, such as\\nmachine monitoring or machine logging. These Pods have a lifetime that is tied to a machine\\nlifetime: the Pod needs to be running on the machine before other Pods start, and are safe to\\nterminate when the machine is otherwise ready to be rebooted/shutdown.\\nReplicationController\\nReplicaSets are the successors to ReplicationControllers . The two serve the same purpose, and\\nbehave similarly, except that a ReplicationController does not support set-based selector\\nrequirements as described in the labels user guide . As such, ReplicaSets are preferred over\\nReplicationControllers\\nWhat's next\\nLearn about Pods .\\nLearn about Deployments .\\nRun a Stateless Application Using a Deployment , which relies on ReplicaSets to work.• \\n• \\n•\", metadata={'source': './PDFS/Concepts.pdf', 'page': 154}),\n",
       " Document(page_content=\"ReplicaSet  is a top-level resource in the Kubernetes REST API. Read the ReplicaSet  object\\ndefinition to understand the API for replica sets.\\nRead about PodDisruptionBudget  and how you can use it to manage application\\navailability during disruptions.\\nStatefulSets\\nA StatefulSet runs a group of Pods, and maintains a sticky identity for each of those Pods. This\\nis useful for managing applications that need persistent storage or a stable, unique network\\nidentity.\\nStatefulSet is the workload API object used to manage stateful applications.\\nManages the deployment and scaling of a set of Pods , and provides guarantees about the ordering\\nand uniqueness  of these Pods.\\nLike a Deployment , a StatefulSet manages Pods that are based on an identical container spec.\\nUnlike a Deployment, a StatefulSet maintains a sticky identity for each of its Pods. These pods\\nare created from the same spec, but are not interchangeable: each has a persistent identifier that\\nit maintains across any rescheduling.\\nIf you want to use storage volumes to provide persistence for your workload, you can use a\\nStatefulSet as part of the solution. Although individual Pods in a StatefulSet are susceptible to\\nfailure, the persistent Pod identifiers make it easier to match existing volumes to the new Pods\\nthat replace any that have failed.\\nUsing StatefulSets\\nStatefulSets are valuable for applications that require one or more of the following.\\nStable, unique network identifiers.\\nStable, persistent storage.\\nOrdered, graceful deployment and scaling.\\nOrdered, automated rolling updates.\\nIn the above, stable is synonymous with persistence across Pod (re)scheduling. If an application\\ndoesn't require any stable identifiers or ordered deployment, deletion, or scaling, you should\\ndeploy your application using a workload object that provides a set of stateless replicas. \\nDeployment  or ReplicaSet  may be better suited to your stateless needs.\\nLimitations\\nThe storage for a given Pod must either be provisioned by a PersistentVolume Provisioner\\nbased on the requested storage class , or pre-provisioned by an admin.\\nDeleting and/or scaling a StatefulSet down will not delete the volumes associated with the\\nStatefulSet. This is done to ensure data safety, which is generally more valuable than an\\nautomatic purge of all related StatefulSet resources.\\nStatefulSets currently require a Headless Service  to be responsible for the network\\nidentity of the Pods. You are responsible for creating this Service.• \\n• \\n• \\n• \\n• \\n• \\n• \\n• \\n•\", metadata={'source': './PDFS/Concepts.pdf', 'page': 155}),\n",
       " Document(page_content='StatefulSets do not provide any guarantees on the termination of pods when a StatefulSet\\nis deleted. To achieve ordered and graceful termination of the pods in the StatefulSet, it is\\npossible to scale the StatefulSet down to 0 prior to deletion.\\nWhen using Rolling Updates  with the default Pod Management Policy  (OrderedReady ),\\nit\\'s possible to get into a broken state that requires manual intervention to repair .\\nComponents\\nThe example below demonstrates the components of a StatefulSet.\\napiVersion : v1\\nkind: Service\\nmetadata :\\n  name : nginx\\n  labels :\\n    app: nginx\\nspec:\\n  ports :\\n  - port: 80\\n    name : web\\n  clusterIP : None\\n  selector :\\n    app: nginx\\n---\\napiVersion : apps/v1\\nkind: StatefulSet\\nmetadata :\\n  name : web\\nspec:\\n  selector :\\n    matchLabels :\\n      app: nginx  # has to match .spec.template.metadata.labels\\n  serviceName : \"nginx\"\\n  replicas : 3 # by default is 1\\n  minReadySeconds : 10 # by default is 0\\n  template :\\n    metadata :\\n      labels :\\n        app: nginx  # has to match .spec.selector.matchLabels\\n    spec:\\n      terminationGracePeriodSeconds : 10\\n      containers :\\n      - name : nginx\\n        image : registry.k8s.io/nginx-slim:0.8\\n        ports :\\n        - containerPort : 80\\n          name : web\\n        volumeMounts :\\n        - name : www\\n          mountPath : /usr/share/nginx/html\\n  volumeClaimTemplates :\\n  - metadata :• \\n•', metadata={'source': './PDFS/Concepts.pdf', 'page': 156}),\n",
       " Document(page_content='name : www\\n    spec:\\n      accessModes : [ \"ReadWriteOnce\"  ]\\n      storageClassName : \"my-storage-class\"\\n      resources :\\n        requests :\\n          storage : 1Gi\\nIn the above example:\\nA Headless Service, named nginx , is used to control the network domain.\\nThe StatefulSet, named web, has a Spec that indicates that 3 replicas of the nginx\\ncontainer will be launched in unique Pods.\\nThe volumeClaimTemplates  will provide stable storage using PersistentVolumes\\nprovisioned by a PersistentVolume Provisioner.\\nThe name of a StatefulSet object must be a valid DNS label .\\nPod Selector\\nYou must set the .spec.selector  field of a StatefulSet to match the labels of\\nits .spec.template.metadata.labels . Failing to specify a matching Pod Selector will result in a\\nvalidation error during StatefulSet creation.\\nVolume Claim Templates\\nYou can set the .spec.volumeClaimTemplates  which can provide stable storage using \\nPersistentVolumes  provisioned by a PersistentVolume Provisioner.\\nMinimum ready seconds\\nFEATURE STATE:  Kubernetes v1.25 [stable]\\n.spec.minReadySeconds  is an optional field that specifies the minimum number of seconds for\\nwhich a newly created Pod should be running and ready without any of its containers crashing,\\nfor it to be considered available. This is used to check progression of a rollout when using a \\nRolling Update  strategy. This field defaults to 0 (the Pod will be considered available as soon as\\nit is ready). To learn more about when a Pod is considered ready, see Container Probes .\\nPod Identity\\nStatefulSet Pods have a unique identity that consists of an ordinal, a stable network identity,\\nand stable storage. The identity sticks to the Pod, regardless of which node it\\'s (re)scheduled on.\\nOrdinal Index\\nFor a StatefulSet with N replicas , each Pod in the StatefulSet will be assigned an integer ordinal,\\nthat is unique over the Set. By default, pods will be assigned ordinals from 0 up through N-1.\\nThe StatefulSet controller will also add a pod label with this index: apps.kubernetes.io/pod-\\nindex .• \\n• \\n•', metadata={'source': './PDFS/Concepts.pdf', 'page': 157}),\n",
       " Document(page_content='Start ordinal\\nFEATURE STATE:  Kubernetes v1.27 [beta]\\n.spec.ordinals  is an optional field that allows you to configure the integer ordinals assigned to\\neach Pod. It defaults to nil. You must enable the StatefulSetStartOrdinal  feature gate  to use this\\nfield. Once enabled, you can configure the following options:\\n.spec.ordinals.start : If the .spec.ordinals.start  field is set, Pods will be assigned ordinals\\nfrom .spec.ordinals.start  up through .spec.ordinals.start + .spec.replicas - 1 .\\nStable Network ID\\nEach Pod in a StatefulSet derives its hostname from the name of the StatefulSet and the ordinal\\nof the Pod. The pattern for the constructed hostname is $(statefulset name)-$(ordinal) . The\\nexample above will create three Pods named web-0,web-1,web-2 . A StatefulSet can use a \\nHeadless Service  to control the domain of its Pods. The domain managed by this Service takes\\nthe form: $(service name).$(namespace).svc.cluster.local , where \"cluster.local\" is the cluster\\ndomain. As each Pod is created, it gets a matching DNS subdomain, taking the form: $\\n(podname).$(governing service domain) , where the governing service is defined by the \\nserviceName  field on the StatefulSet.\\nDepending on how DNS is configured in your cluster, you may not be able to look up the DNS\\nname for a newly-run Pod immediately. This behavior can occur when other clients in the\\ncluster have already sent queries for the hostname of the Pod before it was created. Negative\\ncaching (normal in DNS) means that the results of previous failed lookups are remembered and\\nreused, even after the Pod is running, for at least a few seconds.\\nIf you need to discover Pods promptly after they are created, you have a few options:\\nQuery the Kubernetes API directly (for example, using a watch) rather than relying on\\nDNS lookups.\\nDecrease the time of caching in your Kubernetes DNS provider (typically this means\\nediting the config map for CoreDNS, which currently caches for 30 seconds).\\nAs mentioned in the limitations  section, you are responsible for creating the Headless Service\\nresponsible for the network identity of the pods.\\nHere are some examples of choices for Cluster Domain, Service name, StatefulSet name, and\\nhow that affects the DNS names for the StatefulSet\\'s Pods.\\nCluster\\nDomainService\\n(ns/\\nname)StatefulSet\\n(ns/name)StatefulSet Domain Pod DNSPod\\nHostname\\ncluster.localdefault/\\nnginxdefault/web nginx.default.svc.cluster.localweb-\\n{0..N-1}.nginx.default.svc.cluster.localweb-\\n{0..N-1}\\ncluster.localfoo/\\nnginxfoo/web nginx.foo.svc.cluster.localweb-\\n{0..N-1}.nginx.foo.svc.cluster.localweb-\\n{0..N-1}\\nkube.localfoo/\\nnginxfoo/web nginx.foo.svc.kube.local web-{0..N-1}.nginx.foo.svc.kube.localweb-\\n{0..N-1}\\nNote:  Cluster Domain will be set to cluster.local  unless otherwise configured .• \\n• \\n•', metadata={'source': './PDFS/Concepts.pdf', 'page': 158}),\n",
       " Document(page_content=\"Stable Storage\\nFor each VolumeClaimTemplate entry defined in a StatefulSet, each Pod receives one\\nPersistentVolumeClaim. In the nginx example above, each Pod receives a single\\nPersistentVolume with a StorageClass of my-storage-class  and 1 GiB of provisioned storage. If\\nno StorageClass is specified, then the default StorageClass will be used. When a Pod is\\n(re)scheduled onto a node, its volumeMounts  mount the PersistentVolumes associated with its\\nPersistentVolume Claims. Note that, the PersistentVolumes associated with the Pods'\\nPersistentVolume Claims are not deleted when the Pods, or StatefulSet are deleted. This must be\\ndone manually.\\nPod Name Label\\nWhen the StatefulSet controller  creates a Pod, it adds a label, statefulset.kubernetes.io/pod-\\nname , that is set to the name of the Pod. This label allows you to attach a Service to a specific\\nPod in the StatefulSet.\\nPod index label\\nFEATURE STATE:  Kubernetes v1.28 [beta]\\nWhen the StatefulSet controller  creates a Pod, the new Pod is labelled with apps.kubernetes.io/\\npod-index . The value of this label is the ordinal index of the Pod. This label allows you to route\\ntraffic to a particular pod index, filter logs/metrics using the pod index label, and more. Note the\\nfeature gate PodIndexLabel  must be enabled for this feature, and it is enabled by default.\\nDeployment and Scaling Guarantees\\nFor a StatefulSet with N replicas, when Pods are being deployed, they are created\\nsequentially, in order from {0..N-1}.\\nWhen Pods are being deleted, they are terminated in reverse order, from {N-1..0}.\\nBefore a scaling operation is applied to a Pod, all of its predecessors must be Running and\\nReady.\\nBefore a Pod is terminated, all of its successors must be completely shutdown.\\nThe StatefulSet should not specify a pod.Spec.TerminationGracePeriodSeconds  of 0. This\\npractice is unsafe and strongly discouraged. For further explanation, please refer to force\\ndeleting StatefulSet Pods .\\nWhen the nginx example above is created, three Pods will be deployed in the order web-0,\\nweb-1, web-2. web-1 will not be deployed before web-0 is Running and Ready , and web-2 will\\nnot be deployed until web-1 is Running and Ready. If web-0 should fail, after web-1 is Running\\nand Ready, but before web-2 is launched, web-2 will not be launched until web-0 is successfully\\nrelaunched and becomes Running and Ready.\\nIf a user were to scale the deployed example by patching the StatefulSet such that replicas=1 ,\\nweb-2 would be terminated first. web-1 would not be terminated until web-2 is fully shutdown\\nand deleted. If web-0 were to fail after web-2 has been terminated and is completely shutdown,\\nbut prior to web-1's termination, web-1 would not be terminated until web-0 is Running and\\nReady.• \\n• \\n• \\n•\", metadata={'source': './PDFS/Concepts.pdf', 'page': 159}),\n",
       " Document(page_content=\"Pod Management Policies\\nStatefulSet allows you to relax its ordering guarantees while preserving its uniqueness and\\nidentity guarantees via its .spec.podManagementPolicy  field.\\nOrderedReady Pod Management\\nOrderedReady  pod management is the default for StatefulSets. It implements the behavior\\ndescribed above .\\nParallel Pod Management\\nParallel  pod management tells the StatefulSet controller to launch or terminate all Pods in\\nparallel, and to not wait for Pods to become Running and Ready or completely terminated prior\\nto launching or terminating another Pod. This option only affects the behavior for scaling\\noperations. Updates are not affected.\\nUpdate strategies\\nA StatefulSet's .spec.updateStrategy  field allows you to configure and disable automated rolling\\nupdates for containers, labels, resource request/limits, and annotations for the Pods in a\\nStatefulSet. There are two possible values:\\nOnDelete\\nWhen a StatefulSet's .spec.updateStrategy.type  is set to OnDelete , the StatefulSet\\ncontroller will not automatically update the Pods in a StatefulSet. Users must manually\\ndelete Pods to cause the controller to create new Pods that reflect modifications made to a\\nStatefulSet's .spec.template .\\nRollingUpdate\\nThe RollingUpdate  update strategy implements automated, rolling updates for the Pods in\\na StatefulSet. This is the default update strategy.\\nRolling Updates\\nWhen a StatefulSet's .spec.updateStrategy.type  is set to RollingUpdate , the StatefulSet controller\\nwill delete and recreate each Pod in the StatefulSet. It will proceed in the same order as Pod\\ntermination (from the largest ordinal to the smallest), updating each Pod one at a time.\\nThe Kubernetes control plane waits until an updated Pod is Running and Ready prior to\\nupdating its predecessor. If you have set .spec.minReadySeconds  (see Minimum Ready Seconds ),\\nthe control plane additionally waits that amount of time after the Pod turns ready, before\\nmoving on.\\nPartitioned rolling updates\\nThe RollingUpdate  update strategy can be partitioned, by specifying\\na .spec.updateStrategy.rollingUpdate.partition . If a partition is specified, all Pods with an ordinal\\nthat is greater than or equal to the partition will be updated when the StatefulSet's\\n.spec.template  is updated. All Pods with an ordinal that is less than the partition will not be\\nupdated, and, even if they are deleted, they will be recreated at the previous version. If a\\nStatefulSet's .spec.updateStrategy.rollingUpdate.partition  is greater than its .spec.replicas ,\", metadata={'source': './PDFS/Concepts.pdf', 'page': 160}),\n",
       " Document(page_content=\"updates to its .spec.template  will not be propagated to its Pods. In most cases you will not need\\nto use a partition, but they are useful if you want to stage an update, roll out a canary, or\\nperform a phased roll out.\\nMaximum unavailable Pods\\nFEATURE STATE:  Kubernetes v1.24 [alpha]\\nYou can control the maximum number of Pods that can be unavailable during an update by\\nspecifying the .spec.updateStrategy.rollingUpdate.maxUnavailable  field. The value can be an\\nabsolute number (for example, 5) or a percentage of desired Pods (for example, 10%). Absolute\\nnumber is calculated from the percentage value by rounding it up. This field cannot be 0. The\\ndefault setting is 1.\\nThis field applies to all Pods in the range 0 to replicas - 1 . If there is any unavailable Pod in the\\nrange 0 to replicas - 1 , it will be counted towards maxUnavailable .\\nNote:  The maxUnavailable  field is in Alpha stage and it is honored only by API servers that are\\nrunning with the MaxUnavailableStatefulSet  feature gate  enabled.\\nForced rollback\\nWhen using Rolling Updates  with the default Pod Management Policy  (OrderedReady ), it's\\npossible to get into a broken state that requires manual intervention to repair.\\nIf you update the Pod template to a configuration that never becomes Running and Ready (for\\nexample, due to a bad binary or application-level configuration error), StatefulSet will stop the\\nrollout and wait.\\nIn this state, it's not enough to revert the Pod template to a good configuration. Due to a known\\nissue , StatefulSet will continue to wait for the broken Pod to become Ready (which never\\nhappens) before it will attempt to revert it back to the working configuration.\\nAfter reverting the template, you must also delete any Pods that StatefulSet had already\\nattempted to run with the bad configuration. StatefulSet will then begin to recreate the Pods\\nusing the reverted template.\\nPersistentVolumeClaim retention\\nFEATURE STATE:  Kubernetes v1.27 [beta]\\nThe optional .spec.persistentVolumeClaimRetentionPolicy  field controls if and how PVCs are\\ndeleted during the lifecycle of a StatefulSet. You must enable the StatefulSetAutoDeletePVC  \\nfeature gate  on the API server and the controller manager to use this field. Once enabled, there\\nare two policies you can configure for each StatefulSet:\\nwhenDeleted\\nconfigures the volume retention behavior that applies when the StatefulSet is deleted\\nwhenScaled\\nconfigures the volume retention behavior that applies when the replica count of the\\nStatefulSet is reduced; for example, when scaling down the set.\\nFor each policy that you can configure, you can set the value to either Delete  or Retain .\", metadata={'source': './PDFS/Concepts.pdf', 'page': 161}),\n",
       " Document(page_content='Delete\\nThe PVCs created from the StatefulSet volumeClaimTemplate  are deleted for each Pod\\naffected by the policy. With the whenDeleted  policy all PVCs from the \\nvolumeClaimTemplate  are deleted after their Pods have been deleted. With the \\nwhenScaled  policy, only PVCs corresponding to Pod replicas being scaled down are\\ndeleted, after their Pods have been deleted.\\nRetain  (default)\\nPVCs from the volumeClaimTemplate  are not affected when their Pod is deleted. This is\\nthe behavior before this new feature.\\nBear in mind that these policies only  apply when Pods are being removed due to the\\nStatefulSet being deleted or scaled down. For example, if a Pod associated with a StatefulSet\\nfails due to node failure, and the control plane creates a replacement Pod, the StatefulSet retains\\nthe existing PVC. The existing volume is unaffected, and the cluster will attach it to the node\\nwhere the new Pod is about to launch.\\nThe default for policies is Retain , matching the StatefulSet behavior before this new feature.\\nHere is an example policy.\\napiVersion : apps/v1\\nkind: StatefulSet\\n...\\nspec:\\n  persistentVolumeClaimRetentionPolicy :\\n    whenDeleted : Retain\\n    whenScaled : Delete\\n...\\nThe StatefulSet controller  adds owner references  to its PVCs, which are then deleted by the \\ngarbage collector  after the Pod is terminated. This enables the Pod to cleanly unmount all\\nvolumes before the PVCs are deleted (and before the backing PV and volume are deleted,\\ndepending on the retain policy). When you set the whenDeleted  policy to Delete , an owner\\nreference to the StatefulSet instance is placed on all PVCs associated with that StatefulSet.\\nThe whenScaled  policy must delete PVCs only when a Pod is scaled down, and not when a Pod\\nis deleted for another reason. When reconciling, the StatefulSet controller compares its desired\\nreplica count to the actual Pods present on the cluster. Any StatefulSet Pod whose id greater\\nthan the replica count is condemned and marked for deletion. If the whenScaled  policy is \\nDelete , the condemned Pods are first set as owners to the associated StatefulSet template PVCs,\\nbefore the Pod is deleted. This causes the PVCs to be garbage collected after only the\\ncondemned Pods have terminated.\\nThis means that if the controller crashes and restarts, no Pod will be deleted before its owner\\nreference has been updated appropriate to the policy. If a condemned Pod is force-deleted while\\nthe controller is down, the owner reference may or may not have been set up, depending on\\nwhen the controller crashed. It may take several reconcile loops to update the owner references,\\nso some condemned Pods may have set up owner references and others may not. For this\\nreason we recommend waiting for the controller to come back up, which will verify owner\\nreferences before terminating Pods. If that is not possible, the operator should verify the owner\\nreferences on PVCs to ensure the expected objects are deleted when Pods are force-deleted.', metadata={'source': './PDFS/Concepts.pdf', 'page': 162}),\n",
       " Document(page_content=\"Replicas\\n.spec.replicas  is an optional field that specifies the number of desired Pods. It defaults to 1.\\nShould you manually scale a deployment, example via kubectl scale statefulset statefulset --\\nreplicas=X , and then you update that StatefulSet based on a manifest (for example: by running \\nkubectl apply -f statefulset.yaml ), then applying that manifest overwrites the manual scaling\\nthat you previously did.\\nIf a HorizontalPodAutoscaler  (or any similar API for horizontal scaling) is managing scaling for\\na Statefulset, don't set .spec.replicas . Instead, allow the Kubernetes control plane  to manage\\nthe .spec.replicas  field automatically.\\nWhat's next\\nLearn about Pods .\\nFind out how to use StatefulSets\\nFollow an example of deploying a stateful application .\\nFollow an example of deploying Cassandra with Stateful Sets .\\nFollow an example of running a replicated stateful application .\\nLearn how to scale a StatefulSet .\\nLearn what's involved when you delete a StatefulSet .\\nLearn how to configure a Pod to use a volume for storage .\\nLearn how to configure a Pod to use a PersistentVolume for storage .\\nStatefulSet  is a top-level resource in the Kubernetes REST API. Read the StatefulSet  object\\ndefinition to understand the API for stateful sets.\\nRead about PodDisruptionBudget  and how you can use it to manage application\\navailability during disruptions.\\nDaemonSet\\nA DaemonSet defines Pods that provide node-local facilities. These might be fundamental to the\\noperation of your cluster, such as a networking helper tool, or be part of an add-on.\\nA DaemonSet  ensures that all (or some) Nodes run a copy of a Pod. As nodes are added to the\\ncluster, Pods are added to them. As nodes are removed from the cluster, those Pods are garbage\\ncollected. Deleting a DaemonSet will clean up the Pods it created.\\nSome typical uses of a DaemonSet are:\\nrunning a cluster storage daemon on every node\\nrunning a logs collection daemon on every node\\nrunning a node monitoring daemon on every node\\nIn a simple case, one DaemonSet, covering all nodes, would be used for each type of daemon. A\\nmore complex setup might use multiple DaemonSets for a single type of daemon, but with\\ndifferent flags and/or different memory and cpu requests for different hardware types.• \\n• \\n◦ \\n◦ \\n◦ \\n◦ \\n◦ \\n◦ \\n◦ \\n• \\n• \\n• \\n• \\n•\", metadata={'source': './PDFS/Concepts.pdf', 'page': 163}),\n",
       " Document(page_content='Writing a DaemonSet Spec\\nCreate a DaemonSet\\nYou can describe a DaemonSet in a YAML file. For example, the daemonset.yaml  file below\\ndescribes a DaemonSet that runs the fluentd-elasticsearch Docker image:\\ncontrollers/daemonset.yaml  \\napiVersion : apps/v1\\nkind: DaemonSet\\nmetadata :\\n  name : fluentd-elasticsearch\\n  namespace : kube-system\\n  labels :\\n    k8s-app : fluentd-logging\\nspec:\\n  selector :\\n    matchLabels :\\n      name : fluentd-elasticsearch\\n  template :\\n    metadata :\\n      labels :\\n        name : fluentd-elasticsearch\\n    spec:\\n      tolerations :\\n      # these tolerations are to have the daemonset runnable on control plane nodes\\n      # remove them if your control plane nodes should not run pods\\n      - key: node-role.kubernetes.io/control-plane\\n        operator : Exists\\n        effect : NoSchedule\\n      - key: node-role.kubernetes.io/master\\n        operator : Exists\\n        effect : NoSchedule\\n      containers :\\n      - name : fluentd-elasticsearch\\n        image : quay.io/fluentd_elasticsearch/fluentd:v2.5.2\\n        resources :\\n          limits :\\n            memory : 200Mi\\n          requests :\\n            cpu: 100m\\n            memory : 200Mi\\n        volumeMounts :\\n        - name : varlog\\n          mountPath : /var/log\\n      # it may be desirable to set a high priority class to ensure that a DaemonSet Pod\\n      # preempts running Pods\\n      # priorityClassName: important\\n      terminationGracePeriodSeconds : 30\\n      volumes :\\n      - name : varlog', metadata={'source': './PDFS/Concepts.pdf', 'page': 164}),\n",
       " Document(page_content='hostPath :\\n          path: /var/log\\nCreate a DaemonSet based on the YAML file:\\nkubectl apply -f https://k8s.io/examples/controllers/daemonset.yaml\\nRequired Fields\\nAs with all other Kubernetes config, a DaemonSet needs apiVersion , kind, and metadata  fields.\\nFor general information about working with config files, see running stateless applications  and \\nobject management using kubectl .\\nThe name of a DaemonSet object must be a valid DNS subdomain name .\\nA DaemonSet also needs a .spec  section.\\nPod Template\\nThe .spec.template  is one of the required fields in .spec .\\nThe .spec.template  is a pod template . It has exactly the same schema as a Pod, except it is nested\\nand does not have an apiVersion  or kind.\\nIn addition to required fields for a Pod, a Pod template in a DaemonSet has to specify\\nappropriate labels (see pod selector ).\\nA Pod Template in a DaemonSet must have a RestartPolicy  equal to Always , or be unspecified,\\nwhich defaults to Always .\\nPod Selector\\nThe .spec.selector  field is a pod selector. It works the same as the .spec.selector  of a Job.\\nYou must specify a pod selector that matches the labels of the .spec.template . Also, once a\\nDaemonSet is created, its .spec.selector  can not be mutated. Mutating the pod selector can lead\\nto the unintentional orphaning of Pods, and it was found to be confusing to users.\\nThe .spec.selector  is an object consisting of two fields:\\nmatchLabels  - works the same as the .spec.selector  of a ReplicationController .\\nmatchExpressions  - allows to build more sophisticated selectors by specifying key, list of\\nvalues and an operator that relates the key and values.\\nWhen the two are specified the result is ANDed.\\nThe .spec.selector  must match the .spec.template.metadata.labels . Config with these two not\\nmatching will be rejected by the API.\\nRunning Pods on select Nodes\\nIf you specify a .spec.template.spec.nodeSelector , then the DaemonSet controller will create\\nPods on nodes which match that node selector . Likewise if you specify• \\n•', metadata={'source': './PDFS/Concepts.pdf', 'page': 165}),\n",
       " Document(page_content=\"a .spec.template.spec.affinity , then DaemonSet controller will create Pods on nodes which\\nmatch that node affinity . If you do not specify either, then the DaemonSet controller will create\\nPods on all nodes.\\nHow Daemon Pods are scheduled\\nA DaemonSet can be used to ensure that all eligible nodes run a copy of a Pod. The DaemonSet\\ncontroller creates a Pod for each eligible node and adds the spec.affinity.nodeAffinity  field of the\\nPod to match the target host. After the Pod is created, the default scheduler typically takes over\\nand then binds the Pod to the target host by setting the .spec.nodeName  field. If the new Pod\\ncannot fit on the node, the default scheduler may preempt (evict) some of the existing Pods\\nbased on the priority  of the new Pod.\\nNote:  If it's important that the DaemonSet pod run on each node, it's often desirable to set\\nthe .spec.template.spec.priorityClassName  of the DaemonSet to a PriorityClass  with a higher\\npriority to ensure that this eviction occurs.\\nThe user can specify a different scheduler for the Pods of the DaemonSet, by setting the\\n.spec.template.spec.schedulerName  field of the DaemonSet.\\nThe original node affinity specified at the .spec.template.spec.affinity.nodeAffinity  field (if\\nspecified) is taken into consideration by the DaemonSet controller when evaluating the eligible\\nnodes, but is replaced on the created Pod with the node affinity that matches the name of the\\neligible node.\\nnodeAffinity :\\n  requiredDuringSchedulingIgnoredDuringExecution :\\n    nodeSelectorTerms :\\n    - matchFields :\\n      - key: metadata.name\\n        operator : In\\n        values :\\n        - target-host-name\\nTaints and tolerations\\nThe DaemonSet controller automatically adds a set of tolerations  to DaemonSet Pods:\\nTolerations for DaemonSet pods\\nToleration key Effect Details\\nnode.kubernetes.io/\\nnot-readyNoExecuteDaemonSet Pods can be scheduled onto nodes that are not\\nhealthy or ready to accept Pods. Any DaemonSet Pods\\nrunning on such nodes will not be evicted.\\nnode.kubernetes.io/\\nunreachableNoExecuteDaemonSet Pods can be scheduled onto nodes that are\\nunreachable from the node controller. Any DaemonSet\\nPods running on such nodes will not be evicted.\\nnode.kubernetes.io/\\ndisk-pressureNoScheduleDaemonSet Pods can be scheduled onto nodes with disk\\npressure issues.\\nnode.kubernetes.io/\\nmemory-pressureNoScheduleDaemonSet Pods can be scheduled onto nodes with\\nmemory pressure issues.\\nnode.kubernetes.io/pid-\\npressureNoScheduleDaemonSet Pods can be scheduled onto nodes with\\nprocess pressure issues.\", metadata={'source': './PDFS/Concepts.pdf', 'page': 166}),\n",
       " Document(page_content='Toleration key Effect Details\\nnode.kubernetes.io/\\nunschedulableNoScheduleDaemonSet Pods can be scheduled onto nodes that are\\nunschedulable.\\nnode.kubernetes.io/\\nnetwork-unavailableNoScheduleOnly added for DaemonSet Pods that request host\\nnetworking , i.e., Pods having spec.hostNetwork: true .\\nSuch DaemonSet Pods can be scheduled onto nodes with\\nunavailable network.\\nYou can add your own tolerations to the Pods of a DaemonSet as well, by defining these in the\\nPod template of the DaemonSet.\\nBecause the DaemonSet controller sets the node.kubernetes.io/unschedulable:NoSchedule\\ntoleration automatically, Kubernetes can run DaemonSet Pods on nodes that are marked as \\nunschedulable .\\nIf you use a DaemonSet to provide an important node-level function, such as cluster\\nnetworking , it is helpful that Kubernetes places DaemonSet Pods on nodes before they are\\nready. For example, without that special toleration, you could end up in a deadlock situation\\nwhere the node is not marked as ready because the network plugin is not running there, and at\\nthe same time the network plugin is not running on that node because the node is not yet\\nready.\\nCommunicating with Daemon Pods\\nSome possible patterns for communicating with Pods in a DaemonSet are:\\nPush : Pods in the DaemonSet are configured to send updates to another service, such as\\na stats database. They do not have clients.\\nNodeIP and Known Port : Pods in the DaemonSet can use a hostPort , so that the pods\\nare reachable via the node IPs. Clients know the list of node IPs somehow, and know the\\nport by convention.\\nDNS : Create a headless service  with the same pod selector, and then discover\\nDaemonSets using the endpoints  resource or retrieve multiple A records from DNS.\\nService : Create a service with the same Pod selector, and use the service to reach a\\ndaemon on a random node. (No way to reach specific node.)\\nUpdating a DaemonSet\\nIf node labels are changed, the DaemonSet will promptly add Pods to newly matching nodes\\nand delete Pods from newly not-matching nodes.\\nYou can modify the Pods that a DaemonSet creates. However, Pods do not allow all fields to be\\nupdated. Also, the DaemonSet controller will use the original template the next time a node\\n(even with the same name) is created.\\nYou can delete a DaemonSet. If you specify --cascade=orphan  with kubectl , then the Pods will\\nbe left on the nodes. If you subsequently create a new DaemonSet with the same selector, the\\nnew DaemonSet adopts the existing Pods. If any Pods need replacing the DaemonSet replaces\\nthem according to its updateStrategy .\\nYou can perform a rolling update  on a DaemonSet.• \\n• \\n• \\n•', metadata={'source': './PDFS/Concepts.pdf', 'page': 167}),\n",
       " Document(page_content=\"Alternatives to DaemonSet\\nInit scripts\\nIt is certainly possible to run daemon processes by directly starting them on a node (e.g. using \\ninit, upstartd , or systemd ). This is perfectly fine. However, there are several advantages to\\nrunning such processes via a DaemonSet:\\nAbility to monitor and manage logs for daemons in the same way as applications.\\nSame config language and tools (e.g. Pod templates, kubectl ) for daemons and\\napplications.\\nRunning daemons in containers with resource limits increases isolation between daemons\\nfrom app containers. However, this can also be accomplished by running the daemons in\\na container but not in a Pod.\\nBare Pods\\nIt is possible to create Pods directly which specify a particular node to run on. However, a\\nDaemonSet replaces Pods that are deleted or terminated for any reason, such as in the case of\\nnode failure or disruptive node maintenance, such as a kernel upgrade. For this reason, you\\nshould use a DaemonSet rather than creating individual Pods.\\nStatic Pods\\nIt is possible to create Pods by writing a file to a certain directory watched by Kubelet. These\\nare called static pods . Unlike DaemonSet, static Pods cannot be managed with kubectl or other\\nKubernetes API clients. Static Pods do not depend on the apiserver, making them useful in\\ncluster bootstrapping cases. Also, static Pods may be deprecated in the future.\\nDeployments\\nDaemonSets are similar to Deployments  in that they both create Pods, and those Pods have\\nprocesses which are not expected to terminate (e.g. web servers, storage servers).\\nUse a Deployment for stateless services, like frontends, where scaling up and down the number\\nof replicas and rolling out updates are more important than controlling exactly which host the\\nPod runs on. Use a DaemonSet when it is important that a copy of a Pod always run on all or\\ncertain hosts, if the DaemonSet provides node-level functionality that allows other Pods to run\\ncorrectly on that particular node.\\nFor example, network plugins  often include a component that runs as a DaemonSet. The\\nDaemonSet component makes sure that the node where it's running has working cluster\\nnetworking.\\nWhat's next\\nLearn about Pods .\\nLearn about static Pods , which are useful for running Kubernetes control plane\\ncomponents.\\nFind out how to use DaemonSets\\nPerform a rolling update on a DaemonSet• \\n• \\n• \\n• \\n◦ \\n• \\n◦\", metadata={'source': './PDFS/Concepts.pdf', 'page': 168}),\n",
       " Document(page_content='Perform a rollback on a DaemonSet  (for example, if a roll out didn\\'t work how you\\nexpected).\\nUnderstand how Kubernetes assigns Pods to Nodes .\\nLearn about device plugins  and add ons , which often run as DaemonSets.\\nDaemonSet  is a top-level resource in the Kubernetes REST API. Read the DaemonSet\\nobject definition to understand the API for daemon sets.\\nJobs\\nJobs represent one-off tasks that run to completion and then stop.\\nA Job creates one or more Pods and will continue to retry execution of the Pods until a\\nspecified number of them successfully terminate. As pods successfully complete, the Job tracks\\nthe successful completions. When a specified number of successful completions is reached, the\\ntask (ie, Job) is complete. Deleting a Job will clean up the Pods it created. Suspending a Job will\\ndelete its active Pods until the Job is resumed again.\\nA simple case is to create one Job object in order to reliably run one Pod to completion. The Job\\nobject will start a new Pod if the first Pod fails or is deleted (for example due to a node\\nhardware failure or a node reboot).\\nYou can also use a Job to run multiple Pods in parallel.\\nIf you want to run a Job (either a single task, or several in parallel) on a schedule, see CronJob .\\nRunning an example Job\\nHere is an example Job config. It computes π to 2000 places and prints it out. It takes around 10s\\nto complete.\\ncontrollers/job.yaml  \\napiVersion : batch/v1\\nkind: Job\\nmetadata :\\n  name : pi\\nspec:\\n  template :\\n    spec:\\n      containers :\\n      - name : pi\\n        image : perl:5.34.0\\n        command : [\"perl\" ,  \"-Mbignum=bpi\" , \"-wle\" , \"print bpi(2000)\" ]\\n      restartPolicy : Never\\n  backoffLimit : 4\\nYou can run the example with this command:\\nkubectl apply -f https://kubernetes.io/examples/controllers/job.yaml\\nThe output is similar to this:◦ \\n• \\n• \\n•', metadata={'source': './PDFS/Concepts.pdf', 'page': 169}),\n",
       " Document(page_content='job.batch/pi created\\nCheck on the status of the Job with kubectl :\\nkubectl describe job pi\\nkubectl get job pi -o yaml\\nName:           pi\\nNamespace:      default\\nSelector:       batch.kubernetes.io/controller-uid =c9948307-e56d-4b5d-8302-ae2d7b7da67c\\nLabels:         batch.kubernetes.io/controller-uid =c9948307-e56d-4b5d-8302-ae2d7b7da67c\\n                batch.kubernetes.io/job-name =pi\\n                ...\\nAnnotations:    batch.kubernetes.io/job-tracking: \"\"\\nParallelism:    1\\nCompletions:    1\\nStart Time:     Mon, 02 Dec 2019 15:20:11 +0200\\nCompleted At:   Mon, 02 Dec 2019 15:21:16 +0200\\nDuration:       65s\\nPods Statuses:  0 Running / 1 Succeeded / 0 Failed\\nPod Template:\\n  Labels:  batch.kubernetes.io/controller-uid =c9948307-e56d-4b5d-8302-ae2d7b7da67c\\n           batch.kubernetes.io/job-name =pi\\n  Containers:\\n   pi:\\n    Image:      perl:5.34.0\\n    Port:       <none>\\n    Host Port:  <none>\\n    Command:\\n      perl\\n      -Mbignum =bpi\\n      -wle\\n      print bpi (2000)\\n    Environment:  <none>\\n    Mounts:       <none>\\n  Volumes:        <none>\\nEvents:\\n  Type    Reason            Age   From            Message\\n  ----    ------            ----  ----            -------\\n  Normal  SuccessfulCreate  21s   job-controller  Created pod: pi-xf9p4\\n  Normal  Completed         18s   job-controller  Job completed\\napiVersion: batch/v1\\nkind: Job\\nmetadata:\\n  annotations: batch.kubernetes.io/job-tracking: \"\"\\n             ...  \\n  creationTimestamp: \"2022-11-10T17:53:53Z\"\\n  generation: 1\\n  labels:\\n    batch.kubernetes.io/controller-uid: 863452e6-270d-420e-9b94-53a54146c223• \\n•', metadata={'source': './PDFS/Concepts.pdf', 'page': 170}),\n",
       " Document(page_content='batch.kubernetes.io/job-name: pi\\n  name: pi\\n  namespace: default\\n  resourceVersion: \"4751\"\\n  uid: 204fb678-040b-497f-9266-35ffa8716d14\\nspec:\\n  backoffLimit: 4\\n  completionMode: NonIndexed\\n  completions: 1\\n  parallelism: 1\\n  selector:\\n    matchLabels:\\n      batch.kubernetes.io/controller-uid: 863452e6-270d-420e-9b94-53a54146c223\\n  suspend: false\\n  template:\\n    metadata:\\n      creationTimestamp: null\\n      labels:\\n        batch.kubernetes.io/controller-uid: 863452e6-270d-420e-9b94-53a54146c223\\n        batch.kubernetes.io/job-name: pi\\n    spec:\\n      containers:\\n      - command:\\n        - perl\\n        - -Mbignum =bpi\\n        - -wle\\n        - print bpi (2000)\\n        image: perl:5.34.0\\n        imagePullPolicy: IfNotPresent\\n        name: pi\\n        resources: {}\\n        terminationMessagePath: /dev/termination-log\\n        terminationMessagePolicy: File\\n      dnsPolicy: ClusterFirst\\n      restartPolicy: Never\\n      schedulerName: default-scheduler\\n      securityContext: {}\\n      terminationGracePeriodSeconds: 30\\nstatus:\\n  active: 1\\n  ready: 0\\n  startTime: \"2022-11-10T17:53:57Z\"\\n  uncountedTerminatedPods: {}\\nTo view completed Pods of a Job, use kubectl get pods .\\nTo list all the Pods that belong to a Job in a machine readable form, you can use a command like\\nthis:\\npods =$(kubectl get pods --selector =batch.kubernetes.io/job-name =pi --output =jsonpath =\\'{.items\\n[*].metadata.name}\\' )\\necho  $pods', metadata={'source': './PDFS/Concepts.pdf', 'page': 171}),\n",
       " Document(page_content='The output is similar to this:\\npi-5rwd7\\nHere, the selector is the same as the selector for the Job. The --output=jsonpath  option specifies\\nan expression with the name from each Pod in the returned list.\\nView the standard output of one of the pods:\\nkubectl logs $pods\\nAnother way to view the logs of a Job:\\nkubectl logs jobs/pi\\nThe output is similar to this:\\n3.1415926535897932384626433832795028841971693993751058209749445923078164062862089986\\n280348253421170679821480865132823066470938446095505822317253594081284811174502841027\\n019385211055596446229489549303819644288109756659334461284756482337867831652712019091\\n456485669234603486104543266482133936072602491412737245870066063155881748815209209628\\n292540917153643678925903600113305305488204665213841469519415116094330572703657595919\\n530921861173819326117931051185480744623799627495673518857527248912279381830119491298\\n336733624406566430860213949463952247371907021798609437027705392171762931767523846748\\n184676694051320005681271452635608277857713427577896091736371787214684409012249534301\\n465495853710507922796892589235420199561121290219608640344181598136297747713099605187\\n072113499999983729780499510597317328160963185950244594553469083026425223082533446850\\n352619311881710100031378387528865875332083814206171776691473035982534904287554687311\\n595628638823537875937519577818577805321712268066130019278766111959092164201989380952\\n572010654858632788659361533818279682303019520353018529689957736225994138912497217752\\n834791315155748572424541506959508295331168617278558890750983817546374649393192550604\\n009277016711390098488240128583616035637076601047101819429555961989467678374494482553\\n797747268471040475346462080466842590694912933136770289891521047521620569660240580381\\n501935112533824300355876402474964732639141992726042699227967823547816360093417216412\\n199245863150302861829745557067498385054945885869269956909272107975093029553211653449\\n872027559602364806654991198818347977535663698074265425278625518184175746728909777727\\n938000816470600161452491921732172147723501414419735685481613611573525521334757418494\\n684385233239073941433345477624168625189835694855620992192221842725502542568876717904\\n946016534668049886272327917860857843838279679766814541009538837863609506800642251252\\n051173929848960841284886269456042419652850222106611863067442786220391949450471237137\\n869609563643719172874677646575739624138908658326459958133904780275901\\nWriting a Job spec\\nAs with all other Kubernetes config, a Job needs apiVersion , kind, and metadata  fields.\\nWhen the control plane creates new Pods for a Job, the .metadata.name  of the Job is part of the\\nbasis for naming those Pods. The name of a Job must be a valid DNS subdomain  value, but this\\ncan produce unexpected results for the Pod hostnames. For best compatibility, the name should\\nfollow the more restrictive rules for a DNS label . Even when the name is a DNS subdomain, the\\nname must be no longer than 63 characters.\\nA Job also needs a .spec  section .', metadata={'source': './PDFS/Concepts.pdf', 'page': 172}),\n",
       " Document(page_content='Job Labels\\nJob labels will have batch.kubernetes.io/  prefix for job-name  and controller-uid .\\nPod Template\\nThe .spec.template  is the only required field of the .spec .\\nThe .spec.template  is a pod template . It has exactly the same schema as a Pod, except it is nested\\nand does not have an apiVersion  or kind.\\nIn addition to required fields for a Pod, a pod template in a Job must specify appropriate labels\\n(see pod selector ) and an appropriate restart policy.\\nOnly a RestartPolicy  equal to Never  or OnFailure  is allowed.\\nPod selector\\nThe .spec.selector  field is optional. In almost all cases you should not specify it. See section \\nspecifying your own pod selector .\\nParallel execution for Jobs\\nThere are three main types of task suitable to run as a Job:\\nNon-parallel Jobs\\nnormally, only one Pod is started, unless the Pod fails.\\nthe Job is complete as soon as its Pod terminates successfully.\\nParallel Jobs with a fixed completion count :\\nspecify a non-zero positive value for .spec.completions .\\nthe Job represents the overall task, and is complete when there are\\n.spec.completions  successful Pods.\\nwhen using .spec.completionMode=\"Indexed\" , each Pod gets a different index in the\\nrange 0 to .spec.completions-1 .\\nParallel Jobs with a work queue :\\ndo not specify .spec.completions , default to .spec.parallelism .\\nthe Pods must coordinate amongst themselves or an external service to determine\\nwhat each should work on. For example, a Pod might fetch a batch of up to N items\\nfrom the work queue.\\neach Pod is independently capable of determining whether or not all its peers are\\ndone, and thus that the entire Job is done.\\nwhen any Pod from the Job terminates with success, no new Pods are created.\\nonce at least one Pod has terminated with success and all Pods are terminated, then\\nthe Job is completed with success.\\nonce any Pod has exited with success, no other Pod should still be doing any work\\nfor this task or writing any output. They should all be in the process of exiting.\\nFor a non-parallel  Job, you can leave both .spec.completions  and .spec.parallelism  unset. When\\nboth are unset, both are defaulted to 1.\\nFor a fixed completion count  Job, you should set .spec.completions  to the number of completions\\nneeded. You can set .spec.parallelism , or leave it unset and it will default to 1.1. \\n◦ \\n◦ \\n2. \\n◦ \\n◦ \\n◦ \\n3. \\n◦ \\n◦ \\n◦ \\n◦ \\n◦ \\n◦', metadata={'source': './PDFS/Concepts.pdf', 'page': 173}),\n",
       " Document(page_content='For a work queue  Job, you must leave .spec.completions  unset, and set .spec.parallelism  to a\\nnon-negative integer.\\nFor more information about how to make use of the different types of job, see the job patterns\\nsection.\\nControlling parallelism\\nThe requested parallelism ( .spec.parallelism ) can be set to any non-negative value. If it is\\nunspecified, it defaults to 1. If it is specified as 0, then the Job is effectively paused until it is\\nincreased.\\nActual parallelism (number of pods running at any instant) may be more or less than requested\\nparallelism, for a variety of reasons:\\nFor fixed completion count  Jobs, the actual number of pods running in parallel will not\\nexceed the number of remaining completions. Higher values of .spec.parallelism  are\\neffectively ignored.\\nFor work queue  Jobs, no new Pods are started after any Pod has succeeded -- remaining\\nPods are allowed to complete, however.\\nIf the Job Controller  has not had time to react.\\nIf the Job controller failed to create Pods for any reason (lack of ResourceQuota , lack of\\npermission, etc.), then there may be fewer pods than requested.\\nThe Job controller may throttle new Pod creation due to excessive previous pod failures in\\nthe same Job.\\nWhen a Pod is gracefully shut down, it takes time to stop.\\nCompletion mode\\nFEATURE STATE:  Kubernetes v1.24 [stable]\\nJobs with fixed completion count  - that is, jobs that have non null .spec.completions  - can have a\\ncompletion mode that is specified in .spec.completionMode :\\nNonIndexed  (default): the Job is considered complete when there have\\nbeen .spec.completions  successfully completed Pods. In other words, each Pod completion\\nis homologous to each other. Note that Jobs that have null .spec.completions  are\\nimplicitly NonIndexed .\\nIndexed : the Pods of a Job get an associated completion index from 0\\nto .spec.completions-1 . The index is available through four mechanisms:\\nThe Pod annotation batch.kubernetes.io/job-completion-index .\\nThe Pod label batch.kubernetes.io/job-completion-index  (for v1.28 and later). Note\\nthe feature gate PodIndexLabel  must be enabled to use this label, and it is enabled\\nby default.\\nAs part of the Pod hostname, following the pattern $(job-name)-$(index) . When\\nyou use an Indexed Job in combination with a Service , Pods within the Job can use\\nthe deterministic hostnames to address each other via DNS. For more information\\nabout how to configure this, see Job with Pod-to-Pod Communication .\\nFrom the containerized task, in the environment variable \\nJOB_COMPLETION_INDEX .• \\n• \\n• \\n• \\n• \\n• \\n• \\n• \\n◦ \\n◦ \\n◦ \\n◦', metadata={'source': './PDFS/Concepts.pdf', 'page': 174}),\n",
       " Document(page_content='The Job is considered complete when there is one successfully completed Pod for each\\nindex. For more information about how to use this mode, see Indexed Job for Parallel\\nProcessing with Static Work Assignment .\\nNote:  Although rare, more than one Pod could be started for the same index (due to various\\nreasons such as node failures, kubelet restarts, or Pod evictions). In this case, only the first Pod\\nthat completes successfully will count towards the completion count and update the status of\\nthe Job. The other Pods that are running or completed for the same index will be deleted by the\\nJob controller once they are detected.\\nHandling Pod and container failures\\nA container in a Pod may fail for a number of reasons, such as because the process in it exited\\nwith a non-zero exit code, or the container was killed for exceeding a memory limit, etc. If this\\nhappens, and the .spec.template.spec.restartPolicy = \"OnFailure\" , then the Pod stays on the\\nnode, but the container is re-run. Therefore, your program needs to handle the case when it is\\nrestarted locally, or else specify .spec.template.spec.restartPolicy = \"Never\" . See pod lifecycle  for\\nmore information on restartPolicy .\\nAn entire Pod can also fail, for a number of reasons, such as when the pod is kicked off the\\nnode (node is upgraded, rebooted, deleted, etc.), or if a container of the Pod fails and the\\n.spec.template.spec.restartPolicy = \"Never\" . When a Pod fails, then the Job controller starts a\\nnew Pod. This means that your application needs to handle the case when it is restarted in a\\nnew pod. In particular, it needs to handle temporary files, locks, incomplete output and the like\\ncaused by previous runs.\\nBy default, each pod failure is counted towards the .spec.backoffLimit  limit, see pod backoff\\nfailure policy . However, you can customize handling of pod failures by setting the Job\\'s pod\\nfailure policy .\\nAdditionally, you can choose to count the pod failures independently for each index of an \\nIndexed  Job by setting the .spec.backoffLimitPerIndex  field (for more information, see backoff\\nlimit per index ).\\nNote that even if you specify .spec.parallelism = 1  and .spec.completions = 1\\nand .spec.template.spec.restartPolicy = \"Never\" , the same program may sometimes be started\\ntwice.\\nIf you do specify .spec.parallelism  and .spec.completions  both greater than 1, then there may be\\nmultiple pods running at once. Therefore, your pods must also be tolerant of concurrency.\\nWhen the feature gates  PodDisruptionConditions  and JobPodFailurePolicy  are both enabled,\\nand the .spec.podFailurePolicy  field is set, the Job controller does not consider a terminating\\nPod (a pod that has a .metadata.deletionTimestamp  field set) as a failure until that Pod is\\nterminal (its .status.phase  is Failed  or Succeeded ). However, the Job controller creates a\\nreplacement Pod as soon as the termination becomes apparent. Once the pod terminates, the\\nJob controller evaluates .backoffLimit  and .podFailurePolicy  for the relevant Job, taking this\\nnow-terminated Pod into consideration.\\nIf either of these requirements is not satisfied, the Job controller counts a terminating Pod as an\\nimmediate failure, even if that Pod later terminates with phase: \"Succeeded\" .', metadata={'source': './PDFS/Concepts.pdf', 'page': 175}),\n",
       " Document(page_content='Pod backoff failure policy\\nThere are situations where you want to fail a Job after some amount of retries due to a logical\\nerror in configuration etc. To do so, set .spec.backoffLimit  to specify the number of retries\\nbefore considering a Job as failed. The back-off limit is set by default to 6. Failed Pods associated\\nwith the Job are recreated by the Job controller with an exponential back-off delay (10s, 20s, 40s\\n...) capped at six minutes.\\nThe number of retries is calculated in two ways:\\nThe number of Pods with .status.phase = \"Failed\" .\\nWhen using restartPolicy = \"OnFailure\" , the number of retries in all the containers of\\nPods with .status.phase  equal to Pending  or Running .\\nIf either of the calculations reaches the .spec.backoffLimit , the Job is considered failed.\\nNote:  If your job has restartPolicy = \"OnFailure\" , keep in mind that your Pod running the Job\\nwill be terminated once the job backoff limit has been reached. This can make debugging the\\nJob\\'s executable more difficult. We suggest setting restartPolicy = \"Never\"  when debugging the\\nJob or using a logging system to ensure output from failed Jobs is not lost inadvertently.\\nBackoff limit per index\\nFEATURE STATE:  Kubernetes v1.28 [alpha]\\nNote:  You can only configure the backoff limit per index for an Indexed  Job, if you have the \\nJobBackoffLimitPerIndex  feature gate  enabled in your cluster.\\nWhen you run an indexed  Job, you can choose to handle retries for pod failures independently\\nfor each index. To do so, set the .spec.backoffLimitPerIndex  to specify the maximal number of\\npod failures per index.\\nWhen the per-index backoff limit is exceeded for an index, Kuberentes considers the index as\\nfailed and adds it to the .status.failedIndexes  field. The succeeded indexes, those with a\\nsuccessfully executed pods, are recorded in the .status.completedIndexes  field, regardless of\\nwhether you set the backoffLimitPerIndex  field.\\nNote that a failing index does not interrupt execution of other indexes. Once all indexes finish\\nfor a Job where you specified a backoff limit per index, if at least one of those indexes did fail,\\nthe Job controller marks the overall Job as failed, by setting the Failed condition in the status.\\nThe Job gets marked as failed even if some, potentially nearly all, of the indexes were processed\\nsuccessfully.\\nYou can additionally limit the maximal number of indexes marked failed by setting the\\n.spec.maxFailedIndexes  field. When the number of failed indexes exceeds the maxFailedIndexes\\nfield, the Job controller triggers termination of all remaining running Pods for that Job. Once all\\npods are terminated, the entire Job is marked failed by the Job controller, by setting the Failed\\ncondition in the Job status.\\nHere is an example manifest for a Job that defines a backoffLimitPerIndex :\\n/controllers/job-backoff-limit-per-index-example.yaml  \\napiVersion : batch/v1\\nkind: Job• \\n•', metadata={'source': './PDFS/Concepts.pdf', 'page': 176}),\n",
       " Document(page_content='metadata :\\n  name : job-backoff-limit-per-index-example\\nspec:\\n  completions : 10\\n  parallelism : 3\\n  completionMode : Indexed  # required for the feature\\n  backoffLimitPerIndex : 1  # maximal number of failures per index\\n  maxFailedIndexes : 5      # maximal number of failed indexes before terminating the Job \\nexecution\\n  template :\\n    spec:\\n      restartPolicy : Never  # required for the feature\\n      containers :\\n      - name : example\\n        image : python\\n        command :           # The jobs fails as there is at least one failed index\\n                           # (all even indexes fail in here), yet all indexes\\n                           # are executed as maxFailedIndexes is not exceeded.\\n        - python3\\n        - -c\\n        - |\\n          import os, sys\\n          print(\"Hello world\")\\n          if int(os.environ.get(\"JOB_COMPLETION_INDEX\")) % 2 == 0:\\n            sys.exit(1)           \\nIn the example above, the Job controller allows for one restart for each of the indexes. When the\\ntotal number of failed indexes exceeds 5, then the entire Job is terminated.\\nOnce the job is finished, the Job status looks as follows:\\nkubectl get -o yaml job job-backoff-limit-per-index-example\\n  status :\\n    completedIndexes : 1,3,5,7,9\\n    failedIndexes : 0,2,4,6,8\\n    succeeded : 5          # 1 succeeded pod for each of 5 succeeded indexes\\n    failed : 10            # 2 failed pods (1 retry) for each of 5 failed indexes\\n    conditions :\\n    - message : Job has failed indexes\\n      reason : FailedIndexes\\n      status : \"True\"\\n      type: Failed\\nAdditionally, you may want to use the per-index backoff along with a pod failure policy . When\\nusing per-index backoff, there is a new FailIndex  action available which allows you to avoid\\nunnecessary retries within an index.\\nPod failure policy\\nFEATURE STATE:  Kubernetes v1.26 [beta]\\nNote:  You can only configure a Pod failure policy for a Job if you have the JobPodFailurePolicy  \\nfeature gate  enabled in your cluster. Additionally, it is recommended to enable the', metadata={'source': './PDFS/Concepts.pdf', 'page': 177}),\n",
       " Document(page_content='PodDisruptionConditions  feature gate in order to be able to detect and handle Pod disruption\\nconditions in the Pod failure policy (see also: Pod disruption conditions ). Both feature gates are\\navailable in Kubernetes 1.28.\\nA Pod failure policy, defined with the .spec.podFailurePolicy  field, enables your cluster to\\nhandle Pod failures based on the container exit codes and the Pod conditions.\\nIn some situations, you may want to have a better control when handling Pod failures than the\\ncontrol provided by the Pod backoff failure policy , which is based on the\\nJob\\'s .spec.backoffLimit . These are some examples of use cases:\\nTo optimize costs of running workloads by avoiding unnecessary Pod restarts, you can\\nterminate a Job as soon as one of its Pods fails with an exit code indicating a software\\nbug.\\nTo guarantee that your Job finishes even if there are disruptions, you can ignore Pod\\nfailures caused by disruptions (such as preemption , API-initiated eviction  or taint -based\\neviction) so that they don\\'t count towards the .spec.backoffLimit  limit of retries.\\nYou can configure a Pod failure policy, in the .spec.podFailurePolicy  field, to meet the above use\\ncases. This policy can handle Pod failures based on the container exit codes and the Pod\\nconditions.\\nHere is a manifest for a Job that defines a podFailurePolicy :\\n/controllers/job-pod-failure-policy-example.yaml  \\napiVersion : batch/v1\\nkind: Job\\nmetadata :\\n  name : job-pod-failure-policy-example\\nspec:\\n  completions : 12\\n  parallelism : 3\\n  template :\\n    spec:\\n      restartPolicy : Never\\n      containers :\\n      - name : main\\n        image : docker.io/library/bash:5\\n        command : [\"bash\" ]        # example command simulating a bug which triggers the FailJob \\naction\\n        args:\\n        - -c\\n        - echo \"Hello world!\" && sleep 5 && exit 42\\n  backoffLimit : 6\\n  podFailurePolicy :\\n    rules :\\n    - action : FailJob\\n      onExitCodes :\\n        containerName : main      # optional\\n        operator: In             # one of : In, NotIn\\n        values : [42]\\n    - action: Ignore             # one of : Ignore, FailJob, Count• \\n•', metadata={'source': './PDFS/Concepts.pdf', 'page': 178}),\n",
       " Document(page_content=\"onPodConditions :\\n      - type: DisruptionTarget   # indicates Pod disruption\\nIn the example above, the first rule of the Pod failure policy specifies that the Job should be\\nmarked failed if the main  container fails with the 42 exit code. The following are the rules for\\nthe main  container specifically:\\nan exit code of 0 means that the container succeeded\\nan exit code of 42 means that the entire Job  failed\\nany other exit code represents that the container failed, and hence the entire Pod. The\\nPod will be re-created if the total number of restarts is below backoffLimit . If the \\nbackoffLimit  is reached the entire Job  failed.\\nNote:  Because the Pod template specifies a restartPolicy: Never , the kubelet does not restart\\nthe main  container in that particular Pod.\\nThe second rule of the Pod failure policy, specifying the Ignore  action for failed Pods with\\ncondition DisruptionTarget  excludes Pod disruptions from being counted towards\\nthe .spec.backoffLimit  limit of retries.\\nNote:  If the Job failed, either by the Pod failure policy or Pod backoff failure policy, and the Job\\nis running multiple Pods, Kubernetes terminates all the Pods in that Job that are still Pending or\\nRunning.\\nThese are some requirements and semantics of the API:\\nif you want to use a .spec.podFailurePolicy  field for a Job, you must also define that Job's\\npod template with .spec.restartPolicy  set to Never .\\nthe Pod failure policy rules you specify under spec.podFailurePolicy.rules  are evaluated in\\norder. Once a rule matches a Pod failure, the remaining rules are ignored. When no rule\\nmatches the Pod failure, the default handling applies.\\nyou may want to restrict a rule to a specific container by specifying its name\\ninspec.podFailurePolicy.rules[*].onExitCodes.containerName . When not specified the rule\\napplies to all containers. When specified, it should match one the container or \\ninitContainer  names in the Pod template.\\nyou may specify the action taken when a Pod failure policy is matched by \\nspec.podFailurePolicy.rules[*].action . Possible values are:\\nFailJob : use to indicate that the Pod's job should be marked as Failed and all\\nrunning Pods should be terminated.\\nIgnore : use to indicate that the counter towards the .spec.backoffLimit  should not\\nbe incremented and a replacement Pod should be created.\\nCount : use to indicate that the Pod should be handled in the default way. The\\ncounter towards the .spec.backoffLimit  should be incremented.\\nFailIndex : use this action along with backoff limit per index  to avoid unnecessary\\nretries within the index of a failed pod.\\nNote:  When you use a podFailurePolicy , the job controller only matches Pods in the Failed\\nphase. Pods with a deletion timestamp that are not in a terminal phase ( Failed  or Succeeded ) are\\nconsidered still terminating. This implies that terminating pods retain a tracking finalizer  until\\nthey reach a terminal phase. Since Kubernetes 1.27, Kubelet transitions deleted pods to a\\nterminal phase (see: Pod Phase ). This ensures that deleted pods have their finalizers removed by\\nthe Job controller.• \\n• \\n• \\n• \\n• \\n• \\n• \\n◦ \\n◦ \\n◦ \\n◦\", metadata={'source': './PDFS/Concepts.pdf', 'page': 179}),\n",
       " Document(page_content='Note:  Starting with Kubernetes v1.28, when Pod failure policy is used, the Job controller\\nrecreates terminating Pods only once these Pods reach the terminal Failed  phase. This behavior\\nis similar to podReplacementPolicy: Failed . For more information, see Pod replacement policy .\\nJob termination and cleanup\\nWhen a Job completes, no more Pods are created, but the Pods are usually  not deleted either.\\nKeeping them around allows you to still view the logs of completed pods to check for errors,\\nwarnings, or other diagnostic output. The job object also remains after it is completed so that\\nyou can view its status. It is up to the user to delete old jobs after noting their status. Delete the\\njob with kubectl  (e.g. kubectl delete jobs/pi  or kubectl delete -f ./job.yaml ). When you delete the\\njob using kubectl , all the pods it created are deleted too.\\nBy default, a Job will run uninterrupted unless a Pod fails ( restartPolicy=Never ) or a Container\\nexits in error ( restartPolicy=OnFailure ), at which point the Job defers to the .spec.backoffLimit\\ndescribed above. Once .spec.backoffLimit  has been reached the Job will be marked as failed and\\nany running Pods will be terminated.\\nAnother way to terminate a Job is by setting an active deadline. Do this by setting the\\n.spec.activeDeadlineSeconds  field of the Job to a number of seconds. The activeDeadlineSeconds\\napplies to the duration of the job, no matter how many Pods are created. Once a Job reaches \\nactiveDeadlineSeconds , all of its running Pods are terminated and the Job status will become \\ntype: Failed  with reason: DeadlineExceeded .\\nNote that a Job\\'s .spec.activeDeadlineSeconds  takes precedence over its .spec.backoffLimit .\\nTherefore, a Job that is retrying one or more failed Pods will not deploy additional Pods once it\\nreaches the time limit specified by activeDeadlineSeconds , even if the backoffLimit  is not yet\\nreached.\\nExample:\\napiVersion : batch/v1\\nkind: Job\\nmetadata :\\n  name : pi-with-timeout\\nspec:\\n  backoffLimit : 5\\n  activeDeadlineSeconds : 100\\n  template :\\n    spec:\\n      containers :\\n      - name : pi\\n        image : perl:5.34.0\\n        command : [\"perl\" , \"-Mbignum=bpi\" , \"-wle\" , \"print bpi(2000)\" ]\\n      restartPolicy : Never\\nNote that both the Job spec and the Pod template spec  within the Job have an \\nactiveDeadlineSeconds  field. Ensure that you set this field at the proper level.\\nKeep in mind that the restartPolicy  applies to the Pod, and not to the Job itself: there is no\\nautomatic Job restart once the Job status is type: Failed . That is, the Job termination\\nmechanisms activated with .spec.activeDeadlineSeconds  and .spec.backoffLimit  result in a\\npermanent Job failure that requires manual intervention to resolve.', metadata={'source': './PDFS/Concepts.pdf', 'page': 180}),\n",
       " Document(page_content='Clean up finished jobs automatically\\nFinished Jobs are usually no longer needed in the system. Keeping them around in the system\\nwill put pressure on the API server. If the Jobs are managed directly by a higher level controller,\\nsuch as CronJobs , the Jobs can be cleaned up by CronJobs based on the specified capacity-based\\ncleanup policy.\\nTTL mechanism for finished Jobs\\nFEATURE STATE:  Kubernetes v1.23 [stable]\\nAnother way to clean up finished Jobs (either Complete  or Failed ) automatically is to use a TTL\\nmechanism provided by a TTL controller  for finished resources, by specifying\\nthe .spec.ttlSecondsAfterFinished  field of the Job.\\nWhen the TTL controller cleans up the Job, it will delete the Job cascadingly, i.e. delete its\\ndependent objects, such as Pods, together with the Job. Note that when the Job is deleted, its\\nlifecycle guarantees, such as finalizers, will be honored.\\nFor example:\\napiVersion : batch/v1\\nkind: Job\\nmetadata :\\n  name : pi-with-ttl\\nspec:\\n  ttlSecondsAfterFinished : 100\\n  template :\\n    spec:\\n      containers :\\n      - name : pi\\n        image : perl:5.34.0\\n        command : [\"perl\" , \"-Mbignum=bpi\" , \"-wle\" , \"print bpi(2000)\" ]\\n      restartPolicy : Never\\nThe Job pi-with-ttl  will be eligible to be automatically deleted, 100 seconds after it finishes.\\nIf the field is set to 0, the Job will be eligible to be automatically deleted immediately after it\\nfinishes. If the field is unset, this Job won\\'t be cleaned up by the TTL controller after it finishes.\\nNote:\\nIt is recommended to set ttlSecondsAfterFinished  field because unmanaged jobs (Jobs that you\\ncreated directly, and not indirectly through other workload APIs such as CronJob) have a\\ndefault deletion policy of orphanDependents  causing Pods created by an unmanaged Job to be\\nleft around after that Job is fully deleted. Even though the control plane  eventually garbage\\ncollects  the Pods from a deleted Job after they either fail or complete, sometimes those lingering\\npods may cause cluster performance degradation or in worst case cause the cluster to go offline\\ndue to this degradation.\\nYou can use LimitRanges  and ResourceQuotas  to place a cap on the amount of resources that a\\nparticular namespace can consume.', metadata={'source': './PDFS/Concepts.pdf', 'page': 181}),\n",
       " Document(page_content='Job patterns\\nThe Job object can be used to process a set of independent but related work items . These might\\nbe emails to be sent, frames to be rendered, files to be transcoded, ranges of keys in a NoSQL\\ndatabase to scan, and so on.\\nIn a complex system, there may be multiple different sets of work items. Here we are just\\nconsidering one set of work items that the user wants to manage together — a batch job .\\nThere are several different patterns for parallel computation, each with strengths and\\nweaknesses. The tradeoffs are:\\nOne Job object for each work item, versus a single Job object for all work items. One Job\\nper work item creates some overhead for the user and for the system to manage large\\nnumbers of Job objects. A single Job for all work items is better for large numbers of\\nitems.\\nNumber of Pods created equals number of work items, versus each Pod can process\\nmultiple work items. When the number of Pods equals the number of work items, the\\nPods typically requires less modification to existing code and containers. Having each\\nPod process multiple work items is better for large numbers of items.\\nSeveral approaches use a work queue. This requires running a queue service, and\\nmodifications to the existing program or container to make it use the work queue. Other\\napproaches are easier to adapt to an existing containerised application.\\nWhen the Job is associated with a headless Service , you can enable the Pods within a Job\\nto communicate with each other to collaborate in a computation.\\nThe tradeoffs are summarized here, with columns 2 to 4 corresponding to the above tradeoffs.\\nThe pattern names are also links to examples and more detailed description.\\nPatternSingle Job\\nobjectFewer pods than work\\nitems?Use app\\nunmodified?\\nQueue with Pod Per Work Item sometimes\\nQueue with Variable Pod Count \\nIndexed Job with Static Work\\nAssignment \\nJob with Pod-to-Pod\\nCommunication sometimes sometimes\\nJob Template Expansion \\nWhen you specify completions with .spec.completions , each Pod created by the Job controller\\nhas an identical spec. This means that all pods for a task will have the same command line and\\nthe same image, the same volumes, and (almost) the same environment variables. These\\npatterns are different ways to arrange for pods to work on different things.\\nThis table shows the required settings for .spec.parallelism  and .spec.completions  for each of the\\npatterns. Here, W is the number of work items.\\nPattern .spec.completions .spec.parallelism\\nQueue with Pod Per Work Item W any\\nQueue with Variable Pod Count null any\\nIndexed Job with Static Work Assignment W any• \\n• \\n• \\n•', metadata={'source': './PDFS/Concepts.pdf', 'page': 182}),\n",
       " Document(page_content='Pattern .spec.completions .spec.parallelism\\nJob with Pod-to-Pod Communication W W\\nJob Template Expansion 1 should be 1\\nAdvanced usage\\nSuspending a Job\\nFEATURE STATE:  Kubernetes v1.24 [stable]\\nWhen a Job is created, the Job controller will immediately begin creating Pods to satisfy the\\nJob\\'s requirements and will continue to do so until the Job is complete. However, you may want\\nto temporarily suspend a Job\\'s execution and resume it later, or start Jobs in suspended state\\nand have a custom controller decide later when to start them.\\nTo suspend a Job, you can update the .spec.suspend  field of the Job to true; later, when you\\nwant to resume it again, update it to false. Creating a Job with .spec.suspend  set to true will\\ncreate it in the suspended state.\\nWhen a Job is resumed from suspension, its .status.startTime  field will be reset to the current\\ntime. This means that the .spec.activeDeadlineSeconds  timer will be stopped and reset when a\\nJob is suspended and resumed.\\nWhen you suspend a Job, any running Pods that don\\'t have a status of Completed  will be \\nterminated . with a SIGTERM signal. The Pod\\'s graceful termination period will be honored and\\nyour Pod must handle this signal in this period. This may involve saving progress for later or\\nundoing changes. Pods terminated this way will not count towards the Job\\'s completions  count.\\nAn example Job definition in the suspended state can be like so:\\nkubectl get job myjob -o yaml\\napiVersion : batch/v1\\nkind: Job\\nmetadata :\\n  name : myjob\\nspec:\\n  suspend : true\\n  parallelism : 1\\n  completions : 5\\n  template :\\n    spec:\\n      ...\\nYou can also toggle Job suspension by patching the Job using the command line.\\nSuspend an active Job:\\nkubectl patch job/myjob --type =strategic --patch \\'{\"spec\":{\"suspend\":true}}\\'\\nResume a suspended Job:\\nkubectl patch job/myjob --type =strategic --patch \\'{\"spec\":{\"suspend\":false}}\\'', metadata={'source': './PDFS/Concepts.pdf', 'page': 183}),\n",
       " Document(page_content='The Job\\'s status can be used to determine if a Job is suspended or has been suspended in the\\npast:\\nkubectl get jobs/myjob -o yaml\\napiVersion : batch/v1\\nkind: Job\\n# .metadata and .spec omitted\\nstatus :\\n  conditions :\\n  - lastProbeTime : \"2021-02-05T13:14:33Z\"\\n    lastTransitionTime : \"2021-02-05T13:14:33Z\"\\n    status : \"True\"\\n    type: Suspended\\n  startTime : \"2021-02-05T13:13:48Z\"\\nThe Job condition of type \"Suspended\" with status \"True\" means the Job is suspended; the \\nlastTransitionTime  field can be used to determine how long the Job has been suspended for. If\\nthe status of that condition is \"False\", then the Job was previously suspended and is now\\nrunning. If such a condition does not exist in the Job\\'s status, the Job has never been stopped.\\nEvents are also created when the Job is suspended and resumed:\\nkubectl describe jobs/myjob\\nName:           myjob\\n...\\nEvents:\\n  Type    Reason            Age   From            Message\\n  ----    ------            ----  ----            -------\\n  Normal  SuccessfulCreate  12m   job-controller  Created pod: myjob-hlrpl\\n  Normal  SuccessfulDelete  11m   job-controller  Deleted pod: myjob-hlrpl\\n  Normal  Suspended         11m   job-controller  Job suspended\\n  Normal  SuccessfulCreate  3s    job-controller  Created pod: myjob-jvb44\\n  Normal  Resumed           3s    job-controller  Job resumed\\nThe last four events, particularly the \"Suspended\" and \"Resumed\" events, are directly a result of\\ntoggling the .spec.suspend  field. In the time between these two events, we see that no Pods\\nwere created, but Pod creation restarted as soon as the Job was resumed.\\nMutable Scheduling Directives\\nFEATURE STATE:  Kubernetes v1.27 [stable]\\nIn most cases, a parallel job will want the pods to run with constraints, like all in the same zone,\\nor all either on GPU model x or y but not a mix of both.\\nThe suspend  field is the first step towards achieving those semantics. Suspend allows a custom\\nqueue controller to decide when a job should start; However, once a job is unsuspended, a\\ncustom queue controller has no influence on where the pods of a job will actually land.\\nThis feature allows updating a Job\\'s scheduling directives before it starts, which gives custom\\nqueue controllers the ability to influence pod placement while at the same time offloading', metadata={'source': './PDFS/Concepts.pdf', 'page': 184}),\n",
       " Document(page_content=\"actual pod-to-node assignment to kube-scheduler. This is allowed only for suspended Jobs that\\nhave never been unsuspended before.\\nThe fields in a Job's pod template that can be updated are node affinity, node selector,\\ntolerations, labels, annotations and scheduling gates .\\nSpecifying your own Pod selector\\nNormally, when you create a Job object, you do not specify .spec.selector . The system defaulting\\nlogic adds this field when the Job is created. It picks a selector value that will not overlap with\\nany other jobs.\\nHowever, in some cases, you might need to override this automatically set selector. To do this,\\nyou can specify the .spec.selector  of the Job.\\nBe very careful when doing this. If you specify a label selector which is not unique to the pods\\nof that Job, and which matches unrelated Pods, then pods of the unrelated job may be deleted,\\nor this Job may count other Pods as completing it, or one or both Jobs may refuse to create Pods\\nor run to completion. If a non-unique selector is chosen, then other controllers (e.g.\\nReplicationController) and their Pods may behave in unpredictable ways too. Kubernetes will\\nnot stop you from making a mistake when specifying .spec.selector .\\nHere is an example of a case when you might want to use this feature.\\nSay Job old is already running. You want existing Pods to keep running, but you want the rest\\nof the Pods it creates to use a different pod template and for the Job to have a new name. You\\ncannot update the Job because these fields are not updatable. Therefore, you delete Job old but \\nleave its pods running , using kubectl delete jobs/old --cascade=orphan . Before deleting it, you\\nmake a note of what selector it uses:\\nkubectl get job old -o yaml\\nThe output is similar to this:\\nkind: Job\\nmetadata :\\n  name : old\\n  ...\\nspec:\\n  selector :\\n    matchLabels :\\n      batch.kubernetes.io/controller-uid : a8f3d00d-c6d2-11e5-9f87-42010af00002\\n  ...\\nThen you create a new Job with name new and you explicitly specify the same selector. Since\\nthe existing Pods have label batch.kubernetes.io/controller-uid=a8f3d00d-\\nc6d2-11e5-9f87-42010af00002 , they are controlled by Job new as well.\\nYou need to specify manualSelector: true  in the new Job since you are not using the selector\\nthat the system normally generates for you automatically.\\nkind: Job\\nmetadata :\\n  name : new\", metadata={'source': './PDFS/Concepts.pdf', 'page': 185}),\n",
       " Document(page_content='...\\nspec:\\n  manualSelector : true\\n  selector :\\n    matchLabels :\\n      batch.kubernetes.io/controller-uid : a8f3d00d-c6d2-11e5-9f87-42010af00002\\n  ...\\nThe new Job itself will have a different uid from a8f3d00d-c6d2-11e5-9f87-42010af00002 . Setting \\nmanualSelector: true  tells the system that you know what you are doing and to allow this\\nmismatch.\\nJob tracking with finalizers\\nFEATURE STATE:  Kubernetes v1.26 [stable]\\nThe control plane keeps track of the Pods that belong to any Job and notices if any such Pod is\\nremoved from the API server. To do that, the Job controller creates Pods with the finalizer \\nbatch.kubernetes.io/job-tracking . The controller removes the finalizer only after the Pod has\\nbeen accounted for in the Job status, allowing the Pod to be removed by other controllers or\\nusers.\\nNote:  See My pod stays terminating  if you observe that pods from a Job are stucked with the\\ntracking finalizer.\\nElastic Indexed Jobs\\nFEATURE STATE:  Kubernetes v1.27 [beta]\\nYou can scale Indexed Jobs up or down by mutating both .spec.parallelism\\nand .spec.completions  together such that .spec.parallelism == .spec.completions . When the \\nElasticIndexedJob feature gate  on the API server  is disabled, .spec.completions  is immutable.\\nUse cases for elastic Indexed Jobs include batch workloads which require scaling an indexed\\nJob, such as MPI, Horovord, Ray, and PyTorch training jobs.\\nDelayed creation of replacement pods\\nFEATURE STATE:  Kubernetes v1.28 [alpha]\\nNote:  You can only set podReplacementPolicy  on Jobs if you enable the \\nJobPodReplacementPolicy  feature gate .\\nBy default, the Job controller recreates Pods as soon they either fail or are terminating (have a\\ndeletion timestamp). This means that, at a given time, when some of the Pods are terminating,\\nthe number of running Pods for a Job can be greater than parallelism  or greater than one Pod\\nper index (if you are using an Indexed Job).\\nYou may choose to create replacement Pods only when the terminating Pod is fully terminal\\n(has status.phase: Failed ). To do this, set the .spec.podReplacementPolicy: Failed . The default\\nreplacement policy depends on whether the Job has a podFailurePolicy  set. With no Pod failure\\npolicy defined for a Job, omitting the podReplacementPolicy  field selects the \\nTerminatingOrFailed  replacement policy: the control plane creates replacement Pods\\nimmediately upon Pod deletion (as soon as the control plane sees that a Pod for this Job has', metadata={'source': './PDFS/Concepts.pdf', 'page': 186}),\n",
       " Document(page_content='deletionTimestamp  set). For Jobs with a Pod failure policy set, the default \\npodReplacementPolicy  is Failed , and no other value is permitted. See Pod failure policy  to learn\\nmore about Pod failure policies for Jobs.\\nkind: Job\\nmetadata :\\n  name : new\\n  ...\\nspec:\\n  podReplacementPolicy : Failed\\n  ...\\nProvided your cluster has the feature gate enabled, you can inspect the .status.terminating  field\\nof a Job. The value of the field is the number of Pods owned by the Job that are currently\\nterminating.\\nkubectl get jobs/myjob -o yaml\\napiVersion : batch/v1\\nkind: Job\\n# .metadata and .spec omitted\\nstatus :\\n  terminating : 3 # three Pods are terminating and have not yet reached the Failed phase\\nAlternatives\\nBare Pods\\nWhen the node that a Pod is running on reboots or fails, the pod is terminated and will not be\\nrestarted. However, a Job will create new Pods to replace terminated ones. For this reason, we\\nrecommend that you use a Job rather than a bare Pod, even if your application requires only a\\nsingle Pod.\\nReplication Controller\\nJobs are complementary to Replication Controllers . A Replication Controller manages Pods\\nwhich are not expected to terminate (e.g. web servers), and a Job manages Pods that are\\nexpected to terminate (e.g. batch tasks).\\nAs discussed in Pod Lifecycle , Job is only appropriate for pods with RestartPolicy  equal to \\nOnFailure  or Never . (Note: If RestartPolicy  is not set, the default value is Always .)\\nSingle Job starts controller Pod\\nAnother pattern is for a single Job to create a Pod which then creates other Pods, acting as a\\nsort of custom controller for those Pods. This allows the most flexibility, but may be somewhat\\ncomplicated to get started with and offers less integration with Kubernetes.\\nOne example of this pattern would be a Job which starts a Pod which runs a script that in turn\\nstarts a Spark master controller (see spark example ), runs a spark driver, and then cleans up.', metadata={'source': './PDFS/Concepts.pdf', 'page': 187}),\n",
       " Document(page_content=\"An advantage of this approach is that the overall process gets the completion guarantee of a Job\\nobject, but maintains complete control over what Pods are created and how work is assigned to\\nthem.\\nWhat's next\\nLearn about Pods .\\nRead about different ways of running Jobs:\\nCoarse Parallel Processing Using a Work Queue\\nFine Parallel Processing Using a Work Queue\\nUse an indexed Job for parallel processing with static work assignment\\nCreate multiple Jobs based on a template: Parallel Processing using Expansions\\nFollow the links within Clean up finished jobs automatically  to learn more about how\\nyour cluster can clean up completed and / or failed tasks.\\nJob is part of the Kubernetes REST API. Read the Job object definition to understand the\\nAPI for jobs.\\nRead about CronJob , which you can use to define a series of Jobs that will run based on a\\nschedule, similar to the UNIX tool cron.\\nPractice how to configure handling of retriable and non-retriable pod failures using \\npodFailurePolicy , based on the step-by-step examples .\\nAutomatic Cleanup for Finished Jobs\\nA time-to-live mechanism to clean up old Jobs that have finished execution.\\nFEATURE STATE:  Kubernetes v1.23 [stable]\\nWhen your Job has finished, it's useful to keep that Job in the API (and not immediately delete\\nthe Job) so that you can tell whether the Job succeeded or failed.\\nKubernetes' TTL-after-finished controller  provides a TTL (time to live) mechanism to limit the\\nlifetime of Job objects that have finished execution.\\nCleanup for finished Jobs\\nThe TTL-after-finished controller is only supported for Jobs. You can use this mechanism to\\nclean up finished Jobs (either Complete  or Failed ) automatically by specifying\\nthe .spec.ttlSecondsAfterFinished  field of a Job, as in this example .\\nThe TTL-after-finished controller assumes that a Job is eligible to be cleaned up TTL seconds\\nafter the Job has finished. The timer starts once the status condition of the Job changes to show\\nthat the Job is either Complete  or Failed ; once the TTL has expired, that Job becomes eligible\\nfor cascading  removal. When the TTL-after-finished controller cleans up a job, it will delete it\\ncascadingly, that is to say it will delete its dependent objects together with it.\\nKubernetes honors object lifecycle guarantees on the Job, such as waiting for finalizers .\\nYou can set the TTL seconds at any time. Here are some examples for setting the\\n.spec.ttlSecondsAfterFinished  field of a Job:\\nSpecify this field in the Job manifest, so that a Job can be cleaned up automatically some\\ntime after it finishes.• \\n• \\n◦ \\n◦ \\n◦ \\n◦ \\n• \\n• \\n• \\n• \\n•\", metadata={'source': './PDFS/Concepts.pdf', 'page': 188}),\n",
       " Document(page_content=\"Manually set this field of existing, already finished Jobs, so that they become eligible for\\ncleanup.\\nUse a mutating admission webhook  to set this field dynamically at Job creation time.\\nCluster administrators can use this to enforce a TTL policy for finished jobs.\\nUse a mutating admission webhook  to set this field dynamically after the Job has finished,\\nand choose different TTL values based on job status, labels. For this case, the webhook\\nneeds to detect changes to the .status  of the Job and only set a TTL when the Job is being\\nmarked as completed.\\nWrite your own controller to manage the cleanup TTL for Jobs that match a particular \\nselector-selector .\\nCaveats\\nUpdating TTL for finished Jobs\\nYou can modify the TTL period, e.g. .spec.ttlSecondsAfterFinished  field of Jobs, after the job is\\ncreated or has finished. If you extend the TTL period after the existing ttlSecondsAfterFinished\\nperiod has expired, Kubernetes doesn't guarantee to retain that Job, even if an update to extend\\nthe TTL returns a successful API response.\\nTime skew\\nBecause the TTL-after-finished controller uses timestamps stored in the Kubernetes jobs to\\ndetermine whether the TTL has expired or not, this feature is sensitive to time skew in your\\ncluster, which may cause the control plane to clean up Job objects at the wrong time.\\nClocks aren't always correct, but the difference should be very small. Please be aware of this\\nrisk when setting a non-zero TTL.\\nWhat's next\\nRead Clean up Jobs automatically\\nRefer to the Kubernetes Enhancement Proposal  (KEP) for adding this mechanism.\\nCronJob\\nA CronJob starts one-time Jobs on a repeating schedule.\\nFEATURE STATE:  Kubernetes v1.21 [stable]\\nA CronJob  creates Jobs on a repeating schedule.\\nCronJob is meant for performing regular scheduled actions such as backups, report generation,\\nand so on. One CronJob object is like one line of a crontab  (cron table) file on a Unix system. It\\nruns a job periodically on a given schedule, written in Cron  format.\\nCronJobs have limitations and idiosyncrasies. For example, in certain circumstances, a single\\nCronJob can create multiple concurrent Jobs. See the limitations  below.• \\n• \\n• \\n• \\n• \\n•\", metadata={'source': './PDFS/Concepts.pdf', 'page': 189}),\n",
       " Document(page_content='When the control plane creates new Jobs and (indirectly) Pods for a CronJob, the\\n.metadata.name  of the CronJob is part of the basis for naming those Pods. The name of a\\nCronJob must be a valid DNS subdomain  value, but this can produce unexpected results for the\\nPod hostnames. For best compatibility, the name should follow the more restrictive rules for a \\nDNS label . Even when the name is a DNS subdomain, the name must be no longer than 52\\ncharacters. This is because the CronJob controller will automatically append 11 characters to\\nthe name you provide and there is a constraint that the length of a Job name is no more than 63\\ncharacters.\\nExample\\nThis example CronJob manifest prints the current time and a hello message every minute:\\napplication/job/cronjob.yaml  \\napiVersion : batch/v1\\nkind: CronJob\\nmetadata :\\n  name : hello\\nspec:\\n  schedule : \"* * * * *\"\\n  jobTemplate :\\n    spec:\\n      template :\\n        spec:\\n          containers :\\n          - name : hello\\n            image : busybox:1.28\\n            imagePullPolicy : IfNotPresent\\n            command :\\n            - /bin/sh\\n            - -c\\n            - date; echo Hello from the Kubernetes cluster\\n          restartPolicy : OnFailure\\n(Running Automated Tasks with a CronJob  takes you through this example in more detail).\\nWriting a CronJob spec\\nSchedule syntax\\nThe .spec.schedule  field is required. The value of that field follows the Cron  syntax:\\n# ┌───────────── minute (0 - 59)\\n# │ ┌───────────── hour (0 - 23)\\n# │ │ ┌───────────── day of the month (1 - 31)\\n# │ │ │ ┌───────────── month (1 - 12)\\n# │ │ │ │ ┌───────────── day of the week (0 - 6) (Sunday to Saturday;\\n# │ │ │ │ │                                   7 is also Sunday on some systems)\\n# │ │ │ │ │                                   OR sun, mon, tue, wed, thu, fri, sat\\n# │ │ │ │ │\\n# * * * * *', metadata={'source': './PDFS/Concepts.pdf', 'page': 190}),\n",
       " Document(page_content='For example, 0 0 13 * 5  states that the task must be started every Friday at midnight, as well as\\non the 13th of each month at midnight.\\nThe format also includes extended \"Vixie cron\" step values. As explained in the FreeBSD\\nmanual :\\nStep values can be used in conjunction with ranges. Following a range with /\\n<number>  specifies skips of the number\\'s value through the range. For example, \\n0-23/2  can be used in the hours field to specify command execution every other\\nhour (the alternative in the V7 standard is 0,2,4,6,8,10,12,14,16,18,20,22 ). Steps are\\nalso permitted after an asterisk, so if you want to say \"every two hours\", just use */\\n2.\\nNote:  A question mark ( ?) in the schedule has the same meaning as an asterisk *, that is, it\\nstands for any of available value for a given field.\\nOther than the standard syntax, some macros like @monthly  can also be used:\\nEntry DescriptionEquivalent\\nto\\n@yearly (or\\n@annually)Run once a year at midnight of 1 January 0 0 1 1 *\\n@monthlyRun once a month at midnight of the first day of the\\nmonth0 0 1 * *\\n@weekly Run once a week at midnight on Sunday morning 0 0 * * 0\\n@daily (or @midnight) Run once a day at midnight 0 0 * * *\\n@hourly Run once an hour at the beginning of the hour 0 * * * *\\nTo generate CronJob schedule expressions, you can also use web tools like crontab.guru .\\nJob template\\nThe .spec.jobTemplate  defines a template for the Jobs that the CronJob creates, and it is\\nrequired. It has exactly the same schema as a Job, except that it is nested and does not have an \\napiVersion  or kind. You can specify common metadata for the templated Jobs, such as labels  or \\nannotations . For information about writing a Job .spec , see Writing a Job Spec .\\nDeadline for delayed job start\\nThe .spec.startingDeadlineSeconds  field is optional. This field defines a deadline (in whole\\nseconds) for starting the Job, if that Job misses its scheduled time for any reason.\\nAfter missing the deadline, the CronJob skips that instance of the Job (future occurrences are\\nstill scheduled). For example, if you have a backup job that runs twice a day, you might allow it\\nto start up to 8 hours late, but no later, because a backup taken any later wouldn\\'t be useful: you\\nwould instead prefer to wait for the next scheduled run.\\nFor Jobs that miss their configured deadline, Kubernetes treats them as failed Jobs. If you don\\'t\\nspecify startingDeadlineSeconds  for a CronJob, the Job occurrences have no deadline.', metadata={'source': './PDFS/Concepts.pdf', 'page': 191}),\n",
       " Document(page_content=\"If the .spec.startingDeadlineSeconds  field is set (not null), the CronJob controller measures the\\ntime between when a job is expected to be created and now. If the difference is higher than that\\nlimit, it will skip this execution.\\nFor example, if it is set to 200, it allows a job to be created for up to 200 seconds after the actual\\nschedule.\\nConcurrency policy\\nThe .spec.concurrencyPolicy  field is also optional. It specifies how to treat concurrent\\nexecutions of a job that is created by this CronJob. The spec may specify only one of the\\nfollowing concurrency policies:\\nAllow  (default): The CronJob allows concurrently running jobs\\nForbid : The CronJob does not allow concurrent runs; if it is time for a new job run and\\nthe previous job run hasn't finished yet, the CronJob skips the new job run\\nReplace : If it is time for a new job run and the previous job run hasn't finished yet, the\\nCronJob replaces the currently running job run with a new job run\\nNote that concurrency policy only applies to the jobs created by the same cron job. If there are\\nmultiple CronJobs, their respective jobs are always allowed to run concurrently.\\nSchedule suspension\\nYou can suspend execution of Jobs for a CronJob, by setting the optional .spec.suspend  field to\\ntrue. The field defaults to false.\\nThis setting does not affect Jobs that the CronJob has already started.\\nIf you do set that field to true, all subsequent executions are suspended (they remain scheduled,\\nbut the CronJob controller does not start the Jobs to run the tasks) until you unsuspend the\\nCronJob.\\nCaution:  Executions that are suspended during their scheduled time count as missed jobs.\\nWhen .spec.suspend  changes from true to false on an existing CronJob without a starting\\ndeadline , the missed jobs are scheduled immediately.\\nJobs history limits\\nThe .spec.successfulJobsHistoryLimit  and .spec.failedJobsHistoryLimit  fields are optional. These\\nfields specify how many completed and failed jobs should be kept. By default, they are set to 3\\nand 1 respectively. Setting a limit to 0 corresponds to keeping none of the corresponding kind\\nof jobs after they finish.\\nFor another way to clean up jobs automatically, see Clean up finished jobs automatically .\\nTime zones\\nFEATURE STATE:  Kubernetes v1.27 [stable]\\nFor CronJobs with no time zone specified, the kube-controller-manager  interprets schedules\\nrelative to its local time zone.• \\n• \\n•\", metadata={'source': './PDFS/Concepts.pdf', 'page': 192}),\n",
       " Document(page_content='You can specify a time zone for a CronJob by setting .spec.timeZone  to the name of a valid time\\nzone . For example, setting .spec.timeZone: \"Etc/UTC\"  instructs Kubernetes to interpret the\\nschedule relative to Coordinated Universal Time.\\nA time zone database from the Go standard library is included in the binaries and used as a\\nfallback in case an external database is not available on the system.\\nCronJob limitations\\nUnsupported TimeZone specification\\nThe implementation of the CronJob API in Kubernetes 1.28 lets you set the .spec.schedule  field\\nto include a timezone; for example: CRON_TZ=UTC * * * * *  or TZ=UTC * * * * * .\\nSpecifying a timezone that way is not officially supported  (and never has been).\\nIf you try to set a schedule that includes TZ or CRON_TZ  timezone specification, Kubernetes\\nreports a warning  to the client. Future versions of Kubernetes will prevent setting the unofficial\\ntimezone mechanism entirely.\\nModifying a CronJob\\nBy design, a CronJob contains a template for new Jobs. If you modify an existing CronJob, the\\nchanges you make will apply to new Jobs that start to run after your modification is complete.\\nJobs (and their Pods) that have already started continue to run without changes. That is, the\\nCronJob does not update existing Jobs, even if those remain running.\\nJob creation\\nA CronJob creates a Job object approximately once per execution time of its schedule. The\\nscheduling is approximate because there are certain circumstances where two Jobs might be\\ncreated, or no Job might be created. Kubernetes tries to avoid those situations, but does not\\ncompletely prevent them. Therefore, the Jobs that you define should be idempotent .\\nIf startingDeadlineSeconds  is set to a large value or left unset (the default) and if \\nconcurrencyPolicy  is set to Allow , the jobs will always run at least once.\\nCaution:  If startingDeadlineSeconds  is set to a value less than 10 seconds, the CronJob may not\\nbe scheduled. This is because the CronJob controller checks things every 10 seconds.\\nFor every CronJob, the CronJob Controller  checks how many schedules it missed in the\\nduration from its last scheduled time until now. If there are more than 100 missed schedules,\\nthen it does not start the job and logs the error.\\nCannot determine if job needs to be started. Too many missed start time (> 100). Set or \\ndecrease .spec.startingDeadlineSeconds or check clock skew.\\nIt is important to note that if the startingDeadlineSeconds  field is set (not nil), the controller\\ncounts how many missed jobs occurred from the value of startingDeadlineSeconds  until now\\nrather than from the last scheduled time until now. For example, if startingDeadlineSeconds  is \\n200, the controller counts how many missed jobs occurred in the last 200 seconds.', metadata={'source': './PDFS/Concepts.pdf', 'page': 193}),\n",
       " Document(page_content=\"A CronJob is counted as missed if it has failed to be created at its scheduled time. For example,\\nif concurrencyPolicy  is set to Forbid  and a CronJob was attempted to be scheduled when there\\nwas a previous schedule still running, then it would count as missed.\\nFor example, suppose a CronJob is set to schedule a new Job every one minute beginning at \\n08:30:00 , and its startingDeadlineSeconds  field is not set. If the CronJob controller happens to be\\ndown from 08:29:00  to 10:21:00 , the job will not start as the number of missed jobs which\\nmissed their schedule is greater than 100.\\nTo illustrate this concept further, suppose a CronJob is set to schedule a new Job every one\\nminute beginning at 08:30:00 , and its startingDeadlineSeconds  is set to 200 seconds. If the\\nCronJob controller happens to be down for the same period as the previous example ( 08:29:00\\nto 10:21:00 ,) the Job will still start at 10:22:00. This happens as the controller now checks how\\nmany missed schedules happened in the last 200 seconds (i.e., 3 missed schedules), rather than\\nfrom the last scheduled time until now.\\nThe CronJob is only responsible for creating Jobs that match its schedule, and the Job in turn is\\nresponsible for the management of the Pods it represents.\\nWhat's next\\nLearn about Pods  and Jobs, two concepts that CronJobs rely upon.\\nRead about the detailed format  of CronJob .spec.schedule  fields.\\nFor instructions on creating and working with CronJobs, and for an example of a CronJob\\nmanifest, see Running automated tasks with CronJobs .\\nCronJob  is part of the Kubernetes REST API. Read the CronJob  API reference for more\\ndetails.\\nReplicationController\\nLegacy API for managing workloads that can scale horizontally. Superseded by the Deployment\\nand ReplicaSet APIs.\\nNote:  A Deployment  that configures a ReplicaSet  is now the recommended way to set up\\nreplication.\\nA ReplicationController  ensures that a specified number of pod replicas are running at any one\\ntime. In other words, a ReplicationController makes sure that a pod or a homogeneous set of\\npods is always up and available.\\nHow a ReplicationController works\\nIf there are too many pods, the ReplicationController terminates the extra pods. If there are too\\nfew, the ReplicationController starts more pods. Unlike manually created pods, the pods\\nmaintained by a ReplicationController are automatically replaced if they fail, are deleted, or are\\nterminated. For example, your pods are re-created on a node after disruptive maintenance such\\nas a kernel upgrade. For this reason, you should use a ReplicationController even if your\\napplication requires only a single pod. A ReplicationController is similar to a process\\nsupervisor, but instead of supervising individual processes on a single node, the\\nReplicationController supervises multiple pods across multiple nodes.• \\n• \\n• \\n•\", metadata={'source': './PDFS/Concepts.pdf', 'page': 194}),\n",
       " Document(page_content='ReplicationController is often abbreviated to \"rc\" in discussion, and as a shortcut in kubectl\\ncommands.\\nA simple case is to create one ReplicationController object to reliably run one instance of a Pod\\nindefinitely. A more complex use case is to run several identical replicas of a replicated service,\\nsuch as web servers.\\nRunning an example ReplicationController\\nThis example ReplicationController config runs three copies of the nginx web server.\\ncontrollers/replication.yaml  \\napiVersion : v1\\nkind: ReplicationController\\nmetadata :\\n  name : nginx\\nspec:\\n  replicas : 3\\n  selector :\\n    app: nginx\\n  template :\\n    metadata :\\n      name : nginx\\n      labels :\\n        app: nginx\\n    spec:\\n      containers :\\n      - name : nginx\\n        image : nginx\\n        ports :\\n        - containerPort : 80\\nRun the example job by downloading the example file and then running this command:\\nkubectl apply -f https://k8s.io/examples/controllers/replication.yaml\\nThe output is similar to this:\\nreplicationcontroller/nginx created\\nCheck on the status of the ReplicationController using this command:\\nkubectl describe replicationcontrollers/nginx\\nThe output is similar to this:\\nName:        nginx\\nNamespace:   default\\nSelector:    app=nginx\\nLabels:      app=nginx\\nAnnotations:    <none>\\nReplicas:    3 current / 3 desired', metadata={'source': './PDFS/Concepts.pdf', 'page': 195}),\n",
       " Document(page_content='Pods Status: 0 Running / 3 Waiting / 0 Succeeded / 0 Failed\\nPod Template:\\n  Labels:       app=nginx\\n  Containers:\\n   nginx:\\n    Image:              nginx\\n    Port:               80/TCP\\n    Environment:        <none>\\n    Mounts:             <none>\\n  Volumes:              <none>\\nEvents:\\n  FirstSeen       LastSeen     Count    From                        SubobjectPath    Type      Reason              \\nMessage\\n  ---------       --------     -----    ----                        -------------    ----      ------              -------\\n  20s             20s          1        {replication-controller }                    Normal    SuccessfulCreate    \\nCreated pod: nginx-qrm3m\\n  20s             20s          1        {replication-controller }                    Normal    SuccessfulCreate    \\nCreated pod: nginx-3ntk0\\n  20s             20s          1        {replication-controller }                    Normal    SuccessfulCreate    \\nCreated pod: nginx-4ok8v\\nHere, three pods are created, but none is running yet, perhaps because the image is being\\npulled. A little later, the same command may show:\\nPods Status:    3 Running / 0 Waiting / 0 Succeeded / 0 Failed\\nTo list all the pods that belong to the ReplicationController in a machine readable form, you can\\nuse a command like this:\\npods =$(kubectl get pods --selector =app=nginx --output =jsonpath ={.items..metadata.name })\\necho  $pods\\nThe output is similar to this:\\nnginx-3ntk0 nginx-4ok8v nginx-qrm3m\\nHere, the selector is the same as the selector for the ReplicationController (seen in the kubectl \\ndescribe  output), and in a different form in replication.yaml . The --output=jsonpath  option\\nspecifies an expression with the name from each pod in the returned list.\\nWriting a ReplicationController Manifest\\nAs with all other Kubernetes config, a ReplicationController needs apiVersion , kind, and \\nmetadata  fields.\\nWhen the control plane creates new Pods for a ReplicationController, the .metadata.name  of the\\nReplicationController is part of the basis for naming those Pods. The name of a\\nReplicationController must be a valid DNS subdomain  value, but this can produce unexpected\\nresults for the Pod hostnames. For best compatibility, the name should follow the more\\nrestrictive rules for a DNS label .\\nFor general information about working with configuration files, see object management .', metadata={'source': './PDFS/Concepts.pdf', 'page': 196}),\n",
       " Document(page_content='A ReplicationController also needs a .spec  section .\\nPod Template\\nThe .spec.template  is the only required field of the .spec .\\nThe .spec.template  is a pod template . It has exactly the same schema as a Pod, except it is nested\\nand does not have an apiVersion  or kind.\\nIn addition to required fields for a Pod, a pod template in a ReplicationController must specify\\nappropriate labels and an appropriate restart policy. For labels, make sure not to overlap with\\nother controllers. See pod selector .\\nOnly a .spec.template.spec.restartPolicy  equal to Always  is allowed, which is the default if not\\nspecified.\\nFor local container restarts, ReplicationControllers delegate to an agent on the node, for\\nexample the Kubelet .\\nLabels on the ReplicationController\\nThe ReplicationController can itself have labels ( .metadata.labels ). Typically, you would set\\nthese the same as the .spec.template.metadata.labels ; if .metadata.labels  is not specified then it\\ndefaults to .spec.template.metadata.labels . However, they are allowed to be different, and\\nthe .metadata.labels  do not affect the behavior of the ReplicationController.\\nPod Selector\\nThe .spec.selector  field is a label selector . A ReplicationController manages all the pods with\\nlabels that match the selector. It does not distinguish between pods that it created or deleted\\nand pods that another person or process created or deleted. This allows the\\nReplicationController to be replaced without affecting the running pods.\\nIf specified, the .spec.template.metadata.labels  must be equal to the .spec.selector , or it will be\\nrejected by the API. If .spec.selector  is unspecified, it will be defaulted\\nto .spec.template.metadata.labels .\\nAlso you should not normally create any pods whose labels match this selector, either directly,\\nwith another ReplicationController, or with another controller such as Job. If you do so, the\\nReplicationController thinks that it created the other pods. Kubernetes does not stop you from\\ndoing this.\\nIf you do end up with multiple controllers that have overlapping selectors, you will have to\\nmanage the deletion yourself (see below ).\\nMultiple Replicas\\nYou can specify how many pods should run concurrently by setting .spec.replicas  to the\\nnumber of pods you would like to have running concurrently. The number running at any time\\nmay be higher or lower, such as if the replicas were just increased or decreased, or if a pod is\\ngracefully shutdown, and a replacement starts early.\\nIf you do not specify .spec.replicas , then it defaults to 1.', metadata={'source': './PDFS/Concepts.pdf', 'page': 197}),\n",
       " Document(page_content=\"Working with ReplicationControllers\\nDeleting a ReplicationController and its Pods\\nTo delete a ReplicationController and all its pods, use kubectl delete . Kubectl will scale the\\nReplicationController to zero and wait for it to delete each pod before deleting the\\nReplicationController itself. If this kubectl command is interrupted, it can be restarted.\\nWhen using the REST API or client library , you need to do the steps explicitly (scale replicas to\\n0, wait for pod deletions, then delete the ReplicationController).\\nDeleting only a ReplicationController\\nYou can delete a ReplicationController without affecting any of its pods.\\nUsing kubectl, specify the --cascade=orphan  option to kubectl delete .\\nWhen using the REST API or client library , you can delete the ReplicationController object.\\nOnce the original is deleted, you can create a new ReplicationController to replace it. As long as\\nthe old and new .spec.selector  are the same, then the new one will adopt the old pods. However,\\nit will not make any effort to make existing pods match a new, different pod template. To\\nupdate pods to a new spec in a controlled way, use a rolling update .\\nIsolating pods from a ReplicationController\\nPods may be removed from a ReplicationController's target set by changing their labels. This\\ntechnique may be used to remove pods from service for debugging and data recovery. Pods that\\nare removed in this way will be replaced automatically (assuming that the number of replicas is\\nnot also changed).\\nCommon usage patterns\\nRescheduling\\nAs mentioned above, whether you have 1 pod you want to keep running, or 1000, a\\nReplicationController will ensure that the specified number of pods exists, even in the event of\\nnode failure or pod termination (for example, due to an action by another control agent).\\nScaling\\nThe ReplicationController enables scaling the number of replicas up or down, either manually\\nor by an auto-scaling control agent, by updating the replicas  field.\\nRolling updates\\nThe ReplicationController is designed to facilitate rolling updates to a service by replacing pods\\none-by-one.\\nAs explained in #1353 , the recommended approach is to create a new ReplicationController\\nwith 1 replica, scale the new (+1) and old (-1) controllers one by one, and then delete the old\", metadata={'source': './PDFS/Concepts.pdf', 'page': 198}),\n",
       " Document(page_content=\"controller after it reaches 0 replicas. This predictably updates the set of pods regardless of\\nunexpected failures.\\nIdeally, the rolling update controller would take application readiness into account, and would\\nensure that a sufficient number of pods were productively serving at any given time.\\nThe two ReplicationControllers would need to create pods with at least one differentiating label,\\nsuch as the image tag of the primary container of the pod, since it is typically image updates\\nthat motivate rolling updates.\\nMultiple release tracks\\nIn addition to running multiple releases of an application while a rolling update is in progress,\\nit's common to run multiple releases for an extended period of time, or even continuously, using\\nmultiple release tracks. The tracks would be differentiated by labels.\\nFor instance, a service might target all pods with tier in (frontend), environment in (prod) . Now\\nsay you have 10 replicated pods that make up this tier. But you want to be able to 'canary' a\\nnew version of this component. You could set up a ReplicationController with replicas  set to 9\\nfor the bulk of the replicas, with labels tier=frontend, environment=prod, track=stable , and\\nanother ReplicationController with replicas  set to 1 for the canary, with labels tier=frontend, \\nenvironment=prod, track=canary . Now the service is covering both the canary and non-canary\\npods. But you can mess with the ReplicationControllers separately to test things out, monitor\\nthe results, etc.\\nUsing ReplicationControllers with Services\\nMultiple ReplicationControllers can sit behind a single service, so that, for example, some traffic\\ngoes to the old version, and some goes to the new version.\\nA ReplicationController will never terminate on its own, but it isn't expected to be as long-lived\\nas services. Services may be composed of pods controlled by multiple ReplicationControllers,\\nand it is expected that many ReplicationControllers may be created and destroyed over the\\nlifetime of a service (for instance, to perform an update of pods that run the service). Both\\nservices themselves and their clients should remain oblivious to the ReplicationControllers that\\nmaintain the pods of the services.\\nWriting programs for Replication\\nPods created by a ReplicationController are intended to be fungible and semantically identical,\\nthough their configurations may become heterogeneous over time. This is an obvious fit for\\nreplicated stateless servers, but ReplicationControllers can also be used to maintain availability\\nof master-elected, sharded, and worker-pool applications. Such applications should use dynamic\\nwork assignment mechanisms, such as the RabbitMQ work queues , as opposed to static/one-\\ntime customization of the configuration of each pod, which is considered an anti-pattern. Any\\npod customization performed, such as vertical auto-sizing of resources (for example, cpu or\\nmemory), should be performed by another online controller process, not unlike the\\nReplicationController itself.\", metadata={'source': './PDFS/Concepts.pdf', 'page': 199}),\n",
       " Document(page_content='Responsibilities of the ReplicationController\\nThe ReplicationController ensures that the desired number of pods matches its label selector\\nand are operational. Currently, only terminated pods are excluded from its count. In the future, \\nreadiness  and other information available from the system may be taken into account, we may\\nadd more controls over the replacement policy, and we plan to emit events that could be used\\nby external clients to implement arbitrarily sophisticated replacement and/or scale-down\\npolicies.\\nThe ReplicationController is forever constrained to this narrow responsibility. It itself will not\\nperform readiness nor liveness probes. Rather than performing auto-scaling, it is intended to be\\ncontrolled by an external auto-scaler (as discussed in #492), which would change its replicas\\nfield. We will not add scheduling policies (for example, spreading ) to the ReplicationController.\\nNor should it verify that the pods controlled match the currently specified template, as that\\nwould obstruct auto-sizing and other automated processes. Similarly, completion deadlines,\\nordering dependencies, configuration expansion, and other features belong elsewhere. We even\\nplan to factor out the mechanism for bulk pod creation ( #170).\\nThe ReplicationController is intended to be a composable building-block primitive. We expect\\nhigher-level APIs and/or tools to be built on top of it and other complementary primitives for\\nuser convenience in the future. The \"macro\" operations currently supported by kubectl (run,\\nscale) are proof-of-concept examples of this. For instance, we could imagine something like \\nAsgard  managing ReplicationControllers, auto-scalers, services, scheduling policies, canaries,\\netc.\\nAPI Object\\nReplication controller is a top-level resource in the Kubernetes REST API. More details about\\nthe API object can be found at: ReplicationController API object .\\nAlternatives to ReplicationController\\nReplicaSet\\nReplicaSet  is the next-generation ReplicationController that supports the new set-based label\\nselector . It\\'s mainly used by Deployment  as a mechanism to orchestrate pod creation, deletion\\nand updates. Note that we recommend using Deployments instead of directly using Replica\\nSets, unless you require custom update orchestration or don\\'t require updates at all.\\nDeployment (Recommended)\\nDeployment  is a higher-level API object that updates its underlying Replica Sets and their Pods.\\nDeployments are recommended if you want the rolling update functionality, because they are\\ndeclarative, server-side, and have additional features.\\nBare Pods\\nUnlike in the case where a user directly created pods, a ReplicationController replaces pods that\\nare deleted or terminated for any reason, such as in the case of node failure or disruptive node\\nmaintenance, such as a kernel upgrade. For this reason, we recommend that you use a', metadata={'source': './PDFS/Concepts.pdf', 'page': 200}),\n",
       " Document(page_content=\"ReplicationController even if your application requires only a single pod. Think of it similarly\\nto a process supervisor, only it supervises multiple pods across multiple nodes instead of\\nindividual processes on a single node. A ReplicationController delegates local container restarts\\nto some agent on the node, such as the kubelet.\\nJob\\nUse a Job instead of a ReplicationController for pods that are expected to terminate on their\\nown (that is, batch jobs).\\nDaemonSet\\nUse a DaemonSet  instead of a ReplicationController for pods that provide a machine-level\\nfunction, such as machine monitoring or machine logging. These pods have a lifetime that is\\ntied to a machine lifetime: the pod needs to be running on the machine before other pods start,\\nand are safe to terminate when the machine is otherwise ready to be rebooted/shutdown.\\nWhat's next\\nLearn about Pods .\\nLearn about Deployment , the replacement for ReplicationController.\\nReplicationController  is part of the Kubernetes REST API. Read the ReplicationController\\nobject definition to understand the API for replication controllers.\\nServices, Load Balancing, and Networking\\nConcepts and resources behind networking in Kubernetes.\\nThe Kubernetes network model\\nEvery Pod in a cluster gets its own unique cluster-wide IP address. This means you do not need\\nto explicitly create links between Pods  and you almost never need to deal with mapping\\ncontainer ports to host ports.\\nThis creates a clean, backwards-compatible model where Pods  can be treated much like VMs or\\nphysical hosts from the perspectives of port allocation, naming, service discovery, load\\nbalancing , application configuration, and migration.\\nKubernetes imposes the following fundamental requirements on any networking\\nimplementation (barring any intentional network segmentation policies):\\npods can communicate with all other pods on any other node  without NAT\\nagents on a node (e.g. system daemons, kubelet) can communicate with all pods on that\\nnode\\nNote: For those platforms that support Pods  running in the host network (e.g. Linux), when\\npods are attached to the host network of a node they can still communicate with all pods on all\\nnodes without NAT.\\nThis model is not only less complex overall, but it is principally compatible with the desire for\\nKubernetes to enable low-friction porting of apps from VMs to containers. If your job• \\n• \\n• \\n• \\n•\", metadata={'source': './PDFS/Concepts.pdf', 'page': 201}),\n",
       " Document(page_content='previously ran in a VM, your VM had an IP and could talk to other VMs in your project. This is\\nthe same basic model.\\nKubernetes IP addresses exist at the Pod scope - containers within a Pod share their network\\nnamespaces - including their IP address and MAC address. This means that containers within a \\nPod can all reach each other\\'s ports on localhost . This also means that containers within a Pod\\nmust coordinate port usage, but this is no different from processes in a VM. This is called the\\n\"IP-per-pod\" model.\\nHow this is implemented is a detail of the particular container runtime in use.\\nIt is possible to request ports on the Node  itself which forward to your Pod (called host ports),\\nbut this is a very niche operation. How that forwarding is implemented is also a detail of the\\ncontainer runtime. The Pod itself is blind to the existence or non-existence of host ports.\\nKubernetes networking addresses four concerns:\\nContainers within a Pod use networking to communicate  via loopback.\\nCluster networking provides communication between different Pods.\\nThe Service  API lets you expose an application running in Pods  to be reachable from\\noutside your cluster.\\nIngress  provides extra functionality specifically for exposing HTTP applications,\\nwebsites and APIs.\\nGateway API  is an add-on  that provides an expressive, extensible, and role-oriented\\nfamily of API kinds for modeling service networking.\\nYou can also use Services to publish services only for consumption inside your cluster .\\nThe Connecting Applications with Services  tutorial lets you learn about Services and\\nKubernetes networking with a hands-on example.\\nCluster Networking  explains how to set up networking for your cluster, and also provides an\\noverview of the technologies involved.\\nService\\nExpose an application running in your cluster behind a single outward-facing endpoint, even\\nwhen the workload is split across multiple backends.\\nIngress\\nMake your HTTP (or HTTPS) network service available using a protocol-aware configuration\\nmechanism, that understands web concepts like URIs, hostnames, paths, and more. The Ingress\\nconcept lets you map traffic to different backends based on rules you define via the Kubernetes\\nAPI.\\nIngress Controllers\\nIn order for an Ingress  to work in your cluster, there must be an ingress controller  running. You\\nneed to select at least one ingress controller and make sure it is set up in your cluster. This page\\nlists common ingress controllers that you can deploy.• \\n• \\n• \\n◦ \\n◦ \\n•', metadata={'source': './PDFS/Concepts.pdf', 'page': 202}),\n",
       " Document(page_content='Gateway API\\nGateway API is a family of API kinds that provide dynamic infrastructure provisioning and\\nadvanced traffic routing.\\nEndpointSlices\\nThe EndpointSlice API is the mechanism that Kubernetes uses to let your Service scale to\\nhandle large numbers of backends, and allows the cluster to update its list of healthy backends\\nefficiently.\\nNetwork Policies\\nIf you want to control traffic flow at the IP address or port level (OSI layer 3 or 4),\\nNetworkPolicies allow you to specify rules for traffic flow within your cluster, and also between\\nPods and the outside world. Your cluster must use a network plugin that supports\\nNetworkPolicy enforcement.\\nDNS for Services and Pods\\nYour workload can discover Services within your cluster using DNS; this page explains how\\nthat works.\\nIPv4/IPv6 dual-stack\\nKubernetes lets you configure single-stack IPv4 networking, single-stack IPv6 networking, or\\ndual stack networking with both network families active. This page explains how.\\nTopology Aware Routing\\nTopology Aware Routing  provides a mechanism to help keep network traffic within the zone\\nwhere it originated. Preferring same-zone traffic between Pods in your cluster can help with\\nreliability, performance (network latency and throughput), or cost.\\nNetworking on Windows\\nService ClusterIP allocation\\nService Internal Traffic Policy\\nIf two Pods in your cluster want to communicate, and both Pods are actually running on the\\nsame node, use Service Internal Traffic Policy  to keep network traffic within that node. Avoiding\\na round trip via the cluster network can help with reliability, performance (network latency and\\nthroughput), or cost.\\nService\\nExpose an application running in your cluster behind a single outward-facing endpoint, even\\nwhen the workload is split across multiple backends.', metadata={'source': './PDFS/Concepts.pdf', 'page': 203}),\n",
       " Document(page_content='In Kubernetes, a Service is a method for exposing a network application that is running as one\\nor more Pods  in your cluster.\\nA key aim of Services in Kubernetes is that you don\\'t need to modify your existing application\\nto use an unfamiliar service discovery mechanism. You can run code in Pods, whether this is a\\ncode designed for a cloud-native world, or an older app you\\'ve containerized. You use a Service\\nto make that set of Pods available on the network so that clients can interact with it.\\nIf you use a Deployment  to run your app, that Deployment can create and destroy Pods\\ndynamically. From one moment to the next, you don\\'t know how many of those Pods are\\nworking and healthy; you might not even know what those healthy Pods are named.\\nKubernetes Pods  are created and destroyed to match the desired state of your cluster. Pods are\\nephemeral resources (you should not expect that an individual Pod is reliable and durable).\\nEach Pod gets its own IP address (Kubernetes expects network plugins to ensure this). For a\\ngiven Deployment in your cluster, the set of Pods running in one moment in time could be\\ndifferent from the set of Pods running that application a moment later.\\nThis leads to a problem: if some set of Pods (call them \"backends\") provides functionality to\\nother Pods (call them \"frontends\") inside your cluster, how do the frontends find out and keep\\ntrack of which IP address to connect to, so that the frontend can use the backend part of the\\nworkload?\\nEnter Services .\\nServices in Kubernetes\\nThe Service API, part of Kubernetes, is an abstraction to help you expose groups of Pods over a\\nnetwork. Each Service object defines a logical set of endpoints (usually these endpoints are\\nPods) along with a policy about how to make those pods accessible.\\nFor example, consider a stateless image-processing backend which is running with 3 replicas.\\nThose replicas are fungible—frontends do not care which backend they use. While the actual\\nPods that compose the backend set may change, the frontend clients should not need to be\\naware of that, nor should they need to keep track of the set of backends themselves.\\nThe Service abstraction enables this decoupling.\\nThe set of Pods targeted by a Service is usually determined by a selector  that you define. To\\nlearn about other ways to define Service endpoints, see Services without  selectors .\\nIf your workload speaks HTTP, you might choose to use an Ingress  to control how web traffic\\nreaches that workload. Ingress is not a Service type, but it acts as the entry point for your\\ncluster. An Ingress lets you consolidate your routing rules into a single resource, so that you\\ncan expose multiple components of your workload, running separately in your cluster, behind a\\nsingle listener.\\nThe Gateway  API for Kubernetes provides extra capabilities beyond Ingress and Service. You\\ncan add Gateway to your cluster - it is a family of extension APIs, implemented using \\nCustomResourceDefinitions  - and then use these to configure access to network services that\\nare running in your cluster.', metadata={'source': './PDFS/Concepts.pdf', 'page': 204}),\n",
       " Document(page_content='Cloud-native service discovery\\nIf you\\'re able to use Kubernetes APIs for service discovery in your application, you can query\\nthe API server  for matching EndpointSlices. Kubernetes updates the EndpointSlices for a\\nService whenever the set of Pods in a Service changes.\\nFor non-native applications, Kubernetes offers ways to place a network port or load balancer in\\nbetween your application and the backend Pods.\\nEither way, your workload can use these service discovery  mechanisms to find the target it\\nwants to connect to.\\nDefining a Service\\nA Service is an object  (the same way that a Pod or a ConfigMap is an object). You can create,\\nview or modify Service definitions using the Kubernetes API. Usually you use a tool such as \\nkubectl  to make those API calls for you.\\nFor example, suppose you have a set of Pods that each listen on TCP port 9376 and are labelled\\nas app.kubernetes.io/name=MyApp . You can define a Service to publish that TCP listener:\\napiVersion : v1\\nkind: Service\\nmetadata :\\n  name : my-service\\nspec:\\n  selector :\\n    app.kubernetes.io/name : MyApp\\n  ports :\\n    - protocol : TCP\\n      port: 80\\n      targetPort : 9376\\nApplying this manifest creates a new Service named \"my-service\" with the default ClusterIP \\nservice type . The Service targets TCP port 9376 on any Pod with the app.kubernetes.io/name: \\nMyApp  label.\\nKubernetes assigns this Service an IP address (the cluster IP ), that is used by the virtual IP\\naddress mechanism. For more details on that mechanism, read Virtual IPs and Service Proxies .\\nThe controller for that Service continuously scans for Pods that match its selector, and then\\nmakes any necessary updates to the set of EndpointSlices for the Service.\\nThe name of a Service object must be a valid RFC 1035 label name .\\nNote:  A Service can map any incoming port to a targetPort . By default and for convenience,\\nthe targetPort  is set to the same value as the port field.\\nPort definitions\\nPort definitions in Pods have names, and you can reference these names in the targetPort\\nattribute of a Service. For example, we can bind the targetPort  of the Service to the Pod port in\\nthe following way:', metadata={'source': './PDFS/Concepts.pdf', 'page': 205}),\n",
       " Document(page_content='apiVersion : v1\\nkind: Pod\\nmetadata :\\n  name : nginx\\n  labels :\\n    app.kubernetes.io/name : proxy\\nspec:\\n  containers :\\n  - name : nginx\\n    image : nginx:stable\\n    ports :\\n      - containerPort : 80\\n        name : http-web-svc\\n---\\napiVersion : v1\\nkind: Service\\nmetadata :\\n  name : nginx-service\\nspec:\\n  selector :\\n    app.kubernetes.io/name : proxy\\n  ports :\\n  - name : name-of-service-port\\n    protocol : TCP\\n    port: 80\\n    targetPort : http-web-svc\\nThis works even if there is a mixture of Pods in the Service using a single configured name,\\nwith the same network protocol available via different port numbers. This offers a lot of\\nflexibility for deploying and evolving your Services. For example, you can change the port\\nnumbers that Pods expose in the next version of your backend software, without breaking\\nclients.\\nThe default protocol for Services is TCP; you can also use any other supported protocol .\\nBecause many Services need to expose more than one port, Kubernetes supports multiple port\\ndefinitions  for a single Service. Each port definition can have the same protocol , or a different\\none.\\nServices without selectors\\nServices most commonly abstract access to Kubernetes Pods thanks to the selector, but when\\nused with a corresponding set of EndpointSlices  objects and without a selector, the Service can\\nabstract other kinds of backends, including ones that run outside the cluster.\\nFor example:\\nYou want to have an external database cluster in production, but in your test\\nenvironment you use your own databases.\\nYou want to point your Service to a Service in a different Namespace  or on another\\ncluster.• \\n•', metadata={'source': './PDFS/Concepts.pdf', 'page': 206}),\n",
       " Document(page_content='You are migrating a workload to Kubernetes. While evaluating the approach, you run\\nonly a portion of your backends in Kubernetes.\\nIn any of these scenarios you can define a Service without  specifying a selector to match Pods.\\nFor example:\\napiVersion : v1\\nkind: Service\\nmetadata :\\n  name : my-service\\nspec:\\n  ports :\\n    - protocol : TCP\\n      port: 80\\n      targetPort : 9376\\nBecause this Service has no selector, the corresponding EndpointSlice (and legacy Endpoints)\\nobjects are not created automatically. You can map the Service to the network address and port\\nwhere it\\'s running, by adding an EndpointSlice object manually. For example:\\napiVersion : discovery.k8s.io/v1\\nkind: EndpointSlice\\nmetadata :\\n  name : my-service-1  # by convention, use the name of the Service\\n                     # as a prefix for the name of the EndpointSlice\\n  labels :\\n    # You should set the \"kubernetes.io/service-name\" label.\\n    # Set its value to match the name of the Service\\n    kubernetes.io/service-name : my-service\\naddressType : IPv4\\nports :\\n  - name : \\'\\' # empty because port 9376 is not assigned as a well-known\\n             # port (by IANA)\\n    appProtocol : http\\n    protocol : TCP\\n    port: 9376\\nendpoints :\\n  - addresses :\\n      - \"10.4.5.6\"\\n  - addresses :\\n      - \"10.1.2.3\"\\nCustom EndpointSlices\\nWhen you create an EndpointSlice  object for a Service, you can use any name for the\\nEndpointSlice. Each EndpointSlice in a namespace must have a unique name. You link an\\nEndpointSlice to a Service by setting the kubernetes.io/service-name  label  on that\\nEndpointSlice.\\nNote:\\nThe endpoint IPs must not  be: loopback (127.0.0.0/8 for IPv4, ::1/128 for IPv6), or link-local\\n(169.254.0.0/16 and 224.0.0.0/24 for IPv4, fe80::/64 for IPv6).•', metadata={'source': './PDFS/Concepts.pdf', 'page': 207}),\n",
       " Document(page_content='The endpoint IP addresses cannot be the cluster IPs of other Kubernetes Services, because kube-\\nproxy  doesn\\'t support virtual IPs as a destination.\\nFor an EndpointSlice that you create yourself, or in your own code, you should also pick a value\\nto use for the label endpointslice.kubernetes.io/managed-by . If you create your own controller\\ncode to manage EndpointSlices, consider using a value similar to \"my-domain.example/name-\\nof-controller\" . If you are using a third party tool, use the name of the tool in all-lowercase and\\nchange spaces and other punctuation to dashes ( -). If people are directly using a tool such as \\nkubectl  to manage EndpointSlices, use a name that describes this manual management, such as \\n\"staff\"  or \"cluster-admins\" . You should avoid using the reserved value \"controller\" , which\\nidentifies EndpointSlices managed by Kubernetes\\' own control plane.\\nAccessing a Service without a selector\\nAccessing a Service without a selector works the same as if it had a selector. In the example  for\\na Service without a selector, traffic is routed to one of the two endpoints defined in the\\nEndpointSlice manifest: a TCP connection to 10.1.2.3 or 10.4.5.6, on port 9376.\\nNote:  The Kubernetes API server does not allow proxying to endpoints that are not mapped to\\npods. Actions such as kubectl proxy <service-name>  where the service has no selector will fail\\ndue to this constraint. This prevents the Kubernetes API server from being used as a proxy to\\nendpoints the caller may not be authorized to access.\\nAn ExternalName  Service is a special case of Service that does not have selectors and uses DNS\\nnames instead. For more information, see the ExternalName  section.\\nEndpointSlices\\nFEATURE STATE:  Kubernetes v1.21 [stable]\\nEndpointSlices  are objects that represent a subset (a slice) of the backing network endpoints for\\na Service.\\nYour Kubernetes cluster tracks how many endpoints each EndpointSlice represents. If there are\\nso many endpoints for a Service that a threshold is reached, then Kubernetes adds another\\nempty EndpointSlice and stores new endpoint information there. By default, Kubernetes makes\\na new EndpointSlice once the existing EndpointSlices all contain at least 100 endpoints.\\nKubernetes does not make the new EndpointSlice until an extra endpoint needs to be added.\\nSee EndpointSlices  for more information about this API.\\nEndpoints\\nIn the Kubernetes API, an Endpoints  (the resource kind is plural) defines a list of network\\nendpoints, typically referenced by a Service to define which Pods the traffic can be sent to.\\nThe EndpointSlice API is the recommended replacement for Endpoints.\\nOver-capacity endpoints\\nKubernetes limits the number of endpoints that can fit in a single Endpoints object. When there\\nare over 1000 backing endpoints for a Service, Kubernetes truncates the data in the Endpoints', metadata={'source': './PDFS/Concepts.pdf', 'page': 208}),\n",
       " Document(page_content='object. Because a Service can be linked with more than one EndpointSlice, the 1000 backing\\nendpoint limit only affects the legacy Endpoints API.\\nIn that case, Kubernetes selects at most 1000 possible backend endpoints to store into the\\nEndpoints object, and sets an annotation  on the Endpoints: endpoints.kubernetes.io/over-\\ncapacity: truncated . The control plane also removes that annotation if the number of backend\\nPods drops below 1000.\\nTraffic is still sent to backends, but any load balancing mechanism that relies on the legacy\\nEndpoints API only sends traffic to at most 1000 of the available backing endpoints.\\nThe same API limit means that you cannot manually update an Endpoints to have more than\\n1000 endpoints.\\nApplication protocol\\nFEATURE STATE:  Kubernetes v1.20 [stable]\\nThe appProtocol  field provides a way to specify an application protocol for each Service port.\\nThis is used as a hint for implementations to offer richer behavior for protocols that they\\nunderstand. The value of this field is mirrored by the corresponding Endpoints and\\nEndpointSlice objects.\\nThis field follows standard Kubernetes label syntax. Valid values are one of:\\nIANA standard service names .\\nImplementation-defined prefixed names such as mycompany.com/my-custom-protocol .\\nKubernetes-defined prefixed names:\\nProtocol Description\\nkubernetes.io/h2c HTTP/2 over cleartext as described in RFC 7540\\nMulti-port Services\\nFor some Services, you need to expose more than one port. Kubernetes lets you configure\\nmultiple port definitions on a Service object. When using multiple ports for a Service, you must\\ngive all of your ports names so that these are unambiguous. For example:\\napiVersion : v1\\nkind: Service\\nmetadata :\\n  name : my-service\\nspec:\\n  selector :\\n    app.kubernetes.io/name : MyApp\\n  ports :\\n    - name : http\\n      protocol : TCP\\n      port: 80\\n      targetPort : 9376\\n    - name : https• \\n• \\n•', metadata={'source': './PDFS/Concepts.pdf', 'page': 209}),\n",
       " Document(page_content='protocol : TCP\\n      port: 443\\n      targetPort : 9377\\nNote:\\nAs with Kubernetes names  in general, names for ports must only contain lowercase\\nalphanumeric characters and -. Port names must also start and end with an alphanumeric\\ncharacter.\\nFor example, the names 123-abc  and web are valid, but 123_abc  and -web  are not.\\nService type\\nFor some parts of your application (for example, frontends) you may want to expose a Service\\nonto an external IP address, one that\\'s accessible from outside of your cluster.\\nKubernetes Service types allow you to specify what kind of Service you want.\\nThe available type values and their behaviors are:\\nClusterIP\\nExposes the Service on a cluster-internal IP. Choosing this value makes the Service only\\nreachable from within the cluster. This is the default that is used if you don\\'t explicitly\\nspecify a type for a Service. You can expose the Service to the public internet using an \\nIngress  or a Gateway .\\nNodePort\\nExposes the Service on each Node\\'s IP at a static port (the NodePort ). To make the node\\nport available, Kubernetes sets up a cluster IP address, the same as if you had requested a\\nService of type: ClusterIP .\\nLoadBalancer\\nExposes the Service externally using an external load balancer. Kubernetes does not\\ndirectly offer a load balancing component; you must provide one, or you can integrate\\nyour Kubernetes cluster with a cloud provider.\\nExternalName\\nMaps the Service to the contents of the externalName  field (for example, to the hostname \\napi.foo.bar.example ). The mapping configures your cluster\\'s DNS server to return a \\nCNAME  record with that external hostname value. No proxying of any kind is set up.\\nThe type field in the Service API is designed as nested functionality - each level adds to the\\nprevious. However there is an exception to this nested design. You can define a LoadBalancer\\nService by disabling the load balancer NodePort  allocation .\\ntype: ClusterIP\\nThis default Service type assigns an IP address from a pool of IP addresses that your cluster has\\nreserved for that purpose.\\nSeveral of the other types for Service build on the ClusterIP  type as a foundation.\\nIf you define a Service that has the .spec.clusterIP  set to \"None\"  then Kubernetes does not\\nassign an IP address. See headless Services  for more information.', metadata={'source': './PDFS/Concepts.pdf', 'page': 210}),\n",
       " Document(page_content=\"Choosing your own IP address\\nYou can specify your own cluster IP address as part of a Service  creation request. To do this, set\\nthe .spec.clusterIP  field. For example, if you already have an existing DNS entry that you wish\\nto reuse, or legacy systems that are configured for a specific IP address and difficult to re-\\nconfigure.\\nThe IP address that you choose must be a valid IPv4 or IPv6 address from within the service-\\ncluster-ip-range  CIDR range that is configured for the API server. If you try to create a Service\\nwith an invalid clusterIP  address value, the API server will return a 422 HTTP status code to\\nindicate that there's a problem.\\nRead avoiding collisions  to learn how Kubernetes helps reduce the risk and impact of two\\ndifferent Services both trying to use the same IP address.\\ntype: NodePort\\nIf you set the type field to NodePort , the Kubernetes control plane allocates a port from a range\\nspecified by --service-node-port-range  flag (default: 30000-32767). Each node proxies that port\\n(the same port number on every Node) into your Service. Your Service reports the allocated\\nport in its .spec.ports[*].nodePort  field.\\nUsing a NodePort gives you the freedom to set up your own load balancing solution, to\\nconfigure environments that are not fully supported by Kubernetes, or even to expose one or\\nmore nodes' IP addresses directly.\\nFor a node port Service, Kubernetes additionally allocates a port (TCP, UDP or SCTP to match\\nthe protocol of the Service). Every node in the cluster configures itself to listen on that assigned\\nport and to forward traffic to one of the ready endpoints associated with that Service. You'll be\\nable to contact the type: NodePort  Service, from outside the cluster, by connecting to any node\\nusing the appropriate protocol (for example: TCP), and the appropriate port (as assigned to that\\nService).\\nChoosing your own port\\nIf you want a specific port number, you can specify a value in the nodePort  field. The control\\nplane will either allocate you that port or report that the API transaction failed. This means that\\nyou need to take care of possible port collisions yourself. You also have to use a valid port\\nnumber, one that's inside the range configured for NodePort use.\\nHere is an example manifest for a Service of type: NodePort  that specifies a NodePort value\\n(30007, in this example):\\napiVersion : v1\\nkind: Service\\nmetadata :\\n  name : my-service\\nspec:\\n  type: NodePort\\n  selector :\\n    app.kubernetes.io/name : MyApp\\n  ports :\\n    - port: 80\", metadata={'source': './PDFS/Concepts.pdf', 'page': 211}),\n",
       " Document(page_content=\"# By default and for convenience, the `targetPort` is set to\\n      # the same value as the `port` field.\\n      targetPort : 80\\n      # Optional field\\n      # By default and for convenience, the Kubernetes control plane\\n      # will allocate a port from a range (default: 30000-32767)\\n      nodePort : 30007\\nReserve Nodeport ranges to avoid collisions\\nFEATURE STATE:  Kubernetes v1.28 [beta]\\nThe policy for assigning ports to NodePort services applies to both the auto-assignment and the\\nmanual assignment scenarios. When a user wants to create a NodePort service that uses a\\nspecific port, the target port may conflict with another port that has already been assigned. In\\nthis case, you can enable the feature gate ServiceNodePortStaticSubrange , which allows you to\\nuse a different port allocation strategy for NodePort Services. The port range for NodePort\\nservices is divided into two bands. Dynamic port assignment uses the upper band by default,\\nand it may use the lower band once the upper band has been exhausted. Users can then allocate\\nfrom the lower band with a lower risk of port collision.\\nCustom IP address configuration for type: NodePort  Services\\nYou can set up nodes in your cluster to use a particular IP address for serving node port\\nservices. You might want to do this if each node is connected to multiple networks (for\\nexample: one network for application traffic, and another network for traffic between nodes and\\nthe control plane).\\nIf you want to specify particular IP address(es) to proxy the port, you can set the --nodeport-\\naddresses  flag for kube-proxy or the equivalent nodePortAddresses  field of the kube-proxy\\nconfiguration file  to particular IP block(s).\\nThis flag takes a comma-delimited list of IP blocks (e.g. 10.0.0.0/8 , 192.0.2.0/25 ) to specify IP\\naddress ranges that kube-proxy should consider as local to this node.\\nFor example, if you start kube-proxy with the --nodeport-addresses=127.0.0.0/8  flag, kube-proxy\\nonly selects the loopback interface for NodePort Services. The default for --nodeport-addresses\\nis an empty list. This means that kube-proxy should consider all available network interfaces\\nfor NodePort. (That's also compatible with earlier Kubernetes releases.)\\nNote:  This Service is visible as <NodeIP>:spec.ports[*].nodePort\\nand .spec.clusterIP:spec.ports[*].port . If the --nodeport-addresses  flag for kube-proxy or the\\nequivalent field in the kube-proxy configuration file is set, <NodeIP>  would be a filtered node\\nIP address (or possibly IP addresses).\\ntype: LoadBalancer\\nOn cloud providers which support external load balancers, setting the type field to \\nLoadBalancer  provisions a load balancer for your Service. The actual creation of the load\\nbalancer happens asynchronously, and information about the provisioned balancer is published\\nin the Service's .status.loadBalancer  field. For example:\", metadata={'source': './PDFS/Concepts.pdf', 'page': 212}),\n",
       " Document(page_content=\"apiVersion : v1\\nkind: Service\\nmetadata :\\n  name : my-service\\nspec:\\n  selector :\\n    app.kubernetes.io/name : MyApp\\n  ports :\\n    - protocol : TCP\\n      port: 80\\n      targetPort : 9376\\n  clusterIP : 10.0.171.239\\n  type: LoadBalancer\\nstatus :\\n  loadBalancer :\\n    ingress :\\n    - ip: 192.0.2.127\\nTraffic from the external load balancer is directed at the backend Pods. The cloud provider\\ndecides how it is load balanced.\\nTo implement a Service of type: LoadBalancer , Kubernetes typically starts off by making the\\nchanges that are equivalent to you requesting a Service of type: NodePort . The cloud-controller-\\nmanager component then configures the external load balancer to forward traffic to that\\nassigned node port.\\nYou can configure a load balanced Service to omit  assigning a node port, provided that the\\ncloud provider implementation supports this.\\nSome cloud providers allow you to specify the loadBalancerIP . In those cases, the load-balancer\\nis created with the user-specified loadBalancerIP . If the loadBalancerIP  field is not specified, the\\nload balancer is set up with an ephemeral IP address. If you specify a loadBalancerIP  but your\\ncloud provider does not support the feature, the loadbalancerIP  field that you set is ignored.\\nNote:\\nThe.spec.loadBalancerIP  field for a Service was deprecated in Kubernetes v1.24.\\nThis field was under-specified and its meaning varies across implementations. It also cannot\\nsupport dual-stack networking. This field may be removed in a future API version.\\nIf you're integrating with a provider that supports specifying the load balancer IP address(es)\\nfor a Service via a (provider specific) annotation, you should switch to doing that.\\nIf you are writing code for a load balancer integration with Kubernetes, avoid using this field.\\nYou can integrate with Gateway  rather than Service, or you can define your own (provider\\nspecific) annotations on the Service that specify the equivalent detail.\\nLoad balancers with mixed protocol types\\nFEATURE STATE:  Kubernetes v1.26 [stable]\", metadata={'source': './PDFS/Concepts.pdf', 'page': 213}),\n",
       " Document(page_content='By default, for LoadBalancer type of Services, when there is more than one port defined, all\\nports must have the same protocol, and the protocol must be one which is supported by the\\ncloud provider.\\nThe feature gate MixedProtocolLBService  (enabled by default for the kube-apiserver as of v1.24)\\nallows the use of different protocols for LoadBalancer type of Services, when there is more than\\none port defined.\\nNote:  The set of protocols that can be used for load balanced Services is defined by your cloud\\nprovider; they may impose restrictions beyond what the Kubernetes API enforces.\\nDisabling load balancer NodePort allocation\\nFEATURE STATE:  Kubernetes v1.24 [stable]\\nYou can optionally disable node port allocation for a Service of type: LoadBalancer , by setting\\nthe field spec.allocateLoadBalancerNodePorts  to false. This should only be used for load\\nbalancer implementations that route traffic directly to pods as opposed to using node ports. By\\ndefault, spec.allocateLoadBalancerNodePorts  is true and type LoadBalancer Services will\\ncontinue to allocate node ports. If spec.allocateLoadBalancerNodePorts  is set to false on an\\nexisting Service with allocated node ports, those node ports will not be de-allocated\\nautomatically. You must explicitly remove the nodePorts  entry in every Service port to de-\\nallocate those node ports.\\nSpecifying class of load balancer implementation\\nFEATURE STATE:  Kubernetes v1.24 [stable]\\nFor a Service with type set to LoadBalancer , the .spec.loadBalancerClass  field enables you to use\\na load balancer implementation other than the cloud provider default.\\nBy default, .spec.loadBalancerClass  is not set and a LoadBalancer  type of Service uses the cloud\\nprovider\\'s default load balancer implementation if the cluster is configured with a cloud\\nprovider using the --cloud-provider  component flag.\\nIf you specify .spec.loadBalancerClass , it is assumed that a load balancer implementation that\\nmatches the specified class is watching for Services. Any default load balancer implementation\\n(for example, the one provided by the cloud provider) will ignore Services that have this field\\nset. spec.loadBalancerClass  can be set on a Service of type LoadBalancer  only. Once set, it\\ncannot be changed. The value of spec.loadBalancerClass  must be a label-style identifier, with an\\noptional prefix such as \" internal-vip \" or \" example.com/internal-vip \". Unprefixed names are\\nreserved for end-users.\\nInternal load balancer\\nIn a mixed environment it is sometimes necessary to route traffic from Services inside the same\\n(virtual) network address block.\\nIn a split-horizon DNS environment you would need two Services to be able to route both\\nexternal and internal traffic to your endpoints.', metadata={'source': './PDFS/Concepts.pdf', 'page': 214}),\n",
       " Document(page_content='To set an internal load balancer, add one of the following annotations to your Service\\ndepending on the cloud service provider you\\'re using:\\nDefault\\nGCP\\nAWS\\nAzure\\nIBM Cloud\\nOpenStack\\nBaidu Cloud\\nTencent Cloud\\nAlibaba Cloud\\nOCI\\nSelect one of the tabs.\\nmetadata :\\n  name : my-service\\n  annotations :\\n      networking.gke.io/load-balancer-type : \"Internal\"\\nmetadata :\\n    name : my-service\\n    annotations :\\n        service.beta.kubernetes.io/aws-load-balancer-internal : \"true\"\\nmetadata :\\n  name : my-service\\n  annotations :\\n      service.beta.kubernetes.io/azure-load-balancer-internal : \"true\"\\nmetadata :\\n  name : my-service\\n  annotations :\\n      service.kubernetes.io/ibm-load-balancer-cloud-provider-ip-type : \"private\"\\nmetadata :\\n  name : my-service\\n  annotations :\\n    service.beta.kubernetes.io/openstack-internal-load-balancer : \"true\"\\nmetadata :\\n  name : my-service\\n  annotations :\\n    service.beta.kubernetes.io/cce-load-balancer-internal-vpc : \"true\"\\nmetadata :\\n  annotations :\\n    service.kubernetes.io/qcloud-loadbalancer-internal-subnetid : subnet-xxxxx\\nmetadata :\\n  annotations :\\n    service.beta.kubernetes.io/alibaba-cloud-loadbalancer-address-type : \"intranet\"• \\n• \\n• \\n• \\n• \\n• \\n• \\n• \\n• \\n•', metadata={'source': './PDFS/Concepts.pdf', 'page': 215}),\n",
       " Document(page_content='metadata :\\n  name : my-service\\n  annotations :\\n      service.beta.kubernetes.io/oci-load-balancer-internal : true\\ntype: ExternalName\\nServices of type ExternalName map a Service to a DNS name, not to a typical selector such as \\nmy-service  or cassandra . You specify these Services with the spec.externalName  parameter.\\nThis Service definition, for example, maps the my-service  Service in the prod  namespace to \\nmy.database.example.com :\\napiVersion : v1\\nkind: Service\\nmetadata :\\n  name : my-service\\n  namespace : prod\\nspec:\\n  type: ExternalName\\n  externalName : my.database.example.com\\nNote:\\nA Service of type: ExternalName  accepts an IPv4 address string, but treats that string as a DNS\\nname comprised of digits, not as an IP address (the internet does not however allow such names\\nin DNS). Services with external names that resemble IPv4 addresses are not resolved by DNS\\nservers.\\nIf you want to map a Service directly to a specific IP address, consider using headless Services .\\nWhen looking up the host my-service.prod.svc.cluster.local , the cluster DNS Service returns a \\nCNAME  record with the value my.database.example.com . Accessing my-service  works in the\\nsame way as other Services but with the crucial difference that redirection happens at the DNS\\nlevel rather than via proxying or forwarding. Should you later decide to move your database\\ninto your cluster, you can start its Pods, add appropriate selectors or endpoints, and change the\\nService\\'s type.\\nCaution:\\nYou may have trouble using ExternalName for some common protocols, including HTTP and\\nHTTPS. If you use ExternalName then the hostname used by clients inside your cluster is\\ndifferent from the name that the ExternalName references.\\nFor protocols that use hostnames this difference may lead to errors or unexpected responses.\\nHTTP requests will have a Host:  header that the origin server does not recognize; TLS servers\\nwill not be able to provide a certificate matching the hostname that the client connected to.\\nHeadless Services\\nSometimes you don\\'t need load-balancing and a single Service IP. In this case, you can create\\nwhat are termed headless Services , by explicitly specifying \"None\"  for the cluster IP address\\n(.spec.clusterIP ).', metadata={'source': './PDFS/Concepts.pdf', 'page': 216}),\n",
       " Document(page_content='You can use a headless Service to interface with other service discovery mechanisms, without\\nbeing tied to Kubernetes\\' implementation.\\nFor headless Services, a cluster IP is not allocated, kube-proxy does not handle these Services,\\nand there is no load balancing or proxying done by the platform for them. How DNS is\\nautomatically configured depends on whether the Service has selectors defined:\\nWith selectors\\nFor headless Services that define selectors, the endpoints controller creates EndpointSlices in\\nthe Kubernetes API, and modifies the DNS configuration to return A or AAAA records (IPv4 or\\nIPv6 addresses) that point directly to the Pods backing the Service.\\nWithout selectors\\nFor headless Services that do not define selectors, the control plane does not create\\nEndpointSlice objects. However, the DNS system looks for and configures either:\\nDNS CNAME records for type: ExternalName  Services.\\nDNS A / AAAA records for all IP addresses of the Service\\'s ready endpoints, for all\\nService types other than ExternalName .\\nFor IPv4 endpoints, the DNS system creates A records.\\nFor IPv6 endpoints, the DNS system creates AAAA records.\\nWhen you define a headless Service without a selector, the port must match the targetPort .\\nDiscovering services\\nFor clients running inside your cluster, Kubernetes supports two primary modes of finding a\\nService: environment variables and DNS.\\nEnvironment variables\\nWhen a Pod is run on a Node, the kubelet adds a set of environment variables for each active\\nService. It adds {SVCNAME}_SERVICE_HOST  and {SVCNAME}_SERVICE_PORT  variables,\\nwhere the Service name is upper-cased and dashes are converted to underscores. It also\\nsupports variables (see makeLinkVariables ) that are compatible with Docker Engine\\'s \" legacy\\ncontainer links \" feature.\\nFor example, the Service redis-primary  which exposes TCP port 6379 and has been allocated\\ncluster IP address 10.0.0.11, produces the following environment variables:\\nREDIS_PRIMARY_SERVICE_HOST =10.0.0.11\\nREDIS_PRIMARY_SERVICE_PORT =6379\\nREDIS_PRIMARY_PORT =tcp://10.0.0.11:6379\\nREDIS_PRIMARY_PORT_6379_TCP =tcp://10.0.0.11:6379\\nREDIS_PRIMARY_PORT_6379_TCP_PROTO =tcp\\nREDIS_PRIMARY_PORT_6379_TCP_PORT =6379\\nREDIS_PRIMARY_PORT_6379_TCP_ADDR =10.0.0.11\\nNote:• \\n• \\n◦ \\n◦', metadata={'source': './PDFS/Concepts.pdf', 'page': 217}),\n",
       " Document(page_content='When you have a Pod that needs to access a Service, and you are using the environment\\nvariable method to publish the port and cluster IP to the client Pods, you must create the\\nService before  the client Pods come into existence. Otherwise, those client Pods won\\'t have their\\nenvironment variables populated.\\nIf you only use DNS to discover the cluster IP for a Service, you don\\'t need to worry about this\\nordering issue.\\nKubernetes also supports and provides variables that are compatible with Docker Engine\\'s\\n\"legacy container links \" feature. You can read makeLinkVariables  to see how this is implemented\\nin Kubernetes.\\nDNS\\nYou can (and almost always should) set up a DNS service for your Kubernetes cluster using an \\nadd-on .\\nA cluster-aware DNS server, such as CoreDNS, watches the Kubernetes API for new Services\\nand creates a set of DNS records for each one. If DNS has been enabled throughout your cluster\\nthen all Pods should automatically be able to resolve Services by their DNS name.\\nFor example, if you have a Service called my-service  in a Kubernetes namespace my-ns , the\\ncontrol plane and the DNS Service acting together create a DNS record for my-service.my-ns .\\nPods in the my-ns  namespace should be able to find the service by doing a name lookup for my-\\nservice  (my-service.my-ns  would also work).\\nPods in other namespaces must qualify the name as my-service.my-ns . These names will resolve\\nto the cluster IP assigned for the Service.\\nKubernetes also supports DNS SRV (Service) records for named ports. If the my-service.my-ns\\nService has a port named http with the protocol set to TCP, you can do a DNS SRV query for \\n_http._tcp.my-service.my-ns  to discover the port number for http, as well as the IP address.\\nThe Kubernetes DNS server is the only way to access ExternalName  Services. You can find more\\ninformation about ExternalName  resolution in DNS for Services and Pods .\\nVirtual IP addressing mechanism\\nRead Virtual IPs and Service Proxies  explains the mechanism Kubernetes provides to expose a\\nService with a virtual IP address.\\nTraffic policies\\nYou can set the .spec.internalTrafficPolicy  and .spec.externalTrafficPolicy  fields to control how\\nKubernetes routes traffic to healthy (“ready”) backends.\\nSee Traffic Policies  for more details.', metadata={'source': './PDFS/Concepts.pdf', 'page': 218}),\n",
       " Document(page_content='Session stickiness\\nIf you want to make sure that connections from a particular client are passed to the same Pod\\neach time, you can configure session affinity based on the client\\'s IP address. Read session\\naffinity  to learn more.\\nExternal IPs\\nIf there are external IPs that route to one or more cluster nodes, Kubernetes Services can be\\nexposed on those externalIPs . When network traffic arrives into the cluster, with the external IP\\n(as destination IP) and the port matching that Service, rules and routes that Kubernetes has\\nconfigured ensure that the traffic is routed to one of the endpoints for that Service.\\nWhen you define a Service, you can specify externalIPs  for any service type . In the example\\nbelow, the Service named \"my-service\"  can be accessed by clients using TCP, on \\n\"198.51.100.32:80\"  (calculated from .spec.externalIPs[]  and .spec.ports[].port ).\\napiVersion : v1\\nkind: Service\\nmetadata :\\n  name : my-service\\nspec:\\n  selector :\\n    app.kubernetes.io/name : MyApp\\n  ports :\\n    - name : http\\n      protocol : TCP\\n      port: 80\\n      targetPort : 49152\\n  externalIPs :\\n    - 198.51.100.32\\nNote:  Kubernetes does not manage allocation of externalIPs ; these are the responsibility of the\\ncluster administrator.\\nAPI Object\\nService is a top-level resource in the Kubernetes REST API. You can find more details about the \\nService API object .\\nWhat\\'s next\\nLearn more about Services and how they fit into Kubernetes:\\nFollow the Connecting Applications with Services  tutorial.\\nRead about Ingress , which exposes HTTP and HTTPS routes from outside the cluster to\\nServices within your cluster.\\nRead about Gateway , an extension to Kubernetes that provides more flexibility than\\nIngress.• \\n• \\n•', metadata={'source': './PDFS/Concepts.pdf', 'page': 219}),\n",
       " Document(page_content='For more context, read the following:\\nVirtual IPs and Service Proxies\\nEndpointSlices\\nService API reference\\nEndpointSlice API reference\\nEndpoint API reference (legacy)\\nIngress\\nMake your HTTP (or HTTPS) network service available using a protocol-aware configuration\\nmechanism, that understands web concepts like URIs, hostnames, paths, and more. The Ingress\\nconcept lets you map traffic to different backends based on rules you define via the Kubernetes\\nAPI.\\nFEATURE STATE:  Kubernetes v1.19 [stable]\\nAn API object that manages external access to the services in a cluster, typically HTTP.\\nIngress may provide load balancing, SSL termination and name-based virtual hosting.\\nNote:  Ingress is frozen. New features are being added to the Gateway API .\\nTerminology\\nFor clarity, this guide defines the following terms:\\nNode: A worker machine in Kubernetes, part of a cluster.\\nCluster: A set of Nodes that run containerized applications managed by Kubernetes. For\\nthis example, and in most common Kubernetes deployments, nodes in the cluster are not\\npart of the public internet.\\nEdge router: A router that enforces the firewall policy for your cluster. This could be a\\ngateway managed by a cloud provider or a physical piece of hardware.\\nCluster network: A set of links, logical or physical, that facilitate communication within a\\ncluster according to the Kubernetes networking model .\\nService: A Kubernetes Service  that identifies a set of Pods using label  selectors. Unless\\nmentioned otherwise, Services are assumed to have virtual IPs only routable within the\\ncluster network.\\nWhat is Ingress?\\nIngress  exposes HTTP and HTTPS routes from outside the cluster to services  within the cluster.\\nTraffic routing is controlled by rules defined on the Ingress resource.\\nHere is a simple example where an Ingress sends all its traffic to one Service:\\ningress-diagram\\nFigure. Ingress• \\n• \\n• \\n• \\n• \\n• \\n• \\n• \\n• \\n•', metadata={'source': './PDFS/Concepts.pdf', 'page': 220}),\n",
       " Document(page_content=\"An Ingress may be configured to give Services externally-reachable URLs, load balance traffic,\\nterminate SSL / TLS, and offer name-based virtual hosting. An Ingress controller  is responsible\\nfor fulfilling the Ingress, usually with a load balancer, though it may also configure your edge\\nrouter or additional frontends to help handle the traffic.\\nAn Ingress does not expose arbitrary ports or protocols. Exposing services other than HTTP\\nand HTTPS to the internet typically uses a service of type Service.Type=NodePort  or \\nService.Type=LoadBalancer .\\nPrerequisites\\nYou must have an Ingress controller  to satisfy an Ingress. Only creating an Ingress resource has\\nno effect.\\nYou may need to deploy an Ingress controller such as ingress-nginx . You can choose from a\\nnumber of Ingress controllers .\\nIdeally, all Ingress controllers should fit the reference specification. In reality, the various\\nIngress controllers operate slightly differently.\\nNote:  Make sure you review your Ingress controller's documentation to understand the caveats\\nof choosing it.\\nThe Ingress resource\\nA minimal Ingress resource example:\\nservice/networking/minimal-ingress.yaml  \\napiVersion : networking.k8s.io/v1\\nkind: Ingress\\nmetadata :\\n  name : minimal-ingress\\n  annotations :\\n    nginx.ingress.kubernetes.io/rewrite-target : /\\nspec:\\n  ingressClassName : nginx-example\\n  rules :\\n  - http:\\n      paths :\\n      - path: /testpath\\n        pathType : Prefix\\n        backend :\\n          service :\\n            name : test\\n            port:\\n              number : 80\\nAn Ingress needs apiVersion , kind, metadata  and spec fields. The name of an Ingress object\\nmust be a valid DNS subdomain name . For general information about working with config files,\\nsee deploying applications , configuring containers , managing resources . Ingress frequently uses\\nannotations to configure some options depending on the Ingress controller, an example of\", metadata={'source': './PDFS/Concepts.pdf', 'page': 221}),\n",
       " Document(page_content='which is the rewrite-target annotation . Different Ingress controllers  support different\\nannotations. Review the documentation for your choice of Ingress controller to learn which\\nannotations are supported.\\nThe Ingress spec  has all the information needed to configure a load balancer or proxy server.\\nMost importantly, it contains a list of rules matched against all incoming requests. Ingress\\nresource only supports rules for directing HTTP(S) traffic.\\nIf the ingressClassName  is omitted, a default Ingress class  should be defined.\\nThere are some ingress controllers, that work without the definition of a default IngressClass .\\nFor example, the Ingress-NGINX controller can be configured with a flag --watch-ingress-\\nwithout-class . It is recommended  though, to specify the default IngressClass  as shown below .\\nIngress rules\\nEach HTTP rule contains the following information:\\nAn optional host. In this example, no host is specified, so the rule applies to all inbound\\nHTTP traffic through the IP address specified. If a host is provided (for example,\\nfoo.bar.com), the rules apply to that host.\\nA list of paths (for example, /testpath ), each of which has an associated backend defined\\nwith a service.name  and a service.port.name  or service.port.number . Both the host and\\npath must match the content of an incoming request before the load balancer directs\\ntraffic to the referenced Service.\\nA backend is a combination of Service and port names as described in the Service doc  or\\na custom resource backend  by way of a CRD . HTTP (and HTTPS) requests to the Ingress\\nthat match the host and path of the rule are sent to the listed backend.\\nA defaultBackend  is often configured in an Ingress controller to service any requests that do not\\nmatch a path in the spec.\\nDefaultBackend\\nAn Ingress with no rules sends all traffic to a single default backend and .spec.defaultBackend  is\\nthe backend that should handle requests in that case. The defaultBackend  is conventionally a\\nconfiguration option of the Ingress controller  and is not specified in your Ingress resources. If\\nno .spec.rules  are specified, .spec.defaultBackend  must be specified. If defaultBackend  is not set,\\nthe handling of requests that do not match any of the rules will be up to the ingress controller\\n(consult the documentation for your ingress controller to find out how it handles this case).\\nIf none of the hosts or paths match the HTTP request in the Ingress objects, the traffic is routed\\nto your default backend.\\nResource backends\\nA Resource  backend is an ObjectRef to another Kubernetes resource within the same\\nnamespace as the Ingress object. A Resource  is a mutually exclusive setting with Service, and\\nwill fail validation if both are specified. A common usage for a Resource  backend is to ingress\\ndata to an object storage backend with static assets.\\nservice/networking/ingress-resource-backend.yaml  • \\n• \\n•', metadata={'source': './PDFS/Concepts.pdf', 'page': 222}),\n",
       " Document(page_content='apiVersion : networking.k8s.io/v1\\nkind: Ingress\\nmetadata :\\n  name : ingress-resource-backend\\nspec:\\n  defaultBackend :\\n    resource :\\n      apiGroup : k8s.example.com\\n      kind: StorageBucket\\n      name : static-assets\\n  rules :\\n    - http:\\n        paths :\\n          - path: /icons\\n            pathType : ImplementationSpecific\\n            backend :\\n              resource :\\n                apiGroup : k8s.example.com\\n                kind: StorageBucket\\n                name : icon-assets\\nAfter creating the Ingress above, you can view it with the following command:\\nkubectl describe ingress ingress-resource-backend\\nName:             ingress-resource-backend\\nNamespace:        default\\nAddress:\\nDefault backend:  APIGroup: k8s.example.com, Kind: StorageBucket, Name: static-assets\\nRules:\\n  Host        Path  Backends\\n  ----        ----  --------\\n  *\\n              /icons   APIGroup: k8s.example.com, Kind: StorageBucket, Name: icon-assets\\nAnnotations:  <none>\\nEvents:       <none>\\nPath types\\nEach path in an Ingress is required to have a corresponding path type. Paths that do not include\\nan explicit pathType  will fail validation. There are three supported path types:\\nImplementationSpecific : With this path type, matching is up to the IngressClass.\\nImplementations can treat this as a separate pathType  or treat it identically to Prefix  or \\nExact  path types.\\nExact : Matches the URL path exactly and with case sensitivity.\\nPrefix : Matches based on a URL path prefix split by /. Matching is case sensitive and done\\non a path element by element basis. A path element refers to the list of labels in the path\\nsplit by the / separator. A request is a match for path p if every p is an element-wise\\nprefix of p of the request path.• \\n• \\n•', metadata={'source': './PDFS/Concepts.pdf', 'page': 223}),\n",
       " Document(page_content='Note:  If the last element of the path is a substring of the last element in request path, it is\\nnot a match (for example: /foo/bar  matches /foo/bar/baz , but does not match /foo/barbaz ).\\nExamples\\nKind Path(s) Request path(s) Matches?\\nPrefix / (all paths) Yes\\nExact /foo /foo Yes\\nExact /foo /bar No\\nExact /foo /foo/ No\\nExact /foo/ /foo No\\nPrefix /foo /foo, /foo/ Yes\\nPrefix /foo/ /foo, /foo/ Yes\\nPrefix /aaa/bb /aaa/bbb No\\nPrefix /aaa/bbb /aaa/bbb Yes\\nPrefix /aaa/bbb/ /aaa/bbb Yes, ignores trailing slash\\nPrefix /aaa/bbb /aaa/bbb/ Yes, matches trailing slash\\nPrefix /aaa/bbb /aaa/bbb/ccc Yes, matches subpath\\nPrefix /aaa/bbb /aaa/bbbxyz No, does not match string prefix\\nPrefix /, /aaa /aaa/ccc Yes, matches /aaa prefix\\nPrefix /, /aaa, /aaa/bbb /aaa/bbb Yes, matches /aaa/bbb  prefix\\nPrefix /, /aaa, /aaa/bbb /ccc Yes, matches / prefix\\nPrefix /aaa /ccc No, uses default backend\\nMixed /foo (Prefix), /foo (Exact) /foo Yes, prefers Exact\\nMultiple matches\\nIn some cases, multiple paths within an Ingress will match a request. In those cases precedence\\nwill be given first to the longest matching path. If two paths are still equally matched,\\nprecedence will be given to paths with an exact path type over prefix path type.\\nHostname wildcards\\nHosts can be precise matches (for example “ foo.bar.com ”) or a wildcard (for example\\n“*.foo.com ”). Precise matches require that the HTTP host header matches the host field.\\nWildcard matches require the HTTP host header is equal to the suffix of the wildcard rule.\\nHost Host header Match?\\n*.foo.com bar.foo.com Matches based on shared suffix\\n*.foo.com baz.bar.foo.com No match, wildcard only covers a single DNS label\\n*.foo.com foo.com No match, wildcard only covers a single DNS label\\nservice/networking/ingress-wildcard-host.yaml  \\napiVersion : networking.k8s.io/v1\\nkind: Ingress\\nmetadata :\\n  name : ingress-wildcard-host\\nspec:', metadata={'source': './PDFS/Concepts.pdf', 'page': 224}),\n",
       " Document(page_content='rules :\\n  - host: \"foo.bar.com\"\\n    http:\\n      paths :\\n      - pathType : Prefix\\n        path: \"/bar\"\\n        backend :\\n          service :\\n            name : service1\\n            port:\\n              number : 80\\n  - host: \"*.foo.com\"\\n    http:\\n      paths :\\n      - pathType : Prefix\\n        path: \"/foo\"\\n        backend :\\n          service :\\n            name : service2\\n            port:\\n              number : 80\\nIngress class\\nIngresses can be implemented by different controllers, often with different configuration. Each\\nIngress should specify a class, a reference to an IngressClass resource that contains additional\\nconfiguration including the name of the controller that should implement the class.\\nservice/networking/external-lb.yaml  \\napiVersion : networking.k8s.io/v1\\nkind: IngressClass\\nmetadata :\\n  name : external-lb\\nspec:\\n  controller : example.com/ingress-controller\\n  parameters :\\n    apiGroup : k8s.example.com\\n    kind: IngressParameters\\n    name : external-lb\\nThe .spec.parameters  field of an IngressClass lets you reference another resource that provides\\nconfiguration related to that IngressClass.\\nThe specific type of parameters to use depends on the ingress controller that you specify in the\\n.spec.controller  field of the IngressClass.\\nIngressClass scope\\nDepending on your ingress controller, you may be able to use parameters that you set cluster-\\nwide, or just for one namespace.\\nCluster•', metadata={'source': './PDFS/Concepts.pdf', 'page': 225}),\n",
       " Document(page_content='Namespaced\\nThe default scope for IngressClass parameters is cluster-wide.\\nIf you set the .spec.parameters  field and don\\'t set .spec.parameters.scope , or if you\\nset .spec.parameters.scope  to Cluster , then the IngressClass refers to a cluster-scoped resource.\\nThe kind (in combination the apiGroup ) of the parameters refers to a cluster-scoped API\\n(possibly a custom resource), and the name  of the parameters identifies a specific cluster scoped\\nresource for that API.\\nFor example:\\n---\\napiVersion : networking.k8s.io/v1\\nkind: IngressClass\\nmetadata :\\n  name : external-lb-1\\nspec:\\n  controller : example.com/ingress-controller\\n  parameters :\\n    # The parameters for this IngressClass are specified in a\\n    # ClusterIngressParameter (API group k8s.example.net) named\\n    # \"external-config-1\". This definition tells Kubernetes to\\n    # look for a cluster-scoped parameter resource.\\n    scope : Cluster\\n    apiGroup : k8s.example.net\\n    kind: ClusterIngressParameter\\n    name : external-config-1\\nFEATURE STATE:  Kubernetes v1.23 [stable]\\nIf you set the .spec.parameters  field and set .spec.parameters.scope  to Namespace , then the\\nIngressClass refers to a namespaced-scoped resource. You must also set the namespace  field\\nwithin .spec.parameters  to the namespace that contains the parameters you want to use.\\nThe kind (in combination the apiGroup ) of the parameters refers to a namespaced API (for\\nexample: ConfigMap), and the name  of the parameters identifies a specific resource in the\\nnamespace you specified in namespace .\\nNamespace-scoped parameters help the cluster operator delegate control over the configuration\\n(for example: load balancer settings, API gateway definition) that is used for a workload. If you\\nused a cluster-scoped parameter then either:\\nthe cluster operator team needs to approve a different team\\'s changes every time there\\'s a\\nnew configuration change being applied.\\nthe cluster operator must define specific access controls, such as RBAC  roles and\\nbindings, that let the application team make changes to the cluster-scoped parameters\\nresource.\\nThe IngressClass API itself is always cluster-scoped.\\nHere is an example of an IngressClass that refers to parameters that are namespaced:• \\n• \\n•', metadata={'source': './PDFS/Concepts.pdf', 'page': 226}),\n",
       " Document(page_content='---\\napiVersion : networking.k8s.io/v1\\nkind: IngressClass\\nmetadata :\\n  name : external-lb-2\\nspec:\\n  controller : example.com/ingress-controller\\n  parameters :\\n    # The parameters for this IngressClass are specified in an\\n    # IngressParameter (API group k8s.example.com) named \"external-config\",\\n    # that\\'s in the \"external-configuration\" namespace.\\n    scope : Namespace\\n    apiGroup : k8s.example.com\\n    kind: IngressParameter\\n    namespace : external-configuration\\n    name : external-config\\nDeprecated annotation\\nBefore the IngressClass resource and ingressClassName  field were added in Kubernetes 1.18,\\nIngress classes were specified with a kubernetes.io/ingress.class  annotation on the Ingress. This\\nannotation was never formally defined, but was widely supported by Ingress controllers.\\nThe newer ingressClassName  field on Ingresses is a replacement for that annotation, but is not\\na direct equivalent. While the annotation was generally used to reference the name of the\\nIngress controller that should implement the Ingress, the field is a reference to an IngressClass\\nresource that contains additional Ingress configuration, including the name of the Ingress\\ncontroller.\\nDefault IngressClass\\nYou can mark a particular IngressClass as default for your cluster. Setting the \\ningressclass.kubernetes.io/is-default-class  annotation to true on an IngressClass resource will\\nensure that new Ingresses without an ingressClassName  field specified will be assigned this\\ndefault IngressClass.\\nCaution:  If you have more than one IngressClass marked as the default for your cluster, the\\nadmission controller prevents creating new Ingress objects that don\\'t have an \\ningressClassName  specified. You can resolve this by ensuring that at most 1 IngressClass is\\nmarked as default in your cluster.\\nThere are some ingress controllers, that work without the definition of a default IngressClass .\\nFor example, the Ingress-NGINX controller can be configured with a flag --watch-ingress-\\nwithout-class . It is recommended  though, to specify the default IngressClass :\\nservice/networking/default-ingressclass.yaml  \\napiVersion : networking.k8s.io/v1\\nkind: IngressClass\\nmetadata :\\n  labels :\\n    app.kubernetes.io/component : controller\\n  name : nginx-example', metadata={'source': './PDFS/Concepts.pdf', 'page': 227}),\n",
       " Document(page_content='annotations :\\n    ingressclass.kubernetes.io/is-default-class : \"true\"\\nspec:\\n  controller : k8s.io/ingress-nginx\\nTypes of Ingress\\nIngress backed by a single Service\\nThere are existing Kubernetes concepts that allow you to expose a single Service (see \\nalternatives ). You can also do this with an Ingress by specifying a default backend  with no rules.\\nservice/networking/test-ingress.yaml  \\napiVersion : networking.k8s.io/v1\\nkind: Ingress\\nmetadata :\\n  name : test-ingress\\nspec:\\n  defaultBackend :\\n    service :\\n      name : test\\n      port:\\n        number : 80\\nIf you create it using kubectl apply -f  you should be able to view the state of the Ingress you\\nadded:\\nkubectl get ingress test-ingress\\nNAME           CLASS         HOSTS   ADDRESS         PORTS   AGE\\ntest-ingress   external-lb   *       203.0.113.123   80      59s\\nWhere 203.0.113.123  is the IP allocated by the Ingress controller to satisfy this Ingress.\\nNote:  Ingress controllers and load balancers may take a minute or two to allocate an IP address.\\nUntil that time, you often see the address listed as <pending> .\\nSimple fanout\\nA fanout configuration routes traffic from a single IP address to more than one Service, based\\non the HTTP URI being requested. An Ingress allows you to keep the number of load balancers\\ndown to a minimum. For example, a setup like:\\ningress-fanout-diagram\\nFigure. Ingress Fan Out\\nIt would require an Ingress such as:\\nservice/networking/simple-fanout-example.yaml', metadata={'source': './PDFS/Concepts.pdf', 'page': 228}),\n",
       " Document(page_content='apiVersion : networking.k8s.io/v1\\nkind: Ingress\\nmetadata :\\n  name : simple-fanout-example\\nspec:\\n  rules :\\n  - host: foo.bar.com\\n    http:\\n      paths :\\n      - path: /foo\\n        pathType : Prefix\\n        backend :\\n          service :\\n            name : service1\\n            port:\\n              number : 4200\\n      - path: /bar\\n        pathType : Prefix\\n        backend :\\n          service :\\n            name : service2\\n            port:\\n              number : 8080\\nWhen you create the Ingress with kubectl apply -f :\\nkubectl describe ingress simple-fanout-example\\nName:             simple-fanout-example\\nNamespace:        default\\nAddress:          178.91.123.132\\nDefault backend:  default-http-backend:80 (10.8.2.3:8080)\\nRules:\\n  Host         Path  Backends\\n  ----         ----  --------\\n  foo.bar.com\\n               /foo   service1:4200 (10.8.0.90:4200)\\n               /bar   service2:8080 (10.8.0.91:8080)\\nEvents:\\n  Type     Reason  Age                From                     Message\\n  ----     ------  ----               ----                     -------\\n  Normal   ADD     22s                loadbalancer-controller  default/test\\nThe Ingress controller provisions an implementation-specific load balancer that satisfies the\\nIngress, as long as the Services ( service1 , service2 ) exist. When it has done so, you can see the\\naddress of the load balancer at the Address field.\\nNote:  Depending on the Ingress controller  you are using, you may need to create a default-\\nhttp-backend Service .', metadata={'source': './PDFS/Concepts.pdf', 'page': 229}),\n",
       " Document(page_content='Name based virtual hosting\\nName-based virtual hosts support routing HTTP traffic to multiple host names at the same IP\\naddress.\\ningress-namebase-diagram\\nFigure. Ingress Name Based Virtual hosting\\nThe following Ingress tells the backing load balancer to route requests based on the Host\\nheader .\\nservice/networking/name-virtual-host-ingress.yaml  \\napiVersion : networking.k8s.io/v1\\nkind: Ingress\\nmetadata :\\n  name : name-virtual-host-ingress\\nspec:\\n  rules :\\n  - host: foo.bar.com\\n    http:\\n      paths :\\n      - pathType : Prefix\\n        path: \"/\"\\n        backend :\\n          service :\\n            name : service1\\n            port:\\n              number : 80\\n  - host: bar.foo.com\\n    http:\\n      paths :\\n      - pathType : Prefix\\n        path: \"/\"\\n        backend :\\n          service :\\n            name : service2\\n            port:\\n              number : 80\\nIf you create an Ingress resource without any hosts defined in the rules, then any web traffic to\\nthe IP address of your Ingress controller can be matched without a name based virtual host\\nbeing required.\\nFor example, the following Ingress routes traffic requested for first.bar.com  to service1 , \\nsecond.bar.com  to service2 , and any traffic whose request host header doesn\\'t match \\nfirst.bar.com  and second.bar.com  to service3 .\\nservice/networking/name-virtual-host-ingress-no-third-host.yaml  \\napiVersion : networking.k8s.io/v1\\nkind: Ingress\\nmetadata :', metadata={'source': './PDFS/Concepts.pdf', 'page': 230}),\n",
       " Document(page_content='name : name-virtual-host-ingress-no-third-host\\nspec:\\n  rules :\\n  - host: first.bar.com\\n    http:\\n      paths :\\n      - pathType : Prefix\\n        path: \"/\"\\n        backend :\\n          service :\\n            name : service1\\n            port:\\n              number : 80\\n  - host: second.bar.com\\n    http:\\n      paths :\\n      - pathType : Prefix\\n        path: \"/\"\\n        backend :\\n          service :\\n            name : service2\\n            port:\\n              number : 80\\n  - http:\\n      paths :\\n      - pathType : Prefix\\n        path: \"/\"\\n        backend :\\n          service :\\n            name : service3\\n            port:\\n              number : 80\\nTLS\\nYou can secure an Ingress by specifying a Secret  that contains a TLS private key and certificate.\\nThe Ingress resource only supports a single TLS port, 443, and assumes TLS termination at the\\ningress point (traffic to the Service and its Pods is in plaintext). If the TLS configuration section\\nin an Ingress specifies different hosts, they are multiplexed on the same port according to the\\nhostname specified through the SNI TLS extension (provided the Ingress controller supports\\nSNI). The TLS secret must contain keys named tls.crt  and tls.key  that contain the certificate and\\nprivate key to use for TLS. For example:\\napiVersion : v1\\nkind: Secret\\nmetadata :\\n  name : testsecret-tls\\n  namespace : default\\ndata:\\n  tls.crt : base64 encoded cert\\n  tls.key : base64 encoded key\\ntype: kubernetes.io/tls', metadata={'source': './PDFS/Concepts.pdf', 'page': 231}),\n",
       " Document(page_content=\"Referencing this secret in an Ingress tells the Ingress controller to secure the channel from the\\nclient to the load balancer using TLS. You need to make sure the TLS secret you created came\\nfrom a certificate that contains a Common Name (CN), also known as a Fully Qualified Domain\\nName (FQDN) for https-example.foo.com .\\nNote:  Keep in mind that TLS will not work on the default rule because the certificates would\\nhave to be issued for all the possible sub-domains. Therefore, hosts  in the tls section need to\\nexplicitly match the host in the rules  section.\\nservice/networking/tls-example-ingress.yaml  \\napiVersion : networking.k8s.io/v1\\nkind: Ingress\\nmetadata :\\n  name : tls-example-ingress\\nspec:\\n  tls:\\n  - hosts :\\n      - https-example.foo.com\\n    secretName : testsecret-tls\\n  rules :\\n  - host: https-example.foo.com\\n    http:\\n      paths :\\n      - path: /\\n        pathType : Prefix\\n        backend :\\n          service :\\n            name : service1\\n            port:\\n              number : 80\\nNote:  There is a gap between TLS features supported by various Ingress controllers. Please\\nrefer to documentation on nginx , GCE , or any other platform specific Ingress controller to\\nunderstand how TLS works in your environment.\\nLoad balancing\\nAn Ingress controller is bootstrapped with some load balancing policy settings that it applies to\\nall Ingress, such as the load balancing algorithm, backend weight scheme, and others. More\\nadvanced load balancing concepts (e.g. persistent sessions, dynamic weights) are not yet\\nexposed through the Ingress. You can instead get these features through the load balancer used\\nfor a Service.\\nIt's also worth noting that even though health checks are not exposed directly through the\\nIngress, there exist parallel concepts in Kubernetes such as readiness probes  that allow you to\\nachieve the same end result. Please review the controller specific documentation to see how\\nthey handle health checks (for example: nginx , or GCE ).\\nUpdating an Ingress\\nTo update an existing Ingress to add a new Host, you can update it by editing the resource:\\nkubectl describe ingress test\", metadata={'source': './PDFS/Concepts.pdf', 'page': 232}),\n",
       " Document(page_content='Name:             test\\nNamespace:        default\\nAddress:          178.91.123.132\\nDefault backend:  default-http-backend:80 (10.8.2.3:8080)\\nRules:\\n  Host         Path  Backends\\n  ----         ----  --------\\n  foo.bar.com\\n               /foo   service1:80 (10.8.0.90:80)\\nAnnotations:\\n  nginx.ingress.kubernetes.io/rewrite-target:  /\\nEvents:\\n  Type     Reason  Age                From                     Message\\n  ----     ------  ----               ----                     -------\\n  Normal   ADD     35s                loadbalancer-controller  default/test\\nkubectl edit ingress test\\nThis pops up an editor with the existing configuration in YAML format. Modify it to include the\\nnew Host:\\nspec:\\n  rules :\\n  - host: foo.bar.com\\n    http:\\n      paths :\\n      - backend :\\n          service :\\n            name : service1\\n            port:\\n              number : 80\\n        path: /foo\\n        pathType : Prefix\\n  - host: bar.baz.com\\n    http:\\n      paths :\\n      - backend :\\n          service :\\n            name : service2\\n            port:\\n              number : 80\\n        path: /foo\\n        pathType : Prefix\\n..\\nAfter you save your changes, kubectl updates the resource in the API server, which tells the\\nIngress controller to reconfigure the load balancer.\\nVerify this:\\nkubectl describe ingress test\\nName:             test\\nNamespace:        default', metadata={'source': './PDFS/Concepts.pdf', 'page': 233}),\n",
       " Document(page_content=\"Address:          178.91.123.132\\nDefault backend:  default-http-backend:80 (10.8.2.3:8080)\\nRules:\\n  Host         Path  Backends\\n  ----         ----  --------\\n  foo.bar.com\\n               /foo   service1:80 (10.8.0.90:80)\\n  bar.baz.com\\n               /foo   service2:80 (10.8.0.91:80)\\nAnnotations:\\n  nginx.ingress.kubernetes.io/rewrite-target:  /\\nEvents:\\n  Type     Reason  Age                From                     Message\\n  ----     ------  ----               ----                     -------\\n  Normal   ADD     45s                loadbalancer-controller  default/test\\nYou can achieve the same outcome by invoking kubectl replace -f  on a modified Ingress YAML\\nfile.\\nFailing across availability zones\\nTechniques for spreading traffic across failure domains differ between cloud providers. Please\\ncheck the documentation of the relevant Ingress controller  for details.\\nAlternatives\\nYou can expose a Service in multiple ways that don't directly involve the Ingress resource:\\nUse Service.Type=LoadBalancer\\nUse Service.Type=NodePort\\nWhat's next\\nLearn about the Ingress  API\\nLearn about Ingress controllers\\nSet up Ingress on Minikube with the NGINX Controller\\nIngress Controllers\\nIn order for an Ingress  to work in your cluster, there must be an ingress controller  running. You\\nneed to select at least one ingress controller and make sure it is set up in your cluster. This page\\nlists common ingress controllers that you can deploy.\\nIn order for the Ingress resource to work, the cluster must have an ingress controller running.\\nUnlike other types of controllers which run as part of the kube-controller-manager  binary,\\nIngress controllers are not started automatically with a cluster. Use this page to choose the\\ningress controller implementation that best fits your cluster.\\nKubernetes as a project supports and maintains AWS , GCE , and nginx  ingress controllers.• \\n• \\n• \\n• \\n•\", metadata={'source': './PDFS/Concepts.pdf', 'page': 234}),\n",
       " Document(page_content=\"Additional controllers\\nNote:  This section links to third party projects that provide functionality required by\\nKubernetes. The Kubernetes project authors aren't responsible for these projects, which are\\nlisted alphabetically. To add a project to this list, read the content guide  before submitting a\\nchange. More information.\\nAKS Application Gateway Ingress Controller  is an ingress controller that configures the \\nAzure Application Gateway .\\nAlibaba Cloud MSE Ingress  is an ingress controller that configures the Alibaba Cloud\\nNative Gateway , which is also the commercial version of Higress .\\nApache APISIX ingress controller  is an Apache APISIX -based ingress controller.\\nAvi Kubernetes Operator  provides L4-L7 load-balancing using VMware NSX Advanced\\nLoad Balancer .\\nBFE Ingress Controller  is a BFE-based ingress controller.\\nCilium Ingress Controller  is an ingress controller powered by Cilium .\\nThe Citrix ingress controller  works with Citrix Application Delivery Controller.\\nContour  is an Envoy  based ingress controller.\\nEmissary-Ingress  API Gateway is an Envoy -based ingress controller.\\nEnRoute  is an Envoy  based API gateway that can run as an ingress controller.\\nEasegress IngressController  is an Easegress  based API gateway that can run as an ingress\\ncontroller.\\nF5 BIG-IP Container Ingress Services for Kubernetes  lets you use an Ingress to configure\\nF5 BIG-IP virtual servers.\\nFortiADC Ingress Controller  support the Kubernetes Ingress resources and allows you to\\nmanage FortiADC objects from Kubernetes\\nGloo  is an open-source ingress controller based on Envoy , which offers API gateway\\nfunctionality.\\nHAProxy Ingress  is an ingress controller for HAProxy .\\nHigress  is an Envoy  based API gateway that can run as an ingress controller.\\nThe HAProxy Ingress Controller for Kubernetes  is also an ingress controller for HAProxy .\\nIstio Ingress  is an Istio based ingress controller.\\nThe Kong Ingress Controller for Kubernetes  is an ingress controller driving Kong\\nGateway .\\nKusk Gateway  is an OpenAPI-driven ingress controller based on Envoy .\\nThe NGINX Ingress Controller for Kubernetes  works with the NGINX  webserver (as a\\nproxy).\\nThe ngrok Kubernetes Ingress Controller  is an open source controller for adding secure\\npublic access to your K8s services using the ngrok platform .\\nThe OCI Native Ingress Controller  is an Ingress controller for Oracle Cloud Infrastructure\\nwhich allows you to manage the OCI Load Balancer .\\nThe Pomerium Ingress Controller  is based on Pomerium , which offers context-aware\\naccess policy.\\nSkipper  HTTP router and reverse proxy for service composition, including use cases like\\nKubernetes Ingress, designed as a library to build your custom proxy.\\nThe Traefik Kubernetes Ingress provider  is an ingress controller for the Traefik  proxy.\\nTyk Operator  extends Ingress with Custom Resources to bring API Management\\ncapabilities to Ingress. Tyk Operator works with the Open Source Tyk Gateway & Tyk\\nCloud control plane.\\nVoyager  is an ingress controller for HAProxy .\\nWallarm Ingress Controller  is an Ingress Controller that provides WAAP (WAF) and API\\nSecurity capabilities.• \\n• \\n• \\n• \\n• \\n• \\n• \\n• \\n• \\n• \\n• \\n• \\n• \\n• \\n• \\n• \\n• \\n• \\n• \\n• \\n• \\n• \\n• \\n• \\n• \\n• \\n• \\n• \\n•\", metadata={'source': './PDFS/Concepts.pdf', 'page': 235}),\n",
       " Document(page_content='Using multiple Ingress controllers\\nYou may deploy any number of ingress controllers using ingress class  within a cluster. Note\\nthe .metadata.name  of your ingress class resource. When you create an ingress you would need\\nthat name to specify the ingressClassName  field on your Ingress object (refer to IngressSpec v1\\nreference ). ingressClassName  is a replacement of the older annotation method .\\nIf you do not specify an IngressClass for an Ingress, and your cluster has exactly one\\nIngressClass marked as default, then Kubernetes applies  the cluster\\'s default IngressClass to the\\nIngress. You mark an IngressClass as default by setting the ingressclass.kubernetes.io/is-default-\\nclass  annotation  on that IngressClass, with the string value \"true\" .\\nIdeally, all ingress controllers should fulfill this specification, but the various ingress controllers\\noperate slightly differently.\\nNote:  Make sure you review your ingress controller\\'s documentation to understand the caveats\\nof choosing it.\\nWhat\\'s next\\nLearn more about Ingress .\\nSet up Ingress on Minikube with the NGINX Controller .\\nGateway API\\nGateway API is a family of API kinds that provide dynamic infrastructure provisioning and\\nadvanced traffic routing.\\nMake network services available by using an extensible, role-oriented, protocol-aware\\nconfiguration mechanism. Gateway API  is an add-on  containing API kinds  that provide\\ndynamic infrastructure provisioning and advanced traffic routing.\\nDesign principles\\nThe following principles shaped the design and architecture of Gateway API:\\nRole-oriented:  Gateway API kinds are modeled after organizational roles that are\\nresponsible for managing Kubernetes service networking:\\nInfrastructure Provider:  Manages infrastructure that allows multiple isolated\\nclusters to serve multiple tenants, e.g. a cloud provider.\\nCluster Operator:  Manages clusters and is typically concerned with policies,\\nnetwork access, application permissions, etc.\\nApplication Developer:  Manages an application running in a cluster and is\\ntypically concerned with application-level configuration and Service  composition.\\nPortable:  Gateway API specifications are defined as custom resources  and are supported\\nby many implementations .\\nExpressive:  Gateway API kinds support functionality for common traffic routing use\\ncases such as header-based matching, traffic weighting, and others that were only\\npossible in Ingress  by using custom annotations.• \\n• \\n• \\n◦ \\n◦ \\n◦ \\n• \\n•', metadata={'source': './PDFS/Concepts.pdf', 'page': 236}),\n",
       " Document(page_content=\"Extensible:  Gateway allows for custom resources to be linked at various layers of the\\nAPI. This makes granular customization possible at the appropriate places within the API\\nstructure.\\nResource model\\nGateway API has three stable API kinds:\\nGatewayClass:  Defines a set of gateways with common configuration and managed by a\\ncontroller that implements the class.\\nGateway:  Defines an instance of traffic handling infrastructure, such as cloud load\\nbalancer.\\nHTTPRoute:  Defines HTTP-specific rules for mapping traffic from a Gateway listener to\\na representation of backend network endpoints. These endpoints are often represented as\\na Service .\\nGateway API is organized into different API kinds that have interdependent relationships to\\nsupport the role-oriented nature of organizations. A Gateway object is associated with exactly\\none GatewayClass; the GatewayClass describes the gateway controller responsible for\\nmanaging Gateways of this class. One or more route kinds such as HTTPRoute, are then\\nassociated to Gateways. A Gateway can filter the routes that may be attached to its listeners ,\\nforming a bidirectional trust model with routes.\\nThe following figure illustrates the relationships of the three stable Gateway API kinds:\\nA figure illustrating the relationships of the three stable Gateway API kinds\\nGatewayClass\\nGateways can be implemented by different controllers, often with different configurations. A\\nGateway must reference a GatewayClass that contains the name of the controller that\\nimplements the class.\\nA minimal GatewayClass example:\\napiVersion : gateway.networking.k8s.io/v1\\nkind: GatewayClass\\nmetadata :\\n  name : example-class\\nspec:\\n  controllerName : example.com/gateway-controller\\nIn this example, a controller that has implemented Gateway API is configured to manage\\nGatewayClasses with the controller name example.com/gateway-controller . Gateways of this\\nclass will be managed by the implementation's controller.\\nSee the GatewayClass  reference for a full definition of this API kind.• \\n• \\n• \\n•\", metadata={'source': './PDFS/Concepts.pdf', 'page': 237}),\n",
       " Document(page_content='Gateway\\nA Gateway describes an instance of traffic handling infrastructure. It defines a network\\nendpoint that can be used for processing traffic, i.e. filtering, balancing, splitting, etc. for\\nbackends such as a Service. For example, a Gateway may represent a cloud load balancer or an\\nin-cluster proxy server that is configured to accept HTTP traffic.\\nA minimal Gateway resource example:\\napiVersion : gateway.networking.k8s.io/v1\\nkind: Gateway\\nmetadata :\\n  name : example-gateway\\nspec:\\n  gatewayClassName : example-class\\n  listeners :\\n  - name : http\\n    protocol : HTTP\\n    port: 80\\nIn this example, an instance of traffic handling infrastructure is programmed to listen for HTTP\\ntraffic on port 80. Since the addresses  field is unspecified, an address or hostname is assigned to\\nthe Gateway by the implementation\\'s controller. This address is used as a network endpoint for\\nprocessing traffic of backend network endpoints defined in routes.\\nSee the Gateway  reference for a full definition of this API kind.\\nHTTPRoute\\nThe HTTPRoute kind specifies routing behavior of HTTP requests from a Gateway listener to\\nbackend network endpoints. For a Service backend, an implementation may represent the\\nbackend network endpoint as a Service IP or the backing Endpoints of the Service. An\\nHTTPRoute represents configuration that is applied to the underlying Gateway\\nimplementation. For example, defining a new HTTPRoute may result in configuring additional\\ntraffic routes in a cloud load balancer or in-cluster proxy server.\\nA minimal HTTPRoute example:\\napiVersion : gateway.networking.k8s.io/v1\\nkind: HTTPRoute\\nmetadata :\\n  name : example-httproute\\nspec:\\n  parentRefs :\\n  - name : example-gateway\\n  hostnames :\\n  - \"www.example.com\"\\n  rules :\\n  - matches :\\n    - path:\\n        type: PathPrefix\\n        value : /login\\n    backendRefs :', metadata={'source': './PDFS/Concepts.pdf', 'page': 238}),\n",
       " Document(page_content=\"- name : example-svc\\n      port: 8080\\nIn this example, HTTP traffic from Gateway example-gateway  with the Host: header set to \\nwww.example.com  and the request path specified as /login  will be routed to Service example-\\nsvc on port 8080.\\nSee the HTTPRoute  reference for a full definition of this API kind.\\nRequest flow\\nHere is a simple example of HTTP traffic being routed to a Service by using a Gateway and an\\nHTTPRoute:\\nA diagram that provides an example of HTTP traffic being routed to a Service by\\nusing a Gateway and an HTTPRoute\\nIn this example, the request flow for a Gateway implemented as a reverse proxy is:\\nThe client starts to prepare an HTTP request for the URL http://www.example.com\\nThe client's DNS resolver queries for the destination name and learns a mapping to one or\\nmore IP addresses associated with the Gateway.\\nThe client sends a request to the Gateway IP address; the reverse proxy receives the\\nHTTP request and uses the Host: header to match a configuration that was derived from\\nthe Gateway and attached HTTPRoute.\\nOptionally, the reverse proxy can perform request header and/or path matching based on\\nmatch rules of the HTTPRoute.\\nOptionally, the reverse proxy can modify the request; for example, to add or remove\\nheaders, based on filter rules of the HTTPRoute.\\nLastly, the reverse proxy forwards the request to one or more backends.\\nConformance\\nGateway API covers a broad set of features and is widely implemented. This combination\\nrequires clear conformance definitions and tests to ensure that the API provides a consistent\\nexperience wherever it is used.\\nSee the conformance  documentation to understand details such as release channels, support\\nlevels, and running conformance tests.\\nMigrating from Ingress\\nGateway API is the successor to the Ingress  API. However, it does not include the Ingress kind.\\nAs a result, a one-time conversion from your existing Ingress resources to Gateway API\\nresources is necessary.\\nRefer to the ingress migration  guide for details on migrating Ingress resources to Gateway API\\nresources.1. \\n2. \\n3. \\n4. \\n5. \\n6.\", metadata={'source': './PDFS/Concepts.pdf', 'page': 239}),\n",
       " Document(page_content='What\\'s next\\nInstead of Gateway API resources being natively implemented by Kubernetes, the specifications\\nare defined as Custom Resources  supported by a wide range of implementations . Install  the\\nGateway API CRDs or follow the installation instructions of your selected implementation.\\nAfter installing an implementation, use the Getting Started  guide to help you quickly start\\nworking with Gateway API.\\nNote:  Make sure to review the documentation of your selected implementation to understand\\nany caveats.\\nRefer to the API specification  for additional details of all Gateway API kinds.\\nEndpointSlices\\nThe EndpointSlice API is the mechanism that Kubernetes uses to let your Service scale to\\nhandle large numbers of backends, and allows the cluster to update its list of healthy backends\\nefficiently.\\nFEATURE STATE:  Kubernetes v1.21 [stable]\\nKubernetes\\' EndpointSlice  API provides a way to track network endpoints within a Kubernetes\\ncluster. EndpointSlices offer a more scalable and extensible alternative to Endpoints .\\nEndpointSlice API\\nIn Kubernetes, an EndpointSlice contains references to a set of network endpoints. The control\\nplane automatically creates EndpointSlices for any Kubernetes Service that has a selector\\nspecified. These EndpointSlices include references to all the Pods that match the Service\\nselector. EndpointSlices group network endpoints together by unique combinations of protocol,\\nport number, and Service name. The name of a EndpointSlice object must be a valid DNS\\nsubdomain name .\\nAs an example, here\\'s a sample EndpointSlice object, that\\'s owned by the example  Kubernetes\\nService.\\napiVersion : discovery.k8s.io/v1\\nkind: EndpointSlice\\nmetadata :\\n  name : example-abc\\n  labels :\\n    kubernetes.io/service-name : example\\naddressType : IPv4\\nports :\\n  - name : http\\n    protocol : TCP\\n    port: 80\\nendpoints :\\n  - addresses :\\n      - \"10.1.2.3\"\\n    conditions :\\n      ready : true', metadata={'source': './PDFS/Concepts.pdf', 'page': 240}),\n",
       " Document(page_content=\"hostname : pod-1\\n    nodeName : node-1\\n    zone : us-west2-a\\nBy default, the control plane creates and manages EndpointSlices to have no more than 100\\nendpoints each. You can configure this with the --max-endpoints-per-slice  kube-controller-\\nmanager  flag, up to a maximum of 1000.\\nEndpointSlices can act as the source of truth for kube-proxy  when it comes to how to route\\ninternal traffic.\\nAddress types\\nEndpointSlices support three address types:\\nIPv4\\nIPv6\\nFQDN (Fully Qualified Domain Name)\\nEach EndpointSlice  object represents a specific IP address type. If you have a Service that is\\navailable via IPv4 and IPv6, there will be at least two EndpointSlice  objects (one for IPv4, and\\none for IPv6).\\nConditions\\nThe EndpointSlice API stores conditions about endpoints that may be useful for consumers. The\\nthree conditions are ready , serving , and terminating .\\nReady\\nready  is a condition that maps to a Pod's Ready  condition. A running Pod with the Ready\\ncondition set to True  should have this EndpointSlice condition also set to true. For\\ncompatibility reasons, ready  is NEVER true when a Pod is terminating. Consumers should refer\\nto the serving  condition to inspect the readiness of terminating Pods. The only exception to this\\nrule is for Services with spec.publishNotReadyAddresses  set to true. Endpoints for these\\nServices will always have the ready  condition set to true.\\nServing\\nFEATURE STATE:  Kubernetes v1.26 [stable]\\nThe serving  condition is almost identical to the ready  condition. The difference is that\\nconsumers of the EndpointSlice API should check the serving  condition if they care about pod\\nreadiness while the pod is also terminating.\\nNote:  Although serving  is almost identical to ready , it was added to prevent breaking the\\nexisting meaning of ready . It may be unexpected for existing clients if ready  could be true for\\nterminating endpoints, since historically terminating endpoints were never included in the\\nEndpoints or EndpointSlice API to begin with. For this reason, ready  is always  false for\\nterminating endpoints, and a new condition serving  was added in v1.20 so that clients can track\\nreadiness for terminating pods independent of the existing semantics for ready .• \\n• \\n•\", metadata={'source': './PDFS/Concepts.pdf', 'page': 241}),\n",
       " Document(page_content='Terminating\\nFEATURE STATE:  Kubernetes v1.22 [beta]\\nTerminating  is a condition that indicates whether an endpoint is terminating. For pods, this is\\nany pod that has a deletion timestamp set.\\nTopology information\\nEach endpoint within an EndpointSlice can contain relevant topology information. The\\ntopology information includes the location of the endpoint and information about the\\ncorresponding Node and zone. These are available in the following per endpoint fields on\\nEndpointSlices:\\nnodeName  - The name of the Node this endpoint is on.\\nzone  - The zone this endpoint is in.\\nNote:\\nIn the v1 API, the per endpoint topology  was effectively removed in favor of the dedicated\\nfields nodeName  and zone .\\nSetting arbitrary topology fields on the endpoint  field of an EndpointSlice  resource has been\\ndeprecated and is not supported in the v1 API. Instead, the v1 API supports setting individual \\nnodeName  and zone  fields. These fields are automatically translated between API versions. For\\nexample, the value of the \"topology.kubernetes.io/zone\"  key in the topology  field in the v1beta1\\nAPI is accessible as the zone  field in the v1 API.\\nManagement\\nMost often, the control plane (specifically, the endpoint slice controller ) creates and manages\\nEndpointSlice objects. There are a variety of other use cases for EndpointSlices, such as service\\nmesh implementations, that could result in other entities or controllers managing additional\\nsets of EndpointSlices.\\nTo ensure that multiple entities can manage EndpointSlices without interfering with each other,\\nKubernetes defines the label  endpointslice.kubernetes.io/managed-by , which indicates the\\nentity managing an EndpointSlice. The endpoint slice controller sets endpointslice-\\ncontroller.k8s.io  as the value for this label on all EndpointSlices it manages. Other entities\\nmanaging EndpointSlices should also set a unique value for this label.\\nOwnership\\nIn most use cases, EndpointSlices are owned by the Service that the endpoint slice object tracks\\nendpoints for. This ownership is indicated by an owner reference on each EndpointSlice as well\\nas a kubernetes.io/service-name  label that enables simple lookups of all EndpointSlices\\nbelonging to a Service.• \\n•', metadata={'source': './PDFS/Concepts.pdf', 'page': 242}),\n",
       " Document(page_content=\"EndpointSlice mirroring\\nIn some cases, applications create custom Endpoints resources. To ensure that these\\napplications do not need to concurrently write to both Endpoints and EndpointSlice resources,\\nthe cluster's control plane mirrors most Endpoints resources to corresponding EndpointSlices.\\nThe control plane mirrors Endpoints resources unless:\\nthe Endpoints resource has a endpointslice.kubernetes.io/skip-mirror  label set to true.\\nthe Endpoints resource has a control-plane.alpha.kubernetes.io/leader  annotation.\\nthe corresponding Service resource does not exist.\\nthe corresponding Service resource has a non-nil selector.\\nIndividual Endpoints resources may translate into multiple EndpointSlices. This will occur if an\\nEndpoints resource has multiple subsets or includes endpoints with multiple IP families (IPv4\\nand IPv6). A maximum of 1000 addresses per subset will be mirrored to EndpointSlices.\\nDistribution of EndpointSlices\\nEach EndpointSlice has a set of ports that applies to all endpoints within the resource. When\\nnamed ports are used for a Service, Pods may end up with different target port numbers for the\\nsame named port, requiring different EndpointSlices. This is similar to the logic behind how\\nsubsets are grouped with Endpoints.\\nThe control plane tries to fill EndpointSlices as full as possible, but does not actively rebalance\\nthem. The logic is fairly straightforward:\\nIterate through existing EndpointSlices, remove endpoints that are no longer desired and\\nupdate matching endpoints that have changed.\\nIterate through EndpointSlices that have been modified in the first step and fill them up\\nwith any new endpoints needed.\\nIf there's still new endpoints left to add, try to fit them into a previously unchanged slice\\nand/or create new ones.\\nImportantly, the third step prioritizes limiting EndpointSlice updates over a perfectly full\\ndistribution of EndpointSlices. As an example, if there are 10 new endpoints to add and 2\\nEndpointSlices with room for 5 more endpoints each, this approach will create a new\\nEndpointSlice instead of filling up the 2 existing EndpointSlices. In other words, a single\\nEndpointSlice creation is preferrable to multiple EndpointSlice updates.\\nWith kube-proxy running on each Node and watching EndpointSlices, every change to an\\nEndpointSlice becomes relatively expensive since it will be transmitted to every Node in the\\ncluster. This approach is intended to limit the number of changes that need to be sent to every\\nNode, even if it may result with multiple EndpointSlices that are not full.\\nIn practice, this less than ideal distribution should be rare. Most changes processed by the\\nEndpointSlice controller will be small enough to fit in an existing EndpointSlice, and if not, a\\nnew EndpointSlice is likely going to be necessary soon anyway. Rolling updates of\\nDeployments also provide a natural repacking of EndpointSlices with all Pods and their\\ncorresponding endpoints getting replaced.• \\n• \\n• \\n• \\n1. \\n2. \\n3.\", metadata={'source': './PDFS/Concepts.pdf', 'page': 243}),\n",
       " Document(page_content=\"Duplicate endpoints\\nDue to the nature of EndpointSlice changes, endpoints may be represented in more than one\\nEndpointSlice at the same time. This naturally occurs as changes to different EndpointSlice\\nobjects can arrive at the Kubernetes client watch / cache at different times.\\nNote:\\nClients of the EndpointSlice API must iterate through all the existing EndpointSlices associated\\nto a Service and build a complete list of unique network endpoints. It is important to mention\\nthat endpoints may be duplicated in different EndpointSlices.\\nYou can find a reference implementation for how to perform this endpoint aggregation and\\ndeduplication as part of the EndpointSliceCache  code within kube-proxy .\\nComparison with Endpoints\\nThe original Endpoints API provided a simple and straightforward way of tracking network\\nendpoints in Kubernetes. As Kubernetes clusters and Services  grew to handle more traffic and\\nto send more traffic to more backend Pods, the limitations of that original API became more\\nvisible. Most notably, those included challenges with scaling to larger numbers of network\\nendpoints.\\nSince all network endpoints for a Service were stored in a single Endpoints object, those\\nEndpoints objects could get quite large. For Services that stayed stable (the same set of\\nendpoints over a long period of time) the impact was less noticeable; even then, some use cases\\nof Kubernetes weren't well served.\\nWhen a Service had a lot of backend endpoints and the workload was either scaling frequently,\\nor rolling out new changes frequently, each update to the single Endpoints object for that\\nService meant a lot of traffic between Kubernetes cluster components (within the control plane,\\nand also between nodes and the API server). This extra traffic also had a cost in terms of CPU\\nuse.\\nWith EndpointSlices, adding or removing a single Pod triggers the same number  of updates to\\nclients that are watching for changes, but the size of those update message is much smaller at\\nlarge scale.\\nEndpointSlices also enabled innovation around new features such dual-stack networking and\\ntopology-aware routing.\\nWhat's next\\nFollow the Connecting Applications with Services  tutorial\\nRead the API reference  for the EndpointSlice API\\nRead the API reference  for the Endpoints API\\nNetwork Policies\\nIf you want to control traffic flow at the IP address or port level (OSI layer 3 or 4),\\nNetworkPolicies allow you to specify rules for traffic flow within your cluster, and also between• \\n• \\n•\", metadata={'source': './PDFS/Concepts.pdf', 'page': 244}),\n",
       " Document(page_content='Pods and the outside world. Your cluster must use a network plugin that supports\\nNetworkPolicy enforcement.\\nIf you want to control traffic flow at the IP address or port level for TCP, UDP, and SCTP\\nprotocols, then you might consider using Kubernetes NetworkPolicies for particular\\napplications in your cluster. NetworkPolicies are an application-centric construct which allow\\nyou to specify how a pod is allowed to communicate with various network \"entities\" (we use\\nthe word \"entity\" here to avoid overloading the more common terms such as \"endpoints\" and\\n\"services\", which have specific Kubernetes connotations) over the network. NetworkPolicies\\napply to a connection with a pod on one or both ends, and are not relevant to other\\nconnections.\\nThe entities that a Pod can communicate with are identified through a combination of the\\nfollowing 3 identifiers:\\nOther pods that are allowed (exception: a pod cannot block access to itself)\\nNamespaces that are allowed\\nIP blocks (exception: traffic to and from the node where a Pod is running is always\\nallowed, regardless of the IP address of the Pod or the node)\\nWhen defining a pod- or namespace- based NetworkPolicy, you use a selector  to specify what\\ntraffic is allowed to and from the Pod(s) that match the selector.\\nMeanwhile, when IP based NetworkPolicies are created, we define policies based on IP blocks\\n(CIDR ranges).\\nPrerequisites\\nNetwork policies are implemented by the network plugin . To use network policies, you must be\\nusing a networking solution which supports NetworkPolicy. Creating a NetworkPolicy resource\\nwithout a controller that implements it will have no effect.\\nThe Two Sorts of Pod Isolation\\nThere are two sorts of isolation for a pod: isolation for egress, and isolation for ingress. They\\nconcern what connections may be established. \"Isolation\" here is not absolute, rather it means\\n\"some restrictions apply\". The alternative, \"non-isolated for $direction\", means that no\\nrestrictions apply in the stated direction. The two sorts of isolation (or not) are declared\\nindependently, and are both relevant for a connection from one pod to another.\\nBy default, a pod is non-isolated for egress; all outbound connections are allowed. A pod is\\nisolated for egress if there is any NetworkPolicy that both selects the pod and has \"Egress\" in its\\npolicyTypes ; we say that such a policy applies to the pod for egress. When a pod is isolated for\\negress, the only allowed connections from the pod are those allowed by the egress  list of some\\nNetworkPolicy that applies to the pod for egress. The effects of those egress  lists combine\\nadditively.\\nBy default, a pod is non-isolated for ingress; all inbound connections are allowed. A pod is\\nisolated for ingress if there is any NetworkPolicy that both selects the pod and has \"Ingress\" in\\nits policyTypes ; we say that such a policy applies to the pod for ingress. When a pod is isolated\\nfor ingress, the only allowed connections into the pod are those from the pod\\'s node and those1. \\n2. \\n3.', metadata={'source': './PDFS/Concepts.pdf', 'page': 245}),\n",
       " Document(page_content='allowed by the ingress  list of some NetworkPolicy that applies to the pod for ingress. The\\neffects of those ingress  lists combine additively.\\nNetwork policies do not conflict; they are additive. If any policy or policies apply to a given pod\\nfor a given direction, the connections allowed in that direction from that pod is the union of\\nwhat the applicable policies allow. Thus, order of evaluation does not affect the policy result.\\nFor a connection from a source pod to a destination pod to be allowed, both the egress policy\\non the source pod and the ingress policy on the destination pod need to allow the connection. If\\neither side does not allow the connection, it will not happen.\\nThe NetworkPolicy resource\\nSee the NetworkPolicy  reference for a full definition of the resource.\\nAn example NetworkPolicy might look like this:\\nservice/networking/networkpolicy.yaml  \\napiVersion : networking.k8s.io/v1\\nkind: NetworkPolicy\\nmetadata :\\n  name : test-network-policy\\n  namespace : default\\nspec:\\n  podSelector :\\n    matchLabels :\\n      role: db\\n  policyTypes :\\n    - Ingress\\n    - Egress\\n  ingress :\\n    - from :\\n        - ipBlock :\\n            cidr: 172.17.0.0 /16\\n            except :\\n              - 172.17.1.0 /24\\n        - namespaceSelector :\\n            matchLabels :\\n              project : myproject\\n        - podSelector :\\n            matchLabels :\\n              role: frontend\\n      ports :\\n        - protocol : TCP\\n          port: 6379\\n  egress :\\n    - to:\\n        - ipBlock :\\n            cidr: 10.0.0.0 /24\\n      ports :\\n        - protocol : TCP', metadata={'source': './PDFS/Concepts.pdf', 'page': 246}),\n",
       " Document(page_content='port: 5978\\nNote:  POSTing this to the API server for your cluster will have no effect unless your chosen\\nnetworking solution supports network policy.\\nMandatory Fields : As with all other Kubernetes config, a NetworkPolicy needs apiVersion , \\nkind, and metadata  fields. For general information about working with config files, see \\nConfigure a Pod to Use a ConfigMap , and Object Management .\\nspec : NetworkPolicy spec has all the information needed to define a particular network policy\\nin the given namespace.\\npodSelector : Each NetworkPolicy includes a podSelector  which selects the grouping of pods to\\nwhich the policy applies. The example policy selects pods with the label \"role=db\". An empty \\npodSelector  selects all pods in the namespace.\\npolicyTypes : Each NetworkPolicy includes a policyTypes  list which may include either \\nIngress , Egress , or both. The policyTypes  field indicates whether or not the given policy applies\\nto ingress traffic to selected pod, egress traffic from selected pods, or both. If no policyTypes  are\\nspecified on a NetworkPolicy then by default Ingress  will always be set and Egress  will be set if\\nthe NetworkPolicy has any egress rules.\\ningress : Each NetworkPolicy may include a list of allowed ingress  rules. Each rule allows traffic\\nwhich matches both the from  and ports  sections. The example policy contains a single rule,\\nwhich matches traffic on a single port, from one of three sources, the first specified via an \\nipBlock , the second via a namespaceSelector  and the third via a podSelector .\\negress : Each NetworkPolicy may include a list of allowed egress  rules. Each rule allows traffic\\nwhich matches both the to and ports  sections. The example policy contains a single rule, which\\nmatches traffic on a single port to any destination in 10.0.0.0/24 .\\nSo, the example NetworkPolicy:\\nisolates role=db  pods in the default  namespace for both ingress and egress traffic (if they\\nweren\\'t already isolated)\\n(Ingress rules) allows connections to all pods in the default  namespace with the label \\nrole=db  on TCP port 6379 from:\\nany pod in the default  namespace with the label role=frontend\\nany pod in a namespace with the label project=myproject\\nIP addresses in the ranges 172.17.0.0 –172.17.0.255  and 172.17.2.0 –172.17.255.255  (ie,\\nall of 172.17.0.0/16  except 172.17.1.0/24 )\\n(Egress rules) allows connections from any pod in the default  namespace with the label \\nrole=db  to CIDR 10.0.0.0/24  on TCP port 5978\\nSee the Declare Network Policy  walkthrough for further examples.\\nBehavior of to and from  selectors\\nThere are four kinds of selectors that can be specified in an ingress  from  section or egress  to\\nsection:1. \\n2. \\n◦ \\n◦ \\n◦ \\n3.', metadata={'source': './PDFS/Concepts.pdf', 'page': 247}),\n",
       " Document(page_content='podSelector : This selects particular Pods in the same namespace as the NetworkPolicy which\\nshould be allowed as ingress sources or egress destinations.\\nnamespaceSelector : This selects particular namespaces for which all Pods should be allowed\\nas ingress sources or egress destinations.\\nnamespaceSelector  and podSelector : A single to/from  entry that specifies both \\nnamespaceSelector  and podSelector  selects particular Pods within particular namespaces. Be\\ncareful to use correct YAML syntax. For example:\\n  ...\\n  ingress :\\n  - from :\\n    - namespaceSelector :\\n        matchLabels :\\n          user: alice\\n      podSelector :\\n        matchLabels :\\n          role: client\\n  ...\\nThis policy contains a single from  element allowing connections from Pods with the label \\nrole=client  in namespaces with the label user=alice . But the following policy is different:\\n  ...\\n  ingress :\\n  - from :\\n    - namespaceSelector :\\n        matchLabels :\\n          user: alice\\n    - podSelector :\\n        matchLabels :\\n          role: client\\n  ...\\nIt contains two elements in the from  array, and allows connections from Pods in the local\\nNamespace with the label role=client , or from any Pod in any namespace with the label \\nuser=alice .\\nWhen in doubt, use kubectl describe  to see how Kubernetes has interpreted the policy.\\n ipBlock : This selects particular IP CIDR ranges to allow as ingress sources or egress\\ndestinations. These should be cluster-external IPs, since Pod IPs are ephemeral and\\nunpredictable.\\nCluster ingress and egress mechanisms often require rewriting the source or destination IP of\\npackets. In cases where this happens, it is not defined whether this happens before or after\\nNetworkPolicy processing, and the behavior may be different for different combinations of\\nnetwork plugin, cloud provider, Service  implementation, etc.\\nIn the case of ingress, this means that in some cases you may be able to filter incoming packets\\nbased on the actual original source IP, while in other cases, the \"source IP\" that the\\nNetworkPolicy acts on may be the IP of a LoadBalancer  or of the Pod\\'s node, etc.', metadata={'source': './PDFS/Concepts.pdf', 'page': 248}),\n",
       " Document(page_content='For egress, this means that connections from pods to Service  IPs that get rewritten to cluster-\\nexternal IPs may or may not be subject to ipBlock -based policies.\\nDefault policies\\nBy default, if no policies exist in a namespace, then all ingress and egress traffic is allowed to\\nand from pods in that namespace. The following examples let you change the default behavior\\nin that namespace.\\nDefault deny all ingress traffic\\nYou can create a \"default\" ingress isolation policy for a namespace by creating a NetworkPolicy\\nthat selects all pods but does not allow any ingress traffic to those pods.\\nservice/networking/network-policy-default-deny-ingress.yaml  \\n---\\napiVersion : networking.k8s.io/v1\\nkind: NetworkPolicy\\nmetadata :\\n  name : default-deny-ingress\\nspec:\\n  podSelector : {}\\n  policyTypes :\\n  - Ingress\\nThis ensures that even pods that aren\\'t selected by any other NetworkPolicy will still be isolated\\nfor ingress. This policy does not affect isolation for egress from any pod.\\nAllow all ingress traffic\\nIf you want to allow all incoming connections to all pods in a namespace, you can create a\\npolicy that explicitly allows that.\\nservice/networking/network-policy-allow-all-ingress.yaml  \\n---\\napiVersion : networking.k8s.io/v1\\nkind: NetworkPolicy\\nmetadata :\\n  name : allow-all-ingress\\nspec:\\n  podSelector : {}\\n  ingress :\\n  - {}\\n  policyTypes :\\n  - Ingress\\nWith this policy in place, no additional policy or policies can cause any incoming connection to\\nthose pods to be denied. This policy has no effect on isolation for egress from any pod.', metadata={'source': './PDFS/Concepts.pdf', 'page': 249}),\n",
       " Document(page_content='Default deny all egress traffic\\nYou can create a \"default\" egress isolation policy for a namespace by creating a NetworkPolicy\\nthat selects all pods but does not allow any egress traffic from those pods.\\nservice/networking/network-policy-default-deny-egress.yaml  \\n---\\napiVersion : networking.k8s.io/v1\\nkind: NetworkPolicy\\nmetadata :\\n  name : default-deny-egress\\nspec:\\n  podSelector : {}\\n  policyTypes :\\n  - Egress\\nThis ensures that even pods that aren\\'t selected by any other NetworkPolicy will not be allowed\\negress traffic. This policy does not change the ingress isolation behavior of any pod.\\nAllow all egress traffic\\nIf you want to allow all connections from all pods in a namespace, you can create a policy that\\nexplicitly allows all outgoing connections from pods in that namespace.\\nservice/networking/network-policy-allow-all-egress.yaml  \\n---\\napiVersion : networking.k8s.io/v1\\nkind: NetworkPolicy\\nmetadata :\\n  name : allow-all-egress\\nspec:\\n  podSelector : {}\\n  egress :\\n  - {}\\n  policyTypes :\\n  - Egress\\nWith this policy in place, no additional policy or policies can cause any outgoing connection\\nfrom those pods to be denied. This policy has no effect on isolation for ingress to any pod.\\nDefault deny all ingress and all egress traffic\\nYou can create a \"default\" policy for a namespace which prevents all ingress AND egress traffic\\nby creating the following NetworkPolicy in that namespace.\\nservice/networking/network-policy-default-deny-all.yaml  \\n---\\napiVersion : networking.k8s.io/v1\\nkind: NetworkPolicy\\nmetadata :', metadata={'source': './PDFS/Concepts.pdf', 'page': 250}),\n",
       " Document(page_content=\"name : default-deny-all\\nspec:\\n  podSelector : {}\\n  policyTypes :\\n  - Ingress\\n  - Egress\\nThis ensures that even pods that aren't selected by any other NetworkPolicy will not be allowed\\ningress or egress traffic.\\nNetwork traffic filtering\\nNetworkPolicy is defined for layer 4  connections (TCP, UDP, and optionally SCTP). For all the\\nother protocols, the behaviour may vary across network plugins.\\nNote:  You must be using a CNI plugin that supports SCTP protocol NetworkPolicies.\\nWhen a deny all  network policy is defined, it is only guaranteed to deny TCP, UDP and SCTP\\nconnections. For other protocols, such as ARP or ICMP, the behaviour is undefined. The same\\napplies to allow rules: when a specific pod is allowed as ingress source or egress destination, it\\nis undefined what happens with (for example) ICMP packets. Protocols such as ICMP may be\\nallowed by some network plugins and denied by others.\\nTargeting a range of ports\\nFEATURE STATE:  Kubernetes v1.25 [stable]\\nWhen writing a NetworkPolicy, you can target a range of ports instead of a single port.\\nThis is achievable with the usage of the endPort  field, as the following example:\\nservice/networking/networkpolicy-multiport-egress.yaml  \\napiVersion : networking.k8s.io/v1\\nkind: NetworkPolicy\\nmetadata :\\n  name : multi-port-egress\\n  namespace : default\\nspec:\\n  podSelector :\\n    matchLabels :\\n      role: db\\n  policyTypes :\\n    - Egress\\n  egress :\\n    - to:\\n        - ipBlock :\\n            cidr: 10.0.0.0 /24\\n      ports :\\n        - protocol : TCP\\n          port: 32000\", metadata={'source': './PDFS/Concepts.pdf', 'page': 251}),\n",
       " Document(page_content='endPort : 32768\\nThe above rule allows any Pod with label role=db  on the namespace default  to communicate\\nwith any IP within the range 10.0.0.0/24  over TCP, provided that the target port is between the\\nrange 32000 and 32768.\\nThe following restrictions apply when using this field:\\nThe endPort  field must be equal to or greater than the port field.\\nendPort  can only be defined if port is also defined.\\nBoth ports must be numeric.\\nNote:  Your cluster must be using a CNI plugin that supports the endPort  field in NetworkPolicy\\nspecifications. If your network plugin  does not support the endPort  field and you specify a\\nNetworkPolicy with that, the policy will be applied only for the single port field.\\nTargeting multiple namespaces by label\\nIn this scenario, your Egress  NetworkPolicy targets more than one namespace using their label\\nnames. For this to work, you need to label the target namespaces. For example:\\n kubectl label namespace frontend namespace =frontend\\n kubectl label namespace backend namespace =backend\\nAdd the labels under namespaceSelector  in your NetworkPolicy document. For example:\\napiVersion : networking.k8s.io/v1\\nkind: NetworkPolicy\\nmetadata :\\n  name : egress-namespaces\\nspec:\\n  podSelector :\\n    matchLabels :\\n      app: myapp\\n  policyTypes :\\n  - Egress\\n  egress :\\n  - to:\\n    - namespaceSelector :\\n        matchExpressions :\\n        - key: namespace\\n          operator : In\\n          values : [\"frontend\" , \"backend\" ]\\nNote:  It is not possible to directly specify the name of the namespaces in a NetworkPolicy. You\\nmust use a namespaceSelector  with matchLabels  or matchExpressions  to select the namespaces\\nbased on their labels.• \\n• \\n•', metadata={'source': './PDFS/Concepts.pdf', 'page': 252}),\n",
       " Document(page_content='Targeting a Namespace by its name\\nThe Kubernetes control plane sets an immutable label kubernetes.io/metadata.name  on all\\nnamespaces, the value of the label is the namespace name.\\nWhile NetworkPolicy cannot target a namespace by its name with some object field, you can\\nuse the standardized label to target a specific namespace.\\nPod lifecycle\\nNote:  The following applies to clusters with a conformant networking plugin and a conformant\\nimplementation of NetworkPolicy.\\nWhen a new NetworkPolicy object is created, it may take some time for a network plugin to\\nhandle the new object. If a pod that is affected by a NetworkPolicy is created before the\\nnetwork plugin has completed NetworkPolicy handling, that pod may be started unprotected,\\nand isolation rules will be applied when the NetworkPolicy handling is completed.\\nOnce the NetworkPolicy is handled by a network plugin,\\nAll newly created pods affected by a given NetworkPolicy will be isolated before they are\\nstarted. Implementations of NetworkPolicy must ensure that filtering is effective\\nthroughout the Pod lifecycle, even from the very first instant that any container in that\\nPod is started. Because they are applied at Pod level, NetworkPolicies apply equally to\\ninit containers, sidecar containers, and regular containers.\\nAllow rules will be applied eventually after the isolation rules (or may be applied at the\\nsame time). In the worst case, a newly created pod may have no network connectivity at\\nall when it is first started, if isolation rules were already applied, but no allow rules were\\napplied yet.\\nEvery created NetworkPolicy will be handled by a network plugin eventually, but there is no\\nway to tell from the Kubernetes API when exactly that happens.\\nTherefore, pods must be resilient against being started up with different network connectivity\\nthan expected. If you need to make sure the pod can reach certain destinations before being\\nstarted, you can use an init container  to wait for those destinations to be reachable before\\nkubelet starts the app containers.\\nEvery NetworkPolicy will be applied to all selected pods eventually. Because the network\\nplugin may implement NetworkPolicy in a distributed manner, it is possible that pods may see a\\nslightly inconsistent view of network policies when the pod is first created, or when pods or\\npolicies change. For example, a newly-created pod that is supposed to be able to reach both Pod\\nA on Node 1 and Pod B on Node 2 may find that it can reach Pod A immediately, but cannot\\nreach Pod B until a few seconds later.1. \\n2.', metadata={'source': './PDFS/Concepts.pdf', 'page': 253}),\n",
       " Document(page_content=\"NetworkPolicy and hostNetwork  pods\\nNetworkPolicy behaviour for hostNetwork  pods is undefined, but it should be limited to 2\\npossibilities:\\nThe network plugin can distinguish hostNetwork  pod traffic from all other traffic\\n(including being able to distinguish traffic from different hostNetwork  pods on the same\\nnode), and will apply NetworkPolicy to hostNetwork  pods just like it does to pod-\\nnetwork pods.\\nThe network plugin cannot properly distinguish hostNetwork  pod traffic, and so it\\nignores hostNetwork  pods when matching podSelector  and namespaceSelector . Traffic to/\\nfrom hostNetwork  pods is treated the same as all other traffic to/from the node IP. (This is\\nthe most common implementation.)\\nThis applies when\\na hostNetwork  pod is selected by spec.podSelector .\\n  ...\\n  spec:\\n    podSelector :\\n      matchLabels :\\n        role: client\\n  ...\\na hostNetwork  pod is selected by a podSelector  or namespaceSelector  in an ingress  or \\negress  rule.\\n  ...\\n  ingress :\\n    - from :\\n      - podSelector :\\n          matchLabels :\\n            role: client\\n  ...\\nAt the same time, since hostNetwork  pods have the same IP addresses as the nodes they reside\\non, their connections will be treated as node connections. For example, you can allow traffic\\nfrom a hostNetwork  Pod using an ipBlock  rule.\\nWhat you can't do with network policies (at least, not yet)\\nAs of Kubernetes 1.28, the following functionality does not exist in the NetworkPolicy API, but\\nyou might be able to implement workarounds using Operating System components (such as\\nSELinux, OpenVSwitch, IPTables, and so on) or Layer 7 technologies (Ingress controllers,\\nService Mesh implementations) or admission controllers. In case you are new to network\\nsecurity in Kubernetes, its worth noting that the following User Stories cannot (yet) be\\nimplemented using the NetworkPolicy API.\\nForcing internal cluster traffic to go through a common gateway (this might be best\\nserved with a service mesh or other proxy).\\nAnything TLS related (use a service mesh or ingress controller for this).• \\n• \\n1. \\n2. \\n• \\n•\", metadata={'source': './PDFS/Concepts.pdf', 'page': 254}),\n",
       " Document(page_content='Node specific policies (you can use CIDR notation for these, but you cannot target nodes\\nby their Kubernetes identities specifically).\\nTargeting of services by name (you can, however, target pods or namespaces by their \\nlabels , which is often a viable workaround).\\nCreation or management of \"Policy requests\" that are fulfilled by a third party.\\nDefault policies which are applied to all namespaces or pods (there are some third party\\nKubernetes distributions and projects which can do this).\\nAdvanced policy querying and reachability tooling.\\nThe ability to log network security events (for example connections that are blocked or\\naccepted).\\nThe ability to explicitly deny policies (currently the model for NetworkPolicies are deny\\nby default, with only the ability to add allow rules).\\nThe ability to prevent loopback or incoming host traffic (Pods cannot currently block\\nlocalhost access, nor do they have the ability to block access from their resident node).\\nWhat\\'s next\\nSee the Declare Network Policy  walkthrough for further examples.\\nSee more recipes  for common scenarios enabled by the NetworkPolicy resource.\\nDNS for Services and Pods\\nYour workload can discover Services within your cluster using DNS; this page explains how\\nthat works.\\nKubernetes creates DNS records for Services and Pods. You can contact Services with consistent\\nDNS names instead of IP addresses.\\nKubernetes publishes information about Pods and Services which is used to program DNS.\\nKubelet configures Pods\\' DNS so that running containers can lookup Services by name rather\\nthan IP.\\nServices defined in the cluster are assigned DNS names. By default, a client Pod\\'s DNS search\\nlist includes the Pod\\'s own namespace and the cluster\\'s default domain.\\nNamespaces of Services\\nA DNS query may return different results based on the namespace of the Pod making it. DNS\\nqueries that don\\'t specify a namespace are limited to the Pod\\'s namespace. Access Services in\\nother namespaces by specifying it in the DNS query.\\nFor example, consider a Pod in a test namespace. A data Service is in the prod  namespace.\\nA query for data returns no results, because it uses the Pod\\'s test namespace.\\nA query for data.prod  returns the intended result, because it specifies the namespace.\\nDNS queries may be expanded using the Pod\\'s /etc/resolv.conf . Kubelet configures this file for\\neach Pod. For example, a query for just data may be expanded to data.test.svc.cluster.local . The\\nvalues of the search  option are used to expand queries. To learn more about DNS queries, see \\nthe resolv.conf  manual page.• \\n• \\n• \\n• \\n• \\n• \\n• \\n• \\n• \\n•', metadata={'source': './PDFS/Concepts.pdf', 'page': 255}),\n",
       " Document(page_content='nameserver 10.32.0.10\\nsearch <namespace>.svc.cluster.local svc.cluster.local cluster.local\\noptions ndots:5\\nIn summary, a Pod in the test namespace can successfully resolve either data.prod  or \\ndata.prod.svc.cluster.local .\\nDNS Records\\nWhat objects get DNS records?\\nServices\\nPods\\nThe following sections detail the supported DNS record types and layout that is supported. Any\\nother layout or names or queries that happen to work are considered implementation details\\nand are subject to change without warning. For more up-to-date specification, see Kubernetes\\nDNS-Based Service Discovery .\\nServices\\nA/AAAA records\\n\"Normal\" (not headless) Services are assigned DNS A and/or AAAA records, depending on the\\nIP family or families of the Service, with a name of the form my-svc.my-namespace.svc.cluster-\\ndomain.example . This resolves to the cluster IP of the Service.\\nHeadless Services  (without a cluster IP) Services are also assigned DNS A and/or AAAA\\nrecords, with a name of the form my-svc.my-namespace.svc.cluster-domain.example . Unlike\\nnormal Services, this resolves to the set of IPs of all of the Pods selected by the Service. Clients\\nare expected to consume the set or else use standard round-robin selection from the set.\\nSRV records\\nSRV Records are created for named ports that are part of normal or headless services. For each\\nnamed port, the SRV record has the form _port-name._port-protocol.my-svc.my-\\nnamespace.svc.cluster-domain.example . For a regular Service, this resolves to the port number\\nand the domain name: my-svc.my-namespace.svc.cluster-domain.example . For a headless\\nService, this resolves to multiple answers, one for each Pod that is backing the Service, and\\ncontains the port number and the domain name of the Pod of the form hostname.my-svc.my-\\nnamespace.svc.cluster-domain.example .\\nPods\\nA/AAAA records\\nIn general a Pod has the following DNS resolution:\\npod-ip-address.my-namespace.pod.cluster-domain.example .1. \\n2.', metadata={'source': './PDFS/Concepts.pdf', 'page': 256}),\n",
       " Document(page_content='For example, if a Pod in the default  namespace has the IP address 172.17.0.3, and the domain\\nname for your cluster is cluster.local , then the Pod has a DNS name:\\n172-17-0-3.default.pod.cluster.local .\\nAny Pods exposed by a Service have the following DNS resolution available:\\npod-ip-address.service-name.my-namespace.svc.cluster-domain.example .\\nPod\\'s hostname and subdomain fields\\nCurrently when a Pod is created, its hostname (as observed from within the Pod) is the Pod\\'s \\nmetadata.name  value.\\nThe Pod spec has an optional hostname  field, which can be used to specify a different hostname.\\nWhen specified, it takes precedence over the Pod\\'s name to be the hostname of the Pod (again,\\nas observed from within the Pod). For example, given a Pod with spec.hostname  set to \"my-\\nhost\" , the Pod will have its hostname set to \"my-host\" .\\nThe Pod spec also has an optional subdomain  field which can be used to indicate that the pod is\\npart of sub-group of the namespace. For example, a Pod with spec.hostname  set to \"foo\" , and \\nspec.subdomain  set to \"bar\" , in namespace \"my-namespace\" , will have its hostname set to \"foo\"\\nand its fully qualified domain name (FQDN) set to \"foo.bar.my-namespace.svc.cluster.local\"\\n(once more, as observed from within the Pod).\\nIf there exists a headless Service in the same namespace as the Pod, with the same name as the\\nsubdomain, the cluster\\'s DNS Server also returns A and/or AAAA records for the Pod\\'s fully\\nqualified hostname.\\nExample:\\napiVersion : v1\\nkind: Service\\nmetadata :\\n  name : busybox-subdomain\\nspec:\\n  selector :\\n    name : busybox\\n  clusterIP : None\\n  ports :\\n  - name : foo # name is not required for single-port Services\\n    port: 1234\\n---\\napiVersion : v1\\nkind: Pod\\nmetadata :\\n  name : busybox1\\n  labels :\\n    name : busybox\\nspec:\\n  hostname : busybox-1\\n  subdomain : busybox-subdomain\\n  containers :\\n  - image : busybox:1.28', metadata={'source': './PDFS/Concepts.pdf', 'page': 257}),\n",
       " Document(page_content='command :\\n      - sleep\\n      - \"3600\"\\n    name : busybox\\n---\\napiVersion : v1\\nkind: Pod\\nmetadata :\\n  name : busybox2\\n  labels :\\n    name : busybox\\nspec:\\n  hostname : busybox-2\\n  subdomain : busybox-subdomain\\n  containers :\\n  - image : busybox:1.28\\n    command :\\n      - sleep\\n      - \"3600\"\\n    name : busybox\\nGiven the above Service \"busybox-subdomain\"  and the Pods which set spec.subdomain  to \\n\"busybox-subdomain\" , the first Pod will see its own FQDN as \"busybox-1.busybox-\\nsubdomain.my-namespace.svc.cluster-domain.example\" . DNS serves A and/or AAAA records at\\nthat name, pointing to the Pod\\'s IP. Both Pods \" busybox1 \" and \" busybox2 \" will have their own\\naddress records.\\nAn EndpointSlice  can specify the DNS hostname for any endpoint addresses, along with its IP.\\nNote:  Because A and AAAA records are not created for Pod names, hostname  is required for\\nthe Pod\\'s A or AAAA record to be created. A Pod with no hostname  but with subdomain  will\\nonly create the A or AAAA record for the headless Service ( busybox-subdomain.my-\\nnamespace.svc.cluster-domain.example ), pointing to the Pods\\' IP addresses. Also, the Pod needs\\nto be ready in order to have a record unless publishNotReadyAddresses=True  is set on the\\nService.\\nPod\\'s setHostnameAsFQDN field\\nFEATURE STATE:  Kubernetes v1.22 [stable]\\nWhen a Pod is configured to have fully qualified domain name (FQDN), its hostname is the\\nshort hostname. For example, if you have a Pod with the fully qualified domain name \\nbusybox-1.busybox-subdomain.my-namespace.svc.cluster-domain.example , then by default the \\nhostname  command inside that Pod returns busybox-1  and the hostname --fqdn  command\\nreturns the FQDN.\\nWhen you set setHostnameAsFQDN: true  in the Pod spec, the kubelet writes the Pod\\'s FQDN\\ninto the hostname for that Pod\\'s namespace. In this case, both hostname  and hostname --fqdn\\nreturn the Pod\\'s FQDN.\\nNote:', metadata={'source': './PDFS/Concepts.pdf', 'page': 258}),\n",
       " Document(page_content='In Linux, the hostname field of the kernel (the nodename  field of struct utsname ) is limited to\\n64 characters.\\nIf a Pod enables this feature and its FQDN is longer than 64 character, it will fail to start. The\\nPod will remain in Pending  status ( ContainerCreating  as seen by kubectl ) generating error\\nevents, such as Failed to construct FQDN from Pod hostname and cluster domain, FQDN long-\\nFQDN  is too long (64 characters is the max, 70 characters requested). One way of improving\\nuser experience for this scenario is to create an admission webhook controller  to control FQDN\\nsize when users create top level objects, for example, Deployment.\\nPod\\'s DNS Policy\\nDNS policies can be set on a per-Pod basis. Currently Kubernetes supports the following Pod-\\nspecific DNS policies. These policies are specified in the dnsPolicy  field of a Pod Spec.\\n\"Default \": The Pod inherits the name resolution configuration from the node that the Pods\\nrun on. See related discussion  for more details.\\n\"ClusterFirst \": Any DNS query that does not match the configured cluster domain suffix,\\nsuch as \" www.kubernetes.io \", is forwarded to an upstream nameserver by the DNS server.\\nCluster administrators may have extra stub-domain and upstream DNS servers\\nconfigured. See related discussion  for details on how DNS queries are handled in those\\ncases.\\n\"ClusterFirstWithHostNet \": For Pods running with hostNetwork, you should explicitly set\\nits DNS policy to \" ClusterFirstWithHostNet \". Otherwise, Pods running with hostNetwork\\nand \"ClusterFirst\"  will fallback to the behavior of the \"Default\"  policy.\\nNote: This is not supported on Windows. See below  for details\\n\"None \": It allows a Pod to ignore DNS settings from the Kubernetes environment. All\\nDNS settings are supposed to be provided using the dnsConfig  field in the Pod Spec. See \\nPod\\'s DNS config  subsection below.\\nNote:  \"Default\" is not the default DNS policy. If dnsPolicy  is not explicitly specified, then\\n\"ClusterFirst\" is used.\\nThe example below shows a Pod with its DNS policy set to \" ClusterFirstWithHostNet \" because\\nit has hostNetwork  set to true.\\napiVersion : v1\\nkind: Pod\\nmetadata :\\n  name : busybox\\n  namespace : default\\nspec:\\n  containers :\\n  - image : busybox:1.28\\n    command :\\n      - sleep\\n      - \"3600\"\\n    imagePullPolicy : IfNotPresent\\n    name : busybox\\n  restartPolicy : Always\\n  hostNetwork : true\\n  dnsPolicy : ClusterFirstWithHostNet• \\n• \\n• \\n◦ \\n•', metadata={'source': './PDFS/Concepts.pdf', 'page': 259}),\n",
       " Document(page_content='Pod\\'s DNS Config\\nFEATURE STATE:  Kubernetes v1.14 [stable]\\nPod\\'s DNS Config allows users more control on the DNS settings for a Pod.\\nThe dnsConfig  field is optional and it can work with any dnsPolicy  settings. However, when a\\nPod\\'s dnsPolicy  is set to \" None \", the dnsConfig  field has to be specified.\\nBelow are the properties a user can specify in the dnsConfig  field:\\nnameservers : a list of IP addresses that will be used as DNS servers for the Pod. There can\\nbe at most 3 IP addresses specified. When the Pod\\'s dnsPolicy  is set to \" None \", the list\\nmust contain at least one IP address, otherwise this property is optional. The servers\\nlisted will be combined to the base nameservers generated from the specified DNS policy\\nwith duplicate addresses removed.\\nsearches : a list of DNS search domains for hostname lookup in the Pod. This property is\\noptional. When specified, the provided list will be merged into the base search domain\\nnames generated from the chosen DNS policy. Duplicate domain names are removed.\\nKubernetes allows up to 32 search domains.\\noptions : an optional list of objects where each object may have a name  property\\n(required) and a value  property (optional). The contents in this property will be merged to\\nthe options generated from the specified DNS policy. Duplicate entries are removed.\\nThe following is an example Pod with custom DNS settings:\\nservice/networking/custom-dns.yaml  \\napiVersion : v1\\nkind: Pod\\nmetadata :\\n  namespace : default\\n  name : dns-example\\nspec:\\n  containers :\\n    - name : test\\n      image : nginx\\n  dnsPolicy : \"None\"\\n  dnsConfig :\\n    nameservers :\\n      - 192.0.2.1  # this is an example\\n    searches :\\n      - ns1.svc.cluster-domain.example\\n      - my.dns.search.suffix\\n    options :\\n      - name : ndots\\n        value : \"2\"\\n      - name : edns0\\nWhen the Pod above is created, the container test gets the following contents in its /etc/\\nresolv.conf  file:• \\n• \\n•', metadata={'source': './PDFS/Concepts.pdf', 'page': 260}),\n",
       " Document(page_content=\"nameserver 192.0.2.1\\nsearch ns1.svc.cluster-domain.example my.dns.search.suffix\\noptions ndots:2 edns0\\nFor IPv6 setup, search path and name server should be set up like this:\\nkubectl exec -it dns-example -- cat /etc/resolv.conf\\nThe output is similar to this:\\nnameserver 2001:db8:30::a\\nsearch default.svc.cluster-domain.example svc.cluster-domain.example cluster-domain.example\\noptions ndots:5\\nDNS search domain list limits\\nFEATURE STATE:  Kubernetes 1.28 [stable]\\nKubernetes itself does not limit the DNS Config until the length of the search domain list\\nexceeds 32 or the total length of all search domains exceeds 2048. This limit applies to the\\nnode's resolver configuration file, the Pod's DNS Config, and the merged DNS Config\\nrespectively.\\nNote:\\nSome container runtimes of earlier versions may have their own restrictions on the number of\\nDNS search domains. Depending on the container runtime environment, the pods with a large\\nnumber of DNS search domains may get stuck in the pending state.\\nIt is known that containerd v1.5.5 or earlier and CRI-O v1.21 or earlier have this problem.\\nDNS resolution on Windows nodes\\nClusterFirstWithHostNet is not supported for Pods that run on Windows nodes.\\nWindows treats all names with a . as a FQDN and skips FQDN resolution.\\nOn Windows, there are multiple DNS resolvers that can be used. As these come with\\nslightly different behaviors, using the Resolve-DNSName  powershell cmdlet for name\\nquery resolutions is recommended.\\nOn Linux, you have a DNS suffix list, which is used after resolution of a name as fully\\nqualified has failed. On Windows, you can only have 1 DNS suffix, which is the DNS\\nsuffix associated with that Pod's namespace (example: mydns.svc.cluster.local ). Windows\\ncan resolve FQDNs, Services, or network name which can be resolved with this single\\nsuffix. For example, a Pod spawned in the default  namespace, will have the DNS suffix \\ndefault.svc.cluster.local . Inside a Windows Pod, you can resolve both \\nkubernetes.default.svc.cluster.local  and kubernetes , but not the partially qualified names\\n(kubernetes.default  or kubernetes.default.svc ).\\nWhat's next\\nFor guidance on administering DNS configurations, check Configure DNS Service• \\n• \\n•\", metadata={'source': './PDFS/Concepts.pdf', 'page': 261}),\n",
       " Document(page_content='IPv4/IPv6 dual-stack\\nKubernetes lets you configure single-stack IPv4 networking, single-stack IPv6 networking, or\\ndual stack networking with both network families active. This page explains how.\\nFEATURE STATE:  Kubernetes v1.23 [stable]\\nIPv4/IPv6 dual-stack networking enables the allocation of both IPv4 and IPv6 addresses to Pods\\nand Services .\\nIPv4/IPv6 dual-stack networking is enabled by default for your Kubernetes cluster starting in\\n1.21, allowing the simultaneous assignment of both IPv4 and IPv6 addresses.\\nSupported Features\\nIPv4/IPv6 dual-stack on your Kubernetes cluster provides the following features:\\nDual-stack Pod networking (a single IPv4 and IPv6 address assignment per Pod)\\nIPv4 and IPv6 enabled Services\\nPod off-cluster egress routing (eg. the Internet) via both IPv4 and IPv6 interfaces\\nPrerequisites\\nThe following prerequisites are needed in order to utilize IPv4/IPv6 dual-stack Kubernetes\\nclusters:\\nKubernetes 1.20 or later\\nFor information about using dual-stack services with earlier Kubernetes versions, refer to\\nthe documentation for that version of Kubernetes.\\nProvider support for dual-stack networking (Cloud provider or otherwise must be able to\\nprovide Kubernetes nodes with routable IPv4/IPv6 network interfaces)\\nA network plugin  that supports dual-stack networking.\\nConfigure IPv4/IPv6 dual-stack\\nTo configure IPv4/IPv6 dual-stack, set dual-stack cluster network assignments:\\nkube-apiserver:\\n--service-cluster-ip-range=<IPv4 CIDR>,<IPv6 CIDR>\\nkube-controller-manager:\\n--cluster-cidr=<IPv4 CIDR>,<IPv6 CIDR>\\n--service-cluster-ip-range=<IPv4 CIDR>,<IPv6 CIDR>\\n--node-cidr-mask-size-ipv4|--node-cidr-mask-size-ipv6  defaults to /24 for IPv4 and /\\n64 for IPv6\\nkube-proxy:\\n--cluster-cidr=<IPv4 CIDR>,<IPv6 CIDR>\\nkubelet:\\nwhen there is no --cloud-provider  the administrator can pass a comma-separated\\npair of IP addresses via --node-ip  to manually configure dual-stack .status.addresses• \\n• \\n• \\n• \\n• \\n• \\n• \\n◦ \\n• \\n◦ \\n◦ \\n◦ \\n• \\n◦ \\n• \\n◦', metadata={'source': './PDFS/Concepts.pdf', 'page': 262}),\n",
       " Document(page_content='for that Node. If a Pod runs on that node in HostNetwork mode, the Pod reports\\nthese IP addresses in its .status.podIPs  field. All podIPs  in a node match the IP\\nfamily preference defined by the .status.addresses  field for that Node.\\nNote:\\nAn example of an IPv4 CIDR: 10.244.0.0/16  (though you would supply your own address range)\\nAn example of an IPv6 CIDR: fdXY:IJKL:MNOP:15::/64  (this shows the format but is not a valid\\naddress - see RFC 4193 )\\nFEATURE STATE:  Kubernetes v1.27 [alpha]\\nWhen using an external cloud provider, you can pass a dual-stack --node-ip  value to kubelet if\\nyou enable the CloudDualStackNodeIPs  feature gate in both kubelet and the external cloud\\nprovider. This is only supported for cloud providers that support dual stack clusters.\\nServices\\nYou can create Services  which can use IPv4, IPv6, or both.\\nThe address family of a Service defaults to the address family of the first service cluster IP range\\n(configured via the --service-cluster-ip-range  flag to the kube-apiserver).\\nWhen you define a Service you can optionally configure it as dual stack. To specify the\\nbehavior you want, you set the .spec.ipFamilyPolicy  field to one of the following values:\\nSingleStack : Single-stack service. The control plane allocates a cluster IP for the Service,\\nusing the first configured service cluster IP range.\\nPreferDualStack :\\nAllocates IPv4 and IPv6 cluster IPs for the Service.\\nRequireDualStack : Allocates Service .spec.ClusterIPs  from both IPv4 and IPv6 address\\nranges.\\nSelects the .spec.ClusterIP  from the list of .spec.ClusterIPs  based on the address\\nfamily of the first element in the .spec.ipFamilies  array.\\nIf you would like to define which IP family to use for single stack or define the order of IP\\nfamilies for dual-stack, you can choose the address families by setting an optional field,\\n.spec.ipFamilies , on the Service.\\nNote:  The .spec.ipFamilies  field is conditionally mutable: you can add or remove a secondary IP\\naddress family, but you cannot change the primary IP address family of an existing Service.\\nYou can set .spec.ipFamilies  to any of the following array values:\\n[\"IPv4\"]\\n[\"IPv6\"]\\n[\"IPv4\",\"IPv6\"]  (dual stack)\\n[\"IPv6\",\"IPv4\"]  (dual stack)\\nThe first family you list is used for the legacy .spec.ClusterIP  field.• \\n• \\n◦ \\n• \\n◦ \\n• \\n• \\n• \\n•', metadata={'source': './PDFS/Concepts.pdf', 'page': 263}),\n",
       " Document(page_content='Dual-stack Service configuration scenarios\\nThese examples demonstrate the behavior of various dual-stack Service configuration scenarios.\\nDual-stack options on new Services\\nThis Service specification does not explicitly define .spec.ipFamilyPolicy . When you\\ncreate this Service, Kubernetes assigns a cluster IP for the Service from the first\\nconfigured service-cluster-ip-range  and sets the .spec.ipFamilyPolicy  to SingleStack .\\n(Services without selectors  and headless Services  with selectors will behave in this same\\nway.)\\nservice/networking/dual-stack-default-svc.yaml  \\napiVersion : v1\\nkind: Service\\nmetadata :\\n  name : my-service\\n  labels :\\n    app.kubernetes.io/name : MyApp\\nspec:\\n  selector :\\n    app.kubernetes.io/name : MyApp\\n  ports :\\n    - protocol : TCP\\n      port: 80\\nThis Service specification explicitly defines PreferDualStack  in .spec.ipFamilyPolicy .\\nWhen you create this Service on a dual-stack cluster, Kubernetes assigns both IPv4 and\\nIPv6 addresses for the service. The control plane updates the .spec  for the Service to\\nrecord the IP address assignments. The field .spec.ClusterIPs  is the primary field, and\\ncontains both assigned IP addresses; .spec.ClusterIP  is a secondary field with its value\\ncalculated from .spec.ClusterIPs .\\nFor the .spec.ClusterIP  field, the control plane records the IP address that is from\\nthe same address family as the first service cluster IP range.\\nOn a single-stack cluster, the .spec.ClusterIPs  and .spec.ClusterIP  fields both only\\nlist one address.\\nOn a cluster with dual-stack enabled, specifying RequireDualStack\\nin .spec.ipFamilyPolicy  behaves the same as PreferDualStack .\\nservice/networking/dual-stack-preferred-svc.yaml  \\napiVersion : v1\\nkind: Service\\nmetadata :\\n  name : my-service\\n  labels :\\n    app.kubernetes.io/name : MyApp\\nspec:\\n  ipFamilyPolicy : PreferDualStack\\n  selector :\\n    app.kubernetes.io/name : MyApp\\n  ports :1. \\n2. \\n◦ \\n◦ \\n◦', metadata={'source': './PDFS/Concepts.pdf', 'page': 264}),\n",
       " Document(page_content='- protocol : TCP\\n      port: 80\\nThis Service specification explicitly defines IPv6 and IPv4 in .spec.ipFamilies  as well as\\ndefining PreferDualStack  in .spec.ipFamilyPolicy . When Kubernetes assigns an IPv6 and\\nIPv4 address in .spec.ClusterIPs , .spec.ClusterIP  is set to the IPv6 address because that is\\nthe first element in the .spec.ClusterIPs  array, overriding the default.\\nservice/networking/dual-stack-preferred-ipfamilies-svc.yaml  \\napiVersion : v1\\nkind: Service\\nmetadata :\\n  name : my-service\\n  labels :\\n    app.kubernetes.io/name : MyApp\\nspec:\\n  ipFamilyPolicy : PreferDualStack\\n  ipFamilies :\\n  - IPv6\\n  - IPv4\\n  selector :\\n    app.kubernetes.io/name : MyApp\\n  ports :\\n    - protocol : TCP\\n      port: 80\\nDual-stack defaults on existing Services\\nThese examples demonstrate the default behavior when dual-stack is newly enabled on a cluster\\nwhere Services already exist. (Upgrading an existing cluster to 1.21 or beyond will enable dual-\\nstack.)\\nWhen dual-stack is enabled on a cluster, existing Services (whether IPv4 or IPv6) are\\nconfigured by the control plane to set .spec.ipFamilyPolicy  to SingleStack  and\\nset .spec.ipFamilies  to the address family of the existing Service. The existing Service\\ncluster IP will be stored in .spec.ClusterIPs .\\nservice/networking/dual-stack-default-svc.yaml  \\napiVersion : v1\\nkind: Service\\nmetadata :\\n  name : my-service\\n  labels :\\n    app.kubernetes.io/name : MyApp\\nspec:\\n  selector :\\n    app.kubernetes.io/name : MyApp\\n  ports :\\n    - protocol : TCP\\n      port: 803. \\n1.', metadata={'source': './PDFS/Concepts.pdf', 'page': 265}),\n",
       " Document(page_content='You can validate this behavior by using kubectl to inspect an existing service.\\nkubectl get svc my-service -o yaml\\napiVersion : v1\\nkind: Service\\nmetadata :\\n  labels :\\n    app.kubernetes.io/name : MyApp\\n  name : my-service\\nspec:\\n  clusterIP : 10.0.197.123\\n  clusterIPs :\\n  - 10.0.197.123\\n  ipFamilies :\\n  - IPv4\\n  ipFamilyPolicy : SingleStack\\n  ports :\\n  - port: 80\\n    protocol : TCP\\n    targetPort : 80\\n  selector :\\n    app.kubernetes.io/name : MyApp\\n  type: ClusterIP\\nstatus :\\n  loadBalancer : {}\\nWhen dual-stack is enabled on a cluster, existing headless Services  with selectors are\\nconfigured by the control plane to set .spec.ipFamilyPolicy  to SingleStack  and\\nset .spec.ipFamilies  to the address family of the first service cluster IP range (configured\\nvia the --service-cluster-ip-range  flag to the kube-apiserver) even though .spec.ClusterIP\\nis set to None .\\nservice/networking/dual-stack-default-svc.yaml  \\napiVersion : v1\\nkind: Service\\nmetadata :\\n  name : my-service\\n  labels :\\n    app.kubernetes.io/name : MyApp\\nspec:\\n  selector :\\n    app.kubernetes.io/name : MyApp\\n  ports :\\n    - protocol : TCP\\n      port: 80\\nYou can validate this behavior by using kubectl to inspect an existing headless service\\nwith selectors.\\nkubectl get svc my-service -o yaml2.', metadata={'source': './PDFS/Concepts.pdf', 'page': 266}),\n",
       " Document(page_content='apiVersion : v1\\nkind: Service\\nmetadata :\\n  labels :\\n    app.kubernetes.io/name : MyApp\\n  name : my-service\\nspec:\\n  clusterIP : None\\n  clusterIPs :\\n  - None\\n  ipFamilies :\\n  - IPv4\\n  ipFamilyPolicy : SingleStack\\n  ports :\\n  - port: 80\\n    protocol : TCP\\n    targetPort : 80\\n  selector :\\n    app.kubernetes.io/name : MyApp\\nSwitching Services between single-stack and dual-stack\\nServices can be changed from single-stack to dual-stack and from dual-stack to single-stack.\\nTo change a Service from single-stack to dual-stack, change .spec.ipFamilyPolicy  from \\nSingleStack  to PreferDualStack  or RequireDualStack  as desired. When you change this\\nService from single-stack to dual-stack, Kubernetes assigns the missing address family so\\nthat the Service now has IPv4 and IPv6 addresses.\\nEdit the Service specification updating the .spec.ipFamilyPolicy  from SingleStack  to \\nPreferDualStack .\\nBefore:\\nspec:\\n  ipFamilyPolicy : SingleStack\\nAfter:\\nspec:\\n  ipFamilyPolicy : PreferDualStack\\nTo change a Service from dual-stack to single-stack, change .spec.ipFamilyPolicy  from \\nPreferDualStack  or RequireDualStack  to SingleStack . When you change this Service from\\ndual-stack to single-stack, Kubernetes retains only the first element in the\\n.spec.ClusterIPs  array, and sets .spec.ClusterIP  to that IP address and sets .spec.ipFamilies\\nto the address family of .spec.ClusterIPs .\\nHeadless Services without selector\\nFor Headless Services without selectors  and without .spec.ipFamilyPolicy  explicitly set,\\nthe .spec.ipFamilyPolicy  field defaults to RequireDualStack .1. \\n2.', metadata={'source': './PDFS/Concepts.pdf', 'page': 267}),\n",
       " Document(page_content='Service type LoadBalancer\\nTo provision a dual-stack load balancer for your Service:\\nSet the .spec.type  field to LoadBalancer\\nSet .spec.ipFamilyPolicy  field to PreferDualStack  or RequireDualStack\\nNote:  To use a dual-stack LoadBalancer  type Service, your cloud provider must support IPv4\\nand IPv6 load balancers.\\nEgress traffic\\nIf you want to enable egress traffic in order to reach off-cluster destinations (eg. the public\\nInternet) from a Pod that uses non-publicly routable IPv6 addresses, you need to enable the Pod\\nto use a publicly routed IPv6 address via a mechanism such as transparent proxying or IP\\nmasquerading. The ip-masq-agent  project supports IP masquerading on dual-stack clusters.\\nNote:  Ensure your CNI provider supports IPv6.\\nWindows support\\nKubernetes on Windows does not support single-stack \"IPv6-only\" networking. However, dual-\\nstack IPv4/IPv6 networking for pods and nodes with single-family services is supported.\\nYou can use IPv4/IPv6 dual-stack networking with l2bridge  networks.\\nNote:  Overlay (VXLAN) networks on Windows do not  support dual-stack networking.\\nYou can read more about the different network modes for Windows within the Networking on\\nWindows  topic.\\nWhat\\'s next\\nValidate IPv4/IPv6 dual-stack  networking\\nEnable dual-stack networking using kubeadm\\nTopology Aware Routing\\nTopology Aware Routing  provides a mechanism to help keep network traffic within the zone\\nwhere it originated. Preferring same-zone traffic between Pods in your cluster can help with\\nreliability, performance (network latency and throughput), or cost.\\nFEATURE STATE:  Kubernetes v1.23 [beta]\\nNote:  Prior to Kubernetes 1.27, this feature was known as Topology Aware Hints .\\nTopology Aware Routing  adjusts routing behavior to prefer keeping traffic in the zone it\\noriginated from. In some cases this can help reduce costs or improve network performance.• \\n• \\n• \\n•', metadata={'source': './PDFS/Concepts.pdf', 'page': 268}),\n",
       " Document(page_content='Motivation\\nKubernetes clusters are increasingly deployed in multi-zone environments. Topology Aware\\nRouting  provides a mechanism to help keep traffic within the zone it originated from. When\\ncalculating the endpoints for a Service , the EndpointSlice controller considers the topology\\n(region and zone) of each endpoint and populates the hints field to allocate it to a zone. Cluster\\ncomponents such as kube-proxy  can then consume those hints, and use them to influence how\\nthe traffic is routed (favoring topologically closer endpoints).\\nEnabling Topology Aware Routing\\nNote:  Prior to Kubernetes 1.27, this behavior was controlled using the service.kubernetes.io/\\ntopology-aware-hints  annotation.\\nYou can enable Topology Aware Routing for a Service by setting the service.kubernetes.io/\\ntopology-mode  annotation to Auto . When there are enough endpoints available in each zone,\\nTopology Hints will be populated on EndpointSlices to allocate individual endpoints to specific\\nzones, resulting in traffic being routed closer to where it originated from.\\nWhen it works best\\nThis feature works best when:\\n1. Incoming traffic is evenly distributed\\nIf a large proportion of traffic is originating from a single zone, that traffic could overload the\\nsubset of endpoints that have been allocated to that zone. This feature is not recommended\\nwhen incoming traffic is expected to originate from a single zone.\\n2. The Service has 3 or more endpoints per zone\\nIn a three zone cluster, this means 9 or more endpoints. If there are fewer than 3 endpoints per\\nzone, there is a high (≈50%) probability that the EndpointSlice controller will not be able to\\nallocate endpoints evenly and instead will fall back to the default cluster-wide routing\\napproach.\\nHow It Works\\nThe \"Auto\" heuristic attempts to proportionally allocate a number of endpoints to each zone.\\nNote that this heuristic works best for Services that have a significant number of endpoints.\\nEndpointSlice controller\\nThe EndpointSlice controller is responsible for setting hints on EndpointSlices when this\\nheuristic is enabled. The controller allocates a proportional amount of endpoints to each zone.\\nThis proportion is based on the allocatable  CPU cores for nodes running in that zone. For\\nexample, if one zone had 2 CPU cores and another zone only had 1 CPU core, the controller\\nwould allocate twice as many endpoints to the zone with 2 CPU cores.', metadata={'source': './PDFS/Concepts.pdf', 'page': 269}),\n",
       " Document(page_content='The following example shows what an EndpointSlice looks like when hints have been\\npopulated:\\napiVersion : discovery.k8s.io/v1\\nkind: EndpointSlice\\nmetadata :\\n  name : example-hints\\n  labels :\\n    kubernetes.io/service-name : example-svc\\naddressType : IPv4\\nports :\\n  - name : http\\n    protocol : TCP\\n    port: 80\\nendpoints :\\n  - addresses :\\n      - \"10.1.2.3\"\\n    conditions :\\n      ready : true\\n    hostname : pod-1\\n    zone : zone-a\\n    hints :\\n      forZones :\\n        - name : \"zone-a\"\\nkube-proxy\\nThe kube-proxy component filters the endpoints it routes to based on the hints set by the\\nEndpointSlice controller. In most cases, this means that the kube-proxy is able to route traffic to\\nendpoints in the same zone. Sometimes the controller allocates endpoints from a different zone\\nto ensure more even distribution of endpoints between zones. This would result in some traffic\\nbeing routed to other zones.\\nSafeguards\\nThe Kubernetes control plane and the kube-proxy on each node apply some safeguard rules\\nbefore using Topology Aware Hints. If these don\\'t check out, the kube-proxy selects endpoints\\nfrom anywhere in your cluster, regardless of the zone.\\nInsufficient number of endpoints:  If there are less endpoints than zones in a cluster,\\nthe controller will not assign any hints.\\nImpossible to achieve balanced allocation:  In some cases, it will be impossible to\\nachieve a balanced allocation of endpoints among zones. For example, if zone-a is twice\\nas large as zone-b, but there are only 2 endpoints, an endpoint allocated to zone-a may\\nreceive twice as much traffic as zone-b. The controller does not assign hints if it can\\'t get\\nthis \"expected overload\" value below an acceptable threshold for each zone. Importantly\\nthis is not based on real-time feedback. It is still possible for individual endpoints to\\nbecome overloaded.\\nOne or more Nodes has insufficient information:  If any node does not have a \\ntopology.kubernetes.io/zone  label or is not reporting a value for allocatable CPU, the1. \\n2. \\n3.', metadata={'source': './PDFS/Concepts.pdf', 'page': 270}),\n",
       " Document(page_content=\"control plane does not set any topology-aware endpoint hints and so kube-proxy does\\nnot filter endpoints by zone.\\nOne or more endpoints does not have a zone hint:  When this happens, the kube-\\nproxy assumes that a transition from or to Topology Aware Hints is underway. Filtering\\nendpoints for a Service in this state would be dangerous so the kube-proxy falls back to\\nusing all endpoints.\\nA zone is not represented in hints:  If the kube-proxy is unable to find at least one\\nendpoint with a hint targeting the zone it is running in, it falls back to using endpoints\\nfrom all zones. This is most likely to happen as you add a new zone into your existing\\ncluster.\\nConstraints\\nTopology Aware Hints are not used when internalTrafficPolicy  is set to Local  on a\\nService. It is possible to use both features in the same cluster on different Services, just\\nnot on the same Service.\\nThis approach will not work well for Services that have a large proportion of traffic\\noriginating from a subset of zones. Instead this assumes that incoming traffic will be\\nroughly proportional to the capacity of the Nodes in each zone.\\nThe EndpointSlice controller ignores unready nodes as it calculates the proportions of\\neach zone. This could have unintended consequences if a large portion of nodes are\\nunready.\\nThe EndpointSlice controller ignores nodes with the node-role.kubernetes.io/control-\\nplane  or node-role.kubernetes.io/master  label set. This could be problematic if workloads\\nare also running on those nodes.\\nThe EndpointSlice controller does not take into account tolerations  when deploying or\\ncalculating the proportions of each zone. If the Pods backing a Service are limited to a\\nsubset of Nodes in the cluster, this will not be taken into account.\\nThis may not work well with autoscaling. For example, if a lot of traffic is originating\\nfrom a single zone, only the endpoints allocated to that zone will be handling that traffic.\\nThat could result in Horizontal Pod Autoscaler  either not picking up on this event, or\\nnewly added pods starting in a different zone.\\nCustom heuristics\\nKubernetes is deployed in many different ways, there is no single heuristic for allocating\\nendpoints to zones will work for every use case. A key goal of this feature is to enable custom\\nheuristics to be developed if the built in heuristic does not work for your use case. The first\\nsteps to enable custom heuristics were included in the 1.27 release. This is a limited\\nimplementation that may not yet cover some relevant and plausible situations.\\nWhat's next\\nFollow the Connecting Applications with Services  tutorial4. \\n5. \\n• \\n• \\n• \\n• \\n• \\n• \\n•\", metadata={'source': './PDFS/Concepts.pdf', 'page': 271}),\n",
       " Document(page_content=\"Networking on Windows\\nKubernetes supports running nodes on either Linux or Windows. You can mix both kinds of\\nnode within a single cluster. This page provides an overview to networking specific to the\\nWindows operating system.\\nContainer networking on Windows\\nNetworking for Windows containers is exposed through CNI plugins . Windows containers\\nfunction similarly to virtual machines in regards to networking. Each container has a virtual\\nnetwork adapter (vNIC) which is connected to a Hyper-V virtual switch (vSwitch). The Host\\nNetworking Service (HNS) and the Host Compute Service (HCS) work together to create\\ncontainers and attach container vNICs to networks. HCS is responsible for the management of\\ncontainers whereas HNS is responsible for the management of networking resources such as:\\nVirtual networks (including creation of vSwitches)\\nEndpoints / vNICs\\nNamespaces\\nPolicies including packet encapsulations, load-balancing rules, ACLs, and NAT rules.\\nThe Windows HNS and vSwitch implement namespacing and can create virtual NICs as needed\\nfor a pod or container. However, many configurations such as DNS, routes, and metrics are\\nstored in the Windows registry database rather than as files inside /etc, which is how Linux\\nstores those configurations. The Windows registry for the container is separate from that of the\\nhost, so concepts like mapping /etc/resolv.conf  from the host into a container don't have the\\nsame effect they would on Linux. These must be configured using Windows APIs run in the\\ncontext of that container. Therefore CNI implementations need to call the HNS instead of\\nrelying on file mappings to pass network details into the pod or container.\\nNetwork modes\\nWindows supports five different networking drivers/modes: L2bridge, L2tunnel, Overlay (Beta),\\nTransparent, and NAT. In a heterogeneous cluster with Windows and Linux worker nodes, you\\nneed to select a networking solution that is compatible on both Windows and Linux. The\\nfollowing table lists the out-of-tree plugins are supported on Windows, with recommendations\\non when to use each CNI:\\nNetwork\\nDriverDescriptionContainer\\nPacket\\nModificationsNetwork\\nPluginsNetwork Plugin\\nCharacteristics\\nL2bridgeContainers are\\nattached to an\\nexternal vSwitch.\\nContainers are\\nattached to the\\nunderlay network,\\nalthough the\\nphysical network\\ndoesn't need to\\nlearn the containerMAC is rewritten\\nto host MAC, IP\\nmay be rewritten\\nto host IP using\\nHNS\\nOutboundNAT\\npolicy.win-bridge , \\nAzure-CNI ,\\nFlannel host-\\ngateway uses\\nwin-bridgewin-bridge uses L2bridge\\nnetwork mode, connects\\ncontainers to the underlay\\nof hosts, offering best\\nperformance. Requires\\nuser-defined routes (UDR)\\nfor inter-node\\nconnectivity.• \\n• \\n• \\n•\", metadata={'source': './PDFS/Concepts.pdf', 'page': 272}),\n",
       " Document(page_content='Network\\nDriverDescriptionContainer\\nPacket\\nModificationsNetwork\\nPluginsNetwork Plugin\\nCharacteristics\\nMACs because they\\nare rewritten on\\ningress/egress.\\nL2TunnelThis is a special case\\nof l2bridge, but only\\nused on Azure. All\\npackets are sent to\\nthe virtualization\\nhost where SDN\\npolicy is applied.MAC rewritten, IP\\nvisible on the\\nunderlay networkAzure-CNIAzure-CNI allows\\nintegration of containers\\nwith Azure vNET, and\\nallows them to leverage\\nthe set of capabilities that \\nAzure Virtual Network\\nprovides . For example,\\nsecurely connect to Azure\\nservices or use Azure\\nNSGs. See azure-cni for\\nsome examples\\nOverlayContainers are\\ngiven a vNIC\\nconnected to an\\nexternal vSwitch.\\nEach overlay\\nnetwork gets its\\nown IP subnet,\\ndefined by a custom\\nIP prefix.The\\noverlay network\\ndriver uses VXLAN\\nencapsulation.Encapsulated\\nwith an outer\\nheader.win-overlay ,\\nFlannel\\nVXLAN (uses\\nwin-overlay)win-overlay should be\\nused when virtual\\ncontainer networks are\\ndesired to be isolated from\\nunderlay of hosts (e.g. for\\nsecurity reasons). Allows\\nfor IPs to be re-used for\\ndifferent overlay networks\\n(which have different\\nVNID tags) if you are\\nrestricted on IPs in your\\ndatacenter. This option\\nrequires KB4489899  on\\nWindows Server 2019.\\nTransparent\\n(special use\\ncase for ovn-\\nkubernetes )Requires an\\nexternal vSwitch.\\nContainers are\\nattached to an\\nexternal vSwitch\\nwhich enables intra-\\npod communication\\nvia logical networks\\n(logical switches\\nand routers).Packet is\\nencapsulated\\neither via \\nGENEVE  or STT\\ntunneling to reach\\npods which are\\nnot on the same\\nhost.\\nPackets are\\nforwarded or\\ndropped via the\\ntunnel metadata\\ninformation\\nsupplied by the\\novn network\\ncontroller.\\nNAT is done for\\nnorth-south\\ncommunication.ovn-\\nkubernetesDeploy via ansible .\\nDistributed ACLs can be\\napplied via Kubernetes\\npolicies. IPAM support.\\nLoad-balancing can be\\nachieved without kube-\\nproxy. NATing is done\\nwithout using iptables/\\nnetsh.\\nContainers are\\ngiven a vNICnatIncluded here for\\ncompleteness', metadata={'source': './PDFS/Concepts.pdf', 'page': 273}),\n",
       " Document(page_content='Network\\nDriverDescriptionContainer\\nPacket\\nModificationsNetwork\\nPluginsNetwork Plugin\\nCharacteristics\\nNAT ( not\\nused in\\nKubernetes )connected to an\\ninternal vSwitch.\\nDNS/DHCP is\\nprovided using an\\ninternal component\\ncalled WinNATMAC and IP is\\nrewritten to host\\nMAC/IP.\\nAs outlined above, the Flannel  CNI plugin  is also supported  on Windows via the VXLAN\\nnetwork backend  (Beta support  ; delegates to win-overlay) and host-gateway network\\nbackend  (stable support; delegates to win-bridge).\\nThis plugin supports delegating to one of the reference CNI plugins (win-overlay, win-bridge),\\nto work in conjunction with Flannel daemon on Windows (Flanneld) for automatic node subnet\\nlease assignment and HNS network creation. This plugin reads in its own configuration file\\n(cni.conf), and aggregates it with the environment variables from the FlannelD generated\\nsubnet.env file. It then delegates to one of the reference CNI plugins for network plumbing, and\\nsends the correct configuration containing the node-assigned subnet to the IPAM plugin (for\\nexample: host-local ).\\nFor Node, Pod, and Service objects, the following network flows are supported for TCP/UDP\\ntraffic:\\nPod → Pod (IP)\\nPod → Pod (Name)\\nPod → Service (Cluster IP)\\nPod → Service (PQDN, but only if there are no \".\")\\nPod → Service (FQDN)\\nPod → external (IP)\\nPod → external (DNS)\\nNode → Pod\\nPod → Node\\nIP address management (IPAM)\\nThe following IPAM options are supported on Windows:\\nhost-local\\nazure-vnet-ipam  (for azure-cni only)\\nWindows Server IPAM  (fallback option if no IPAM is set)\\nLoad balancing and Services\\nA Kubernetes Service  is an abstraction that defines a logical set of Pods and a means to access\\nthem over a network. In a cluster that includes Windows nodes, you can use the following\\ntypes of Service:\\nNodePort\\nClusterIP\\nLoadBalancer• \\n• \\n• \\n• \\n• \\n• \\n• \\n• \\n• \\n• \\n• \\n• \\n• \\n• \\n•', metadata={'source': './PDFS/Concepts.pdf', 'page': 274}),\n",
       " Document(page_content='ExternalName\\nWindows container networking differs in some important ways from Linux networking. The \\nMicrosoft documentation for Windows Container Networking  provides additional details and\\nbackground.\\nOn Windows, you can use the following settings to configure Services and load balancing\\nbehavior:\\nWindows Service Settings\\nFeature DescriptionMinimum\\nSupported\\nWindows\\nOS buildHow to enable\\nSession\\naffinityEnsures that connections\\nfrom a particular client are\\npassed to the same Pod\\neach time.Windows\\nServer 2022Set service.spec.sessionAffinity  to\\n\"ClientIP\"\\nDirect Server\\nReturn (DSR)Load balancing mode\\nwhere the IP address\\nfixups and the LBNAT\\noccurs at the container\\nvSwitch port directly;\\nservice traffic arrives with\\nthe source IP set as the\\noriginating pod IP.Windows\\nServer 2019Set the following flags in kube-proxy: \\n--feature-gates=\"WinDSR=true\" --\\nenable-dsr=true\\nPreserve-\\nDestinationSkips DNAT of service\\ntraffic, thereby preserving\\nthe virtual IP of the target\\nservice in packets reaching\\nthe backend Pod. Also\\ndisables node-node\\nforwarding.Windows\\nServer,\\nversion 1903Set \"preserve-destination\": \"true\"  in\\nservice annotations and enable DSR in\\nkube-proxy.\\nIPv4/IPv6\\ndual-stack\\nnetworkingNative IPv4-to-IPv4 in\\nparallel with IPv6-to-IPv6\\ncommunications to, from,\\nand within a clusterWindows\\nServer 2019See IPv4/IPv6 dual-stack\\nClient IP\\npreservationEnsures that source IP of\\nincoming ingress traffic\\ngets preserved. Also\\ndisables node-node\\nforwarding.Windows\\nServer 2019Set service.spec.externalTrafficPolicy  to\\n\"Local\" and enable DSR in kube-proxy\\nWarning:\\nThere are known issue with NodePort Services on overlay networking, if the destination node is\\nrunning Windows Server 2022. To avoid the issue entirely, you can configure the service with \\nexternalTrafficPolicy: Local .\\nThere are known issues with Pod to Pod connectivity on l2bridge network on Windows Server\\n2022 with KB5005619 or higher installed. To workaround the issue and restore Pod to Pod\\nconnectivity, you can disable the WinDSR feature in kube-proxy.•', metadata={'source': './PDFS/Concepts.pdf', 'page': 275}),\n",
       " Document(page_content=\"These issues require OS fixes. Please follow https://github.com/microsoft/Windows-Containers/\\nissues/204  for updates.\\nLimitations\\nThe following networking functionality is not supported on Windows nodes:\\nHost networking mode\\nLocal NodePort access from the node itself (works for other nodes or external clients)\\nMore than 64 backend pods (or unique destination addresses) for a single Service\\nIPv6 communication between Windows pods connected to overlay networks\\nLocal Traffic Policy in non-DSR mode\\nOutbound communication using the ICMP protocol via the win-overlay , win-bridge , or\\nusing the Azure-CNI plugin.\\nSpecifically, the Windows data plane ( VFP) doesn't support ICMP packet transpositions,\\nand this means:\\nICMP packets directed to destinations within the same network (such as pod to pod\\ncommunication via ping) work as expected;\\nTCP/UDP packets work as expected;\\nICMP packets directed to pass through a remote network (e.g. pod to external\\ninternet communication via ping) cannot be transposed and thus will not be routed\\nback to their source;\\nSince TCP/UDP packets can still be transposed, you can substitute ping \\n<destination>  with curl <destination>  when debugging connectivity with the\\noutside world.\\nOther limitations:\\nWindows reference network plugins win-bridge and win-overlay do not implement CNI\\nspec v0.4.0, due to a missing CHECK  implementation.\\nThe Flannel VXLAN CNI plugin has the following limitations on Windows:\\nNode-pod connectivity is only possible for local pods with Flannel v0.12.0 (or\\nhigher).\\nFlannel is restricted to using VNI 4096 and UDP port 4789. See the official Flannel\\nVXLAN  backend docs for more details on these parameters.\\nService ClusterIP allocation\\nIn Kubernetes, Services  are an abstract way to expose an application running on a set of Pods.\\nServices can have a cluster-scoped virtual IP address (using a Service of type: ClusterIP ). Clients\\ncan connect using that virtual IP address, and Kubernetes then load-balances traffic to that\\nService across the different backing Pods.\\nHow Service ClusterIPs are allocated?\\nWhen Kubernetes needs to assign a virtual IP address for a Service, that assignment happens\\none of two ways:\\ndynamically\\nthe cluster's control plane automatically picks a free IP address from within the\\nconfigured IP range for type: ClusterIP  Services.• \\n• \\n• \\n• \\n• \\n• \\n◦ \\n◦ \\n◦ \\n◦ \\n• \\n• \\n◦ \\n◦\", metadata={'source': './PDFS/Concepts.pdf', 'page': 276}),\n",
       " Document(page_content='statically\\nyou specify an IP address of your choice, from within the configured IP range for\\nServices.\\nAcross your whole cluster, every Service ClusterIP  must be unique. Trying to create a Service\\nwith a specific ClusterIP  that has already been allocated will return an error.\\nWhy do you need to reserve Service Cluster IPs?\\nSometimes you may want to have Services running in well-known IP addresses, so other\\ncomponents and users in the cluster can use them.\\nThe best example is the DNS Service for the cluster. As a soft convention, some Kubernetes\\ninstallers assign the 10th IP address from the Service IP range to the DNS service. Assuming\\nyou configured your cluster with Service IP range 10.96.0.0/16 and you want your DNS Service\\nIP to be 10.96.0.10, you\\'d have to create a Service like this:\\napiVersion : v1\\nkind: Service\\nmetadata :\\n  labels :\\n    k8s-app : kube-dns\\n    kubernetes.io/cluster-service : \"true\"\\n    kubernetes.io/name : CoreDNS\\n  name : kube-dns\\n  namespace : kube-system\\nspec:\\n  clusterIP : 10.96.0.10\\n  ports :\\n  - name : dns\\n    port: 53\\n    protocol : UDP\\n    targetPort : 53\\n  - name : dns-tcp\\n    port: 53\\n    protocol : TCP\\n    targetPort : 53\\n  selector :\\n    k8s-app : kube-dns\\n  type: ClusterIP\\nbut as it was explained before, the IP address 10.96.0.10 has not been reserved; if other Services\\nare created before or in parallel with dynamic allocation, there is a chance they can allocate this\\nIP, hence, you will not be able to create the DNS Service because it will fail with a conflict error.\\nHow can you avoid Service ClusterIP conflicts?\\nThe allocation strategy implemented in Kubernetes to allocate ClusterIPs to Services reduces\\nthe risk of collision.\\nThe ClusterIP  range is divided, based on the formula min(max(16, cidrSize / 16), 256) , described\\nas never less than 16 or more than 256 with a graduated step between them .', metadata={'source': './PDFS/Concepts.pdf', 'page': 277}),\n",
       " Document(page_content='Dynamic IP assignment uses the upper band by default, once this has been exhausted it will use\\nthe lower range. This will allow users to use static allocations on the lower band with a low risk\\nof collision.\\nExamples\\nExample 1\\nThis example uses the IP address range: 10.96.0.0/24 (CIDR notation) for the IP addresses of\\nServices.\\nRange Size: 28 - 2 = 254\\nBand Offset: min(max(16, 256/16), 256)  = min(16, 256)  = 16\\nStatic band start: 10.96.0.1\\nStatic band end: 10.96.0.16\\nRange end: 10.96.0.254\\npie showData title 10.96.0.0/24 \"Static\" : 16 \"Dynamic\" : 238\\nJavaScript must be enabled  to view this content\\nExample 2\\nThis example uses the IP address range: 10.96.0.0/20 (CIDR notation) for the IP addresses of\\nServices.\\nRange Size: 212 - 2 = 4094\\nBand Offset: min(max(16, 4096/16), 256)  = min(256, 256)  = 256\\nStatic band start: 10.96.0.1\\nStatic band end: 10.96.1.0\\nRange end: 10.96.15.254\\npie showData title 10.96.0.0/20 \"Static\" : 256 \"Dynamic\" : 3838\\nJavaScript must be enabled  to view this content\\nExample 3\\nThis example uses the IP address range: 10.96.0.0/16 (CIDR notation) for the IP addresses of\\nServices.\\nRange Size: 216 - 2 = 65534\\nBand Offset: min(max(16, 65536/16), 256)  = min(4096, 256)  = 256\\nStatic band start: 10.96.0.1\\nStatic band ends: 10.96.1.0\\nRange end: 10.96.255.254\\npie showData title 10.96.0.0/16 \"Static\" : 256 \"Dynamic\" : 65278\\nJavaScript must be enabled  to view this content', metadata={'source': './PDFS/Concepts.pdf', 'page': 278}),\n",
       " Document(page_content='What\\'s next\\nRead about Service External Traffic Policy\\nRead about Connecting Applications with Services\\nRead about Services\\nService Internal Traffic Policy\\nIf two Pods in your cluster want to communicate, and both Pods are actually running on the\\nsame node, use Service Internal Traffic Policy  to keep network traffic within that node. Avoiding\\na round trip via the cluster network can help with reliability, performance (network latency and\\nthroughput), or cost.\\nFEATURE STATE:  Kubernetes v1.26 [stable]\\nService Internal Traffic Policy  enables internal traffic restrictions to only route internal traffic to\\nendpoints within the node the traffic originated from. The \"internal\" traffic here refers to traffic\\noriginated from Pods in the current cluster. This can help to reduce costs and improve\\nperformance.\\nUsing Service Internal Traffic Policy\\nYou can enable the internal-only traffic policy for a Service , by setting\\nits .spec.internalTrafficPolicy  to Local . This tells kube-proxy to only use node local endpoints\\nfor cluster internal traffic.\\nNote:  For pods on nodes with no endpoints for a given Service, the Service behaves as if it has\\nzero endpoints (for Pods on this node) even if the service does have endpoints on other nodes.\\nThe following example shows what a Service looks like when you set .spec.internalTrafficPolicy\\nto Local :\\napiVersion : v1\\nkind: Service\\nmetadata :\\n  name : my-service\\nspec:\\n  selector :\\n    app.kubernetes.io/name : MyApp\\n  ports :\\n    - protocol : TCP\\n      port: 80\\n      targetPort : 9376\\n  internalTrafficPolicy : Local\\nHow it works\\nThe kube-proxy filters the endpoints it routes to based on the spec.internalTrafficPolicy  setting.\\nWhen it\\'s set to Local , only node local endpoints are considered. When it\\'s Cluster  (the default),\\nor is not set, Kubernetes considers all endpoints.• \\n• \\n•', metadata={'source': './PDFS/Concepts.pdf', 'page': 279}),\n",
       " Document(page_content=\"What's next\\nRead about Topology Aware Routing\\nRead about Service External Traffic Policy\\nFollow the Connecting Applications with Services  tutorial\\nStorage\\nWays to provide both long-term and temporary storage to Pods in your cluster.\\nVolumes\\nPersistent Volumes\\nProjected Volumes\\nEphemeral Volumes\\nStorage Classes\\nDynamic Volume Provisioning\\nVolume Snapshots\\nVolume Snapshot Classes\\nCSI Volume Cloning\\nStorage Capacity\\nNode-specific Volume Limits\\nVolume Health Monitoring\\nWindows Storage\\nVolumes\\nOn-disk files in a container are ephemeral, which presents some problems for non-trivial\\napplications when running in containers. One problem occurs when a container crashes or is\\nstopped. Container state is not saved so all of the files that were created or modified during the\\nlifetime of the container are lost. During a crash, kubelet restarts the container with a clean\\nstate. Another problem occurs when multiple containers are running in a Pod and need to share\\nfiles. It can be challenging to setup and access a shared filesystem across all of the containers.\\nThe Kubernetes volume  abstraction solves both of these problems. Familiarity with Pods  is\\nsuggested.• \\n• \\n•\", metadata={'source': './PDFS/Concepts.pdf', 'page': 280}),\n",
       " Document(page_content='Background\\nKubernetes supports many types of volumes. A Pod can use any number of volume types\\nsimultaneously. Ephemeral volume  types have a lifetime of a pod, but persistent volumes  exist\\nbeyond the lifetime of a pod. When a pod ceases to exist, Kubernetes destroys ephemeral\\nvolumes; however, Kubernetes does not destroy persistent volumes. For any kind of volume in a\\ngiven pod, data is preserved across container restarts.\\nAt its core, a volume is a directory, possibly with some data in it, which is accessible to the\\ncontainers in a pod. How that directory comes to be, the medium that backs it, and the contents\\nof it are determined by the particular volume type used.\\nTo use a volume, specify the volumes to provide for the Pod in .spec.volumes  and declare where\\nto mount those volumes into containers in .spec.containers[*].volumeMounts . A process in a\\ncontainer sees a filesystem view composed from the initial contents of the container image , plus\\nvolumes (if defined) mounted inside the container. The process sees a root filesystem that\\ninitially matches the contents of the container image. Any writes to within that filesystem\\nhierarchy, if allowed, affect what that process views when it performs a subsequent filesystem\\naccess. Volumes mount at the specified paths  within the image. For each container defined\\nwithin a Pod, you must independently specify where to mount each volume that the container\\nuses.\\nVolumes cannot mount within other volumes (but see Using subPath  for a related mechanism).\\nAlso, a volume cannot contain a hard link to anything in a different volume.\\nTypes of volumes\\nKubernetes supports several types of volumes.\\nawsElasticBlockStore (removed)\\nKubernetes 1.28 does not include a awsElasticBlockStore  volume type.\\nThe AWSElasticBlockStore in-tree storage driver was deprecated in the Kubernetes v1.19\\nrelease and then removed entirely in the v1.27 release.\\nThe Kubernetes project suggests that you use the AWS EBS  third party storage driver instead.\\nazureDisk (removed)\\nKubernetes 1.28 does not include a azureDisk  volume type.\\nThe AzureDisk in-tree storage driver was deprecated in the Kubernetes v1.19 release and then\\nremoved entirely in the v1.27 release.\\nThe Kubernetes project suggests that you use the Azure Disk  third party storage driver instead.\\nazureFile (deprecated)\\nFEATURE STATE:  Kubernetes v1.21 [deprecated]\\nThe azureFile  volume type mounts a Microsoft Azure File volume (SMB 2.1 and 3.0) into a pod.', metadata={'source': './PDFS/Concepts.pdf', 'page': 281}),\n",
       " Document(page_content=\"For more details, see the azureFile  volume plugin .\\nazureFile CSI migration\\nFEATURE STATE:  Kubernetes v1.26 [stable]\\nThe CSIMigration  feature for azureFile , when enabled, redirects all plugin operations from the\\nexisting in-tree plugin to the file.csi.azure.com  Container Storage Interface (CSI) Driver. In\\norder to use this feature, the Azure File CSI Driver  must be installed on the cluster and the \\nCSIMigrationAzureFile  feature gates  must be enabled.\\nAzure File CSI driver does not support using same volume with different fsgroups. If \\nCSIMigrationAzureFile  is enabled, using same volume with different fsgroups won't be\\nsupported at all.\\nazureFile CSI migration complete\\nFEATURE STATE:  Kubernetes v1.21 [alpha]\\nTo disable the azureFile  storage plugin from being loaded by the controller manager and the\\nkubelet, set the InTreePluginAzureFileUnregister  flag to true.\\ncephfs\\nFEATURE STATE:  Kubernetes v1.28 [deprecated]\\nNote:  The Kubernetes project suggests that you use the CephFS CSI  third party storage driver\\ninstead.\\nA cephfs  volume allows an existing CephFS volume to be mounted into your Pod. Unlike \\nemptyDir , which is erased when a pod is removed, the contents of a cephfs  volume are\\npreserved and the volume is merely unmounted. This means that a cephfs  volume can be pre-\\npopulated with data, and that data can be shared between pods. The cephfs  volume can be\\nmounted by multiple writers simultaneously.\\nNote:  You must have your own Ceph server running with the share exported before you can\\nuse it.\\nSee the CephFS example  for more details.\\ncinder (removed)\\nKubernetes 1.28 does not include a cinder  volume type.\\nThe OpenStack Cinder in-tree storage driver was deprecated in the Kubernetes v1.11 release\\nand then removed entirely in the v1.26 release.\\nThe Kubernetes project suggests that you use the OpenStack Cinder  third party storage driver\\ninstead.\", metadata={'source': './PDFS/Concepts.pdf', 'page': 282}),\n",
       " Document(page_content='configMap\\nA ConfigMap  provides a way to inject configuration data into pods. The data stored in a\\nConfigMap can be referenced in a volume of type configMap  and then consumed by\\ncontainerized applications running in a pod.\\nWhen referencing a ConfigMap, you provide the name of the ConfigMap in the volume. You\\ncan customize the path to use for a specific entry in the ConfigMap. The following\\nconfiguration shows how to mount the log-config  ConfigMap onto a Pod called configmap-pod :\\napiVersion : v1\\nkind: Pod\\nmetadata :\\n  name : configmap-pod\\nspec:\\n  containers :\\n    - name : test\\n      image : busybox:1.28\\n      command : [\\'sh\\', \\'-c\\', \\'echo \"The app is running!\" && tail -f /dev/null\\' ]\\n      volumeMounts :\\n        - name : config-vol\\n          mountPath : /etc/config\\n  volumes :\\n    - name : config-vol\\n      configMap :\\n        name : log-config\\n        items :\\n          - key: log_level\\n            path: log_level\\nThe log-config  ConfigMap is mounted as a volume, and all contents stored in its log_level  entry\\nare mounted into the Pod at path /etc/config/log_level . Note that this path is derived from the\\nvolume\\'s mountPath  and the path keyed with log_level .\\nNote:\\nYou must create a ConfigMap  before you can use it.\\nA ConfigMap is always mounted as readOnly .\\nA container using a ConfigMap as a subPath  volume mount will not receive ConfigMap\\nupdates.\\nText data is exposed as files using the UTF-8 character encoding. For other character\\nencodings, use binaryData .\\ndownwardAPI\\nA downwardAPI  volume makes downward API  data available to applications. Within the\\nvolume, you can find the exposed data as read-only files in plain text format.\\nNote:  A container using the downward API as a subPath  volume mount does not receive\\nupdates when field values change.• \\n• \\n• \\n•', metadata={'source': './PDFS/Concepts.pdf', 'page': 283}),\n",
       " Document(page_content='See Expose Pod Information to Containers Through Files  to learn more.\\nemptyDir\\nFor a Pod that defines an emptyDir  volume, the volume is created when the Pod is assigned to a\\nnode. As the name says, the emptyDir  volume is initially empty. All containers in the Pod can\\nread and write the same files in the emptyDir  volume, though that volume can be mounted at\\nthe same or different paths in each container. When a Pod is removed from a node for any\\nreason, the data in the emptyDir  is deleted permanently.\\nNote:  A container crashing does not remove a Pod from a node. The data in an emptyDir\\nvolume is safe across container crashes.\\nSome uses for an emptyDir  are:\\nscratch space, such as for a disk-based merge sort\\ncheckpointing a long computation for recovery from crashes\\nholding files that a content-manager container fetches while a webserver container serves\\nthe data\\nThe emptyDir.medium  field controls where emptyDir  volumes are stored. By default emptyDir\\nvolumes are stored on whatever medium that backs the node such as disk, SSD, or network\\nstorage, depending on your environment. If you set the emptyDir.medium  field to \"Memory\" ,\\nKubernetes mounts a tmpfs (RAM-backed filesystem) for you instead. While tmpfs is very fast\\nbe aware that, unlike disks, files you write count against the memory limit of the container that\\nwrote them.\\nA size limit can be specified for the default medium, which limits the capacity of the emptyDir\\nvolume. The storage is allocated from node ephemeral storage . If that is filled up from another\\nsource (for example, log files or image overlays), the emptyDir  may run out of capacity before\\nthis limit.\\nNote:  If the SizeMemoryBackedVolumes  feature gate  is enabled, you can specify a size for\\nmemory backed volumes. If no size is specified, memory backed volumes are sized to node\\nallocatable memory.\\nemptyDir configuration example\\napiVersion : v1\\nkind: Pod\\nmetadata :\\n  name : test-pd\\nspec:\\n  containers :\\n  - image : registry.k8s.io/test-webserver\\n    name : test-container\\n    volumeMounts :\\n    - mountPath : /cache\\n      name : cache-volume\\n  volumes :\\n  - name : cache-volume\\n    emptyDir :\\n      sizeLimit : 500Mi• \\n• \\n•', metadata={'source': './PDFS/Concepts.pdf', 'page': 284}),\n",
       " Document(page_content='fc (fibre channel)\\nAn fc volume type allows an existing fibre channel block storage volume to mount in a Pod.\\nYou can specify single or multiple target world wide names (WWNs) using the parameter \\ntargetWWNs  in your Volume configuration. If multiple WWNs are specified, targetWWNs\\nexpect that those WWNs are from multi-path connections.\\nNote:  You must configure FC SAN Zoning to allocate and mask those LUNs (volumes) to the\\ntarget WWNs beforehand so that Kubernetes hosts can access them.\\nSee the fibre channel example  for more details.\\ngcePersistentDisk (deprecated)\\nFEATURE STATE:  Kubernetes v1.17 [deprecated]\\nA gcePersistentDisk  volume mounts a Google Compute Engine (GCE) persistent disk  (PD) into\\nyour Pod. Unlike emptyDir , which is erased when a pod is removed, the contents of a PD are\\npreserved and the volume is merely unmounted. This means that a PD can be pre-populated\\nwith data, and that data can be shared between pods.\\nNote:  You must create a PD using gcloud  or the GCE API or UI before you can use it.\\nThere are some restrictions when using a gcePersistentDisk :\\nthe nodes on which Pods are running must be GCE VMs\\nthose VMs need to be in the same GCE project and zone as the persistent disk\\nOne feature of GCE persistent disk is concurrent read-only access to a persistent disk. A \\ngcePersistentDisk  volume permits multiple consumers to simultaneously mount a persistent\\ndisk as read-only. This means that you can pre-populate a PD with your dataset and then serve\\nit in parallel from as many Pods as you need. Unfortunately, PDs can only be mounted by a\\nsingle consumer in read-write mode. Simultaneous writers are not allowed.\\nUsing a GCE persistent disk with a Pod controlled by a ReplicaSet will fail unless the PD is\\nread-only or the replica count is 0 or 1.\\nCreating a GCE persistent disk\\nBefore you can use a GCE persistent disk with a Pod, you need to create it.\\ngcloud compute disks create --size =500GB --zone =us-central1-a my-data-disk\\nGCE persistent disk configuration example\\napiVersion : v1\\nkind: Pod\\nmetadata :\\n  name : test-pd\\nspec:\\n  containers :\\n  - image : registry.k8s.io/test-webserver\\n    name : test-container• \\n•', metadata={'source': './PDFS/Concepts.pdf', 'page': 285}),\n",
       " Document(page_content='volumeMounts :\\n    - mountPath : /test-pd\\n      name : test-volume\\n  volumes :\\n  - name : test-volume\\n    # This GCE PD must already exist.\\n    gcePersistentDisk :\\n      pdName : my-data-disk\\n      fsType : ext4\\nRegional persistent disks\\nThe Regional persistent disks  feature allows the creation of persistent disks that are available in\\ntwo zones within the same region. In order to use this feature, the volume must be provisioned\\nas a PersistentVolume; referencing the volume directly from a pod is not supported.\\nManually provisioning a Regional PD PersistentVolume\\nDynamic provisioning is possible using a StorageClass for GCE PD . Before creating a\\nPersistentVolume, you must create the persistent disk:\\ngcloud compute disks create --size =500GB my-data-disk\\n  --region us-central1\\n  --replica-zones us-central1-a,us-central1-b\\nRegional persistent disk configuration example\\napiVersion : v1\\nkind: PersistentVolume\\nmetadata :\\n  name : test-volume\\nspec:\\n  capacity :\\n    storage : 400Gi\\n  accessModes :\\n  - ReadWriteOnce\\n  gcePersistentDisk :\\n    pdName : my-data-disk\\n    fsType : ext4\\n  nodeAffinity :\\n    required :\\n      nodeSelectorTerms :\\n      - matchExpressions :\\n        # failure-domain.beta.kubernetes.io/zone should be used prior to 1.21\\n        - key: topology.kubernetes.io/zone\\n          operator : In\\n          values :\\n          - us-central1-a\\n          - us-central1-b', metadata={'source': './PDFS/Concepts.pdf', 'page': 286}),\n",
       " Document(page_content='GCE CSI migration\\nFEATURE STATE:  Kubernetes v1.25 [stable]\\nThe CSIMigration  feature for GCE PD, when enabled, redirects all plugin operations from the\\nexisting in-tree plugin to the pd.csi.storage.gke.io  Container Storage Interface (CSI) Driver. In\\norder to use this feature, the GCE PD CSI Driver  must be installed on the cluster.\\nGCE CSI migration complete\\nFEATURE STATE:  Kubernetes v1.21 [alpha]\\nTo disable the gcePersistentDisk  storage plugin from being loaded by the controller manager\\nand the kubelet, set the InTreePluginGCEUnregister  flag to true.\\ngitRepo (deprecated)\\nWarning:  The gitRepo  volume type is deprecated. To provision a container with a git repo,\\nmount an EmptyDir  into an InitContainer that clones the repo using git, then mount the \\nEmptyDir  into the Pod\\'s container.\\nA gitRepo  volume is an example of a volume plugin. This plugin mounts an empty directory\\nand clones a git repository into this directory for your Pod to use.\\nHere is an example of a gitRepo  volume:\\napiVersion : v1\\nkind: Pod\\nmetadata :\\n  name : server\\nspec:\\n  containers :\\n  - image : nginx\\n    name : nginx\\n    volumeMounts :\\n    - mountPath : /mypath\\n      name : git-volume\\n  volumes :\\n  - name : git-volume\\n    gitRepo :\\n      repository : \"git@somewhere:me/my-git-repository.git\"\\n      revision : \"22f1d8406d464b0c0874075539c1f2e96c253775\"\\nglusterfs (removed)\\nKubernetes 1.28 does not include a glusterfs  volume type.\\nThe GlusterFS in-tree storage driver was deprecated in the Kubernetes v1.25 release and then\\nremoved entirely in the v1.26 release.', metadata={'source': './PDFS/Concepts.pdf', 'page': 287}),\n",
       " Document(page_content=\"hostPath\\nWarning:\\nHostPath volumes present many security risks, and it is a best practice to avoid the use of\\nHostPaths when possible. When a HostPath volume must be used, it should be scoped to only\\nthe required file or directory, and mounted as ReadOnly.\\nIf restricting HostPath access to specific directories through AdmissionPolicy, volumeMounts\\nMUST be required to use readOnly  mounts for the policy to be effective.\\nA hostPath  volume mounts a file or directory from the host node's filesystem into your Pod.\\nThis is not something that most Pods will need, but it offers a powerful escape hatch for some\\napplications.\\nFor example, some uses for a hostPath  are:\\nrunning a container that needs access to Docker internals; use a hostPath  of /var/lib/\\ndocker\\nrunning cAdvisor in a container; use a hostPath  of /sys\\nallowing a Pod to specify whether a given hostPath  should exist prior to the Pod running,\\nwhether it should be created, and what it should exist as\\nIn addition to the required path property, you can optionally specify a type for a hostPath\\nvolume.\\nThe supported values for field type are:\\nValue Behavior\\nEmpty string (default) is for backward compatibility, which means that no\\nchecks will be performed before mounting the hostPath volume.\\nDirectoryOrCreateIf nothing exists at the given path, an empty directory will be created there\\nas needed with permission set to 0755, having the same group and\\nownership with Kubelet.\\nDirectory A directory must exist at the given path\\nFileOrCreateIf nothing exists at the given path, an empty file will be created there as\\nneeded with permission set to 0644, having the same group and ownership\\nwith Kubelet.\\nFile A file must exist at the given path\\nSocket A UNIX socket must exist at the given path\\nCharDevice A character device must exist at the given path\\nBlockDevice A block device must exist at the given path\\nWatch out when using this type of volume, because:\\nHostPaths can expose privileged system credentials (such as for the Kubelet) or privileged\\nAPIs (such as container runtime socket), which can be used for container escape or to\\nattack other parts of the cluster.\\nPods with identical configuration (such as created from a PodTemplate) may behave\\ndifferently on different nodes due to different files on the nodes• \\n• \\n• \\n• \\n•\", metadata={'source': './PDFS/Concepts.pdf', 'page': 288}),\n",
       " Document(page_content='The files or directories created on the underlying hosts are only writable by root. You\\neither need to run your process as root in a privileged Container  or modify the file\\npermissions on the host to be able to write to a hostPath  volume\\nhostPath configuration example\\napiVersion : v1\\nkind: Pod\\nmetadata :\\n  name : test-pd\\nspec:\\n  containers :\\n  - image : registry.k8s.io/test-webserver\\n    name : test-container\\n    volumeMounts :\\n    - mountPath : /test-pd\\n      name : test-volume\\n  volumes :\\n  - name : test-volume\\n    hostPath :\\n      # directory location on host\\n      path: /data\\n      # this field is optional\\n      type: Directory\\nCaution:  The FileOrCreate  mode does not create the parent directory of the file. If the parent\\ndirectory of the mounted file does not exist, the pod fails to start. To ensure that this mode\\nworks, you can try to mount directories and files separately, as shown in the \\nFileOrCreate configuration .\\nhostPath FileOrCreate configuration example\\napiVersion : v1\\nkind: Pod\\nmetadata :\\n  name : test-webserver\\nspec:\\n  containers :\\n  - name : test-webserver\\n    image : registry.k8s.io/test-webserver:latest\\n    volumeMounts :\\n    - mountPath : /var/local/aaa\\n      name : mydir\\n    - mountPath : /var/local/aaa/1.txt\\n      name : myfile\\n  volumes :\\n  - name : mydir\\n    hostPath :\\n      # Ensure the file directory is created.\\n      path: /var/local/aaa\\n      type: DirectoryOrCreate\\n  - name : myfile•', metadata={'source': './PDFS/Concepts.pdf', 'page': 289}),\n",
       " Document(page_content=\"hostPath :\\n      path: /var/local/aaa/1.txt\\n      type: FileOrCreate\\niscsi\\nAn iscsi volume allows an existing iSCSI (SCSI over IP) volume to be mounted into your Pod.\\nUnlike emptyDir , which is erased when a Pod is removed, the contents of an iscsi volume are\\npreserved and the volume is merely unmounted. This means that an iscsi volume can be pre-\\npopulated with data, and that data can be shared between pods.\\nNote:  You must have your own iSCSI server running with the volume created before you can\\nuse it.\\nA feature of iSCSI is that it can be mounted as read-only by multiple consumers simultaneously.\\nThis means that you can pre-populate a volume with your dataset and then serve it in parallel\\nfrom as many Pods as you need. Unfortunately, iSCSI volumes can only be mounted by a single\\nconsumer in read-write mode. Simultaneous writers are not allowed.\\nSee the iSCSI example  for more details.\\nlocal\\nA local  volume represents a mounted local storage device such as a disk, partition or directory.\\nLocal volumes can only be used as a statically created PersistentVolume. Dynamic provisioning\\nis not supported.\\nCompared to hostPath  volumes, local  volumes are used in a durable and portable manner\\nwithout manually scheduling pods to nodes. The system is aware of the volume's node\\nconstraints by looking at the node affinity on the PersistentVolume.\\nHowever, local  volumes are subject to the availability of the underlying node and are not\\nsuitable for all applications. If a node becomes unhealthy, then the local  volume becomes\\ninaccessible by the pod. The pod using this volume is unable to run. Applications using local\\nvolumes must be able to tolerate this reduced availability, as well as potential data loss,\\ndepending on the durability characteristics of the underlying disk.\\nThe following example shows a PersistentVolume using a local  volume and nodeAffinity :\\napiVersion : v1\\nkind: PersistentVolume\\nmetadata :\\n  name : example-pv\\nspec:\\n  capacity :\\n    storage : 100Gi\\n  volumeMode : Filesystem\\n  accessModes :\\n  - ReadWriteOnce\\n  persistentVolumeReclaimPolicy : Delete\\n  storageClassName : local-storage\\n  local :\", metadata={'source': './PDFS/Concepts.pdf', 'page': 290}),\n",
       " Document(page_content='path: /mnt/disks/ssd1\\n  nodeAffinity :\\n    required :\\n      nodeSelectorTerms :\\n      - matchExpressions :\\n        - key: kubernetes.io/hostname\\n          operator : In\\n          values :\\n          - example-node\\nYou must set a PersistentVolume nodeAffinity  when using local  volumes. The Kubernetes\\nscheduler uses the PersistentVolume nodeAffinity  to schedule these Pods to the correct node.\\nPersistentVolume volumeMode  can be set to \"Block\" (instead of the default value \"Filesystem\")\\nto expose the local volume as a raw block device.\\nWhen using local volumes, it is recommended to create a StorageClass with \\nvolumeBindingMode  set to WaitForFirstConsumer . For more details, see the local StorageClass\\nexample. Delaying volume binding ensures that the PersistentVolumeClaim binding decision\\nwill also be evaluated with any other node constraints the Pod may have, such as node resource\\nrequirements, node selectors, Pod affinity, and Pod anti-affinity.\\nAn external static provisioner can be run separately for improved management of the local\\nvolume lifecycle. Note that this provisioner does not support dynamic provisioning yet. For an\\nexample on how to run an external local provisioner, see the local volume provisioner user\\nguide .\\nNote:  The local PersistentVolume requires manual cleanup and deletion by the user if the\\nexternal static provisioner is not used to manage the volume lifecycle.\\nnfs\\nAn nfs volume allows an existing NFS (Network File System) share to be mounted into a Pod.\\nUnlike emptyDir , which is erased when a Pod is removed, the contents of an nfs volume are\\npreserved and the volume is merely unmounted. This means that an NFS volume can be pre-\\npopulated with data, and that data can be shared between pods. NFS can be mounted by\\nmultiple writers simultaneously.\\napiVersion : v1\\nkind: Pod\\nmetadata :\\n  name : test-pd\\nspec:\\n  containers :\\n  - image : registry.k8s.io/test-webserver\\n    name : test-container\\n    volumeMounts :\\n    - mountPath : /my-nfs-data\\n      name : test-volume\\n  volumes :\\n  - name : test-volume\\n    nfs:\\n      server : my-nfs-server.example.com', metadata={'source': './PDFS/Concepts.pdf', 'page': 291}),\n",
       " Document(page_content='path: /my-nfs-volume\\n      readOnly : true\\nNote:\\nYou must have your own NFS server running with the share exported before you can use it.\\nAlso note that you can\\'t specify NFS mount options in a Pod spec. You can either set mount\\noptions server-side or use /etc/nfsmount.conf . You can also mount NFS volumes via\\nPersistentVolumes which do allow you to set mount options.\\nSee the NFS example  for an example of mounting NFS volumes with PersistentVolumes.\\npersistentVolumeClaim\\nA persistentVolumeClaim  volume is used to mount a PersistentVolume  into a Pod.\\nPersistentVolumeClaims are a way for users to \"claim\" durable storage (such as a GCE\\nPersistentDisk or an iSCSI volume) without knowing the details of the particular cloud\\nenvironment.\\nSee the information about PersistentVolumes  for more details.\\nportworxVolume (deprecated)\\nFEATURE STATE:  Kubernetes v1.25 [deprecated]\\nA portworxVolume  is an elastic block storage layer that runs hyperconverged with Kubernetes. \\nPortworx  fingerprints storage in a server, tiers based on capabilities, and aggregates capacity\\nacross multiple servers. Portworx runs in-guest in virtual machines or on bare metal Linux\\nnodes.\\nA portworxVolume  can be dynamically created through Kubernetes or it can also be pre-\\nprovisioned and referenced inside a Pod. Here is an example Pod referencing a pre-provisioned\\nPortworx volume:\\napiVersion : v1\\nkind: Pod\\nmetadata :\\n  name : test-portworx-volume-pod\\nspec:\\n  containers :\\n  - image : registry.k8s.io/test-webserver\\n    name : test-container\\n    volumeMounts :\\n    - mountPath : /mnt\\n      name : pxvol\\n  volumes :\\n  - name : pxvol\\n    # This Portworx volume must already exist.\\n    portworxVolume :\\n      volumeID : \"pxvol\"\\n      fsType : \"<fs-type>\"', metadata={'source': './PDFS/Concepts.pdf', 'page': 292}),\n",
       " Document(page_content=\"Note:  Make sure you have an existing PortworxVolume with name pxvol  before using it in the\\nPod.\\nFor more details, see the Portworx volume  examples.\\nPortworx CSI migration\\nFEATURE STATE:  Kubernetes v1.25 [beta]\\nThe CSIMigration  feature for Portworx has been added but disabled by default in Kubernetes\\n1.23 since it's in alpha state. It has been beta now since v1.25 but it is still turned off by default.\\nIt redirects all plugin operations from the existing in-tree plugin to the pxd.portworx.com\\nContainer Storage Interface (CSI) Driver. Portworx CSI Driver  must be installed on the cluster.\\nTo enable the feature, set CSIMigrationPortworx=true  in kube-controller-manager and kubelet.\\nprojected\\nA projected volume maps several existing volume sources into the same directory. For more\\ndetails, see projected volumes .\\nrbd\\nFEATURE STATE:  Kubernetes v1.28 [deprecated]\\nNote:  The Kubernetes project suggests that you use the Ceph CSI  third party storage driver\\ninstead, in RBD mode.\\nAn rbd volume allows a Rados Block Device  (RBD) volume to mount into your Pod. Unlike \\nemptyDir , which is erased when a pod is removed, the contents of an rbd volume are preserved\\nand the volume is unmounted. This means that a RBD volume can be pre-populated with data,\\nand that data can be shared between pods.\\nNote:  You must have a Ceph installation running before you can use RBD.\\nA feature of RBD is that it can be mounted as read-only by multiple consumers simultaneously.\\nThis means that you can pre-populate a volume with your dataset and then serve it in parallel\\nfrom as many pods as you need. Unfortunately, RBD volumes can only be mounted by a single\\nconsumer in read-write mode. Simultaneous writers are not allowed.\\nSee the RBD example  for more details.\\nRBD CSI migration\\nFEATURE STATE:  Kubernetes v1.28 [deprecated]\\nThe CSIMigration  feature for RBD , when enabled, redirects all plugin operations from the\\nexisting in-tree plugin to the rbd.csi.ceph.com  CSI driver. In order to use this feature, the Ceph\\nCSI driver  must be installed on the cluster and the CSIMigrationRBD  feature gate  must be\\nenabled. (Note that the csiMigrationRBD  flag has been removed and replaced with \\nCSIMigrationRBD  in release v1.24)\\nNote:\", metadata={'source': './PDFS/Concepts.pdf', 'page': 293}),\n",
       " Document(page_content=\"As a Kubernetes cluster operator that administers storage, here are the prerequisites that you\\nmust complete before you attempt migration to the RBD CSI driver:\\nYou must install the Ceph CSI driver ( rbd.csi.ceph.com ), v3.5.0 or above, into your\\nKubernetes cluster.\\nconsidering the clusterID  field is a required parameter for CSI driver for its operations,\\nbut in-tree StorageClass has monitors  field as a required parameter, a Kubernetes storage\\nadmin has to create a clusterID based on the monitors hash ( ex: #echo -n \\n'<monitors_string>' | md5sum ) in the CSI config map and keep the monitors under this\\nclusterID configuration.\\nAlso, if the value of adminId  in the in-tree Storageclass is different from admin , the \\nadminSecretName  mentioned in the in-tree Storageclass has to be patched with the\\nbase64 value of the adminId  parameter value, otherwise this step can be skipped.\\nsecret\\nA secret  volume is used to pass sensitive information, such as passwords, to Pods. You can store\\nsecrets in the Kubernetes API and mount them as files for use by pods without coupling to\\nKubernetes directly. secret  volumes are backed by tmpfs (a RAM-backed filesystem) so they are\\nnever written to non-volatile storage.\\nNote:\\nYou must create a Secret in the Kubernetes API before you can use it.\\nA Secret is always mounted as readOnly .\\nA container using a Secret as a subPath  volume mount will not receive Secret updates.\\nFor more details, see Configuring Secrets .\\nvsphereVolume (deprecated)\\nNote:  The Kubernetes project recommends using the vSphere CSI  out-of-tree storage driver\\ninstead.\\nA vsphereVolume  is used to mount a vSphere VMDK volume into your Pod. The contents of a\\nvolume are preserved when it is unmounted. It supports both VMFS and VSAN datastore.\\nFor more information, see the vSphere volume  examples.\\nvSphere CSI migration\\nFEATURE STATE:  Kubernetes v1.26 [stable]\\nIn Kubernetes 1.28, all operations for the in-tree vsphereVolume  type are redirected to the \\ncsi.vsphere.vmware.com  CSI driver.\\nvSphere CSI driver  must be installed on the cluster. You can find additional advice on how to\\nmigrate in-tree vsphereVolume  in VMware's documentation page Migrating In-Tree vSphere\\nVolumes to vSphere Container Storage lug-in . If vSphere CSI Driver is not installed volume\\noperations can not be performed on the PV created with the in-tree vsphereVolume  type.• \\n• \\n• \\n• \\n• \\n•\", metadata={'source': './PDFS/Concepts.pdf', 'page': 294}),\n",
       " Document(page_content='You must run vSphere 7.0u2 or later in order to migrate to the vSphere CSI driver.\\nIf you are running a version of Kubernetes other than v1.28, consult the documentation for that\\nversion of Kubernetes.\\nNote:\\nThe following StorageClass parameters from the built-in vsphereVolume  plugin are not\\nsupported by the vSphere CSI driver:\\ndiskformat\\nhostfailurestotolerate\\nforceprovisioning\\ncachereservation\\ndiskstripes\\nobjectspacereservation\\niopslimit\\nExisting volumes created using these parameters will be migrated to the vSphere CSI driver, but\\nnew volumes created by the vSphere CSI driver will not be honoring these parameters.\\nvSphere CSI migration complete\\nFEATURE STATE:  Kubernetes v1.19 [beta]\\nTo turn off the vsphereVolume  plugin from being loaded by the controller manager and the\\nkubelet, you need to set InTreePluginvSphereUnregister  feature flag to true. You must install a \\ncsi.vsphere.vmware.com  CSI driver on all worker nodes.\\nUsing subPath\\nSometimes, it is useful to share one volume for multiple uses in a single pod. The \\nvolumeMounts.subPath  property specifies a sub-path inside the referenced volume instead of its\\nroot.\\nThe following example shows how to configure a Pod with a LAMP stack (Linux Apache\\nMySQL PHP) using a single, shared volume. This sample subPath  configuration is not\\nrecommended for production use.\\nThe PHP application\\'s code and assets map to the volume\\'s html  folder and the MySQL database\\nis stored in the volume\\'s mysql  folder. For example:\\napiVersion : v1\\nkind: Pod\\nmetadata :\\n  name : my-lamp-site\\nspec:\\n    containers :\\n    - name : mysql\\n      image : mysql\\n      env:\\n      - name : MYSQL_ROOT_PASSWORD\\n        value : \"rootpasswd\"• \\n• \\n• \\n• \\n• \\n• \\n•', metadata={'source': './PDFS/Concepts.pdf', 'page': 295}),\n",
       " Document(page_content='volumeMounts :\\n      - mountPath : /var/lib/mysql\\n        name : site-data\\n        subPath : mysql\\n    - name : php\\n      image : php:7.0-apache\\n      volumeMounts :\\n      - mountPath : /var/www/html\\n        name : site-data\\n        subPath : html\\n    volumes :\\n    - name : site-data\\n      persistentVolumeClaim :\\n        claimName : my-lamp-site-data\\nUsing subPath with expanded environment variables\\nFEATURE STATE:  Kubernetes v1.17 [stable]\\nUse the subPathExpr  field to construct subPath  directory names from downward API\\nenvironment variables. The subPath  and subPathExpr  properties are mutually exclusive.\\nIn this example, a Pod uses subPathExpr  to create a directory pod1  within the hostPath\\nvolume /var/log/pods . The hostPath  volume takes the Pod name from the downwardAPI . The\\nhost directory /var/log/pods/pod1  is mounted at /logs  in the container.\\napiVersion : v1\\nkind: Pod\\nmetadata :\\n  name : pod1\\nspec:\\n  containers :\\n  - name : container1\\n    env:\\n    - name : POD_NAME\\n      valueFrom :\\n        fieldRef :\\n          apiVersion : v1\\n          fieldPath : metadata.name\\n    image : busybox:1.28\\n    command : [ \"sh\", \"-c\", \"while [ true ]; do echo \\'Hello\\'; sleep 10; done | tee -a /logs/hello.txt\"  ]\\n    volumeMounts :\\n    - name : workdir1\\n      mountPath : /logs\\n      # The variable expansion uses round brackets (not curly brackets).\\n      subPathExpr : $(POD_NAME)\\n  restartPolicy : Never\\n  volumes :\\n  - name : workdir1\\n    hostPath :\\n      path: /var/log/pods', metadata={'source': './PDFS/Concepts.pdf', 'page': 296}),\n",
       " Document(page_content='Resources\\nThe storage media (such as Disk or SSD) of an emptyDir  volume is determined by the medium\\nof the filesystem holding the kubelet root dir (typically /var/lib/kubelet ). There is no limit on\\nhow much space an emptyDir  or hostPath  volume can consume, and no isolation between\\ncontainers or between pods.\\nTo learn about requesting space using a resource specification, see how to manage resources .\\nOut-of-tree volume plugins\\nThe out-of-tree volume plugins include Container Storage Interface  (CSI), and also FlexVolume\\n(which is deprecated). These plugins enable storage vendors to create custom storage plugins\\nwithout adding their plugin source code to the Kubernetes repository.\\nPreviously, all volume plugins were \"in-tree\". The \"in-tree\" plugins were built, linked, compiled,\\nand shipped with the core Kubernetes binaries. This meant that adding a new storage system to\\nKubernetes (a volume plugin) required checking code into the core Kubernetes code repository.\\nBoth CSI and FlexVolume allow volume plugins to be developed independent of the Kubernetes\\ncode base, and deployed (installed) on Kubernetes clusters as extensions.\\nFor storage vendors looking to create an out-of-tree volume plugin, please refer to the volume\\nplugin FAQ .\\ncsi\\nContainer Storage Interface  (CSI) defines a standard interface for container orchestration\\nsystems (like Kubernetes) to expose arbitrary storage systems to their container workloads.\\nPlease read the CSI design proposal  for more information.\\nNote:  Support for CSI spec versions 0.2 and 0.3 are deprecated in Kubernetes v1.13 and will be\\nremoved in a future release.\\nNote:  CSI drivers may not be compatible across all Kubernetes releases. Please check the\\nspecific CSI driver\\'s documentation for supported deployments steps for each Kubernetes\\nrelease and a compatibility matrix.\\nOnce a CSI compatible volume driver is deployed on a Kubernetes cluster, users may use the csi\\nvolume type to attach or mount the volumes exposed by the CSI driver.\\nA csi volume can be used in a Pod in three different ways:\\nthrough a reference to a PersistentVolumeClaim\\nwith a generic ephemeral volume\\nwith a CSI ephemeral volume  if the driver supports that\\nThe following fields are available to storage administrators to configure a CSI persistent\\nvolume:\\ndriver : A string value that specifies the name of the volume driver to use. This value must\\ncorrespond to the value returned in the GetPluginInfoResponse  by the CSI driver as• \\n• \\n• \\n•', metadata={'source': './PDFS/Concepts.pdf', 'page': 297}),\n",
       " Document(page_content='defined in the CSI spec . It is used by Kubernetes to identify which CSI driver to call out\\nto, and by CSI driver components to identify which PV objects belong to the CSI driver.\\nvolumeHandle : A string value that uniquely identifies the volume. This value must\\ncorrespond to the value returned in the volume.id  field of the CreateVolumeResponse  by\\nthe CSI driver as defined in the CSI spec . The value is passed as volume_id  on all calls to\\nthe CSI volume driver when referencing the volume.\\nreadOnly : An optional boolean value indicating whether the volume is to be\\n\"ControllerPublished\" (attached) as read only. Default is false. This value is passed to the\\nCSI driver via the readonly  field in the ControllerPublishVolumeRequest .\\nfsType : If the PV\\'s VolumeMode  is Filesystem  then this field may be used to specify the\\nfilesystem that should be used to mount the volume. If the volume has not been formatted\\nand formatting is supported, this value will be used to format the volume. This value is\\npassed to the CSI driver via the VolumeCapability  field of \\nControllerPublishVolumeRequest , NodeStageVolumeRequest , and \\nNodePublishVolumeRequest .\\nvolumeAttributes : A map of string to string that specifies static properties of a volume.\\nThis map must correspond to the map returned in the volume.attributes  field of the \\nCreateVolumeResponse  by the CSI driver as defined in the CSI spec . The map is passed to\\nthe CSI driver via the volume_context  field in the ControllerPublishVolumeRequest , \\nNodeStageVolumeRequest , and NodePublishVolumeRequest .\\ncontrollerPublishSecretRef : A reference to the secret object containing sensitive\\ninformation to pass to the CSI driver to complete the CSI ControllerPublishVolume  and \\nControllerUnpublishVolume  calls. This field is optional, and may be empty if no secret is\\nrequired. If the Secret contains more than one secret, all secrets are passed.\\nnodeExpandSecretRef : A reference to the secret containing sensitive information to pass\\nto the CSI driver to complete the CSI NodeExpandVolume  call. This field is optional, and\\nmay be empty if no secret is required. If the object contains more than one secret, all\\nsecrets are passed. When you have configured secret data for node-initiated volume\\nexpansion, the kubelet passes that data via the NodeExpandVolume()  call to the CSI\\ndriver. In order to use the nodeExpandSecretRef  field, your cluster should be running\\nKubernetes version 1.25 or later.\\nIf you are running Kubernetes Version 1.25 or 1.26, you must enable the feature gate\\nnamed CSINodeExpandSecret  for each kube-apiserver and for the kubelet on every node.\\nIn Kubernetes version 1.27 this feature has been enabled by default and no explicit\\nenablement of the feature gate is required. You must also be using a CSI driver that\\nsupports or requires secret data during node-initiated storage resize operations.\\nnodePublishSecretRef : A reference to the secret object containing sensitive information to\\npass to the CSI driver to complete the CSI NodePublishVolume  call. This field is optional,\\nand may be empty if no secret is required. If the secret object contains more than one\\nsecret, all secrets are passed.\\nnodeStageSecretRef : A reference to the secret object containing sensitive information to\\npass to the CSI driver to complete the CSI NodeStageVolume  call. This field is optional,\\nand may be empty if no secret is required. If the Secret contains more than one secret, all\\nsecrets are passed.\\nCSI raw block volume support\\nFEATURE STATE:  Kubernetes v1.18 [stable]\\nVendors with external CSI drivers can implement raw block volume support in Kubernetes\\nworkloads.• \\n• \\n• \\n• \\n• \\n• \\n• \\n• \\n•', metadata={'source': './PDFS/Concepts.pdf', 'page': 298}),\n",
       " Document(page_content='You can set up your PersistentVolume/PersistentVolumeClaim with raw block volume support\\nas usual, without any CSI specific changes.\\nCSI ephemeral volumes\\nFEATURE STATE:  Kubernetes v1.25 [stable]\\nYou can directly configure CSI volumes within the Pod specification. Volumes specified in this\\nway are ephemeral and do not persist across pod restarts. See Ephemeral Volumes  for more\\ninformation.\\nFor more information on how to develop a CSI driver, refer to the kubernetes-csi\\ndocumentation\\nWindows CSI proxy\\nFEATURE STATE:  Kubernetes v1.22 [stable]\\nCSI node plugins need to perform various privileged operations like scanning of disk devices\\nand mounting of file systems. These operations differ for each host operating system. For Linux\\nworker nodes, containerized CSI node plugins are typically deployed as privileged containers.\\nFor Windows worker nodes, privileged operations for containerized CSI node plugins is\\nsupported using csi-proxy , a community-managed, stand-alone binary that needs to be pre-\\ninstalled on each Windows node.\\nFor more details, refer to the deployment guide of the CSI plugin you wish to deploy.\\nMigrating to CSI drivers from in-tree plugins\\nFEATURE STATE:  Kubernetes v1.25 [stable]\\nThe CSIMigration  feature directs operations against existing in-tree plugins to corresponding\\nCSI plugins (which are expected to be installed and configured). As a result, operators do not\\nhave to make any configuration changes to existing Storage Classes, PersistentVolumes or\\nPersistentVolumeClaims (referring to in-tree plugins) when transitioning to a CSI driver that\\nsupersedes an in-tree plugin.\\nThe operations and features that are supported include: provisioning/delete, attach/detach,\\nmount/unmount and resizing of volumes.\\nIn-tree plugins that support CSIMigration  and have a corresponding CSI driver implemented\\nare listed in Types of Volumes .\\nThe following in-tree plugins support persistent storage on Windows nodes:\\nazureFile\\ngcePersistentDisk\\nvsphereVolume\\nflexVolume (deprecated)\\nFEATURE STATE:  Kubernetes v1.23 [deprecated]• \\n• \\n•', metadata={'source': './PDFS/Concepts.pdf', 'page': 299}),\n",
       " Document(page_content=\"FlexVolume is an out-of-tree plugin interface that uses an exec-based model to interface with\\nstorage drivers. The FlexVolume driver binaries must be installed in a pre-defined volume\\nplugin path on each node and in some cases the control plane nodes as well.\\nPods interact with FlexVolume drivers through the flexVolume  in-tree volume plugin. For more\\ndetails, see the FlexVolume README  document.\\nThe following FlexVolume plugins , deployed as PowerShell scripts on the host, support\\nWindows nodes:\\nSMB\\niSCSI\\nNote:\\nFlexVolume is deprecated. Using an out-of-tree CSI driver is the recommended way to integrate\\nexternal storage with Kubernetes.\\nMaintainers of FlexVolume driver should implement a CSI Driver and help to migrate users of\\nFlexVolume drivers to CSI. Users of FlexVolume should move their workloads to use the\\nequivalent CSI Driver.\\nMount propagation\\nMount propagation allows for sharing volumes mounted by a container to other containers in\\nthe same pod, or even to other pods on the same node.\\nMount propagation of a volume is controlled by the mountPropagation  field in \\nContainer.volumeMounts . Its values are:\\nNone  - This volume mount will not receive any subsequent mounts that are mounted to\\nthis volume or any of its subdirectories by the host. In similar fashion, no mounts created\\nby the container will be visible on the host. This is the default mode.\\nThis mode is equal to rprivate  mount propagation as described in mount(8)\\nHowever, the CRI runtime may choose rslave  mount propagation (i.e., HostToContainer )\\ninstead, when rprivate  propagation is not applicable. cri-dockerd (Docker) is known to\\nchoose rslave  mount propagation when the mount source contains the Docker daemon's\\nroot directory ( /var/lib/docker ).\\nHostToContainer  - This volume mount will receive all subsequent mounts that are\\nmounted to this volume or any of its subdirectories.\\nIn other words, if the host mounts anything inside the volume mount, the container will\\nsee it mounted there.\\nSimilarly, if any Pod with Bidirectional  mount propagation to the same volume mounts\\nanything there, the container with HostToContainer  mount propagation will see it.\\nThis mode is equal to rslave  mount propagation as described in the mount(8)• \\n• \\n• \\n•\", metadata={'source': './PDFS/Concepts.pdf', 'page': 300}),\n",
       " Document(page_content=\"Bidirectional  - This volume mount behaves the same the HostToContainer  mount. In\\naddition, all volume mounts created by the container will be propagated back to the host\\nand to all containers of all pods that use the same volume.\\nA typical use case for this mode is a Pod with a FlexVolume or CSI driver or a Pod that\\nneeds to mount something on the host using a hostPath  volume.\\nThis mode is equal to rshared  mount propagation as described in the mount(8)\\nWarning:  Bidirectional  mount propagation can be dangerous. It can damage the host\\noperating system and therefore it is allowed only in privileged containers. Familiarity\\nwith Linux kernel behavior is strongly recommended. In addition, any volume mounts\\ncreated by containers in pods must be destroyed (unmounted) by the containers on\\ntermination.\\nWhat's next\\nFollow an example of deploying WordPress and MySQL with Persistent Volumes .\\nPersistent Volumes\\nThis document describes persistent volumes  in Kubernetes. Familiarity with volumes  is\\nsuggested.\\nIntroduction\\nManaging storage is a distinct problem from managing compute instances. The\\nPersistentVolume subsystem provides an API for users and administrators that abstracts details\\nof how storage is provided from how it is consumed. To do this, we introduce two new API\\nresources: PersistentVolume and PersistentVolumeClaim.\\nA PersistentVolume  (PV) is a piece of storage in the cluster that has been provisioned by an\\nadministrator or dynamically provisioned using Storage Classes . It is a resource in the cluster\\njust like a node is a cluster resource. PVs are volume plugins like Volumes, but have a lifecycle\\nindependent of any individual Pod that uses the PV. This API object captures the details of the\\nimplementation of the storage, be that NFS, iSCSI, or a cloud-provider-specific storage system.\\nA PersistentVolumeClaim  (PVC) is a request for storage by a user. It is similar to a Pod. Pods\\nconsume node resources and PVCs consume PV resources. Pods can request specific levels of\\nresources (CPU and Memory). Claims can request specific size and access modes (e.g., they can\\nbe mounted ReadWriteOnce, ReadOnlyMany or ReadWriteMany, see AccessModes ).\\nWhile PersistentVolumeClaims allow a user to consume abstract storage resources, it is\\ncommon that users need PersistentVolumes with varying properties, such as performance, for\\ndifferent problems. Cluster administrators need to be able to offer a variety of\\nPersistentVolumes that differ in more ways than size and access modes, without exposing users\\nto the details of how those volumes are implemented. For these needs, there is the StorageClass\\nresource.\\nSee the detailed walkthrough with working examples .•\", metadata={'source': './PDFS/Concepts.pdf', 'page': 301}),\n",
       " Document(page_content='Lifecycle of a volume and claim\\nPVs are resources in the cluster. PVCs are requests for those resources and also act as claim\\nchecks to the resource. The interaction between PVs and PVCs follows this lifecycle:\\nProvisioning\\nThere are two ways PVs may be provisioned: statically or dynamically.\\nStatic\\nA cluster administrator creates a number of PVs. They carry the details of the real storage,\\nwhich is available for use by cluster users. They exist in the Kubernetes API and are available\\nfor consumption.\\nDynamic\\nWhen none of the static PVs the administrator created match a user\\'s PersistentVolumeClaim,\\nthe cluster may try to dynamically provision a volume specially for the PVC. This provisioning\\nis based on StorageClasses: the PVC must request a storage class  and the administrator must\\nhave created and configured that class for dynamic provisioning to occur. Claims that request\\nthe class \"\" effectively disable dynamic provisioning for themselves.\\nTo enable dynamic storage provisioning based on storage class, the cluster administrator needs\\nto enable the DefaultStorageClass  admission controller  on the API server. This can be done, for\\nexample, by ensuring that DefaultStorageClass  is among the comma-delimited, ordered list of\\nvalues for the --enable-admission-plugins  flag of the API server component. For more\\ninformation on API server command-line flags, check kube-apiserver  documentation.\\nBinding\\nA user creates, or in the case of dynamic provisioning, has already created, a\\nPersistentVolumeClaim with a specific amount of storage requested and with certain access\\nmodes. A control loop in the control plane watches for new PVCs, finds a matching PV (if\\npossible), and binds them together. If a PV was dynamically provisioned for a new PVC, the\\nloop will always bind that PV to the PVC. Otherwise, the user will always get at least what they\\nasked for, but the volume may be in excess of what was requested. Once bound,\\nPersistentVolumeClaim binds are exclusive, regardless of how they were bound. A PVC to PV\\nbinding is a one-to-one mapping, using a ClaimRef which is a bi-directional binding between\\nthe PersistentVolume and the PersistentVolumeClaim.\\nClaims will remain unbound indefinitely if a matching volume does not exist. Claims will be\\nbound as matching volumes become available. For example, a cluster provisioned with many\\n50Gi PVs would not match a PVC requesting 100Gi. The PVC can be bound when a 100Gi PV is\\nadded to the cluster.\\nUsing\\nPods use claims as volumes. The cluster inspects the claim to find the bound volume and\\nmounts that volume for a Pod. For volumes that support multiple access modes, the user\\nspecifies which mode is desired when using their claim as a volume in a Pod.', metadata={'source': './PDFS/Concepts.pdf', 'page': 302}),\n",
       " Document(page_content=\"Once a user has a claim and that claim is bound, the bound PV belongs to the user for as long as\\nthey need it. Users schedule Pods and access their claimed PVs by including a \\npersistentVolumeClaim  section in a Pod's volumes  block. See Claims As Volumes  for more\\ndetails on this.\\nStorage Object in Use Protection\\nThe purpose of the Storage Object in Use Protection feature is to ensure that\\nPersistentVolumeClaims (PVCs) in active use by a Pod and PersistentVolume (PVs) that are\\nbound to PVCs are not removed from the system, as this may result in data loss.\\nNote:  PVC is in active use by a Pod when a Pod object exists that is using the PVC.\\nIf a user deletes a PVC in active use by a Pod, the PVC is not removed immediately. PVC\\nremoval is postponed until the PVC is no longer actively used by any Pods. Also, if an admin\\ndeletes a PV that is bound to a PVC, the PV is not removed immediately. PV removal is\\npostponed until the PV is no longer bound to a PVC.\\nYou can see that a PVC is protected when the PVC's status is Terminating  and the Finalizers  list\\nincludes kubernetes.io/pvc-protection :\\nkubectl describe pvc hostpath\\nName:          hostpath\\nNamespace:     default\\nStorageClass:  example-hostpath\\nStatus:        Terminating\\nVolume:\\nLabels:        <none>\\nAnnotations:   volume.beta.kubernetes.io/storage-class =example-hostpath\\n               volume.beta.kubernetes.io/storage-provisioner =example.com/hostpath\\nFinalizers:    [kubernetes.io/pvc-protection ]\\n...\\nYou can see that a PV is protected when the PV's status is Terminating  and the Finalizers  list\\nincludes kubernetes.io/pv-protection  too:\\nkubectl describe pv task-pv-volume\\nName:            task-pv-volume\\nLabels:          type=local\\nAnnotations:     <none>\\nFinalizers:      [kubernetes.io/pv-protection ]\\nStorageClass:    standard\\nStatus:          Terminating\\nClaim:\\nReclaim Policy:  Delete\\nAccess Modes:    RWO\\nCapacity:        1Gi\\nMessage:\\nSource:\\n    Type:          HostPath (bare host directory volume )\\n    Path:          /tmp/data\\n    HostPathType:\\nEvents:            <none>\", metadata={'source': './PDFS/Concepts.pdf', 'page': 303}),\n",
       " Document(page_content='Reclaiming\\nWhen a user is done with their volume, they can delete the PVC objects from the API that\\nallows reclamation of the resource. The reclaim policy for a PersistentVolume tells the cluster\\nwhat to do with the volume after it has been released of its claim. Currently, volumes can either\\nbe Retained, Recycled, or Deleted.\\nRetain\\nThe Retain  reclaim policy allows for manual reclamation of the resource. When the\\nPersistentVolumeClaim is deleted, the PersistentVolume still exists and the volume is\\nconsidered \"released\". But it is not yet available for another claim because the previous\\nclaimant\\'s data remains on the volume. An administrator can manually reclaim the volume with\\nthe following steps.\\nDelete the PersistentVolume. The associated storage asset in external infrastructure (such\\nas an AWS EBS or GCE PD volume) still exists after the PV is deleted.\\nManually clean up the data on the associated storage asset accordingly.\\nManually delete the associated storage asset.\\nIf you want to reuse the same storage asset, create a new PersistentVolume with the same\\nstorage asset definition.\\nDelete\\nFor volume plugins that support the Delete  reclaim policy, deletion removes both the\\nPersistentVolume object from Kubernetes, as well as the associated storage asset in the external\\ninfrastructure, such as an AWS EBS or GCE PD volume. Volumes that were dynamically\\nprovisioned inherit the reclaim policy of their StorageClass , which defaults to Delete . The\\nadministrator should configure the StorageClass according to users\\' expectations; otherwise,\\nthe PV must be edited or patched after it is created. See Change the Reclaim Policy of a\\nPersistentVolume .\\nRecycle\\nWarning:  The Recycle  reclaim policy is deprecated. Instead, the recommended approach is to\\nuse dynamic provisioning.\\nIf supported by the underlying volume plugin, the Recycle  reclaim policy performs a basic\\nscrub ( rm -rf /thevolume/* ) on the volume and makes it available again for a new claim.\\nHowever, an administrator can configure a custom recycler Pod template using the Kubernetes\\ncontroller manager command line arguments as described in the reference . The custom recycler\\nPod template must contain a volumes  specification, as shown in the example below:\\napiVersion : v1\\nkind: Pod\\nmetadata :\\n  name : pv-recycler\\n  namespace : default\\nspec:\\n  restartPolicy : Never1. \\n2. \\n3.', metadata={'source': './PDFS/Concepts.pdf', 'page': 304}),\n",
       " Document(page_content='volumes :\\n  - name : vol\\n    hostPath :\\n      path: /any/path/it/will/be/replaced\\n  containers :\\n  - name : pv-recycler\\n    image : \"registry.k8s.io/busybox\"\\n    command : [\"/bin/sh\" , \"-c\", \\n\"test -e /scrub && rm -rf /scrub/..?* /scrub/.[!.]* /scrub/*  && test -z \\\\\"$(ls -A /scrub)\\\\\" || exit 1\" ]\\n    volumeMounts :\\n    - name : vol\\n      mountPath : /scrub\\nHowever, the particular path specified in the custom recycler Pod template in the volumes  part\\nis replaced with the particular path of the volume that is being recycled.\\nPersistentVolume deletion protection finalizer\\nFEATURE STATE:  Kubernetes v1.23 [alpha]\\nFinalizers can be added on a PersistentVolume to ensure that PersistentVolumes having Delete\\nreclaim policy are deleted only after the backing storage are deleted.\\nThe newly introduced finalizers kubernetes.io/pv-controller  and external-\\nprovisioner.volume.kubernetes.io/finalizer  are only added to dynamically provisioned volumes.\\nThe finalizer kubernetes.io/pv-controller  is added to in-tree plugin volumes. The following is an\\nexample\\nkubectl describe pv pvc-74a498d6-3929-47e8-8c02-078c1ece4d78\\nName:            pvc-74a498d6-3929-47e8-8c02-078c1ece4d78\\nLabels:          <none>\\nAnnotations:     kubernetes.io/createdby: vsphere-volume-dynamic-provisioner\\n                 pv.kubernetes.io/bound-by-controller: yes\\n                 pv.kubernetes.io/provisioned-by: kubernetes.io/vsphere-volume\\nFinalizers:      [kubernetes.io/pv-protection kubernetes.io/pv-controller ]\\nStorageClass:    vcp-sc\\nStatus:          Bound\\nClaim:           default/vcp-pvc-1\\nReclaim Policy:  Delete\\nAccess Modes:    RWO\\nVolumeMode:      Filesystem\\nCapacity:        1Gi\\nNode Affinity:   <none>\\nMessage:         \\nSource:\\n    Type:               vSphereVolume (a Persistent Disk resource in vSphere )\\n    VolumePath:         [vsanDatastore ] d49c4a62-166f-ce12-c464-020077ba5d46/kubernetes-\\ndynamic-pvc-74a498d6-3929-47e8-8c02-078c1ece4d78.vmdk\\n    FSType:             ext4\\n    StoragePolicyName:  vSAN Default Storage Policy\\nEvents:                 <none>', metadata={'source': './PDFS/Concepts.pdf', 'page': 305}),\n",
       " Document(page_content='The finalizer external-provisioner.volume.kubernetes.io/finalizer  is added for CSI volumes. The\\nfollowing is an example:\\nName:            pvc-2f0bab97-85a8-4552-8044-eb8be45cf48d\\nLabels:          <none>\\nAnnotations:     pv.kubernetes.io/provisioned-by: csi.vsphere.vmware.com\\nFinalizers:      [kubernetes.io/pv-protection external-provisioner.volume.kubernetes.io/finalizer ]\\nStorageClass:    fast\\nStatus:          Bound\\nClaim:           demo-app/nginx-logs\\nReclaim Policy:  Delete\\nAccess Modes:    RWO\\nVolumeMode:      Filesystem\\nCapacity:        200Mi\\nNode Affinity:   <none>\\nMessage:         \\nSource:\\n    Type:              CSI (a Container Storage Interface (CSI) volume source )\\n    Driver:            csi.vsphere.vmware.com\\n    FSType:            ext4\\n    VolumeHandle:      44830fa8-79b4-406b-8b58-621ba25353fd\\n    ReadOnly:          false\\n    VolumeAttributes:      storage.kubernetes.io/csiProvisionerIdentity =1648442357185-8081-\\ncsi.vsphere.vmware.com\\n                           type=vSphere CNS Block Volume\\nEvents:                <none>\\nWhen the CSIMigration{provider}  feature flag is enabled for a specific in-tree volume plugin,\\nthe kubernetes.io/pv-controller  finalizer is replaced by the external-\\nprovisioner.volume.kubernetes.io/finalizer  finalizer.\\nReserving a PersistentVolume\\nThe control plane can bind PersistentVolumeClaims to matching PersistentVolumes  in the\\ncluster. However, if you want a PVC to bind to a specific PV, you need to pre-bind them.\\nBy specifying a PersistentVolume in a PersistentVolumeClaim, you declare a binding between\\nthat specific PV and PVC. If the PersistentVolume exists and has not reserved\\nPersistentVolumeClaims through its claimRef  field, then the PersistentVolume and\\nPersistentVolumeClaim will be bound.\\nThe binding happens regardless of some volume matching criteria, including node affinity. The\\ncontrol plane still checks that storage class , access modes, and requested storage size are valid.\\napiVersion : v1\\nkind: PersistentVolumeClaim\\nmetadata :\\n  name : foo-pvc\\n  namespace : foo\\nspec:\\n  storageClassName : \"\" \\n# Empty string must be explicitly set otherwise default StorageClass will be set', metadata={'source': './PDFS/Concepts.pdf', 'page': 306}),\n",
       " Document(page_content='volumeName : foo-pv\\n  ...\\nThis method does not guarantee any binding privileges to the PersistentVolume. If other\\nPersistentVolumeClaims could use the PV that you specify, you first need to reserve that\\nstorage volume. Specify the relevant PersistentVolumeClaim in the claimRef  field of the PV so\\nthat other PVCs can not bind to it.\\napiVersion : v1\\nkind: PersistentVolume\\nmetadata :\\n  name : foo-pv\\nspec:\\n  storageClassName : \"\"\\n  claimRef :\\n    name : foo-pvc\\n    namespace : foo\\n  ...\\nThis is useful if you want to consume PersistentVolumes that have their claimPolicy  set to \\nRetain , including cases where you are reusing an existing PV.\\nExpanding Persistent Volumes Claims\\nFEATURE STATE:  Kubernetes v1.24 [stable]\\nSupport for expanding PersistentVolumeClaims (PVCs) is enabled by default. You can expand\\nthe following types of volumes:\\nazureFile (deprecated)\\ncsi\\nflexVolume (deprecated)\\ngcePersistentDisk (deprecated)\\nrbd (deprecated)\\nportworxVolume (deprecated)\\nYou can only expand a PVC if its storage class\\'s allowVolumeExpansion  field is set to true.\\napiVersion : storage.k8s.io/v1\\nkind: StorageClass\\nmetadata :\\n  name : example-vol-default\\nprovisioner : vendor-name.example/magicstorage\\nparameters :\\n  resturl : \"http://192.168.10.100:8080\"\\n  restuser : \"\"\\n  secretNamespace : \"\"\\n  secretName : \"\"\\nallowVolumeExpansion : true\\nTo request a larger volume for a PVC, edit the PVC object and specify a larger size. This triggers\\nexpansion of the volume that backs the underlying PersistentVolume. A new PersistentVolume\\nis never created to satisfy the claim. Instead, an existing volume is resized.• \\n• \\n• \\n• \\n• \\n•', metadata={'source': './PDFS/Concepts.pdf', 'page': 307}),\n",
       " Document(page_content=\"Warning:  Directly editing the size of a PersistentVolume can prevent an automatic resize of\\nthat volume. If you edit the capacity of a PersistentVolume, and then edit the .spec  of a\\nmatching PersistentVolumeClaim to make the size of the PersistentVolumeClaim match the\\nPersistentVolume, then no storage resize happens. The Kubernetes control plane will see that\\nthe desired state of both resources matches, conclude that the backing volume size has been\\nmanually increased and that no resize is necessary.\\nCSI Volume expansion\\nFEATURE STATE:  Kubernetes v1.24 [stable]\\nSupport for expanding CSI volumes is enabled by default but it also requires a specific CSI\\ndriver to support volume expansion. Refer to documentation of the specific CSI driver for more\\ninformation.\\nResizing a volume containing a file system\\nYou can only resize volumes containing a file system if the file system is XFS, Ext3, or Ext4.\\nWhen a volume contains a file system, the file system is only resized when a new Pod is using\\nthe PersistentVolumeClaim in ReadWrite  mode. File system expansion is either done when a\\nPod is starting up or when a Pod is running and the underlying file system supports online\\nexpansion.\\nFlexVolumes (deprecated since Kubernetes v1.23) allow resize if the driver is configured with\\nthe RequiresFSResize  capability to true. The FlexVolume can be resized on Pod restart.\\nResizing an in-use PersistentVolumeClaim\\nFEATURE STATE:  Kubernetes v1.24 [stable]\\nIn this case, you don't need to delete and recreate a Pod or deployment that is using an existing\\nPVC. Any in-use PVC automatically becomes available to its Pod as soon as its file system has\\nbeen expanded. This feature has no effect on PVCs that are not in use by a Pod or deployment.\\nYou must create a Pod that uses the PVC before the expansion can complete.\\nSimilar to other volume types - FlexVolume volumes can also be expanded when in-use by a\\nPod.\\nNote:  FlexVolume resize is possible only when the underlying driver supports resize.\\nNote:  Expanding EBS volumes is a time-consuming operation. Also, there is a per-volume\\nquota of one modification every 6 hours.\\nRecovering from Failure when Expanding Volumes\\nIf a user specifies a new size that is too big to be satisfied by underlying storage system,\\nexpansion of PVC will be continuously retried until user or cluster administrator takes some\\naction. This can be undesirable and hence Kubernetes provides following methods of recovering\\nfrom such failures.\\nManually with Cluster Administrator access\\nBy requesting expansion to smaller size• \\n•\", metadata={'source': './PDFS/Concepts.pdf', 'page': 308}),\n",
       " Document(page_content=\"If expanding underlying storage fails, the cluster administrator can manually recover the\\nPersistent Volume Claim (PVC) state and cancel the resize requests. Otherwise, the resize\\nrequests are continuously retried by the controller without administrator intervention.\\nMark the PersistentVolume(PV) that is bound to the PersistentVolumeClaim(PVC) with \\nRetain  reclaim policy.\\nDelete the PVC. Since PV has Retain  reclaim policy - we will not lose any data when we\\nrecreate the PVC.\\nDelete the claimRef  entry from PV specs, so as new PVC can bind to it. This should make\\nthe PV Available .\\nRe-create the PVC with smaller size than PV and set volumeName  field of the PVC to the\\nname of the PV. This should bind new PVC to existing PV.\\nDon't forget to restore the reclaim policy of the PV.\\nFEATURE STATE:  Kubernetes v1.23 [alpha]\\nNote:  Recovery from failing PVC expansion by users is available as an alpha feature since\\nKubernetes 1.23. The RecoverVolumeExpansionFailure  feature must be enabled for this feature\\nto work. Refer to the feature gate  documentation for more information.\\nIf the feature gates RecoverVolumeExpansionFailure  is enabled in your cluster, and expansion\\nhas failed for a PVC, you can retry expansion with a smaller size than the previously requested\\nvalue. To request a new expansion attempt with a smaller proposed size, edit .spec.resources  for\\nthat PVC and choose a value that is less than the value you previously tried. This is useful if\\nexpansion to a higher value did not succeed because of capacity constraint. If that has\\nhappened, or you suspect that it might have, you can retry expansion by specifying a size that\\nis within the capacity limits of underlying storage provider. You can monitor status of resize\\noperation by watching .status.allocatedResourceStatuses  and events on the PVC.\\nNote that, although you can specify a lower amount of storage than what was requested\\npreviously, the new value must still be higher than .status.capacity . Kubernetes does not\\nsupport shrinking a PVC to less than its current size.\\nTypes of Persistent Volumes\\nPersistentVolume types are implemented as plugins. Kubernetes currently supports the\\nfollowing plugins:\\ncsi - Container Storage Interface (CSI)\\nfc - Fibre Channel (FC) storage\\nhostPath  - HostPath volume (for single node testing only; WILL NOT WORK in a multi-\\nnode cluster; consider using local  volume instead)\\niscsi - iSCSI (SCSI over IP) storage\\nlocal  - local storage devices mounted on nodes.\\nnfs - Network File System (NFS) storage\\nThe following types of PersistentVolume are deprecated. This means that support is still\\navailable but will be removed in a future Kubernetes release.\\nazureFile  - Azure File ( deprecated  in v1.21)\\nflexVolume  - FlexVolume ( deprecated  in v1.23)\\ngcePersistentDisk  - GCE Persistent Disk ( deprecated  in v1.17)\\nportworxVolume  - Portworx volume ( deprecated  in v1.25)\\nvsphereVolume  - vSphere VMDK volume ( deprecated  in v1.19)1. \\n2. \\n3. \\n4. \\n5. \\n• \\n• \\n• \\n• \\n• \\n• \\n• \\n• \\n• \\n• \\n•\", metadata={'source': './PDFS/Concepts.pdf', 'page': 309}),\n",
       " Document(page_content=\"cephfs  - CephFS volume ( deprecated  in v1.28)\\nrbd - Rados Block Device (RBD) volume ( deprecated  in v1.28)\\nOlder versions of Kubernetes also supported the following in-tree PersistentVolume types:\\nawsElasticBlockStore  - AWS Elastic Block Store (EBS) ( not available  in v1.27)\\nazureDisk  - Azure Disk ( not available  in v1.27)\\ncinder  - Cinder (OpenStack block storage) ( not available  in v1.26)\\nphotonPersistentDisk  - Photon controller persistent disk. ( not available  starting v1.15)\\nscaleIO  - ScaleIO volume. ( not available  starting v1.21)\\nflocker  - Flocker storage. ( not available  starting v1.25)\\nquobyte  - Quobyte volume. ( not available  starting v1.25)\\nstorageos  - StorageOS volume. ( not available  starting v1.25)\\nPersistent Volumes\\nEach PV contains a spec and status, which is the specification and status of the volume. The\\nname of a PersistentVolume object must be a valid DNS subdomain name .\\napiVersion : v1\\nkind: PersistentVolume\\nmetadata :\\n  name : pv0003\\nspec:\\n  capacity :\\n    storage : 5Gi\\n  volumeMode : Filesystem\\n  accessModes :\\n    - ReadWriteOnce\\n  persistentVolumeReclaimPolicy : Recycle\\n  storageClassName : slow\\n  mountOptions :\\n    - hard\\n    - nfsvers=4.1\\n  nfs:\\n    path: /tmp\\n    server : 172.17.0.2\\nNote:  Helper programs relating to the volume type may be required for consumption of a\\nPersistentVolume within a cluster. In this example, the PersistentVolume is of type NFS and the\\nhelper program /sbin/mount.nfs is required to support the mounting of NFS filesystems.\\nCapacity\\nGenerally, a PV will have a specific storage capacity. This is set using the PV's capacity\\nattribute. Read the glossary term Quantity  to understand the units expected by capacity .\\nCurrently, storage size is the only resource that can be set or requested. Future attributes may\\ninclude IOPS, throughput, etc.• \\n• \\n• \\n• \\n• \\n• \\n• \\n• \\n• \\n•\", metadata={'source': './PDFS/Concepts.pdf', 'page': 310}),\n",
       " Document(page_content=\"Volume Mode\\nFEATURE STATE:  Kubernetes v1.18 [stable]\\nKubernetes supports two volumeModes  of PersistentVolumes: Filesystem  and Block .\\nvolumeMode  is an optional API parameter. Filesystem  is the default mode used when \\nvolumeMode  parameter is omitted.\\nA volume with volumeMode: Filesystem  is mounted  into Pods into a directory. If the volume is\\nbacked by a block device and the device is empty, Kubernetes creates a filesystem on the device\\nbefore mounting it for the first time.\\nYou can set the value of volumeMode  to Block  to use a volume as a raw block device. Such\\nvolume is presented into a Pod as a block device, without any filesystem on it. This mode is\\nuseful to provide a Pod the fastest possible way to access a volume, without any filesystem\\nlayer between the Pod and the volume. On the other hand, the application running in the Pod\\nmust know how to handle a raw block device. See Raw Block Volume Support  for an example\\non how to use a volume with volumeMode: Block  in a Pod.\\nAccess Modes\\nA PersistentVolume can be mounted on a host in any way supported by the resource provider.\\nAs shown in the table below, providers will have different capabilities and each PV's access\\nmodes are set to the specific modes supported by that particular volume. For example, NFS can\\nsupport multiple read/write clients, but a specific NFS PV might be exported on the server as\\nread-only. Each PV gets its own set of access modes describing that specific PV's capabilities.\\nThe access modes are:\\nReadWriteOnce\\nthe volume can be mounted as read-write by a single node. ReadWriteOnce access mode\\nstill can allow multiple pods to access the volume when the pods are running on the same\\nnode.\\nReadOnlyMany\\nthe volume can be mounted as read-only by many nodes.\\nReadWriteMany\\nthe volume can be mounted as read-write by many nodes.\\nReadWriteOncePod\\nFEATURE STATE:  Kubernetes v1.27 [beta]\\nthe volume can be mounted as read-write by a single Pod. Use ReadWriteOncePod access\\nmode if you want to ensure that only one pod across the whole cluster can read that PVC\\nor write to it. This is only supported for CSI volumes and Kubernetes version 1.22+.\\nThe blog article Introducing Single Pod Access Mode for PersistentVolumes  covers this in more\\ndetail.\\nIn the CLI, the access modes are abbreviated to:\\nRWO - ReadWriteOnce\\nROX - ReadOnlyMany\\nRWX - ReadWriteMany\\nRWOP - ReadWriteOncePod• \\n• \\n• \\n•\", metadata={'source': './PDFS/Concepts.pdf', 'page': 311}),\n",
       " Document(page_content=\"Note:  Kubernetes uses volume access modes to match PersistentVolumeClaims and\\nPersistentVolumes. In some cases, the volume access modes also constrain where the\\nPersistentVolume can be mounted. Volume access modes do not enforce write protection once\\nthe storage has been mounted. Even if the access modes are specified as ReadWriteOnce,\\nReadOnlyMany, or ReadWriteMany, they don't set any constraints on the volume. For example,\\neven if a PersistentVolume is created as ReadOnlyMany, it is no guarantee that it will be read-\\nonly. If the access modes are specified as ReadWriteOncePod, the volume is constrained and can\\nbe mounted on only a single Pod.\\nImportant!  A volume can only be mounted using one access mode at a time, even\\nif it supports many. For example, a GCEPersistentDisk can be mounted as\\nReadWriteOnce by a single node or ReadOnlyMany by many nodes, but not at the\\nsame time.\\nVolume Plugin ReadWriteOnce ReadOnlyMany ReadWriteMany ReadWriteOncePod\\nAzureFile -\\nCephFS -\\nCSIdepends on the\\ndriverdepends on the\\ndriverdepends on the\\ndriverdepends on the\\ndriver\\nFC - -\\nFlexVolume depends on the\\ndriver-\\nGCEPersistentDisk - -\\nGlusterfs -\\nHostPath - - -\\niSCSI - -\\nNFS -\\nRBD - -\\nVsphereVolume -- (works when\\nPods are\\ncollocated)-\\nPortworxVolume - -\\nClass\\nA PV can have a class, which is specified by setting the storageClassName  attribute to the name\\nof a StorageClass . A PV of a particular class can only be bound to PVCs requesting that class. A\\nPV with no storageClassName  has no class and can only be bound to PVCs that request no\\nparticular class.\\nIn the past, the annotation volume.beta.kubernetes.io/storage-class  was used instead of the \\nstorageClassName  attribute. This annotation is still working; however, it will become fully\\ndeprecated in a future Kubernetes release.\\nReclaim Policy\\nCurrent reclaim policies are:\\nRetain -- manual reclamation\\nRecycle -- basic scrub ( rm -rf /thevolume/* )\\nDelete -- associated storage asset such as AWS EBS or GCE PD volume is deleted• \\n• \\n•\", metadata={'source': './PDFS/Concepts.pdf', 'page': 312}),\n",
       " Document(page_content='Currently, only NFS and HostPath support recycling. AWS EBS and GCE PD volumes support\\ndeletion.\\nMount Options\\nA Kubernetes administrator can specify additional mount options for when a Persistent Volume\\nis mounted on a node.\\nNote:  Not all Persistent Volume types support mount options.\\nThe following volume types support mount options:\\nazureFile\\ncephfs  (deprecated  in v1.28)\\ncinder  (deprecated  in v1.18)\\ngcePersistentDisk  (deprecated  in v1.28)\\niscsi\\nnfs\\nrbd (deprecated  in v1.28)\\nvsphereVolume\\nMount options are not validated. If a mount option is invalid, the mount fails.\\nIn the past, the annotation volume.beta.kubernetes.io/mount-options  was used instead of the \\nmountOptions  attribute. This annotation is still working; however, it will become fully\\ndeprecated in a future Kubernetes release.\\nNode Affinity\\nNote:  For most volume types, you do not need to set this field. It is automatically populated for \\nGCE PD  volume block types. You need to explicitly set this for local  volumes.\\nA PV can specify node affinity to define constraints that limit what nodes this volume can be\\naccessed from. Pods that use a PV will only be scheduled to nodes that are selected by the node\\naffinity. To specify node affinity, set nodeAffinity  in the .spec  of a PV. The PersistentVolume  API\\nreference has more details on this field.\\nPhase\\nA PersistentVolume will be in one of the following phases:\\nAvailable\\na free resource that is not yet bound to a claim\\nBound\\nthe volume is bound to a claim\\nReleased\\nthe claim has been deleted, but the associated storage resource is not yet reclaimed by the\\ncluster\\nFailed\\nthe volume has failed its (automated) reclamation\\nYou can see the name of the PVC bound to the PV using kubectl describe persistentvolume \\n<name> .• \\n• \\n• \\n• \\n• \\n• \\n• \\n•', metadata={'source': './PDFS/Concepts.pdf', 'page': 313}),\n",
       " Document(page_content='Phase transition timestamp\\nFEATURE STATE:  Kubernetes v1.28 [alpha]\\nThe .status  field for a PersistentVolume can include an alpha lastPhaseTransitionTime  field. This\\nfield records the timestamp of when the volume last transitioned its phase. For newly created\\nvolumes the phase is set to Pending  and lastPhaseTransitionTime  is set to the current time.\\nNote:  You need to enable the PersistentVolumeLastPhaseTransitionTime  feature gate  to use or\\nsee the lastPhaseTransitionTime  field.\\nPersistentVolumeClaims\\nEach PVC contains a spec and status, which is the specification and status of the claim. The\\nname of a PersistentVolumeClaim object must be a valid DNS subdomain name .\\napiVersion : v1\\nkind: PersistentVolumeClaim\\nmetadata :\\n  name : myclaim\\nspec:\\n  accessModes :\\n    - ReadWriteOnce\\n  volumeMode : Filesystem\\n  resources :\\n    requests :\\n      storage : 8Gi\\n  storageClassName : slow\\n  selector :\\n    matchLabels :\\n      release : \"stable\"\\n    matchExpressions :\\n      - {key: environment, operator: In, values : [dev]}\\nAccess Modes\\nClaims use the same conventions as volumes  when requesting storage with specific access\\nmodes.\\nVolume Modes\\nClaims use the same convention as volumes  to indicate the consumption of the volume as either\\na filesystem or block device.\\nResources\\nClaims, like Pods, can request specific quantities of a resource. In this case, the request is for\\nstorage. The same resource model  applies to both volumes and claims.', metadata={'source': './PDFS/Concepts.pdf', 'page': 314}),\n",
       " Document(page_content='Selector\\nClaims can specify a label selector  to further filter the set of volumes. Only the volumes whose\\nlabels match the selector can be bound to the claim. The selector can consist of two fields:\\nmatchLabels  - the volume must have a label with this value\\nmatchExpressions  - a list of requirements made by specifying key, list of values, and\\noperator that relates the key and values. Valid operators include In, NotIn, Exists, and\\nDoesNotExist.\\nAll of the requirements, from both matchLabels  and matchExpressions , are ANDed together –\\nthey must all be satisfied in order to match.\\nClass\\nA claim can request a particular class by specifying the name of a StorageClass  using the\\nattribute storageClassName . Only PVs of the requested class, ones with the same \\nstorageClassName  as the PVC, can be bound to the PVC.\\nPVCs don\\'t necessarily have to request a class. A PVC with its storageClassName  set equal to \"\"\\nis always interpreted to be requesting a PV with no class, so it can only be bound to PVs with\\nno class (no annotation or one set equal to \"\"). A PVC with no storageClassName  is not quite\\nthe same and is treated differently by the cluster, depending on whether the \\nDefaultStorageClass  admission plugin  is turned on.\\nIf the admission plugin is turned on, the administrator may specify a default\\nStorageClass. All PVCs that have no storageClassName  can be bound only to PVs of that\\ndefault. Specifying a default StorageClass is done by setting the annotation \\nstorageclass.kubernetes.io/is-default-class  equal to true in a StorageClass object. If the\\nadministrator does not specify a default, the cluster responds to PVC creation as if the\\nadmission plugin were turned off. If more than one default StorageClass is specified, the\\nnewest default is used when the PVC is dynamically provisioned.\\nIf the admission plugin is turned off, there is no notion of a default StorageClass. All\\nPVCs that have storageClassName  set to \"\" can be bound only to PVs that have \\nstorageClassName  also set to \"\". However, PVCs with missing storageClassName  can be\\nupdated later once default StorageClass becomes available. If the PVC gets updated it will\\nno longer bind to PVs that have storageClassName  also set to \"\".\\nSee retroactive default StorageClass assignment  for more details.\\nDepending on installation method, a default StorageClass may be deployed to a Kubernetes\\ncluster by addon manager during installation.\\nWhen a PVC specifies a selector  in addition to requesting a StorageClass, the requirements are\\nANDed together: only a PV of the requested class and with the requested labels may be bound\\nto the PVC.\\nNote:  Currently, a PVC with a non-empty selector  can\\'t have a PV dynamically provisioned for\\nit.\\nIn the past, the annotation volume.beta.kubernetes.io/storage-class  was used instead of \\nstorageClassName  attribute. This annotation is still working; however, it won\\'t be supported in\\na future Kubernetes release.• \\n• \\n• \\n•', metadata={'source': './PDFS/Concepts.pdf', 'page': 315}),\n",
       " Document(page_content='Retroactive default StorageClass assignment\\nFEATURE STATE:  Kubernetes v1.28 [stable]\\nYou can create a PersistentVolumeClaim without specifying a storageClassName  for the new\\nPVC, and you can do so even when no default StorageClass exists in your cluster. In this case,\\nthe new PVC creates as you defined it, and the storageClassName  of that PVC remains unset\\nuntil default becomes available.\\nWhen a default StorageClass becomes available, the control plane identifies any existing PVCs\\nwithout storageClassName . For the PVCs that either have an empty value for \\nstorageClassName  or do not have this key, the control plane then updates those PVCs to set \\nstorageClassName  to match the new default StorageClass. If you have an existing PVC where\\nthe storageClassName  is \"\", and you configure a default StorageClass, then this PVC will not get\\nupdated.\\nIn order to keep binding to PVs with storageClassName  set to \"\" (while a default StorageClass is\\npresent), you need to set the storageClassName  of the associated PVC to \"\".\\nThis behavior helps administrators change default StorageClass by removing the old one first\\nand then creating or setting another one. This brief window while there is no default causes\\nPVCs without storageClassName  created at that time to not have any default, but due to the\\nretroactive default StorageClass assignment this way of changing defaults is safe.\\nClaims As Volumes\\nPods access storage by using the claim as a volume. Claims must exist in the same namespace\\nas the Pod using the claim. The cluster finds the claim in the Pod\\'s namespace and uses it to get\\nthe PersistentVolume backing the claim. The volume is then mounted to the host and into the\\nPod.\\napiVersion : v1\\nkind: Pod\\nmetadata :\\n  name : mypod\\nspec:\\n  containers :\\n    - name : myfrontend\\n      image : nginx\\n      volumeMounts :\\n      - mountPath : \"/var/www/html\"\\n        name : mypd\\n  volumes :\\n    - name : mypd\\n      persistentVolumeClaim :\\n        claimName : myclaim\\nA Note on Namespaces\\nPersistentVolumes binds are exclusive, and since PersistentVolumeClaims are namespaced\\nobjects, mounting claims with \"Many\" modes ( ROX , RWX ) is only possible within one\\nnamespace.', metadata={'source': './PDFS/Concepts.pdf', 'page': 316}),\n",
       " Document(page_content='PersistentVolumes typed hostPath\\nA hostPath  PersistentVolume uses a file or directory on the Node to emulate network-attached\\nstorage. See an example of hostPath  typed volume .\\nRaw Block Volume Support\\nFEATURE STATE:  Kubernetes v1.18 [stable]\\nThe following volume plugins support raw block volumes, including dynamic provisioning\\nwhere applicable:\\nCSI\\nFC (Fibre Channel)\\nGCEPersistentDisk (deprecated)\\niSCSI\\nLocal volume\\nOpenStack Cinder\\nRBD (deprecated)\\nRBD (Ceph Block Device; deprecated)\\nVsphereVolume\\nPersistentVolume using a Raw Block Volume\\napiVersion : v1\\nkind: PersistentVolume\\nmetadata :\\n  name : block-pv\\nspec:\\n  capacity :\\n    storage : 10Gi\\n  accessModes :\\n    - ReadWriteOnce\\n  volumeMode : Block\\n  persistentVolumeReclaimPolicy : Retain\\n  fc:\\n    targetWWNs : [\"50060e801049cfd1\" ]\\n    lun: 0\\n    readOnly : false\\nPersistentVolumeClaim requesting a Raw Block Volume\\napiVersion : v1\\nkind: PersistentVolumeClaim\\nmetadata :\\n  name : block-pvc\\nspec:\\n  accessModes :\\n    - ReadWriteOnce\\n  volumeMode : Block\\n  resources :• \\n• \\n• \\n• \\n• \\n• \\n• \\n• \\n•', metadata={'source': './PDFS/Concepts.pdf', 'page': 317}),\n",
       " Document(page_content='requests :\\n      storage : 10Gi\\nPod specification adding Raw Block Device path in container\\napiVersion : v1\\nkind: Pod\\nmetadata :\\n  name : pod-with-block-volume\\nspec:\\n  containers :\\n    - name : fc-container\\n      image : fedora:26\\n      command : [\"/bin/sh\" , \"-c\"]\\n      args: [ \"tail -f /dev/null\"  ]\\n      volumeDevices :\\n        - name : data\\n          devicePath : /dev/xvda\\n  volumes :\\n    - name : data\\n      persistentVolumeClaim :\\n        claimName : block-pvc\\nNote:  When adding a raw block device for a Pod, you specify the device path in the container\\ninstead of a mount path.\\nBinding Block Volumes\\nIf a user requests a raw block volume by indicating this using the volumeMode  field in the\\nPersistentVolumeClaim spec, the binding rules differ slightly from previous releases that didn\\'t\\nconsider this mode as part of the spec. Listed is a table of possible combinations the user and\\nadmin might specify for requesting a raw block device. The table indicates if the volume will be\\nbound or not given the combinations: Volume binding matrix for statically provisioned\\nvolumes:\\nPV volumeMode PVC volumeMode Result\\nunspecified unspecified BIND\\nunspecified Block NO BIND\\nunspecified Filesystem BIND\\nBlock unspecified NO BIND\\nBlock Block BIND\\nBlock Filesystem NO BIND\\nFilesystem Filesystem BIND\\nFilesystem Block NO BIND\\nFilesystem unspecified BIND\\nNote:  Only statically provisioned volumes are supported for alpha release. Administrators\\nshould take care to consider these values when working with raw block devices.', metadata={'source': './PDFS/Concepts.pdf', 'page': 318}),\n",
       " Document(page_content='Volume Snapshot and Restore Volume from Snapshot\\nSupport\\nFEATURE STATE:  Kubernetes v1.20 [stable]\\nVolume snapshots only support the out-of-tree CSI volume plugins. For details, see Volume\\nSnapshots . In-tree volume plugins are deprecated. You can read about the deprecated volume\\nplugins in the Volume Plugin FAQ .\\nCreate a PersistentVolumeClaim from a Volume Snapshot\\napiVersion : v1\\nkind: PersistentVolumeClaim\\nmetadata :\\n  name : restore-pvc\\nspec:\\n  storageClassName : csi-hostpath-sc\\n  dataSource :\\n    name : new-snapshot-test\\n    kind: VolumeSnapshot\\n    apiGroup : snapshot.storage.k8s.io\\n  accessModes :\\n    - ReadWriteOnce\\n  resources :\\n    requests :\\n      storage : 10Gi\\nVolume Cloning\\nVolume Cloning  only available for CSI volume plugins.\\nCreate PersistentVolumeClaim from an existing PVC\\napiVersion : v1\\nkind: PersistentVolumeClaim\\nmetadata :\\n  name : cloned-pvc\\nspec:\\n  storageClassName : my-csi-plugin\\n  dataSource :\\n    name : existing-src-pvc-name\\n    kind: PersistentVolumeClaim\\n  accessModes :\\n    - ReadWriteOnce\\n  resources :\\n    requests :\\n      storage : 10Gi', metadata={'source': './PDFS/Concepts.pdf', 'page': 319}),\n",
       " Document(page_content='Volume populators and data sources\\nFEATURE STATE:  Kubernetes v1.24 [beta]\\nKubernetes supports custom volume populators. To use custom volume populators, you must\\nenable the AnyVolumeDataSource  feature gate  for the kube-apiserver and kube-controller-\\nmanager.\\nVolume populators take advantage of a PVC spec field called dataSourceRef . Unlike the \\ndataSource  field, which can only contain either a reference to another PersistentVolumeClaim\\nor to a VolumeSnapshot, the dataSourceRef  field can contain a reference to any object in the\\nsame namespace, except for core objects other than PVCs. For clusters that have the feature\\ngate enabled, use of the dataSourceRef  is preferred over dataSource .\\nCross namespace data sources\\nFEATURE STATE:  Kubernetes v1.26 [alpha]\\nKubernetes supports cross namespace volume data sources. To use cross namespace volume\\ndata sources, you must enable the AnyVolumeDataSource  and \\nCrossNamespaceVolumeDataSource  feature gates  for the kube-apiserver and kube-controller-\\nmanager. Also, you must enable the CrossNamespaceVolumeDataSource  feature gate for the csi-\\nprovisioner.\\nEnabling the CrossNamespaceVolumeDataSource  feature gate allows you to specify a\\nnamespace in the dataSourceRef field.\\nNote:  When you specify a namespace for a volume data source, Kubernetes checks for a\\nReferenceGrant in the other namespace before accepting the reference. ReferenceGrant is part\\nof the gateway.networking.k8s.io  extension APIs. See ReferenceGrant  in the Gateway API\\ndocumentation for details. This means that you must extend your Kubernetes cluster with at\\nleast ReferenceGrant from the Gateway API before you can use this mechanism.\\nData source references\\nThe dataSourceRef  field behaves almost the same as the dataSource  field. If one is specified\\nwhile the other is not, the API server will give both fields the same value. Neither field can be\\nchanged after creation, and attempting to specify different values for the two fields will result in\\na validation error. Therefore the two fields will always have the same contents.\\nThere are two differences between the dataSourceRef  field and the dataSource  field that users\\nshould be aware of:\\nThe dataSource  field ignores invalid values (as if the field was blank) while the \\ndataSourceRef  field never ignores values and will cause an error if an invalid value is\\nused. Invalid values are any core object (objects with no apiGroup) except for PVCs.\\nThe dataSourceRef  field may contain different types of objects, while the dataSource  field\\nonly allows PVCs and VolumeSnapshots.• \\n•', metadata={'source': './PDFS/Concepts.pdf', 'page': 320}),\n",
       " Document(page_content=\"When the CrossNamespaceVolumeDataSource  feature is enabled, there are additional\\ndifferences:\\nThe dataSource  field only allows local objects, while the dataSourceRef  field allows\\nobjects in any namespaces.\\nWhen namespace is specified, dataSource  and dataSourceRef  are not synced.\\nUsers should always use dataSourceRef  on clusters that have the feature gate enabled, and fall\\nback to dataSource  on clusters that do not. It is not necessary to look at both fields under any\\ncircumstance. The duplicated values with slightly different semantics exist only for backwards\\ncompatibility. In particular, a mixture of older and newer controllers are able to interoperate\\nbecause the fields are the same.\\nUsing volume populators\\nVolume populators are controllers  that can create non-empty volumes, where the contents of\\nthe volume are determined by a Custom Resource. Users create a populated volume by referring\\nto a Custom Resource using the dataSourceRef  field:\\napiVersion : v1\\nkind: PersistentVolumeClaim\\nmetadata :\\n  name : populated-pvc\\nspec:\\n  dataSourceRef :\\n    name : example-name\\n    kind: ExampleDataSource\\n    apiGroup : example.storage.k8s.io\\n  accessModes :\\n    - ReadWriteOnce\\n  resources :\\n    requests :\\n      storage : 10Gi\\nBecause volume populators are external components, attempts to create a PVC that uses one\\ncan fail if not all the correct components are installed. External controllers should generate\\nevents on the PVC to provide feedback on the status of the creation, including warnings if the\\nPVC cannot be created due to some missing component.\\nYou can install the alpha volume data source validator  controller into your cluster. That\\ncontroller generates warning Events on a PVC in the case that no populator is registered to\\nhandle that kind of data source. When a suitable populator is installed for a PVC, it's the\\nresponsibility of that populator controller to report Events that relate to volume creation and\\nissues during the process.\\nUsing a cross-namespace volume data source\\nFEATURE STATE:  Kubernetes v1.26 [alpha]\\nCreate a ReferenceGrant to allow the namespace owner to accept the reference. You define a\\npopulated volume by specifying a cross namespace volume data source using the dataSourceRef\\nfield. You must already have a valid ReferenceGrant in the source namespace:• \\n•\", metadata={'source': './PDFS/Concepts.pdf', 'page': 321}),\n",
       " Document(page_content='apiVersion : gateway.networking.k8s.io/v1beta1\\nkind: ReferenceGrant\\nmetadata :\\n  name : allow-ns1-pvc\\n  namespace : default\\nspec:\\n  from :\\n  - group : \"\"\\n    kind: PersistentVolumeClaim\\n    namespace : ns1\\n  to:\\n  - group : snapshot.storage.k8s.io\\n    kind: VolumeSnapshot\\n    name : new-snapshot-demo\\napiVersion : v1\\nkind: PersistentVolumeClaim\\nmetadata :\\n  name : foo-pvc\\n  namespace : ns1\\nspec:\\n  storageClassName : example\\n  accessModes :\\n  - ReadWriteOnce\\n  resources :\\n    requests :\\n      storage : 1Gi\\n  dataSourceRef :\\n    apiGroup : snapshot.storage.k8s.io\\n    kind: VolumeSnapshot\\n    name : new-snapshot-demo\\n    namespace : default\\n  volumeMode : Filesystem\\nWriting Portable Configuration\\nIf you\\'re writing configuration templates or examples that run on a wide range of clusters and\\nneed persistent storage, it is recommended that you use the following pattern:\\nInclude PersistentVolumeClaim objects in your bundle of config (alongside Deployments,\\nConfigMaps, etc).\\nDo not include PersistentVolume objects in the config, since the user instantiating the\\nconfig may not have permission to create PersistentVolumes.\\nGive the user the option of providing a storage class name when instantiating the\\ntemplate.\\nIf the user provides a storage class name, put that value into the \\npersistentVolumeClaim.storageClassName  field. This will cause the PVC to match\\nthe right storage class if the cluster has StorageClasses enabled by the admin.\\nIf the user does not provide a storage class name, leave the \\npersistentVolumeClaim.storageClassName  field as nil. This will cause a PV to be\\nautomatically provisioned for the user with the default StorageClass in the cluster.• \\n• \\n• \\n◦ \\n◦', metadata={'source': './PDFS/Concepts.pdf', 'page': 322}),\n",
       " Document(page_content=\"Many cluster environments have a default StorageClass installed, or administrators\\ncan create their own default StorageClass.\\nIn your tooling, watch for PVCs that are not getting bound after some time and surface\\nthis to the user, as this may indicate that the cluster has no dynamic storage support (in\\nwhich case the user should create a matching PV) or the cluster has no storage system (in\\nwhich case the user cannot deploy config requiring PVCs).\\nWhat's next\\nLearn more about Creating a PersistentVolume .\\nLearn more about Creating a PersistentVolumeClaim .\\nRead the Persistent Storage design document .\\nAPI references\\nRead about the APIs described in this page:\\nPersistentVolume\\nPersistentVolumeClaim\\nProjected Volumes\\nThis document describes projected volumes  in Kubernetes. Familiarity with volumes  is\\nsuggested.\\nIntroduction\\nA projected  volume maps several existing volume sources into the same directory.\\nCurrently, the following types of volume sources can be projected:\\nsecret\\ndownwardAPI\\nconfigMap\\nserviceAccountToken\\nAll sources are required to be in the same namespace as the Pod. For more details, see the all-\\nin-one volume  design document.\\nExample configuration with a secret, a downwardAPI, and a configMap\\npods/storage/projected-secret-downwardapi-configmap.yaml  \\napiVersion : v1\\nkind: Pod\\nmetadata :\\n  name : volume-test\\nspec:\\n  containers :\\n  - name : container-test• \\n• \\n• \\n• \\n• \\n• \\n• \\n• \\n• \\n•\", metadata={'source': './PDFS/Concepts.pdf', 'page': 323}),\n",
       " Document(page_content='image : busybox:1.28\\n    command : [\"sleep\" , \"3600\" ]\\n    volumeMounts :\\n    - name : all-in-one\\n      mountPath : \"/projected-volume\"\\n      readOnly : true\\n  volumes :\\n  - name : all-in-one\\n    projected :\\n      sources :\\n      - secret :\\n          name : mysecret\\n          items :\\n            - key: username\\n              path: my-group/my-username\\n      - downwardAPI :\\n          items :\\n            - path: \"labels\"\\n              fieldRef :\\n                fieldPath : metadata.labels\\n            - path: \"cpu_limit\"\\n              resourceFieldRef :\\n                containerName : container-test\\n                resource : limits.cpu\\n      - configMap :\\n          name : myconfigmap\\n          items :\\n            - key: config\\n              path: my-group/my-config\\nExample configuration: secrets with a non-default permission mode set\\npods/storage/projected-secrets-nondefault-permission-mode.yaml  \\napiVersion : v1\\nkind: Pod\\nmetadata :\\n  name : volume-test\\nspec:\\n  containers :\\n  - name : container-test\\n    image : busybox:1.28\\n    command : [\"sleep\" , \"3600\" ]\\n    volumeMounts :\\n    - name : all-in-one\\n      mountPath : \"/projected-volume\"\\n      readOnly : true\\n  volumes :\\n  - name : all-in-one\\n    projected :\\n      sources :\\n      - secret :\\n          name : mysecret', metadata={'source': './PDFS/Concepts.pdf', 'page': 324}),\n",
       " Document(page_content='items :\\n            - key: username\\n              path: my-group/my-username\\n      - secret :\\n          name : mysecret2\\n          items :\\n            - key: password\\n              path: my-group/my-password\\n              mode : 511\\nEach projected volume source is listed in the spec under sources . The parameters are nearly the\\nsame with two exceptions:\\nFor secrets, the secretName  field has been changed to name  to be consistent with\\nConfigMap naming.\\nThe defaultMode  can only be specified at the projected level and not for each volume\\nsource. However, as illustrated above, you can explicitly set the mode  for each individual\\nprojection.\\nserviceAccountToken projected volumes\\nYou can inject the token for the current service account  into a Pod at a specified path. For\\nexample:\\npods/storage/projected-service-account-token.yaml  \\napiVersion : v1\\nkind: Pod\\nmetadata :\\n  name : sa-token-test\\nspec:\\n  containers :\\n  - name : container-test\\n    image : busybox:1.28\\n    command : [\"sleep\" , \"3600\" ]\\n    volumeMounts :\\n    - name : token-vol\\n      mountPath : \"/service-account\"\\n      readOnly : true\\n  serviceAccountName : default\\n  volumes :\\n  - name : token-vol\\n    projected :\\n      sources :\\n      - serviceAccountToken :\\n          audience : api\\n          expirationSeconds : 3600\\n          path: token\\nThe example Pod has a projected volume containing the injected service account token.\\nContainers in this Pod can use that token to access the Kubernetes API server, authenticating\\nwith the identity of the pod\\'s ServiceAccount . The audience  field contains the intended\\naudience of the token. A recipient of the token must identify itself with an identifier specified in• \\n•', metadata={'source': './PDFS/Concepts.pdf', 'page': 325}),\n",
       " Document(page_content=\"the audience of the token, and otherwise should reject the token. This field is optional and it\\ndefaults to the identifier of the API server.\\nThe expirationSeconds  is the expected duration of validity of the service account token. It\\ndefaults to 1 hour and must be at least 10 minutes (600 seconds). An administrator can also limit\\nits maximum value by specifying the --service-account-max-token-expiration  option for the\\nAPI server. The path field specifies a relative path to the mount point of the projected volume.\\nNote:  A container using a projected volume source as a subPath  volume mount will not receive\\nupdates for those volume sources.\\nSecurityContext interactions\\nThe proposal  for file permission handling in projected service account volume enhancement\\nintroduced the projected files having the correct owner permissions set.\\nLinux\\nIn Linux pods that have a projected volume and RunAsUser  set in the Pod SecurityContext , the\\nprojected files have the correct ownership set including container user ownership.\\nWhen all containers in a pod have the same runAsUser  set in their PodSecurityContext  or\\ncontainer SecurityContext , then the kubelet ensures that the contents of the \\nserviceAccountToken  volume are owned by that user, and the token file has its permission\\nmode set to 0600.\\nNote:\\nEphemeral containers  added to a Pod after it is created do not change volume permissions that\\nwere set when the pod was created.\\nIf a Pod's serviceAccountToken  volume permissions were set to 0600 because all other\\ncontainers in the Pod have the same runAsUser , ephemeral containers must use the same \\nrunAsUser  to be able to read the token.\\nWindows\\nIn Windows pods that have a projected volume and RunAsUsername  set in the Pod \\nSecurityContext , the ownership is not enforced due to the way user accounts are managed in\\nWindows. Windows stores and manages local user and group accounts in a database file called\\nSecurity Account Manager (SAM). Each container maintains its own instance of the SAM\\ndatabase, to which the host has no visibility into while the container is running. Windows\\ncontainers are designed to run the user mode portion of the OS in isolation from the host,\\nhence the maintenance of a virtual SAM database. As a result, the kubelet running on the host\\ndoes not have the ability to dynamically configure host file ownership for virtualized container\\naccounts. It is recommended that if files on the host machine are to be shared with the\\ncontainer then they should be placed into their own volume mount outside of C:\\\\.\\nBy default, the projected files will have the following ownership as shown for an example\\nprojected volume file:\\nPS C:\\\\> Get-Acl  C:\\\\var\\\\run\\\\secrets\\\\kubernetes.io\\\\serviceaccount\\\\..2021_08_31_22_22_18. 3182300\\n61\\\\ca.crt | Format-List\", metadata={'source': './PDFS/Concepts.pdf', 'page': 326}),\n",
       " Document(page_content=\"Path   : Microsoft.PowerShell.Core\\\\FileSystem::C:\\n\\\\var\\\\run\\\\secrets\\\\kubernetes.io\\\\serviceaccount\\\\..2021_08_31_22_22_18. 318230061 \\\\ca.crt\\nOwner  : BUILTIN\\\\Administrators\\nGroup  : NT AUTHORITY\\\\SYSTEM\\nAccess : NT AUTHORITY\\\\SYSTEM Allow  FullControl\\n         BUILTIN\\\\Administrators Allow  FullControl\\n         BUILTIN\\\\Users Allow  ReadAndExecute, Synchronize\\nAudit  :\\nSddl   : O:BAG :SYD:AI(A;ID;FA;;;SY)(A;ID;FA;;;BA)(A;ID;0x1200a9;;;BU)\\nThis implies all administrator users like ContainerAdministrator  will have read, write and\\nexecute access while, non-administrator users will have read and execute access.\\nNote:\\nIn general, granting the container access to the host is discouraged as it can open the door for\\npotential security exploits.\\nCreating a Windows Pod with RunAsUser  in it's SecurityContext  will result in the Pod being\\nstuck at ContainerCreating  forever. So it is advised to not use the Linux only RunAsUser  option\\nwith Windows Pods.\\nEphemeral Volumes\\nThis document describes ephemeral volumes  in Kubernetes. Familiarity with volumes  is\\nsuggested, in particular PersistentVolumeClaim and PersistentVolume.\\nSome application need additional storage but don't care whether that data is stored persistently\\nacross restarts. For example, caching services are often limited by memory size and can move\\ninfrequently used data into storage that is slower than memory with little impact on overall\\nperformance.\\nOther applications expect some read-only input data to be present in files, like configuration\\ndata or secret keys.\\nEphemeral volumes  are designed for these use cases. Because volumes follow the Pod's lifetime\\nand get created and deleted along with the Pod, Pods can be stopped and restarted without\\nbeing limited to where some persistent volume is available.\\nEphemeral volumes are specified inline  in the Pod spec, which simplifies application\\ndeployment and management.\\nTypes of ephemeral volumes\\nKubernetes supports several different kinds of ephemeral volumes for different purposes:\\nemptyDir : empty at Pod startup, with storage coming locally from the kubelet base\\ndirectory (usually the root disk) or RAM\\nconfigMap , downwardAPI , secret : inject different kinds of Kubernetes data into a Pod\\nCSI ephemeral volumes : similar to the previous volume kinds, but provided by special CSI\\ndrivers which specifically support this feature• \\n• \\n•\", metadata={'source': './PDFS/Concepts.pdf', 'page': 327}),\n",
       " Document(page_content='generic ephemeral volumes , which can be provided by all storage drivers that also\\nsupport persistent volumes\\nemptyDir , configMap , downwardAPI , secret  are provided as local ephemeral storage . They are\\nmanaged by kubelet on each node.\\nCSI ephemeral volumes must  be provided by third-party CSI storage drivers.\\nGeneric ephemeral volumes can be provided by third-party CSI storage drivers, but also by any\\nother storage driver that supports dynamic provisioning. Some CSI drivers are written\\nspecifically for CSI ephemeral volumes and do not support dynamic provisioning: those then\\ncannot be used for generic ephemeral volumes.\\nThe advantage of using third-party drivers is that they can offer functionality that Kubernetes\\nitself does not support, for example storage with different performance characteristics than the\\ndisk that is managed by kubelet, or injecting different data.\\nCSI ephemeral volumes\\nFEATURE STATE:  Kubernetes v1.25 [stable]\\nNote:  CSI ephemeral volumes are only supported by a subset of CSI drivers. The Kubernetes\\nCSI Drivers list  shows which drivers support ephemeral volumes.\\nConceptually, CSI ephemeral volumes are similar to configMap , downwardAPI  and secret\\nvolume types: the storage is managed locally on each node and is created together with other\\nlocal resources after a Pod has been scheduled onto a node. Kubernetes has no concept of\\nrescheduling Pods anymore at this stage. Volume creation has to be unlikely to fail, otherwise\\nPod startup gets stuck. In particular, storage capacity aware Pod scheduling  is not supported for\\nthese volumes. They are currently also not covered by the storage resource usage limits of a\\nPod, because that is something that kubelet can only enforce for storage that it manages itself.\\nHere\\'s an example manifest for a Pod that uses CSI ephemeral storage:\\nkind: Pod\\napiVersion : v1\\nmetadata :\\n  name : my-csi-app\\nspec:\\n  containers :\\n    - name : my-frontend\\n      image : busybox:1.28\\n      volumeMounts :\\n      - mountPath : \"/data\"\\n        name : my-csi-inline-vol\\n      command : [ \"sleep\" , \"1000000\"  ]\\n  volumes :\\n    - name : my-csi-inline-vol\\n      csi:\\n        driver : inline.storage.kubernetes.io\\n        volumeAttributes :\\n          foo: bar•', metadata={'source': './PDFS/Concepts.pdf', 'page': 328}),\n",
       " Document(page_content='The volumeAttributes  determine what volume is prepared by the driver. These attributes are\\nspecific to each driver and not standardized. See the documentation of each CSI driver for\\nfurther instructions.\\nCSI driver restrictions\\nCSI ephemeral volumes allow users to provide volumeAttributes  directly to the CSI driver as\\npart of the Pod spec. A CSI driver allowing volumeAttributes  that are typically restricted to\\nadministrators is NOT suitable for use in an inline ephemeral volume. For example, parameters\\nthat are normally defined in the StorageClass should not be exposed to users through the use of\\ninline ephemeral volumes.\\nCluster administrators who need to restrict the CSI drivers that are allowed to be used as inline\\nvolumes within a Pod spec may do so by:\\nRemoving Ephemeral  from volumeLifecycleModes  in the CSIDriver spec, which prevents\\nthe driver from being used as an inline ephemeral volume.\\nUsing an admission webhook  to restrict how this driver is used.\\nGeneric ephemeral volumes\\nFEATURE STATE:  Kubernetes v1.23 [stable]\\nGeneric ephemeral volumes are similar to emptyDir  volumes in the sense that they provide a\\nper-pod directory for scratch data that is usually empty after provisioning. But they may also\\nhave additional features:\\nStorage can be local or network-attached.\\nVolumes can have a fixed size that Pods are not able to exceed.\\nVolumes may have some initial data, depending on the driver and parameters.\\nTypical operations on volumes are supported assuming that the driver supports them,\\nincluding snapshotting , cloning , resizing , and storage capacity tracking .\\nExample:\\nkind: Pod\\napiVersion : v1\\nmetadata :\\n  name : my-app\\nspec:\\n  containers :\\n    - name : my-frontend\\n      image : busybox:1.28\\n      volumeMounts :\\n      - mountPath : \"/scratch\"\\n        name : scratch-volume\\n      command : [ \"sleep\" , \"1000000\"  ]\\n  volumes :\\n    - name : scratch-volume\\n      ephemeral :\\n        volumeClaimTemplate :\\n          metadata :\\n            labels :• \\n• \\n• \\n• \\n• \\n•', metadata={'source': './PDFS/Concepts.pdf', 'page': 329}),\n",
       " Document(page_content='type: my-frontend-volume\\n          spec:\\n            accessModes : [ \"ReadWriteOnce\"  ]\\n            storageClassName : \"scratch-storage-class\"\\n            resources :\\n              requests :\\n                storage : 1Gi\\nLifecycle and PersistentVolumeClaim\\nThe key design idea is that the parameters for a volume claim  are allowed inside a volume\\nsource of the Pod. Labels, annotations and the whole set of fields for a PersistentVolumeClaim\\nare supported. When such a Pod gets created, the ephemeral volume controller then creates an\\nactual PersistentVolumeClaim object in the same namespace as the Pod and ensures that the\\nPersistentVolumeClaim gets deleted when the Pod gets deleted.\\nThat triggers volume binding and/or provisioning, either immediately if the StorageClass  uses\\nimmediate volume binding or when the Pod is tentatively scheduled onto a node\\n(WaitForFirstConsumer  volume binding mode). The latter is recommended for generic\\nephemeral volumes because then the scheduler is free to choose a suitable node for the Pod.\\nWith immediate binding, the scheduler is forced to select a node that has access to the volume\\nonce it is available.\\nIn terms of resource ownership , a Pod that has generic ephemeral storage is the owner of the\\nPersistentVolumeClaim(s) that provide that ephemeral storage. When the Pod is deleted, the\\nKubernetes garbage collector deletes the PVC, which then usually triggers deletion of the\\nvolume because the default reclaim policy of storage classes is to delete volumes. You can create\\nquasi-ephemeral local storage using a StorageClass with a reclaim policy of retain : the storage\\noutlives the Pod, and in this case you need to ensure that volume clean up happens separately.\\nWhile these PVCs exist, they can be used like any other PVC. In particular, they can be\\nreferenced as data source in volume cloning or snapshotting. The PVC object also holds the\\ncurrent status of the volume.\\nPersistentVolumeClaim naming\\nNaming of the automatically created PVCs is deterministic: the name is a combination of the\\nPod name and volume name, with a hyphen ( -) in the middle. In the example above, the PVC\\nname will be my-app-scratch-volume . This deterministic naming makes it easier to interact\\nwith the PVC because one does not have to search for it once the Pod name and volume name\\nare known.\\nThe deterministic naming also introduces a potential conflict between different Pods (a Pod\\n\"pod-a\" with volume \"scratch\" and another Pod with name \"pod\" and volume \"a-scratch\" both\\nend up with the same PVC name \"pod-a-scratch\") and between Pods and manually created\\nPVCs.\\nSuch conflicts are detected: a PVC is only used for an ephemeral volume if it was created for the\\nPod. This check is based on the ownership relationship. An existing PVC is not overwritten or\\nmodified. But this does not resolve the conflict because without the right PVC, the Pod cannot\\nstart.', metadata={'source': './PDFS/Concepts.pdf', 'page': 330}),\n",
       " Document(page_content='Caution:  Take care when naming Pods and volumes inside the same namespace, so that these\\nconflicts can\\'t occur.\\nSecurity\\nUsing generic ephemeral volumes allows users to create PVCs indirectly if they can create Pods,\\neven if they do not have permission to create PVCs directly. Cluster administrators must be\\naware of this. If this does not fit their security model, they should use an admission webhook\\nthat rejects objects like Pods that have a generic ephemeral volume.\\nThe normal namespace quota for PVCs  still applies, so even if users are allowed to use this new\\nmechanism, they cannot use it to circumvent other policies.\\nWhat\\'s next\\nEphemeral volumes managed by kubelet\\nSee local ephemeral storage .\\nCSI ephemeral volumes\\nFor more information on the design, see the Ephemeral Inline CSI volumes KEP .\\nFor more information on further development of this feature, see the enhancement\\ntracking issue #596 .\\nGeneric ephemeral volumes\\nFor more information on the design, see the Generic ephemeral inline volumes KEP .\\nStorage Classes\\nThis document describes the concept of a StorageClass in Kubernetes. Familiarity with volumes\\nand persistent volumes  is suggested.\\nIntroduction\\nA StorageClass provides a way for administrators to describe the \"classes\" of storage they offer.\\nDifferent classes might map to quality-of-service levels, or to backup policies, or to arbitrary\\npolicies determined by the cluster administrators. Kubernetes itself is unopinionated about\\nwhat classes represent. This concept is sometimes called \"profiles\" in other storage systems.\\nThe StorageClass Resource\\nEach StorageClass contains the fields provisioner , parameters , and reclaimPolicy , which are\\nused when a PersistentVolume belonging to the class needs to be dynamically provisioned.\\nThe name of a StorageClass object is significant, and is how users can request a particular class.\\nAdministrators set the name and other parameters of a class when first creating StorageClass\\nobjects.• \\n• \\n•', metadata={'source': './PDFS/Concepts.pdf', 'page': 331}),\n",
       " Document(page_content='Administrators can specify a default StorageClass only for PVCs that don\\'t request any\\nparticular class to bind to: see the PersistentVolumeClaim section  for details.\\napiVersion : storage.k8s.io/v1\\nkind: StorageClass\\nmetadata :\\n  name : standard\\nprovisioner : kubernetes.io/aws-ebs\\nparameters :\\n  type: gp2\\nreclaimPolicy : Retain\\nallowVolumeExpansion : true\\nmountOptions :\\n  - debug\\nvolumeBindingMode : Immediate\\nDefault StorageClass\\nWhen a PVC does not specify a storageClassName , the default StorageClass is used. The cluster\\ncan only have one default StorageClass. If more than one default StorageClass is accidentally\\nset, the newest default is used when the PVC is dynamically provisioned.\\nFor instructions on setting the default StorageClass, see Change the default StorageClass . Note\\nthat certain cloud providers may already define a default StorageClass.\\nProvisioner\\nEach StorageClass has a provisioner that determines what volume plugin is used for\\nprovisioning PVs. This field must be specified.\\nVolume Plugin Internal Provisioner Config Example\\nAzureFile Azure File\\nCephFS - -\\nFC - -\\nFlexVolume - -\\nGCEPersistentDisk GCE PD\\niSCSI - -\\nNFS - NFS\\nRBD Ceph RBD\\nVsphereVolume vSphere\\nPortworxVolume Portworx Volume\\nLocal - Local\\nYou are not restricted to specifying the \"internal\" provisioners listed here (whose names are\\nprefixed with \"kubernetes.io\" and shipped alongside Kubernetes). You can also run and specify\\nexternal provisioners, which are independent programs that follow a specification  defined by\\nKubernetes. Authors of external provisioners have full discretion over where their code lives,\\nhow the provisioner is shipped, how it needs to be run, what volume plugin it uses (including\\nFlex), etc. The repository kubernetes-sigs/sig-storage-lib-external-provisioner  houses a library\\nfor writing external provisioners that implements the bulk of the specification. Some external', metadata={'source': './PDFS/Concepts.pdf', 'page': 332}),\n",
       " Document(page_content='provisioners are listed under the repository kubernetes-sigs/sig-storage-lib-external-\\nprovisioner .\\nFor example, NFS doesn\\'t provide an internal provisioner, but an external provisioner can be\\nused. There are also cases when 3rd party storage vendors provide their own external\\nprovisioner.\\nReclaim Policy\\nPersistentVolumes that are dynamically created by a StorageClass will have the reclaim policy\\nspecified in the reclaimPolicy  field of the class, which can be either Delete  or Retain . If no \\nreclaimPolicy  is specified when a StorageClass object is created, it will default to Delete .\\nPersistentVolumes that are created manually and managed via a StorageClass will have\\nwhatever reclaim policy they were assigned at creation.\\nAllow Volume Expansion\\nPersistentVolumes can be configured to be expandable. This feature when set to true, allows the\\nusers to resize the volume by editing the corresponding PVC object.\\nThe following types of volumes support volume expansion, when the underlying StorageClass\\nhas the field allowVolumeExpansion  set to true.\\nTable of Volume types and the version of\\nKubernetes they require\\nVolume type Required Kubernetes version\\ngcePersistentDisk 1.11\\nrbd 1.11\\nAzure File 1.11\\nPortworx 1.11\\nFlexVolume 1.13\\nCSI 1.14 (alpha), 1.16 (beta)\\nNote:  You can only use the volume expansion feature to grow a Volume, not to shrink it.\\nMount Options\\nPersistentVolumes that are dynamically created by a StorageClass will have the mount options\\nspecified in the mountOptions  field of the class.\\nIf the volume plugin does not support mount options but mount options are specified,\\nprovisioning will fail. Mount options are not validated on either the class or PV. If a mount\\noption is invalid, the PV mount fails.\\nVolume Binding Mode\\nThe volumeBindingMode  field controls when volume binding and dynamic provisioning  should\\noccur. When unset, \"Immediate\" mode is used by default.\\nThe Immediate  mode indicates that volume binding and dynamic provisioning occurs once the\\nPersistentVolumeClaim is created. For storage backends that are topology-constrained and not', metadata={'source': './PDFS/Concepts.pdf', 'page': 333}),\n",
       " Document(page_content='globally accessible from all Nodes in the cluster, PersistentVolumes will be bound or\\nprovisioned without knowledge of the Pod\\'s scheduling requirements. This may result in\\nunschedulable Pods.\\nA cluster administrator can address this issue by specifying the WaitForFirstConsumer  mode\\nwhich will delay the binding and provisioning of a PersistentVolume until a Pod using the\\nPersistentVolumeClaim is created. PersistentVolumes will be selected or provisioned\\nconforming to the topology that is specified by the Pod\\'s scheduling constraints. These include,\\nbut are not limited to, resource requirements , node selectors , pod affinity and anti-affinity , and \\ntaints and tolerations .\\nThe following plugins support WaitForFirstConsumer  with dynamic provisioning:\\nGCEPersistentDisk\\nThe following plugins support WaitForFirstConsumer  with pre-created PersistentVolume\\nbinding:\\nAll of the above\\nLocal\\nCSI volumes  are also supported with dynamic provisioning and pre-created PVs, but you\\'ll need\\nto look at the documentation for a specific CSI driver to see its supported topology keys and\\nexamples.\\nNote:\\nIf you choose to use WaitForFirstConsumer , do not use nodeName  in the Pod spec to specify\\nnode affinity. If nodeName  is used in this case, the scheduler will be bypassed and PVC will\\nremain in pending  state.\\nInstead, you can use node selector for hostname in this case as shown below.\\napiVersion : v1\\nkind: Pod\\nmetadata :\\n  name : task-pv-pod\\nspec:\\n  nodeSelector :\\n    kubernetes.io/hostname : kube-01\\n  volumes :\\n    - name : task-pv-storage\\n      persistentVolumeClaim :\\n        claimName : task-pv-claim\\n  containers :\\n    - name : task-pv-container\\n      image : nginx\\n      ports :\\n        - containerPort : 80\\n          name : \"http-server\"\\n      volumeMounts :\\n        - mountPath : \"/usr/share/nginx/html\"\\n          name : task-pv-storage• \\n• \\n•', metadata={'source': './PDFS/Concepts.pdf', 'page': 334}),\n",
       " Document(page_content='Allowed Topologies\\nWhen a cluster operator specifies the WaitForFirstConsumer  volume binding mode, it is no\\nlonger necessary to restrict provisioning to specific topologies in most situations. However, if\\nstill required, allowedTopologies  can be specified.\\nThis example demonstrates how to restrict the topology of provisioned volumes to specific\\nzones and should be used as a replacement for the zone  and zones  parameters for the supported\\nplugins.\\napiVersion : storage.k8s.io/v1\\nkind: StorageClass\\nmetadata :\\n  name : standard\\nprovisioner : kubernetes.io/gce-pd\\nparameters :\\n  type: pd-standard\\nvolumeBindingMode : WaitForFirstConsumer\\nallowedTopologies :\\n- matchLabelExpressions :\\n  - key: topology.kubernetes.io/zone\\n    values :\\n    - us-central-1a\\n    - us-central-1b\\nParameters\\nStorage Classes have parameters that describe volumes belonging to the storage class. Different\\nparameters may be accepted depending on the provisioner . For example, the value io1, for the\\nparameter type, and the parameter iopsPerGB  are specific to EBS. When a parameter is omitted,\\nsome default is used.\\nThere can be at most 512 parameters defined for a StorageClass. The total length of the\\nparameters object including its keys and values cannot exceed 256 KiB.\\nAWS EBS\\napiVersion : storage.k8s.io/v1\\nkind: StorageClass\\nmetadata :\\n  name : slow\\nprovisioner : kubernetes.io/aws-ebs\\nparameters :\\n  type: io1\\n  iopsPerGB : \"10\"\\n  fsType : ext4\\ntype: io1, gp2, sc1, st1. See AWS docs  for details. Default: gp2.\\nzone  (Deprecated): AWS zone. If neither zone  nor zones  is specified, volumes are\\ngenerally round-robin-ed across all active zones where Kubernetes cluster has a node. \\nzone  and zones  parameters must not be used at the same time.• \\n•', metadata={'source': './PDFS/Concepts.pdf', 'page': 335}),\n",
       " Document(page_content='zones  (Deprecated): A comma separated list of AWS zone(s). If neither zone  nor zones  is\\nspecified, volumes are generally round-robin-ed across all active zones where Kubernetes\\ncluster has a node. zone  and zones  parameters must not be used at the same time.\\niopsPerGB : only for io1 volumes. I/O operations per second per GiB. AWS volume plugin\\nmultiplies this with size of requested volume to compute IOPS of the volume and caps it\\nat 20 000 IOPS (maximum supported by AWS, see AWS docs ). A string is expected here,\\ni.e. \"10\", not 10.\\nfsType : fsType that is supported by kubernetes. Default: \"ext4\" .\\nencrypted : denotes whether the EBS volume should be encrypted or not. Valid values are \\n\"true\"  or \"false\" . A string is expected here, i.e. \"true\" , not true.\\nkmsKeyId : optional. The full Amazon Resource Name of the key to use when encrypting\\nthe volume. If none is supplied but encrypted  is true, a key is generated by AWS. See\\nAWS docs for valid ARN value.\\nNote:  zone  and zones  parameters are deprecated and replaced with allowedTopologies\\nGCE PD\\napiVersion : storage.k8s.io/v1\\nkind: StorageClass\\nmetadata :\\n  name : slow\\nprovisioner : kubernetes.io/gce-pd\\nparameters :\\n  type: pd-standard\\n  fstype : ext4\\n  replication-type : none\\ntype: pd-standard  or pd-ssd . Default: pd-standard\\nzone  (Deprecated): GCE zone. If neither zone  nor zones  is specified, volumes are\\ngenerally round-robin-ed across all active zones where Kubernetes cluster has a node. \\nzone  and zones  parameters must not be used at the same time.\\nzones  (Deprecated): A comma separated list of GCE zone(s). If neither zone  nor zones  is\\nspecified, volumes are generally round-robin-ed across all active zones where Kubernetes\\ncluster has a node. zone  and zones  parameters must not be used at the same time.\\nfstype : ext4 or xfs. Default: ext4. The defined filesystem type must be supported by the\\nhost operating system.\\nreplication-type : none  or regional-pd . Default: none .\\nIf replication-type  is set to none , a regular (zonal) PD will be provisioned.\\nIf replication-type  is set to regional-pd , a Regional Persistent Disk  will be provisioned. It\\'s\\nhighly recommended to have volumeBindingMode: WaitForFirstConsumer  set, in which case\\nwhen you create a Pod that consumes a PersistentVolumeClaim which uses this StorageClass, a\\nRegional Persistent Disk is provisioned with two zones. One zone is the same as the zone that\\nthe Pod is scheduled in. The other zone is randomly picked from the zones available to the\\ncluster. Disk zones can be further constrained using allowedTopologies .\\nNote:  zone  and zones  parameters are deprecated and replaced with allowedTopologies . When \\nGCE CSI Migration  is enabled, a GCE PD volume can be provisioned in a topology that does not• \\n• \\n• \\n• \\n• \\n• \\n• \\n• \\n• \\n•', metadata={'source': './PDFS/Concepts.pdf', 'page': 336}),\n",
       " Document(page_content='match any nodes, but any pod trying to use that volume will fail to schedule. With legacy pre-\\nmigration GCE PD, in this case an error will be produced instead at provisioning time. GCE CSI\\nMigration is enabled by default beginning from the Kubernetes 1.23 release.\\nNFS\\napiVersion : storage.k8s.io/v1\\nkind: StorageClass\\nmetadata :\\n  name : example-nfs\\nprovisioner : example.com/external-nfs\\nparameters :\\n  server : nfs-server.example.com\\n  path: /share\\n  readOnly : \"false\"\\nserver : Server is the hostname or IP address of the NFS server.\\npath: Path that is exported by the NFS server.\\nreadOnly : A flag indicating whether the storage will be mounted as read only (default\\nfalse).\\nKubernetes doesn\\'t include an internal NFS provisioner. You need to use an external provisioner\\nto create a StorageClass for NFS. Here are some examples:\\nNFS Ganesha server and external provisioner\\nNFS subdir external provisioner\\nvSphere\\nThere are two types of provisioners for vSphere storage classes:\\nCSI provisioner : csi.vsphere.vmware.com\\nvCP provisioner : kubernetes.io/vsphere-volume\\nIn-tree provisioners are deprecated . For more information on the CSI provisioner, see \\nKubernetes vSphere CSI Driver  and vSphereVolume CSI migration .\\nCSI Provisioner\\nThe vSphere CSI StorageClass provisioner works with Tanzu Kubernetes clusters. For an\\nexample, refer to the vSphere CSI repository .\\nvCP Provisioner\\nThe following examples use the VMware Cloud Provider (vCP) StorageClass provisioner.\\nCreate a StorageClass with a user specified disk format.\\napiVersion : storage.k8s.io/v1\\nkind: StorageClass\\nmetadata :\\n  name : fast\\nprovisioner : kubernetes.io/vsphere-volume• \\n• \\n• \\n• \\n• \\n• \\n• \\n1.', metadata={'source': './PDFS/Concepts.pdf', 'page': 337}),\n",
       " Document(page_content='parameters :\\n  diskformat : zeroedthick\\ndiskformat : thin, zeroedthick  and eagerzeroedthick . Default: \"thin\" .\\nCreate a StorageClass with a disk format on a user specified datastore.\\napiVersion : storage.k8s.io/v1\\nkind: StorageClass\\nmetadata :\\n  name : fast\\nprovisioner : kubernetes.io/vsphere-volume\\nparameters :\\n  diskformat : zeroedthick\\n  datastore : VSANDatastore\\ndatastore : The user can also specify the datastore in the StorageClass. The volume will be\\ncreated on the datastore specified in the StorageClass, which in this case is \\nVSANDatastore . This field is optional. If the datastore is not specified, then the volume\\nwill be created on the datastore specified in the vSphere config file used to initialize the\\nvSphere Cloud Provider.\\nStorage Policy Management inside kubernetes\\nUsing existing vCenter SPBM policy\\nOne of the most important features of vSphere for Storage Management is policy\\nbased Management. Storage Policy Based Management (SPBM) is a storage policy\\nframework that provides a single unified control plane across a broad range of data\\nservices and storage solutions. SPBM enables vSphere administrators to overcome\\nupfront storage provisioning challenges, such as capacity planning, differentiated\\nservice levels and managing capacity headroom.\\nThe SPBM policies can be specified in the StorageClass using the \\nstoragePolicyName  parameter.\\nVirtual SAN policy support inside Kubernetes\\nVsphere Infrastructure (VI) Admins will have the ability to specify custom Virtual\\nSAN Storage Capabilities during dynamic volume provisioning. You can now define\\nstorage requirements, such as performance and availability, in the form of storage\\ncapabilities during dynamic volume provisioning. The storage capability\\nrequirements are converted into a Virtual SAN policy which are then pushed down\\nto the Virtual SAN layer when a persistent volume (virtual disk) is being created.\\nThe virtual disk is distributed across the Virtual SAN datastore to meet the\\nrequirements.\\nYou can see Storage Policy Based Management for dynamic provisioning of\\nvolumes  for more details on how to use storage policies for persistent volumes\\nmanagement.\\nThere are few vSphere examples  which you try out for persistent volume management inside\\nKubernetes for vSphere.2. \\n3. \\n◦ \\n◦', metadata={'source': './PDFS/Concepts.pdf', 'page': 338}),\n",
       " Document(page_content='Ceph RBD\\nNote:\\nFEATURE STATE:  Kubernetes v1.28 [deprecated]\\nThis internal provisioner of Ceph RBD is deprecated. Please use CephFS RBD CSI driver .\\napiVersion : storage.k8s.io/v1\\nkind: StorageClass\\nmetadata :\\n  name : fast\\nprovisioner : kubernetes.io/rbd\\nparameters :\\n  monitors : 10.16.153.105 :6789\\n  adminId : kube\\n  adminSecretName : ceph-secret\\n  adminSecretNamespace : kube-system\\n  pool: kube\\n  userId : kube\\n  userSecretName : ceph-secret-user\\n  userSecretNamespace : default\\n  fsType : ext4\\n  imageFormat : \"2\"\\n  imageFeatures : \"layering\"\\nmonitors : Ceph monitors, comma delimited. This parameter is required.\\nadminId : Ceph client ID that is capable of creating images in the pool. Default is \"admin\".\\nadminSecretName : Secret Name for adminId . This parameter is required. The provided\\nsecret must have type \"kubernetes.io/rbd\".\\nadminSecretNamespace : The namespace for adminSecretName . Default is \"default\".\\npool: Ceph RBD pool. Default is \"rbd\".\\nuserId : Ceph client ID that is used to map the RBD image. Default is the same as adminId .\\nuserSecretName : The name of Ceph Secret for userId  to map RBD image. It must exist in\\nthe same namespace as PVCs. This parameter is required. The provided secret must have\\ntype \"kubernetes.io/rbd\", for example created in this way:\\nkubectl create secret generic ceph-secret --type =\"kubernetes.io/rbd\"  \\\\\\n  --from-literal =key=\\'QVFEQ1pMdFhPUnQrSmhBQUFYaERWNHJsZ3BsMmNjcDR6RFZS\\nT0E9PQ==\\'  \\\\\\n  --namespace =kube-system\\nuserSecretNamespace : The namespace for userSecretName .\\nfsType : fsType that is supported by kubernetes. Default: \"ext4\" .\\nimageFormat : Ceph RBD image format, \"1\" or \"2\". Default is \"2\".• \\n• \\n• \\n• \\n• \\n• \\n• \\n• \\n• \\n•', metadata={'source': './PDFS/Concepts.pdf', 'page': 339}),\n",
       " Document(page_content='imageFeatures : This parameter is optional and should only be used if you set \\nimageFormat  to \"2\". Currently supported features are layering  only. Default is \"\", and no\\nfeatures are turned on.\\nAzure Disk\\nAzure Unmanaged Disk storage class\\napiVersion : storage.k8s.io/v1\\nkind: StorageClass\\nmetadata :\\n  name : slow\\nprovisioner : kubernetes.io/azure-disk\\nparameters :\\n  skuName : Standard_LRS\\n  location : eastus\\n  storageAccount : azure_storage_account_name\\nskuName : Azure storage account Sku tier. Default is empty.\\nlocation : Azure storage account location. Default is empty.\\nstorageAccount : Azure storage account name. If a storage account is provided, it must\\nreside in the same resource group as the cluster, and location  is ignored. If a storage\\naccount is not provided, a new storage account will be created in the same resource\\ngroup as the cluster.\\nAzure Disk storage class (starting from v1.7.2)\\napiVersion : storage.k8s.io/v1\\nkind: StorageClass\\nmetadata :\\n  name : slow\\nprovisioner : kubernetes.io/azure-disk\\nparameters :\\n  storageaccounttype : Standard_LRS\\n  kind: managed\\nstorageaccounttype : Azure storage account Sku tier. Default is empty.\\nkind: Possible values are shared , dedicated , and managed  (default). When kind is shared ,\\nall unmanaged disks are created in a few shared storage accounts in the same resource\\ngroup as the cluster. When kind is dedicated , a new dedicated storage account will be\\ncreated for the new unmanaged disk in the same resource group as the cluster. When \\nkind is managed , all managed disks are created in the same resource group as the cluster.\\nresourceGroup : Specify the resource group in which the Azure disk will be created. It\\nmust be an existing resource group name. If it is unspecified, the disk will be placed in the\\nsame resource group as the current Kubernetes cluster.\\nPremium VM can attach both Standard_LRS and Premium_LRS disks, while Standard VM\\ncan only attach Standard_LRS disks.\\nManaged VM can only attach managed disks and unmanaged VM can only attach\\nunmanaged disks.• \\n• \\n• \\n• \\n• \\n• \\n• \\n• \\n•', metadata={'source': './PDFS/Concepts.pdf', 'page': 340}),\n",
       " Document(page_content='Azure File\\napiVersion : storage.k8s.io/v1\\nkind: StorageClass\\nmetadata :\\n  name : azurefile\\nprovisioner : kubernetes.io/azure-file\\nparameters :\\n  skuName : Standard_LRS\\n  location : eastus\\n  storageAccount : azure_storage_account_name\\nskuName : Azure storage account Sku tier. Default is empty.\\nlocation : Azure storage account location. Default is empty.\\nstorageAccount : Azure storage account name. Default is empty. If a storage account is not\\nprovided, all storage accounts associated with the resource group are searched to find one\\nthat matches skuName  and location . If a storage account is provided, it must reside in the\\nsame resource group as the cluster, and skuName  and location  are ignored.\\nsecretNamespace : the namespace of the secret that contains the Azure Storage Account\\nName and Key. Default is the same as the Pod.\\nsecretName : the name of the secret that contains the Azure Storage Account Name and\\nKey. Default is azure-storage-account-<accountName>-secret\\nreadOnly : a flag indicating whether the storage will be mounted as read only. Defaults to\\nfalse which means a read/write mount. This setting will impact the ReadOnly  setting in\\nVolumeMounts as well.\\nDuring storage provisioning, a secret named by secretName  is created for the mounting\\ncredentials. If the cluster has enabled both RBAC  and Controller Roles , add the create\\npermission of resource secret  for clusterrole system:controller:persistent-volume-binder .\\nIn a multi-tenancy context, it is strongly recommended to set the value for secretNamespace\\nexplicitly, otherwise the storage account credentials may be read by other users.\\nPortworx Volume\\napiVersion : storage.k8s.io/v1\\nkind: StorageClass\\nmetadata :\\n  name : portworx-io-priority-high\\nprovisioner : kubernetes.io/portworx-volume\\nparameters :\\n  repl: \"1\"\\n  snap_interval : \"70\"\\n  priority_io : \"high\"\\nfs: filesystem to be laid out: none/xfs/ext4  (default: ext4).\\nblock_size : block size in Kbytes (default: 32).\\nrepl: number of synchronous replicas to be provided in the form of replication factor 1..3\\n(default: 1) A string is expected here i.e. \"1\" and not 1.\\npriority_io : determines whether the volume will be created from higher performance or a\\nlower priority storage high/medium/low  (default: low).• \\n• \\n• \\n• \\n• \\n• \\n• \\n• \\n• \\n•', metadata={'source': './PDFS/Concepts.pdf', 'page': 341}),\n",
       " Document(page_content='snap_interval : clock/time interval in minutes for when to trigger snapshots. Snapshots\\nare incremental based on difference with the prior snapshot, 0 disables snaps (default: 0).\\nA string is expected here i.e. \"70\" and not 70.\\naggregation_level : specifies the number of chunks the volume would be distributed into, 0\\nindicates a non-aggregated volume (default: 0). A string is expected here i.e. \"0\" and not 0\\nephemeral : specifies whether the volume should be cleaned-up after unmount or should\\nbe persistent. emptyDir  use case can set this value to true and persistent volumes  use\\ncase such as for databases like Cassandra should set to false, true/false  (default false). A\\nstring is expected here i.e. \"true\"  and not true.\\nLocal\\napiVersion : storage.k8s.io/v1\\nkind: StorageClass\\nmetadata :\\n  name : local-storage\\nprovisioner : kubernetes.io/no-provisioner\\nvolumeBindingMode : WaitForFirstConsumer\\nLocal volumes do not currently support dynamic provisioning, however a StorageClass should\\nstill be created to delay volume binding until Pod scheduling. This is specified by the \\nWaitForFirstConsumer  volume binding mode.\\nDelaying volume binding allows the scheduler to consider all of a Pod\\'s scheduling constraints\\nwhen choosing an appropriate PersistentVolume for a PersistentVolumeClaim.\\nDynamic Volume Provisioning\\nDynamic volume provisioning allows storage volumes to be created on-demand. Without\\ndynamic provisioning, cluster administrators have to manually make calls to their cloud or\\nstorage provider to create new storage volumes, and then create PersistentVolume  objects  to\\nrepresent them in Kubernetes. The dynamic provisioning feature eliminates the need for cluster\\nadministrators to pre-provision storage. Instead, it automatically provisions storage when it is\\nrequested by users.\\nBackground\\nThe implementation of dynamic volume provisioning is based on the API object StorageClass\\nfrom the API group storage.k8s.io . A cluster administrator can define as many StorageClass\\nobjects as needed, each specifying a volume plugin  (aka provisioner ) that provisions a volume\\nand the set of parameters to pass to that provisioner when provisioning. A cluster administrator\\ncan define and expose multiple flavors of storage (from the same or different storage systems)\\nwithin a cluster, each with a custom set of parameters. This design also ensures that end users\\ndon\\'t have to worry about the complexity and nuances of how storage is provisioned, but still\\nhave the ability to select from multiple storage options.\\nMore information on storage classes can be found here.• \\n• \\n•', metadata={'source': './PDFS/Concepts.pdf', 'page': 342}),\n",
       " Document(page_content='Enabling Dynamic Provisioning\\nTo enable dynamic provisioning, a cluster administrator needs to pre-create one or more\\nStorageClass objects for users. StorageClass objects define which provisioner should be used\\nand what parameters should be passed to that provisioner when dynamic provisioning is\\ninvoked. The name of a StorageClass object must be a valid DNS subdomain name .\\nThe following manifest creates a storage class \"slow\" which provisions standard disk-like\\npersistent disks.\\napiVersion : storage.k8s.io/v1\\nkind: StorageClass\\nmetadata :\\n  name : slow\\nprovisioner : kubernetes.io/gce-pd\\nparameters :\\n  type: pd-standard\\nThe following manifest creates a storage class \"fast\" which provisions SSD-like persistent disks.\\napiVersion : storage.k8s.io/v1\\nkind: StorageClass\\nmetadata :\\n  name : fast\\nprovisioner : kubernetes.io/gce-pd\\nparameters :\\n  type: pd-ssd\\nUsing Dynamic Provisioning\\nUsers request dynamically provisioned storage by including a storage class in their \\nPersistentVolumeClaim . Before Kubernetes v1.6, this was done via the \\nvolume.beta.kubernetes.io/storage-class  annotation. However, this annotation is deprecated\\nsince v1.9. Users now can and should instead use the storageClassName  field of the \\nPersistentVolumeClaim  object. The value of this field must match the name of a StorageClass\\nconfigured by the administrator (see below ).\\nTo select the \"fast\" storage class, for example, a user would create the following\\nPersistentVolumeClaim:\\napiVersion : v1\\nkind: PersistentVolumeClaim\\nmetadata :\\n  name : claim1\\nspec:\\n  accessModes :\\n    - ReadWriteOnce\\n  storageClassName : fast\\n  resources :\\n    requests :\\n      storage : 30Gi', metadata={'source': './PDFS/Concepts.pdf', 'page': 343}),\n",
       " Document(page_content='This claim results in an SSD-like Persistent Disk being automatically provisioned. When the\\nclaim is deleted, the volume is destroyed.\\nDefaulting Behavior\\nDynamic provisioning can be enabled on a cluster such that all claims are dynamically\\nprovisioned if no storage class is specified. A cluster administrator can enable this behavior by:\\nMarking one StorageClass  object as default ;\\nMaking sure that the DefaultStorageClass  admission controller  is enabled on the API\\nserver.\\nAn administrator can mark a specific StorageClass  as default by adding the \\nstorageclass.kubernetes.io/is-default-class  annotation  to it. When a default StorageClass  exists\\nin a cluster and a user creates a PersistentVolumeClaim  with storageClassName  unspecified, the \\nDefaultStorageClass  admission controller automatically adds the storageClassName  field\\npointing to the default storage class.\\nNote that there can be at most one default  storage class on a cluster, or a \\nPersistentVolumeClaim  without storageClassName  explicitly specified cannot be created.\\nTopology Awareness\\nIn Multi-Zone  clusters, Pods can be spread across Zones in a Region. Single-Zone storage\\nbackends should be provisioned in the Zones where Pods are scheduled. This can be\\naccomplished by setting the Volume Binding Mode .\\nVolume Snapshots\\nIn Kubernetes, a VolumeSnapshot  represents a snapshot of a volume on a storage system. This\\ndocument assumes that you are already familiar with Kubernetes persistent volumes .\\nIntroduction\\nSimilar to how API resources PersistentVolume  and PersistentVolumeClaim  are used to\\nprovision volumes for users and administrators, VolumeSnapshotContent  and VolumeSnapshot\\nAPI resources are provided to create volume snapshots for users and administrators.\\nA VolumeSnapshotContent  is a snapshot taken from a volume in the cluster that has been\\nprovisioned by an administrator. It is a resource in the cluster just like a PersistentVolume is a\\ncluster resource.\\nA VolumeSnapshot  is a request for snapshot of a volume by a user. It is similar to a\\nPersistentVolumeClaim.\\nVolumeSnapshotClass  allows you to specify different attributes belonging to a VolumeSnapshot .\\nThese attributes may differ among snapshots taken from the same volume on the storage\\nsystem and therefore cannot be expressed by using the same StorageClass  of a \\nPersistentVolumeClaim .• \\n•', metadata={'source': './PDFS/Concepts.pdf', 'page': 344}),\n",
       " Document(page_content=\"Volume snapshots provide Kubernetes users with a standardized way to copy a volume's\\ncontents at a particular point in time without creating an entirely new volume. This\\nfunctionality enables, for example, database administrators to backup databases before\\nperforming edit or delete modifications.\\nUsers need to be aware of the following when using this feature:\\nAPI Objects VolumeSnapshot , VolumeSnapshotContent , and VolumeSnapshotClass  are \\nCRDs , not part of the core API.\\nVolumeSnapshot  support is only available for CSI drivers.\\nAs part of the deployment process of VolumeSnapshot , the Kubernetes team provides a\\nsnapshot controller to be deployed into the control plane, and a sidecar helper container\\ncalled csi-snapshotter to be deployed together with the CSI driver. The snapshot\\ncontroller watches VolumeSnapshot  and VolumeSnapshotContent  objects and is\\nresponsible for the creation and deletion of VolumeSnapshotContent  object. The sidecar\\ncsi-snapshotter watches VolumeSnapshotContent  objects and triggers CreateSnapshot\\nand DeleteSnapshot  operations against a CSI endpoint.\\nThere is also a validating webhook server which provides tightened validation on\\nsnapshot objects. This should be installed by the Kubernetes distros along with the\\nsnapshot controller and CRDs, not CSI drivers. It should be installed in all Kubernetes\\nclusters that has the snapshot feature enabled.\\nCSI drivers may or may not have implemented the volume snapshot functionality. The\\nCSI drivers that have provided support for volume snapshot will likely use the csi-\\nsnapshotter. See CSI Driver documentation  for details.\\nThe CRDs and snapshot controller installations are the responsibility of the Kubernetes\\ndistribution.\\nLifecycle of a volume snapshot and volume snapshot\\ncontent\\nVolumeSnapshotContents  are resources in the cluster. VolumeSnapshots  are requests for those\\nresources. The interaction between VolumeSnapshotContents  and VolumeSnapshots  follow this\\nlifecycle:\\nProvisioning Volume Snapshot\\nThere are two ways snapshots may be provisioned: pre-provisioned or dynamically provisioned.\\nPre-provisioned\\nA cluster administrator creates a number of VolumeSnapshotContents . They carry the details of\\nthe real volume snapshot on the storage system which is available for use by cluster users. They\\nexist in the Kubernetes API and are available for consumption.\\nDynamic\\nInstead of using a pre-existing snapshot, you can request that a snapshot to be dynamically\\ntaken from a PersistentVolumeClaim. The VolumeSnapshotClass  specifies storage provider-\\nspecific parameters to use when taking a snapshot.• \\n• \\n• \\n• \\n• \\n•\", metadata={'source': './PDFS/Concepts.pdf', 'page': 345}),\n",
       " Document(page_content='Binding\\nThe snapshot controller handles the binding of a VolumeSnapshot  object with an appropriate \\nVolumeSnapshotContent  object, in both pre-provisioned and dynamically provisioned\\nscenarios. The binding is a one-to-one mapping.\\nIn the case of pre-provisioned binding, the VolumeSnapshot will remain unbound until the\\nrequested VolumeSnapshotContent object is created.\\nPersistent Volume Claim as Snapshot Source Protection\\nThe purpose of this protection is to ensure that in-use PersistentVolumeClaim  API objects are\\nnot removed from the system while a snapshot is being taken from it (as this may result in data\\nloss).\\nWhile a snapshot is being taken of a PersistentVolumeClaim, that PersistentVolumeClaim is in-\\nuse. If you delete a PersistentVolumeClaim API object in active use as a snapshot source, the\\nPersistentVolumeClaim object is not removed immediately. Instead, removal of the\\nPersistentVolumeClaim object is postponed until the snapshot is readyToUse or aborted.\\nDelete\\nDeletion is triggered by deleting the VolumeSnapshot  object, and the DeletionPolicy  will be\\nfollowed. If the DeletionPolicy  is Delete , then the underlying storage snapshot will be deleted\\nalong with the VolumeSnapshotContent  object. If the DeletionPolicy  is Retain , then both the\\nunderlying snapshot and VolumeSnapshotContent  remain.\\nVolumeSnapshots\\nEach VolumeSnapshot contains a spec and a status.\\napiVersion : snapshot.storage.k8s.io/v1\\nkind: VolumeSnapshot\\nmetadata :\\n  name : new-snapshot-test\\nspec:\\n  volumeSnapshotClassName : csi-hostpath-snapclass\\n  source :\\n    persistentVolumeClaimName : pvc-test\\npersistentVolumeClaimName  is the name of the PersistentVolumeClaim data source for the\\nsnapshot. This field is required for dynamically provisioning a snapshot.\\nA volume snapshot can request a particular class by specifying the name of a \\nVolumeSnapshotClass  using the attribute volumeSnapshotClassName . If nothing is set, then the\\ndefault class is used if available.\\nFor pre-provisioned snapshots, you need to specify a volumeSnapshotContentName  as the\\nsource for the snapshot as shown in the following example. The volumeSnapshotContentName\\nsource field is required for pre-provisioned snapshots.', metadata={'source': './PDFS/Concepts.pdf', 'page': 346}),\n",
       " Document(page_content='apiVersion : snapshot.storage.k8s.io/v1\\nkind: VolumeSnapshot\\nmetadata :\\n  name : test-snapshot\\nspec:\\n  source :\\n    volumeSnapshotContentName : test-content\\nVolume Snapshot Contents\\nEach VolumeSnapshotContent contains a spec and status. In dynamic provisioning, the\\nsnapshot common controller creates VolumeSnapshotContent  objects. Here is an example:\\napiVersion : snapshot.storage.k8s.io/v1\\nkind: VolumeSnapshotContent\\nmetadata :\\n  name : snapcontent-72d9a349-aacd-42d2-a240-d775650d2455\\nspec:\\n  deletionPolicy : Delete\\n  driver : hostpath.csi.k8s.io\\n  source :\\n    volumeHandle : ee0cfb94-f8d4-11e9-b2d8-0242ac110002\\n  sourceVolumeMode : Filesystem\\n  volumeSnapshotClassName : csi-hostpath-snapclass\\n  volumeSnapshotRef :\\n    name : new-snapshot-test\\n    namespace : default\\n    uid: 72d9a349-aacd-42d2-a240-d775650d2455\\nvolumeHandle  is the unique identifier of the volume created on the storage backend and\\nreturned by the CSI driver during the volume creation. This field is required for dynamically\\nprovisioning a snapshot. It specifies the volume source of the snapshot.\\nFor pre-provisioned snapshots, you (as cluster administrator) are responsible for creating the \\nVolumeSnapshotContent  object as follows.\\napiVersion : snapshot.storage.k8s.io/v1\\nkind: VolumeSnapshotContent\\nmetadata :\\n  name : new-snapshot-content-test\\nspec:\\n  deletionPolicy : Delete\\n  driver : hostpath.csi.k8s.io\\n  source :\\n    snapshotHandle : 7bdd0de3-aaeb-11e8-9aae-0242ac110002\\n  sourceVolumeMode : Filesystem\\n  volumeSnapshotRef :\\n    name : new-snapshot-test\\n    namespace : default\\nsnapshotHandle  is the unique identifier of the volume snapshot created on the storage backend.\\nThis field is required for the pre-provisioned snapshots. It specifies the CSI snapshot id on the\\nstorage system that this VolumeSnapshotContent  represents.', metadata={'source': './PDFS/Concepts.pdf', 'page': 347}),\n",
       " Document(page_content='sourceVolumeMode  is the mode of the volume whose snapshot is taken. The value of the \\nsourceVolumeMode  field can be either Filesystem  or Block . If the source volume mode is not\\nspecified, Kubernetes treats the snapshot as if the source volume\\'s mode is unknown.\\nvolumeSnapshotRef  is the reference of the corresponding VolumeSnapshot . Note that when the \\nVolumeSnapshotContent  is being created as a pre-provisioned snapshot, the VolumeSnapshot\\nreferenced in volumeSnapshotRef  might not exist yet.\\nConverting the volume mode of a Snapshot\\nIf the VolumeSnapshots  API installed on your cluster supports the sourceVolumeMode  field,\\nthen the API has the capability to prevent unauthorized users from converting the mode of a\\nvolume.\\nTo check if your cluster has capability for this feature, run the following command:\\n$ kubectl get crd volumesnapshotcontent -o yaml\\nIf you want to allow users to create a PersistentVolumeClaim  from an existing VolumeSnapshot ,\\nbut with a different volume mode than the source, the annotation \\nsnapshot.storage.kubernetes.io/allow-volume-mode-change: \"true\" needs to be added to the \\nVolumeSnapshotContent  that corresponds to the VolumeSnapshot .\\nFor pre-provisioned snapshots, spec.sourceVolumeMode  needs to be populated by the cluster\\nadministrator.\\nAn example VolumeSnapshotContent  resource with this feature enabled would look like:\\napiVersion : snapshot.storage.k8s.io/v1\\nkind: VolumeSnapshotContent\\nmetadata :\\n  name : new-snapshot-content-test\\n  annotations :\\n    - snapshot.storage.kubernetes.io/allow-volume-mode-change : \"true\"\\nspec:\\n  deletionPolicy : Delete\\n  driver : hostpath.csi.k8s.io\\n  source :\\n    snapshotHandle : 7bdd0de3-aaeb-11e8-9aae-0242ac110002\\n  sourceVolumeMode : Filesystem\\n  volumeSnapshotRef :\\n    name : new-snapshot-test\\n    namespace : default\\nProvisioning Volumes from Snapshots\\nYou can provision a new volume, pre-populated with data from a snapshot, by using the \\ndataSource  field in the PersistentVolumeClaim  object.\\nFor more details, see Volume Snapshot and Restore Volume from Snapshot .', metadata={'source': './PDFS/Concepts.pdf', 'page': 348}),\n",
       " Document(page_content='Volume Snapshot Classes\\nThis document describes the concept of VolumeSnapshotClass in Kubernetes. Familiarity with \\nvolume snapshots  and storage classes  is suggested.\\nIntroduction\\nJust like StorageClass provides a way for administrators to describe the \"classes\" of storage they\\noffer when provisioning a volume, VolumeSnapshotClass provides a way to describe the\\n\"classes\" of storage when provisioning a volume snapshot.\\nThe VolumeSnapshotClass Resource\\nEach VolumeSnapshotClass contains the fields driver , deletionPolicy , and parameters , which are\\nused when a VolumeSnapshot belonging to the class needs to be dynamically provisioned.\\nThe name of a VolumeSnapshotClass object is significant, and is how users can request a\\nparticular class. Administrators set the name and other parameters of a class when first creating\\nVolumeSnapshotClass objects, and the objects cannot be updated once they are created.\\nNote:  Installation of the CRDs is the responsibility of the Kubernetes distribution. Without the\\nrequired CRDs present, the creation of a VolumeSnapshotClass fails.\\napiVersion : snapshot.storage.k8s.io/v1\\nkind: VolumeSnapshotClass\\nmetadata :\\n  name : csi-hostpath-snapclass\\ndriver : hostpath.csi.k8s.io\\ndeletionPolicy : Delete\\nparameters :\\nAdministrators can specify a default VolumeSnapshotClass for VolumeSnapshots that don\\'t\\nrequest any particular class to bind to by adding the snapshot.storage.kubernetes.io/is-default-\\nclass: \"true\"  annotation:\\napiVersion : snapshot.storage.k8s.io/v1\\nkind: VolumeSnapshotClass\\nmetadata :\\n  name : csi-hostpath-snapclass\\n  annotations :\\n    snapshot.storage.kubernetes.io/is-default-class : \"true\"\\ndriver : hostpath.csi.k8s.io\\ndeletionPolicy : Delete\\nparameters :\\nDriver\\nVolume snapshot classes have a driver that determines what CSI volume plugin is used for\\nprovisioning VolumeSnapshots. This field must be specified.', metadata={'source': './PDFS/Concepts.pdf', 'page': 349}),\n",
       " Document(page_content='DeletionPolicy\\nVolume snapshot classes have a deletionPolicy . It enables you to configure what happens to a\\nVolumeSnapshotContent when the VolumeSnapshot object it is bound to is to be deleted. The\\ndeletionPolicy of a volume snapshot class can either be Retain  or Delete . This field must be\\nspecified.\\nIf the deletionPolicy is Delete , then the underlying storage snapshot will be deleted along with\\nthe VolumeSnapshotContent object. If the deletionPolicy is Retain , then both the underlying\\nsnapshot and VolumeSnapshotContent remain.\\nParameters\\nVolume snapshot classes have parameters that describe volume snapshots belonging to the\\nvolume snapshot class. Different parameters may be accepted depending on the driver .\\nCSI Volume Cloning\\nThis document describes the concept of cloning existing CSI Volumes in Kubernetes. Familiarity\\nwith Volumes  is suggested.\\nIntroduction\\nThe CSI Volume Cloning feature adds support for specifying existing PVCs in the dataSource\\nfield to indicate a user would like to clone a Volume .\\nA Clone is defined as a duplicate of an existing Kubernetes Volume that can be consumed as\\nany standard Volume would be. The only difference is that upon provisioning, rather than\\ncreating a \"new\" empty Volume, the back end device creates an exact duplicate of the specified\\nVolume.\\nThe implementation of cloning, from the perspective of the Kubernetes API, adds the ability to\\nspecify an existing PVC as a dataSource during new PVC creation. The source PVC must be\\nbound and available (not in use).\\nUsers need to be aware of the following when using this feature:\\nCloning support ( VolumePVCDataSource ) is only available for CSI drivers.\\nCloning support is only available for dynamic provisioners.\\nCSI drivers may or may not have implemented the volume cloning functionality.\\nYou can only clone a PVC when it exists in the same namespace as the destination PVC\\n(source and destination must be in the same namespace).\\nCloning is supported with a different Storage Class.\\nDestination volume can be the same or a different storage class as the source.\\nDefault storage class can be used and storageClassName omitted in the spec.\\nCloning can only be performed between two volumes that use the same VolumeMode\\nsetting (if you request a block mode volume, the source MUST also be block mode)• \\n• \\n• \\n• \\n• \\n◦ \\n◦ \\n•', metadata={'source': './PDFS/Concepts.pdf', 'page': 350}),\n",
       " Document(page_content=\"Provisioning\\nClones are provisioned like any other PVC with the exception of adding a dataSource that\\nreferences an existing PVC in the same namespace.\\napiVersion : v1\\nkind: PersistentVolumeClaim\\nmetadata :\\n    name : clone-of-pvc-1\\n    namespace : myns\\nspec:\\n  accessModes :\\n  - ReadWriteOnce\\n  storageClassName : cloning\\n  resources :\\n    requests :\\n      storage : 5Gi\\n  dataSource :\\n    kind: PersistentVolumeClaim\\n    name : pvc-1\\nNote:  You must specify a capacity value for spec.resources.requests.storage , and the value you\\nspecify must be the same or larger than the capacity of the source volume.\\nThe result is a new PVC with the name clone-of-pvc-1  that has the exact same content as the\\nspecified source pvc-1 .\\nUsage\\nUpon availability of the new PVC, the cloned PVC is consumed the same as other PVC. It's also\\nexpected at this point that the newly created PVC is an independent object. It can be consumed,\\ncloned, snapshotted, or deleted independently and without consideration for it's original\\ndataSource PVC. This also implies that the source is not linked in any way to the newly created\\nclone, it may also be modified or deleted without affecting the newly created clone.\\nStorage Capacity\\nStorage capacity is limited and may vary depending on the node on which a pod runs: network-\\nattached storage might not be accessible by all nodes, or storage is local to a node to begin with.\\nFEATURE STATE:  Kubernetes v1.24 [stable]\\nThis page describes how Kubernetes keeps track of storage capacity and how the scheduler uses\\nthat information to schedule Pods  onto nodes that have access to enough storage capacity for\\nthe remaining missing volumes. Without storage capacity tracking, the scheduler may choose a\\nnode that doesn't have enough capacity to provision a volume and multiple scheduling retries\\nwill be needed.\", metadata={'source': './PDFS/Concepts.pdf', 'page': 351}),\n",
       " Document(page_content='Before you begin\\nKubernetes v1.28 includes cluster-level API support for storage capacity tracking. To use this\\nyou must also be using a CSI driver that supports capacity tracking. Consult the documentation\\nfor the CSI drivers that you use to find out whether this support is available and, if so, how to\\nuse it. If you are not running Kubernetes v1.28, check the documentation for that version of\\nKubernetes.\\nAPI\\nThere are two API extensions for this feature:\\nCSIStorageCapacity  objects: these get produced by a CSI driver in the namespace where\\nthe driver is installed. Each object contains capacity information for one storage class and\\ndefines which nodes have access to that storage.\\nThe CSIDriverSpec.StorageCapacity  field : when set to true, the Kubernetes scheduler will\\nconsider storage capacity for volumes that use the CSI driver.\\nScheduling\\nStorage capacity information is used by the Kubernetes scheduler if:\\na Pod uses a volume that has not been created yet,\\nthat volume uses a StorageClass  which references a CSI driver and uses \\nWaitForFirstConsumer  volume binding mode , and\\nthe CSIDriver  object for the driver has StorageCapacity  set to true.\\nIn that case, the scheduler only considers nodes for the Pod which have enough storage\\navailable to them. This check is very simplistic and only compares the size of the volume\\nagainst the capacity listed in CSIStorageCapacity  objects with a topology that includes the\\nnode.\\nFor volumes with Immediate  volume binding mode, the storage driver decides where to create\\nthe volume, independently of Pods that will use the volume. The scheduler then schedules Pods\\nonto nodes where the volume is available after the volume has been created.\\nFor CSI ephemeral volumes , scheduling always happens without considering storage capacity.\\nThis is based on the assumption that this volume type is only used by special CSI drivers which\\nare local to a node and do not need significant resources there.\\nRescheduling\\nWhen a node has been selected for a Pod with WaitForFirstConsumer  volumes, that decision is\\nstill tentative. The next step is that the CSI storage driver gets asked to create the volume with a\\nhint that the volume is supposed to be available on the selected node.\\nBecause Kubernetes might have chosen a node based on out-dated capacity information, it is\\npossible that the volume cannot really be created. The node selection is then reset and the\\nKubernetes scheduler tries again to find a node for the Pod.• \\n• \\n• \\n• \\n•', metadata={'source': './PDFS/Concepts.pdf', 'page': 352}),\n",
       " Document(page_content=\"Limitations\\nStorage capacity tracking increases the chance that scheduling works on the first try, but\\ncannot guarantee this because the scheduler has to decide based on potentially out-dated\\ninformation. Usually, the same retry mechanism as for scheduling without any storage capacity\\ninformation handles scheduling failures.\\nOne situation where scheduling can fail permanently is when a Pod uses multiple volumes: one\\nvolume might have been created already in a topology segment which then does not have\\nenough capacity left for another volume. Manual intervention is necessary to recover from this,\\nfor example by increasing capacity or deleting the volume that was already created.\\nWhat's next\\nFor more information on the design, see the Storage Capacity Constraints for Pod\\nScheduling KEP .\\nNode-specific Volume Limits\\nThis page describes the maximum number of volumes that can be attached to a Node for\\nvarious cloud providers.\\nCloud providers like Google, Amazon, and Microsoft typically have a limit on how many\\nvolumes can be attached to a Node. It is important for Kubernetes to respect those limits.\\nOtherwise, Pods scheduled on a Node could get stuck waiting for volumes to attach.\\nKubernetes default limits\\nThe Kubernetes scheduler has default limits on the number of volumes that can be attached to a\\nNode:\\nCloud service Maximum volumes per Node\\nAmazon Elastic Block Store (EBS) 39\\nGoogle Persistent Disk 16\\nMicrosoft Azure Disk Storage 16\\nCustom limits\\nYou can change these limits by setting the value of the KUBE_MAX_PD_VOLS  environment\\nvariable, and then starting the scheduler. CSI drivers might have a different procedure, see their\\ndocumentation on how to customize their limits.\\nUse caution if you set a limit that is higher than the default limit. Consult the cloud provider's\\ndocumentation to make sure that Nodes can actually support the limit you set.\\nThe limit applies to the entire cluster, so it affects all Nodes.•\", metadata={'source': './PDFS/Concepts.pdf', 'page': 353}),\n",
       " Document(page_content='Dynamic volume limits\\nFEATURE STATE:  Kubernetes v1.17 [stable]\\nDynamic volume limits are supported for following volume types.\\nAmazon EBS\\nGoogle Persistent Disk\\nAzure Disk\\nCSI\\nFor volumes managed by in-tree volume plugins, Kubernetes automatically determines the\\nNode type and enforces the appropriate maximum number of volumes for the node. For\\nexample:\\nOn Google Compute Engine , up to 127 volumes can be attached to a node, depending on\\nthe node type .\\nFor Amazon EBS disks on M5,C5,R5,T3 and Z1D instance types, Kubernetes allows only\\n25 volumes to be attached to a Node. For other instance types on Amazon Elastic\\nCompute Cloud (EC2) , Kubernetes allows 39 volumes to be attached to a Node.\\nOn Azure, up to 64 disks can be attached to a node, depending on the node type. For more\\ndetails, refer to Sizes for virtual machines in Azure .\\nIf a CSI storage driver advertises a maximum number of volumes for a Node (using \\nNodeGetInfo ), the kube-scheduler  honors that limit. Refer to the CSI specifications  for\\ndetails.\\nFor volumes managed by in-tree plugins that have been migrated to a CSI driver, the\\nmaximum number of volumes will be the one reported by the CSI driver.\\nVolume Health Monitoring\\nFEATURE STATE:  Kubernetes v1.21 [alpha]\\nCSI volume health monitoring allows CSI Drivers to detect abnormal volume conditions from\\nthe underlying storage systems and report them as events on PVCs  or Pods .\\nVolume health monitoring\\nKubernetes volume health monitoring  is part of how Kubernetes implements the Container\\nStorage Interface (CSI). Volume health monitoring feature is implemented in two components:\\nan External Health Monitor controller, and the kubelet .\\nIf a CSI Driver supports Volume Health Monitoring feature from the controller side, an event\\nwill be reported on the related PersistentVolumeClaim  (PVC) when an abnormal volume\\ncondition is detected on a CSI volume.\\nThe External Health Monitor controller  also watches for node failure events. You can enable\\nnode failure monitoring by setting the enable-node-watcher  flag to true. When the external• \\n• \\n• \\n• \\n• \\n• \\n• \\n• \\n•', metadata={'source': './PDFS/Concepts.pdf', 'page': 354}),\n",
       " Document(page_content=\"health monitor detects a node failure event, the controller reports an Event will be reported on\\nthe PVC to indicate that pods using this PVC are on a failed node.\\nIf a CSI Driver supports Volume Health Monitoring feature from the node side, an Event will be\\nreported on every Pod using the PVC when an abnormal volume condition is detected on a CSI\\nvolume. In addition, Volume Health information is exposed as Kubelet VolumeStats metrics. A\\nnew metric kubelet_volume_stats_health_status_abnormal is added. This metric includes two\\nlabels: namespace  and persistentvolumeclaim . The count is either 1 or 0. 1 indicates the volume\\nis unhealthy, 0 indicates volume is healthy. For more information, please check KEP.\\nNote:  You need to enable the CSIVolumeHealth  feature gate  to use this feature from the node\\nside.\\nWhat's next\\nSee the CSI driver documentation  to find out which CSI drivers have implemented this feature.\\nWindows Storage\\nThis page provides an storage overview specific to the Windows operating system.\\nPersistent storage\\nWindows has a layered filesystem driver to mount container layers and create a copy filesystem\\nbased on NTFS. All file paths in the container are resolved only within the context of that\\ncontainer.\\nWith Docker, volume mounts can only target a directory in the container, and not an\\nindividual file. This limitation does not apply to containerd.\\nVolume mounts cannot project files or directories back to the host filesystem.\\nRead-only filesystems are not supported because write access is always required for the\\nWindows registry and SAM database. However, read-only volumes are supported.\\nVolume user-masks and permissions are not available. Because the SAM is not shared\\nbetween the host & container, there's no mapping between them. All permissions are\\nresolved within the context of the container.\\nAs a result, the following storage functionality is not supported on Windows nodes:\\nVolume subpath mounts: only the entire volume can be mounted in a Windows container\\nSubpath volume mounting for Secrets\\nHost mount projection\\nRead-only root filesystem (mapped volumes still support readOnly )\\nBlock device mapping\\nMemory as the storage medium (for example, emptyDir.medium  set to Memory )\\nFile system features like uid/gid; per-user Linux filesystem permissions\\nSetting secret permissions with DefaultMode  (due to UID/GID dependency)\\nNFS based storage/volume support\\nExpanding the mounted volume (resizefs)\\nKubernetes volumes  enable complex applications, with data persistence and Pod volume\\nsharing requirements, to be deployed on Kubernetes. Management of persistent volumes• \\n• \\n• \\n• \\n• \\n• \\n• \\n• \\n• \\n• \\n• \\n• \\n• \\n•\", metadata={'source': './PDFS/Concepts.pdf', 'page': 355}),\n",
       " Document(page_content=\"associated with a specific storage back-end or protocol includes actions such as provisioning/\\nde-provisioning/resizing of volumes, attaching/detaching a volume to/from a Kubernetes node\\nand mounting/dismounting a volume to/from individual containers in a pod that needs to\\npersist data.\\nVolume management components are shipped as Kubernetes volume plugin . The following\\nbroad classes of Kubernetes volume plugins are supported on Windows:\\nFlexVolume plugins\\nPlease note that FlexVolumes have been deprecated as of 1.23\\nCSI Plugins\\nIn-tree volume plugins\\nThe following in-tree plugins support persistent storage on Windows nodes:\\nazureFile\\ngcePersistentDisk\\nvsphereVolume\\nConfiguration\\nResources that Kubernetes provides for configuring Pods.\\nConfiguration Best Practices\\nConfigMaps\\nSecrets\\nResource Management for Pods and Containers\\nOrganizing Cluster Access Using kubeconfig Files\\nResource Management for Windows nodes\\nConfiguration Best Practices\\nThis document highlights and consolidates configuration best practices that are introduced\\nthroughout the user guide, Getting Started documentation, and examples.\\nThis is a living document. If you think of something that is not on this list but might be useful\\nto others, please don't hesitate to file an issue or submit a PR.\\nGeneral Configuration Tips\\nWhen defining configurations, specify the latest stable API version.• \\n◦ \\n• \\n• \\n• \\n• \\n•\", metadata={'source': './PDFS/Concepts.pdf', 'page': 356}),\n",
       " Document(page_content='Configuration files should be stored in version control before being pushed to the cluster.\\nThis allows you to quickly roll back a configuration change if necessary. It also aids\\ncluster re-creation and restoration.\\nWrite your configuration files using YAML rather than JSON. Though these formats can\\nbe used interchangeably in almost all scenarios, YAML tends to be more user-friendly.\\nGroup related objects into a single file whenever it makes sense. One file is often easier to\\nmanage than several. See the guestbook-all-in-one.yaml  file as an example of this syntax.\\nNote also that many kubectl  commands can be called on a directory. For example, you\\ncan call kubectl apply  on a directory of config files.\\nDon\\'t specify default values unnecessarily: simple, minimal configuration will make\\nerrors less likely.\\nPut object descriptions in annotations, to allow better introspection.\\nNote:\\nThere is a breaking change introduced in the YAML 1.2  boolean values specification with\\nrespect to YAML 1.1 . This is a known issue  in Kubernetes. YAML 1.2 only recognizes true and \\nfalse  as valid booleans, while YAML 1.1 also accepts yes, no, on, and off as booleans. However,\\nKubernetes uses YAML parsers  that are mostly compatible with YAML 1.1, which means that\\nusing yes or no instead of true or false  in a YAML manifest may cause unexpected errors or\\nbehaviors. To avoid this issue, it is recommended to always use true or false  for boolean values\\nin YAML manifests, and to quote any strings that may be confused with booleans, such as \"yes\"\\nor \"no\" .\\nBesides booleans, there are additional specifications changes between YAML versions. Please\\nrefer to the YAML Specification Changes  documentation for a comprehensive list.\\n\"Naked\" Pods versus ReplicaSets, Deployments, and Jobs\\nDon\\'t use naked Pods (that is, Pods not bound to a ReplicaSet  or Deployment ) if you can\\navoid it. Naked Pods will not be rescheduled in the event of a node failure.\\nA Deployment, which both creates a ReplicaSet to ensure that the desired number of Pods\\nis always available, and specifies a strategy to replace Pods (such as RollingUpdate ), is\\nalmost always preferable to creating Pods directly, except for some explicit restartPolicy: \\nNever  scenarios. A Job may also be appropriate.\\nServices\\nCreate a Service  before its corresponding backend workloads (Deployments or\\nReplicaSets), and before any workloads that need to access it. When Kubernetes starts a\\ncontainer, it provides environment variables pointing to all the Services which were\\nrunning when the container was started. For example, if a Service named foo exists, all\\ncontainers will get the following variables in their initial environment:\\nFOO_SERVICE_HOST =<the host the Service is running on>\\nFOO_SERVICE_PORT =<the port the Service is running on>• \\n• \\n• \\n• \\n• \\n• \\n• \\n•', metadata={'source': './PDFS/Concepts.pdf', 'page': 357}),\n",
       " Document(page_content='This does imply an ordering requirement  - any Service  that a Pod wants to access must be\\ncreated before the Pod itself, or else the environment variables will not be populated.\\nDNS does not have this restriction.\\nAn optional (though strongly recommended) cluster add-on  is a DNS server. The DNS\\nserver watches the Kubernetes API for new Services  and creates a set of DNS records for\\neach. If DNS has been enabled throughout the cluster then all Pods  should be able to do\\nname resolution of Services  automatically.\\nDon\\'t specify a hostPort  for a Pod unless it is absolutely necessary. When you bind a Pod\\nto a hostPort , it limits the number of places the Pod can be scheduled, because each\\n<hostIP , hostPort , protocol > combination must be unique. If you don\\'t specify the hostIP\\nand protocol  explicitly, Kubernetes will use 0.0.0.0  as the default hostIP  and TCP as the\\ndefault protocol .\\nIf you only need access to the port for debugging purposes, you can use the apiserver\\nproxy  or kubectl port-forward .\\nIf you explicitly need to expose a Pod\\'s port on the node, consider using a NodePort\\nService before resorting to hostPort .\\nAvoid using hostNetwork , for the same reasons as hostPort .\\nUse headless Services  (which have a ClusterIP  of None ) for service discovery when you\\ndon\\'t need kube-proxy  load balancing.\\nUsing Labels\\nDefine and use labels  that identify semantic attributes  of your application or\\nDeployment, such as { app.kubernetes.io/name: MyApp, tier: frontend, phase: test, \\ndeployment: v3 } . You can use these labels to select the appropriate Pods for other\\nresources; for example, a Service that selects all tier: frontend  Pods, or all phase: test\\ncomponents of app.kubernetes.io/name: MyApp . See the guestbook  app for examples of\\nthis approach.\\nA Service can be made to span multiple Deployments by omitting release-specific labels\\nfrom its selector. When you need to update a running service without downtime, use a \\nDeployment .\\nA desired state of an object is described by a Deployment, and if changes to that spec are \\napplied , the deployment controller changes the actual state to the desired state at a\\ncontrolled rate.\\nUse the Kubernetes common labels  for common use cases. These standardized labels\\nenrich the metadata in a way that allows tools, including kubectl  and dashboard , to work\\nin an interoperable way.\\nYou can manipulate labels for debugging. Because Kubernetes controllers (such as\\nReplicaSet) and Services match to Pods using selector labels, removing the relevant labels\\nfrom a Pod will stop it from being considered by a controller or from being served traffic\\nby a Service. If you remove the labels of an existing Pod, its controller will create a new\\nPod to take its place. This is a useful way to debug a previously \"live\" Pod in a\\n\"quarantine\" environment. To interactively remove or add labels, use kubectl label .• \\n• \\n• \\n• \\n• \\n• \\n•', metadata={'source': './PDFS/Concepts.pdf', 'page': 358}),\n",
       " Document(page_content='Using kubectl\\nUse kubectl apply -f <directory> . This looks for Kubernetes configuration in\\nall .yaml , .yml, and .json  files in <directory>  and passes it to apply .\\nUse label selectors for get and delete  operations instead of specific object names. See the\\nsections on label selectors  and using labels effectively .\\nUse kubectl create deployment  and kubectl expose  to quickly create single-container\\nDeployments and Services. See Use a Service to Access an Application in a Cluster  for an\\nexample.\\nConfigMaps\\nA ConfigMap is an API object used to store non-confidential data in key-value pairs. Pods  can\\nconsume ConfigMaps as environment variables, command-line arguments, or as configuration\\nfiles in a volume .\\nA ConfigMap allows you to decouple environment-specific configuration from your container\\nimages , so that your applications are easily portable.\\nCaution:  ConfigMap does not provide secrecy or encryption. If the data you want to store are\\nconfidential, use a Secret  rather than a ConfigMap, or use additional (third party) tools to keep\\nyour data private.\\nMotivation\\nUse a ConfigMap for setting configuration data separately from application code.\\nFor example, imagine that you are developing an application that you can run on your own\\ncomputer (for development) and in the cloud (to handle real traffic). You write the code to look\\nin an environment variable named DATABASE_HOST . Locally, you set that variable to \\nlocalhost . In the cloud, you set it to refer to a Kubernetes Service  that exposes the database\\ncomponent to your cluster. This lets you fetch a container image running in the cloud and\\ndebug the exact same code locally if needed.\\nNote:  A ConfigMap is not designed to hold large chunks of data. The data stored in a\\nConfigMap cannot exceed 1 MiB. If you need to store settings that are larger than this limit, you\\nmay want to consider mounting a volume or use a separate database or file service.\\nConfigMap object\\nA ConfigMap is an API object  that lets you store configuration for other objects to use. Unlike\\nmost Kubernetes objects that have a spec, a ConfigMap has data and binaryData  fields. These\\nfields accept key-value pairs as their values. Both the data field and the binaryData  are optional.\\nThe data field is designed to contain UTF-8 strings while the binaryData  field is designed to\\ncontain binary data as base64-encoded strings.\\nThe name of a ConfigMap must be a valid DNS subdomain name .• \\n• \\n•', metadata={'source': './PDFS/Concepts.pdf', 'page': 359}),\n",
       " Document(page_content='Each key under the data or the binaryData  field must consist of alphanumeric characters, -, _\\nor .. The keys stored in data must not overlap with the keys in the binaryData  field.\\nStarting from v1.19, you can add an immutable  field to a ConfigMap definition to create an \\nimmutable ConfigMap .\\nConfigMaps and Pods\\nYou can write a Pod spec that refers to a ConfigMap and configures the container(s) in that Pod\\nbased on the data in the ConfigMap. The Pod and the ConfigMap must be in the same \\nnamespace .\\nNote:  The spec of a static Pod  cannot refer to a ConfigMap or any other API objects.\\nHere\\'s an example ConfigMap that has some keys with single values, and other keys where the\\nvalue looks like a fragment of a configuration format.\\napiVersion : v1\\nkind: ConfigMap\\nmetadata :\\n  name : game-demo\\ndata:\\n  # property-like keys; each key maps to a simple value\\n  player_initial_lives : \"3\"\\n  ui_properties_file_name : \"user-interface.properties\"\\n  # file-like keys\\n  game.properties : |\\n    enemy.types=aliens,monsters\\n    player.maximum-lives=5     \\n  user-interface.properties : |\\n    color.good=purple\\n    color.bad=yellow\\n    allow.textmode=true     \\nThere are four different ways that you can use a ConfigMap to configure a container inside a\\nPod:\\nInside a container command and args\\nEnvironment variables for a container\\nAdd a file in read-only volume, for the application to read\\nWrite code to run inside the Pod that uses the Kubernetes API to read a ConfigMap\\nThese different methods lend themselves to different ways of modeling the data being\\nconsumed. For the first three methods, the kubelet  uses the data from the ConfigMap when it\\nlaunches container(s) for a Pod.\\nThe fourth method means you have to write code to read the ConfigMap and its data. However,\\nbecause you\\'re using the Kubernetes API directly, your application can subscribe to get updates\\nwhenever the ConfigMap changes, and react when that happens. By accessing the Kubernetes\\nAPI directly, this technique also lets you access a ConfigMap in a different namespace.\\nHere\\'s an example Pod that uses values from game-demo  to configure a Pod:1. \\n2. \\n3. \\n4.', metadata={'source': './PDFS/Concepts.pdf', 'page': 360}),\n",
       " Document(page_content='configmap/configure-pod.yaml  \\napiVersion : v1\\nkind: Pod\\nmetadata :\\n  name : configmap-demo-pod\\nspec:\\n  containers :\\n    - name : demo\\n      image : alpine\\n      command : [\"sleep\" , \"3600\" ]\\n      env:\\n        # Define the environment variable\\n        - name : PLAYER_INITIAL_LIVES  # Notice that the case is different here\\n                                     # from the key name in the ConfigMap.\\n          valueFrom :\\n            configMapKeyRef :\\n              name : game-demo           # The ConfigMap this value comes from.\\n              key: player_initial_lives  # The key to fetch.\\n        - name : UI_PROPERTIES_FILE_NAME\\n          valueFrom :\\n            configMapKeyRef :\\n              name : game-demo\\n              key: ui_properties_file_name\\n      volumeMounts :\\n      - name : config\\n        mountPath : \"/config\"\\n        readOnly : true\\n  volumes :\\n  # You set volumes at the Pod level, then mount them into containers inside that Pod\\n  - name : config\\n    configMap :\\n      # Provide the name of the ConfigMap you want to mount.\\n      name : game-demo\\n      # An array of keys from the ConfigMap to create as files\\n      items :\\n      - key: \"game.properties\"\\n        path: \"game.properties\"\\n      - key: \"user-interface.properties\"\\n        path: \"user-interface.properties\"\\n        \\nA ConfigMap doesn\\'t differentiate between single line property values and multi-line file-like\\nvalues. What matters is how Pods and other objects consume those values.\\nFor this example, defining a volume and mounting it inside the demo  container as /config\\ncreates two files, /config/game.properties  and /config/user-interface.properties , even though\\nthere are four keys in the ConfigMap. This is because the Pod definition specifies an items  array\\nin the volumes  section. If you omit the items  array entirely, every key in the ConfigMap\\nbecomes a file with the same name as the key, and you get 4 files.', metadata={'source': './PDFS/Concepts.pdf', 'page': 361}),\n",
       " Document(page_content='Using ConfigMaps\\nConfigMaps can be mounted as data volumes. ConfigMaps can also be used by other parts of\\nthe system, without being directly exposed to the Pod. For example, ConfigMaps can hold data\\nthat other parts of the system should use for configuration.\\nThe most common way to use ConfigMaps is to configure settings for containers running in a\\nPod in the same namespace. You can also use a ConfigMap separately.\\nFor example, you might encounter addons  or operators  that adjust their behavior based on a\\nConfigMap.\\nUsing ConfigMaps as files from a Pod\\nTo consume a ConfigMap in a volume in a Pod:\\nCreate a ConfigMap or use an existing one. Multiple Pods can reference the same\\nConfigMap.\\nModify your Pod definition to add a volume under .spec.volumes[] . Name the volume\\nanything, and have a .spec.volumes[].configMap.name  field set to reference your\\nConfigMap object.\\nAdd a .spec.containers[].volumeMounts[]  to each container that needs the ConfigMap.\\nSpecify .spec.containers[].volumeMounts[].readOnly = true\\nand .spec.containers[].volumeMounts[].mountPath  to an unused directory name where\\nyou would like the ConfigMap to appear.\\nModify your image or command line so that the program looks for files in that directory.\\nEach key in the ConfigMap data map becomes the filename under mountPath .\\nThis is an example of a Pod that mounts a ConfigMap in a volume:\\napiVersion : v1\\nkind: Pod\\nmetadata :\\n  name : mypod\\nspec:\\n  containers :\\n  - name : mypod\\n    image : redis\\n    volumeMounts :\\n    - name : foo\\n      mountPath : \"/etc/foo\"\\n      readOnly : true\\n  volumes :\\n  - name : foo\\n    configMap :\\n      name : myconfigmap\\nEach ConfigMap you want to use needs to be referred to in .spec.volumes .\\nIf there are multiple containers in the Pod, then each container needs its own volumeMounts\\nblock, but only one .spec.volumes  is needed per ConfigMap.1. \\n2. \\n3. \\n4.', metadata={'source': './PDFS/Concepts.pdf', 'page': 362}),\n",
       " Document(page_content=\"Mounted ConfigMaps are updated automatically\\nWhen a ConfigMap currently consumed in a volume is updated, projected keys are eventually\\nupdated as well. The kubelet checks whether the mounted ConfigMap is fresh on every periodic\\nsync. However, the kubelet uses its local cache for getting the current value of the ConfigMap.\\nThe type of the cache is configurable using the configMapAndSecretChangeDetectionStrategy\\nfield in the KubeletConfiguration struct . A ConfigMap can be either propagated by watch\\n(default), ttl-based, or by redirecting all requests directly to the API server. As a result, the total\\ndelay from the moment when the ConfigMap is updated to the moment when new keys are\\nprojected to the Pod can be as long as the kubelet sync period + cache propagation delay, where\\nthe cache propagation delay depends on the chosen cache type (it equals to watch propagation\\ndelay, ttl of cache, or zero correspondingly).\\nConfigMaps consumed as environment variables are not updated automatically and require a\\npod restart.\\nNote:  A container using a ConfigMap as a subPath  volume mount will not receive ConfigMap\\nupdates.\\nImmutable ConfigMaps\\nFEATURE STATE:  Kubernetes v1.21 [stable]\\nThe Kubernetes feature Immutable Secrets and ConfigMaps  provides an option to set individual\\nSecrets and ConfigMaps as immutable. For clusters that extensively use ConfigMaps (at least\\ntens of thousands of unique ConfigMap to Pod mounts), preventing changes to their data has\\nthe following advantages:\\nprotects you from accidental (or unwanted) updates that could cause applications outages\\nimproves performance of your cluster by significantly reducing load on kube-apiserver,\\nby closing watches for ConfigMaps marked as immutable.\\nYou can create an immutable ConfigMap by setting the immutable  field to true. For example:\\napiVersion : v1\\nkind: ConfigMap\\nmetadata :\\n  ...\\ndata:\\n  ...\\nimmutable : true\\nOnce a ConfigMap is marked as immutable, it is not possible to revert this change nor to mutate\\nthe contents of the data or the binaryData  field. You can only delete and recreate the\\nConfigMap. Because existing Pods maintain a mount point to the deleted ConfigMap, it is\\nrecommended to recreate these pods.\\nWhat's next\\nRead about Secrets .\\nRead Configure a Pod to Use a ConfigMap .\\nRead about changing a ConfigMap (or any other Kubernetes object)• \\n• \\n• \\n• \\n•\", metadata={'source': './PDFS/Concepts.pdf', 'page': 363}),\n",
       " Document(page_content='Read The Twelve-Factor App  to understand the motivation for separating code from\\nconfiguration.\\nSecrets\\nA Secret is an object that contains a small amount of sensitive data such as a password, a token,\\nor a key. Such information might otherwise be put in a Pod specification or in a container\\nimage . Using a Secret means that you don\\'t need to include confidential data in your application\\ncode.\\nBecause Secrets can be created independently of the Pods that use them, there is less risk of the\\nSecret (and its data) being exposed during the workflow of creating, viewing, and editing Pods.\\nKubernetes, and applications that run in your cluster, can also take additional precautions with\\nSecrets, such as avoiding writing sensitive data to nonvolatile storage.\\nSecrets are similar to ConfigMaps  but are specifically intended to hold confidential data.\\nCaution:\\nKubernetes Secrets are, by default, stored unencrypted in the API server\\'s underlying data store\\n(etcd). Anyone with API access can retrieve or modify a Secret, and so can anyone with access\\nto etcd. Additionally, anyone who is authorized to create a Pod in a namespace can use that\\naccess to read any Secret in that namespace; this includes indirect access such as the ability to\\ncreate a Deployment.\\nIn order to safely use Secrets, take at least the following steps:\\nEnable Encryption at Rest  for Secrets.\\nEnable or configure RBAC rules  with least-privilege access to Secrets.\\nRestrict Secret access to specific containers.\\nConsider using external Secret store providers .\\nFor more guidelines to manage and improve the security of your Secrets, refer to Good\\npractices for Kubernetes Secrets .\\nSee Information security for Secrets  for more details.\\nUses for Secrets\\nYou can use Secrets for purposes such as the following:\\nSet environment variables for a container .\\nProvide credentials such as SSH keys or passwords to Pods .\\nAllow the kubelet to pull container images from private registries .\\nThe Kubernetes control plane also uses Secrets; for example, bootstrap token Secrets  are a\\nmechanism to help automate node registration.\\nUse case: dotfiles in a secret volume\\nYou can make your data \"hidden\" by defining a key that begins with a dot. This key represents a\\ndotfile or \"hidden\" file. For example, when the following Secret is mounted into a volume, • \\n1. \\n2. \\n3. \\n4. \\n• \\n• \\n•', metadata={'source': './PDFS/Concepts.pdf', 'page': 364}),\n",
       " Document(page_content='secret-volume , the volume will contain a single file, called .secret-file , and the dotfile-test-\\ncontainer  will have this file present at the path /etc/secret-volume/.secret-file .\\nNote:  Files beginning with dot characters are hidden from the output of ls -l; you must use ls -\\nla to see them when listing directory contents.\\nsecret/dotfile-secret.yaml  \\napiVersion : v1\\nkind: Secret\\nmetadata :\\n  name : dotfile-secret\\ndata:\\n  .secret-file : dmFsdWUtMg0KDQo=\\n---\\napiVersion : v1\\nkind: Pod\\nmetadata :\\n  name : secret-dotfiles-pod\\nspec:\\n  volumes :\\n    - name : secret-volume\\n      secret :\\n        secretName : dotfile-secret\\n  containers :\\n    - name : dotfile-test-container\\n      image : registry.k8s.io/busybox\\n      command :\\n        - ls\\n        - \"-l\"\\n        - \"/etc/secret-volume\"\\n      volumeMounts :\\n        - name : secret-volume\\n          readOnly : true\\n          mountPath : \"/etc/secret-volume\"\\nUse case: Secret visible to one container in a Pod\\nConsider a program that needs to handle HTTP requests, do some complex business logic, and\\nthen sign some messages with an HMAC. Because it has complex application logic, there might\\nbe an unnoticed remote file reading exploit in the server, which could expose the private key to\\nan attacker.\\nThis could be divided into two processes in two containers: a frontend container which handles\\nuser interaction and business logic, but which cannot see the private key; and a signer\\ncontainer that can see the private key, and responds to simple signing requests from the\\nfrontend (for example, over localhost networking).\\nWith this partitioned approach, an attacker now has to trick the application server into doing\\nsomething rather arbitrary, which may be harder than getting it to read a file.\\nAlternatives to Secrets\\nRather than using a Secret to protect confidential data, you can pick from alternatives.', metadata={'source': './PDFS/Concepts.pdf', 'page': 365}),\n",
       " Document(page_content=\"Here are some of your options:\\nIf your cloud-native component needs to authenticate to another application that you\\nknow is running within the same Kubernetes cluster, you can use a ServiceAccount  and\\nits tokens to identify your client.\\nThere are third-party tools that you can run, either within or outside your cluster, that\\nmanage sensitive data. For example, a service that Pods access over HTTPS, that reveals a\\nSecret if the client correctly authenticates (for example, with a ServiceAccount token).\\nFor authentication, you can implement a custom signer for X.509 certificates, and use \\nCertificateSigningRequests  to let that custom signer issue certificates to Pods that need\\nthem.\\nYou can use a device plugin  to expose node-local encryption hardware to a specific Pod.\\nFor example, you can schedule trusted Pods onto nodes that provide a Trusted Platform\\nModule, configured out-of-band.\\nYou can also combine two or more of those options, including the option to use Secret objects\\nthemselves.\\nFor example: implement (or deploy) an operator  that fetches short-lived session tokens from an\\nexternal service, and then creates Secrets based on those short-lived session tokens. Pods\\nrunning in your cluster can make use of the session tokens, and operator ensures they are valid.\\nThis separation means that you can run Pods that are unaware of the exact mechanisms for\\nissuing and refreshing those session tokens.\\nTypes of Secret\\nWhen creating a Secret, you can specify its type using the type field of the Secret  resource, or\\ncertain equivalent kubectl  command line flags (if available). The Secret type is used to facilitate\\nprogrammatic handling of the Secret data.\\nKubernetes provides several built-in types for some common usage scenarios. These types vary\\nin terms of the validations performed and the constraints Kubernetes imposes on them.\\nBuilt-in Type Usage\\nOpaque arbitrary user-defined data\\nkubernetes.io/service-account-token ServiceAccount token\\nkubernetes.io/dockercfg serialized ~/.dockercfg  file\\nkubernetes.io/dockerconfigjson serialized ~/.docker/config.json  file\\nkubernetes.io/basic-auth credentials for basic authentication\\nkubernetes.io/ssh-auth credentials for SSH authentication\\nkubernetes.io/tls data for a TLS client or server\\nbootstrap.kubernetes.io/token bootstrap token data\\nYou can define and use your own Secret type by assigning a non-empty string as the type value\\nfor a Secret object (an empty string is treated as an Opaque  type).\\nKubernetes doesn't impose any constraints on the type name. However, if you are using one of\\nthe built-in types, you must meet all the requirements defined for that type.\\nIf you are defining a type of Secret that's for public use, follow the convention and structure the\\nSecret type to have your domain name before the name, separated by a /. For example: cloud-\\nhosting.example.net/cloud-api-credentials .• \\n• \\n• \\n•\", metadata={'source': './PDFS/Concepts.pdf', 'page': 366}),\n",
       " Document(page_content=\"Opaque Secrets\\nOpaque  is the default Secret type if you don't explicitly specify a type in a Secret manifest.\\nWhen you create a Secret using kubectl , you must use the generic  subcommand to indicate an \\nOpaque  Secret type. For example, the following command creates an empty Secret of type \\nOpaque :\\nkubectl create secret generic empty-secret\\nkubectl get secret empty-secret\\nThe output looks like:\\nNAME           TYPE     DATA   AGE\\nempty-secret   Opaque   0      2m6s\\nThe DATA  column shows the number of data items stored in the Secret. In this case, 0 means\\nyou have created an empty Secret.\\nServiceAccount token Secrets\\nA kubernetes.io/service-account-token  type of Secret is used to store a token credential that\\nidentifies a ServiceAccount . This is a legacy mechanism that provides long-lived ServiceAccount\\ncredentials to Pods.\\nIn Kubernetes v1.22 and later, the recommended approach is to obtain a short-lived,\\nautomatically rotating ServiceAccount token by using the TokenRequest  API instead. You can\\nget these short-lived tokens using the following methods:\\nCall the TokenRequest  API either directly or by using an API client like kubectl . For\\nexample, you can use the kubectl create token  command.\\nRequest a mounted token in a projected volume  in your Pod manifest. Kubernetes creates\\nthe token and mounts it in the Pod. The token is automatically invalidated when the Pod\\nthat it's mounted in is deleted. For details, see Launch a Pod using service account token\\nprojection .\\nNote:  You should only create a ServiceAccount token Secret if you can't use the TokenRequest\\nAPI to obtain a token, and the security exposure of persisting a non-expiring token credential\\nin a readable API object is acceptable to you. For instructions, see Manually create a long-lived\\nAPI token for a ServiceAccount .\\nWhen using this Secret type, you need to ensure that the kubernetes.io/service-account.name\\nannotation is set to an existing ServiceAccount name. If you are creating both the\\nServiceAccount and the Secret objects, you should create the ServiceAccount object first.\\nAfter the Secret is created, a Kubernetes controller  fills in some other fields such as the \\nkubernetes.io/service-account.uid  annotation, and the token  key in the data field, which is\\npopulated with an authentication token.\\nThe following example configuration declares a ServiceAccount token Secret:\\nsecret/serviceaccount-token-secret.yaml  \\napiVersion : v1\\nkind: Secret• \\n•\", metadata={'source': './PDFS/Concepts.pdf', 'page': 367}),\n",
       " Document(page_content='metadata :\\n  name : secret-sa-sample\\n  annotations :\\n    kubernetes.io/service-account.name : \"sa-name\"\\ntype: kubernetes.io/service-account-token\\ndata:\\n  extra : YmFyCg==\\nAfter creating the Secret, wait for Kubernetes to populate the token  key in the data field.\\nSee the ServiceAccount  documentation for more information on how ServiceAccounts work.\\nYou can also check the automountServiceAccountToken  field and the serviceAccountName  field\\nof the Pod for information on referencing ServiceAccount credentials from within Pods.\\nDocker config Secrets\\nIf you are creating a Secret to store credentials for accessing a container image registry, you\\nmust use one of the following type values for that Secret:\\nkubernetes.io/dockercfg : store a serialized ~/.dockercfg  which is the legacy format for\\nconfiguring Docker command line. The Secret data field contains a .dockercfg  key whose\\nvalue is the content of a base64 encoded ~/.dockercfg  file.\\nkubernetes.io/dockerconfigjson : store a serialized JSON that follows the same format\\nrules as the ~/.docker/config.json  file, which is a new format for ~/.dockercfg . The Secret \\ndata field must contain a .dockerconfigjson  key for which the value is the content of a\\nbase64 encoded ~/.docker/config.json  file.\\nBelow is an example for a kubernetes.io/dockercfg  type of Secret:\\nsecret/dockercfg-secret.yaml  \\napiVersion : v1\\nkind: Secret\\nmetadata :\\n  name : secret-dockercfg\\ntype: kubernetes.io/dockercfg\\ndata:\\n  .dockercfg : |\\n    \\neyJhdXRocyI6eyJodHRwczovL2V4YW1wbGUvdjEvIjp7ImF1dGgiOiJvcGVuc2VzYW1lIn19fQo=    \\n \\nNote:  If you do not want to perform the base64 encoding, you can choose to use the stringData\\nfield instead.\\nWhen you create Docker config Secrets using a manifest, the API server checks whether the\\nexpected key exists in the data field, and it verifies if the value provided can be parsed as a valid\\nJSON. The API server doesn\\'t validate if the JSON actually is a Docker config file.\\nYou can also use kubectl  to create a Secret for accessing a container registry, such as when you\\ndon\\'t have a Docker configuration file:\\nkubectl create secret docker-registry secret-tiger-docker \\\\\\n  --docker-email =tiger@acme.example \\\\• \\n•', metadata={'source': './PDFS/Concepts.pdf', 'page': 368}),\n",
       " Document(page_content='--docker-username =tiger \\\\\\n  --docker-password =pass1234 \\\\\\n  --docker-server =my-registry.example:5000\\nThis command creates a Secret of type kubernetes.io/dockerconfigjson .\\nRetrieve the .data.dockerconfigjson  field from that new Secret and decode the data:\\nkubectl get secret secret-tiger-docker -o jsonpath =\\'{.data.*}\\'  | base64 -d\\nThe output is equivalent to the following JSON document (which is also a valid Docker\\nconfiguration file):\\n{\\n  \"auths\" : {\\n    \"my-registry.example:5000\" : {\\n      \"username\" : \"tiger\" ,\\n      \"password\" : \"pass1234\" ,\\n      \"email\" : \"tiger@acme.example\" ,\\n      \"auth\" : \"dGlnZXI6cGFzczEyMzQ=\"\\n    }\\n  }\\n}\\nCaution:\\nThe auth value there is base64 encoded; it is obscured but not secret. Anyone who can read that\\nSecret can learn the registry access bearer token.\\nIt is suggested to use credential providers  to dynamically and securely provide pull secrets on-\\ndemand.\\nBasic authentication Secret\\nThe kubernetes.io/basic-auth  type is provided for storing credentials needed for basic\\nauthentication. When using this Secret type, the data field of the Secret must contain one of the\\nfollowing two keys:\\nusername : the user name for authentication\\npassword : the password or token for authentication\\nBoth values for the above two keys are base64 encoded strings. You can alternatively provide\\nthe clear text content using the stringData  field in the Secret manifest.\\nThe following manifest is an example of a basic authentication Secret:\\nsecret/basicauth-secret.yaml  \\napiVersion : v1\\nkind: Secret\\nmetadata :\\n  name : secret-basic-auth\\ntype: kubernetes.io/basic-auth\\nstringData :• \\n•', metadata={'source': './PDFS/Concepts.pdf', 'page': 369}),\n",
       " Document(page_content='username : admin  # required field for kubernetes.io/basic-auth\\n  password : t0p-Secret  # required field for kubernetes.io/basic-auth\\nNote:  The stringData  field for a Secret does not work well with server-side apply.\\nThe basic authentication Secret type is provided only for convenience. You can create an \\nOpaque  type for credentials used for basic authentication. However, using the defined and\\npublic Secret type ( kubernetes.io/basic-auth ) helps other people to understand the purpose of\\nyour Secret, and sets a convention for what key names to expect. The Kubernetes API verifies\\nthat the required keys are set for a Secret of this type.\\nSSH authentication Secrets\\nThe builtin type kubernetes.io/ssh-auth  is provided for storing data used in SSH authentication.\\nWhen using this Secret type, you will have to specify a ssh-privatekey  key-value pair in the \\ndata (or stringData ) field as the SSH credential to use.\\nThe following manifest is an example of a Secret used for SSH public/private key\\nauthentication:\\nsecret/ssh-auth-secret.yaml  \\napiVersion : v1\\nkind: Secret\\nmetadata :\\n  name : secret-ssh-auth\\ntype: kubernetes.io/ssh-auth\\ndata:\\n  # the data is abbreviated in this example\\n  ssh-privatekey : |\\n    UG91cmluZzYlRW1vdGljb24lU2N1YmE=     \\nThe SSH authentication Secret type is provided only for convenience. You can create an Opaque\\ntype for credentials used for SSH authentication. However, using the defined and public Secret\\ntype ( kubernetes.io/ssh-auth ) helps other people to understand the purpose of your Secret, and\\nsets a convention for what key names to expect. The Kubernetes API verifies that the required\\nkeys are set for a Secret of this type.\\nCaution:  SSH private keys do not establish trusted communication between an SSH client and\\nhost server on their own. A secondary means of establishing trust is needed to mitigate \"man in\\nthe middle\" attacks, such as a known_hosts  file added to a ConfigMap.\\nTLS Secrets\\nThe kubernetes.io/tls  Secret type is for storing a certificate and its associated key that are\\ntypically used for TLS.\\nOne common use for TLS Secrets is to configure encryption in transit for an Ingress , but you\\ncan also use it with other resources or directly in your workload. When using this type of\\nSecret, the tls.key  and the tls.crt  key must be provided in the data (or stringData ) field of the\\nSecret configuration, although the API server doesn\\'t actually validate the values for each key.', metadata={'source': './PDFS/Concepts.pdf', 'page': 370}),\n",
       " Document(page_content='As an alternative to using stringData , you can use the data field to provide the base64 encoded\\ncertificate and private key. For details, see Constraints on Secret names and data .\\nThe following YAML contains an example config for a TLS Secret:\\nsecret/tls-auth-secret.yaml  \\napiVersion : v1\\nkind: Secret\\nmetadata :\\n  name : secret-tls\\ntype: kubernetes.io/tls\\ndata:\\n  # values are base64 encoded, which obscures them but does NOT provide\\n  # any useful level of confidentiality\\n  tls.crt : |\\n    \\nLS0tLS1CRUdJTiBDRVJUSUZJQ0FURS0tLS0tCk1JSUNVakNDQWJzQ0FnMytNQTBHQ1NxR1N\\nJYjNE\\n    \\nUUVCQlFVQU1JR2JNUXN3Q1FZRFZRUUdFd0pLVURFT01Bd0cKQTFVRUNCTUZWRzlyZVc4e\\nEVEQU9C\\n    \\nZ05WQkFjVEIwTm9kVzh0YTNVeEVUQVBCZ05WQkFvVENFWnlZVzVyTkVSRQpNUmd3Rmd\\nZRFZRUUxF\\n    \\ndzlYWldKRFpYSjBJRk4xY0hCdmNuUXhHREFXQmdOVkJBTVREMFp5WVc1ck5FUkVJRmRsCll\\npQkRR\\n    \\nVEVqTUNFR0NTcUdTSWIzRFFFSkFSWVVjM1Z3Y0c5eWRFQm1jbUZ1YXpSa1pDNWpiMjB3S\\nGhjTk1U\\n    \\nTXcKTVRFeE1EUTFNVE01V2hjTk1UZ3dNVEV3TURRMU1UTTVXakJMTVFzd0NRWURWUV\\nFHREFKS1VE\\n    \\nRVBNQTBHQTFVRQpDQXdHWEZSdmEzbHZNUkV3RHdZRFZRUUtEQWhHY21GdWF6UkVS\\nREVZTUJZR0Ex\\n    \\nVUVBd3dQZDNkM0xtVjRZVzF3CmJHVXVZMjl0TUlHYU1BMEdDU3FHU0liM0RRRUJBUVVB\\nQTRHSUFE\\n    \\nQ0JoQUo5WThFaUhmeHhNL25PbjJTbkkxWHgKRHdPdEJEVDFKRjBReTliMVlKanV2YjdjaTEw\\nZjVN\\n    \\nVm1UQllqMUZTVWZNOU1vejJDVVFZdW4yRFljV29IcFA4ZQpqSG1BUFVrNVd5cDJRN1ArMj\\nh1bklI\\n    \\nQkphVGZlQ09PekZSUFY2MEdTWWUzNmFScG04L3dVVm16eGFLOGtCOWVaCmhPN3F1Tjdt\\nSWQxL2pW\\n    \\ncTNKODhDQXdFQUFUQU5CZ2txaGtpRzl3MEJBUVVGQUFPQmdRQU1meTQzeE15OHh3QTU\\nKVjF2T2NS', metadata={'source': './PDFS/Concepts.pdf', 'page': 371}),\n",
       " Document(page_content='OEtyNWNaSXdtbFhCUU8xeFEzazlxSGtyNFlUY1JxTVQ5WjVKTm1rWHYxK2VSaGcwTi9WM\\nW5NUTRZ\\n    \\nRgpnWXcxbnlESnBnOTduZUV4VzQyeXVlMFlHSDYyV1hYUUhyOVNVREgrRlowVnQvRGZsd\\nklVTWRj\\n    \\nUUFEZjM4aU9zCjlQbG1kb3YrcE0vNCs5a1h5aDhSUEkzZXZ6OS9NQT09Ci0tLS0tRU5EIENFUlR\\nJ\\n    RklDQVRFLS0tLS0K     \\n  # In this example, the key data is not a real PEM-encoded private key\\n  tls.key : |\\n    RXhhbXBsZSBkYXRhIGZvciB0aGUgVExTIGNydCBmaWVsZA==     \\nThe TLS Secret type is provided only for convenience. You can create an Opaque  type for\\ncredentials used for TLS authentication. However, using the defined and public Secret type\\n(kubernetes.io/ssh-auth ) helps ensure the consistency of Secret format in your project. The API\\nserver verifies if the required keys are set for a Secret of this type.\\nTo create a TLS Secret using kubectl , use the tls subcommand:\\nkubectl create secret tls my-tls-secret \\\\\\n  --cert =path/to/cert/file \\\\\\n  --key =path/to/key/file\\nThe public/private key pair must exist before hand. The public key certificate for --cert  must be\\n.PEM encoded and must match the given private key for --key .\\nBootstrap token Secrets\\nThe bootstrap.kubernetes.io/token  Secret type is for tokens used during the node bootstrap\\nprocess. It stores tokens used to sign well-known ConfigMaps.\\nA bootstrap token Secret is usually created in the kube-system  namespace and named in the\\nform bootstrap-token-<token-id>  where <token-id>  is a 6 character string of the token ID.\\nAs a Kubernetes manifest, a bootstrap token Secret might look like the following:\\nsecret/bootstrap-token-secret-base64.yaml  \\napiVersion : v1\\nkind: Secret\\nmetadata :\\n  name : bootstrap-token-5emitj\\n  namespace : kube-system\\ntype: bootstrap.kubernetes.io/token\\ndata:\\n  auth-extra-groups : c3lzdGVtOmJvb3RzdHJhcHBlcnM6a3ViZWFkbTpkZWZhdWx0LW5vZGUt\\ndG9rZW4=\\n  expiration : MjAyMC0wOS0xM1QwNDozOToxMFo=\\n  token-id : NWVtaXRq\\n  token-secret : a3E0Z2lodnN6emduMXAwcg==\\n  usage-bootstrap-authentication : dHJ1ZQ==\\n  usage-bootstrap-signing : dHJ1ZQ==', metadata={'source': './PDFS/Concepts.pdf', 'page': 372}),\n",
       " Document(page_content='A bootstrap token Secret has the following keys specified under data:\\ntoken-id : A random 6 character string as the token identifier. Required.\\ntoken-secret : A random 16 character string as the actual token Secret. Required.\\ndescription : A human-readable string that describes what the token is used for. Optional.\\nexpiration : An absolute UTC time using RFC3339  specifying when the token should be\\nexpired. Optional.\\nusage-bootstrap-<usage> : A boolean flag indicating additional usage for the bootstrap\\ntoken.\\nauth-extra-groups : A comma-separated list of group names that will be authenticated as\\nin addition to the system:bootstrappers  group.\\nYou can alternatively provide the values in the stringData  field of the Secret without base64\\nencoding them:\\nsecret/bootstrap-token-secret-literal.yaml  \\napiVersion : v1\\nkind: Secret\\nmetadata :\\n  # Note how the Secret is named\\n  name : bootstrap-token-5emitj\\n  # A bootstrap token Secret usually resides in the kube-system namespace\\n  namespace : kube-system\\ntype: bootstrap.kubernetes.io/token\\nstringData :\\n  auth-extra-groups : \"system:bootstrappers:kubeadm:default-node-token\"\\n  expiration : \"2020-09-13T04:39:10Z\"\\n  # This token ID is used in the name\\n  token-id : \"5emitj\"\\n  token-secret : \"kq4gihvszzgn1p0r\"\\n  # This token can be used for authentication\\n  usage-bootstrap-authentication : \"true\"\\n  # and it can be used for signing\\n  usage-bootstrap-signing : \"true\"\\nNote:  The stringData  field for a Secret does not work well with server-side apply.\\nWorking with Secrets\\nCreating a Secret\\nThere are several options to create a Secret:\\nUse kubectl\\nUse a configuration file\\nUse the Kustomize tool\\nConstraints on Secret names and data\\nThe name of a Secret object must be a valid DNS subdomain name .• \\n• \\n• \\n• \\n• \\n• \\n• \\n• \\n•', metadata={'source': './PDFS/Concepts.pdf', 'page': 373}),\n",
       " Document(page_content=\"You can specify the data and/or the stringData  field when creating a configuration file for a\\nSecret. The data and the stringData  fields are optional. The values for all keys in the data field\\nhave to be base64-encoded strings. If the conversion to base64 string is not desirable, you can\\nchoose to specify the stringData  field instead, which accepts arbitrary strings as values.\\nThe keys of data and stringData  must consist of alphanumeric characters, -, _ or .. All key-value\\npairs in the stringData  field are internally merged into the data field. If a key appears in both\\nthe data and the stringData  field, the value specified in the stringData  field takes precedence.\\nSize limit\\nIndividual Secrets are limited to 1MiB in size. This is to discourage creation of very large Secrets\\nthat could exhaust the API server and kubelet memory. However, creation of many smaller\\nSecrets could also exhaust memory. You can use a resource quota  to limit the number of Secrets\\n(or other resources) in a namespace.\\nEditing a Secret\\nYou can edit an existing Secret unless it is immutable . To edit a Secret, use one of the following\\nmethods:\\nUse kubectl\\nUse a configuration file\\nYou can also edit the data in a Secret using the Kustomize tool . However, this method creates a\\nnew Secret  object with the edited data.\\nDepending on how you created the Secret, as well as how the Secret is used in your Pods,\\nupdates to existing Secret  objects are propagated automatically to Pods that use the data. For\\nmore information, refer to Using Secrets as files from a Pod  section.\\nUsing a Secret\\nSecrets can be mounted as data volumes or exposed as environment variables  to be used by a\\ncontainer in a Pod. Secrets can also be used by other parts of the system, without being directly\\nexposed to the Pod. For example, Secrets can hold credentials that other parts of the system\\nshould use to interact with external systems on your behalf.\\nSecret volume sources are validated to ensure that the specified object reference actually points\\nto an object of type Secret. Therefore, a Secret needs to be created before any Pods that depend\\non it.\\nIf the Secret cannot be fetched (perhaps because it does not exist, or due to a temporary lack of\\nconnection to the API server) the kubelet periodically retries running that Pod. The kubelet also\\nreports an Event for that Pod, including details of the problem fetching the Secret.\\nOptional Secrets\\nWhen you reference a Secret in a Pod, you can mark the Secret as optional , such as in the\\nfollowing example. If an optional Secret doesn't exist, Kubernetes ignores it.\\nsecret/optional-secret.yaml  • \\n•\", metadata={'source': './PDFS/Concepts.pdf', 'page': 374}),\n",
       " Document(page_content='apiVersion : v1\\nkind: Pod\\nmetadata :\\n  name : mypod\\nspec:\\n  containers :\\n  - name : mypod\\n    image : redis\\n    volumeMounts :\\n    - name : foo\\n      mountPath : \"/etc/foo\"\\n      readOnly : true\\n  volumes :\\n  - name : foo\\n    secret :\\n      secretName : mysecret\\n      optional : true\\nBy default, Secrets are required. None of a Pod\\'s containers will start until all non-optional\\nSecrets are available.\\nIf a Pod references a specific key in a non-optional Secret and that Secret does exist, but is\\nmissing the named key, the Pod fails during startup.\\nUsing Secrets as files from a Pod\\nIf you want to access data from a Secret in a Pod, one way to do that is to have Kubernetes\\nmake the value of that Secret be available as a file inside the filesystem of one or more of the\\nPod\\'s containers.\\nFor instructions, refer to Distribute credentials securely using Secrets .\\nWhen a volume contains data from a Secret, and that Secret is updated, Kubernetes tracks this\\nand updates the data in the volume, using an eventually-consistent approach.\\nNote:  A container using a Secret as a subPath  volume mount does not receive automated Secret\\nupdates.\\nThe kubelet keeps a cache of the current keys and values for the Secrets that are used in\\nvolumes for pods on that node. You can configure the way that the kubelet detects changes\\nfrom the cached values. The configMapAndSecretChangeDetectionStrategy  field in the kubelet\\nconfiguration  controls which strategy the kubelet uses. The default strategy is Watch .\\nUpdates to Secrets can be either propagated by an API watch mechanism (the default), based on\\na cache with a defined time-to-live, or polled from the cluster API server on each kubelet\\nsynchronisation loop.\\nAs a result, the total delay from the moment when the Secret is updated to the moment when\\nnew keys are projected to the Pod can be as long as the kubelet sync period + cache\\npropagation delay, where the cache propagation delay depends on the chosen cache type\\n(following the same order listed in the previous paragraph, these are: watch propagation delay,\\nthe configured cache TTL, or zero for direct polling).', metadata={'source': './PDFS/Concepts.pdf', 'page': 375}),\n",
       " Document(page_content=\"Using Secrets as environment variables\\nTo use a Secret in an environment variable  in a Pod:\\nFor each container in your Pod specification, add an environment variable for each Secret\\nkey that you want to use to the env[].valueFrom.secretKeyRef  field.\\nModify your image and/or command line so that the program looks for values in the\\nspecified environment variables.\\nFor instructions, refer to Define container environment variables using Secret data .\\nInvalid environment variables\\nIf your environment variable definitions in your Pod specification are considered to be invalid\\nenvironment variable names, those keys aren't made available to your container. The Pod is\\nallowed to start.\\nKubernetes adds an Event with the reason set to InvalidVariableNames  and a message that lists\\nthe skipped invalid keys. The following example shows a Pod that refers to a Secret named \\nmysecret , where mysecret  contains 2 invalid keys: 1badkey  and 2alsobad .\\nkubectl get events\\nThe output is similar to:\\nLASTSEEN   FIRSTSEEN   COUNT     NAME            KIND      SUBOBJECT                         \\nTYPE      REASON\\n0s         0s          1         dapi-test-pod   Pod                                         Warning   \\nInvalidEnvironmentVariableNames   kubelet, 127.0.0.1      Keys [1badkey, 2alsobad] from the \\nEnvFrom secret default/mysecret were skipped since they are considered invalid environment \\nvariable names.\\nContainer image pull Secrets\\nIf you want to fetch container images from a private repository, you need a way for the kubelet\\non each node to authenticate to that repository. You can configure image pull Secrets  to make\\nthis possible. These Secrets are configured at the Pod level.\\nUsing imagePullSecrets\\nThe imagePullSecrets  field is a list of references to Secrets in the same namespace. You can use\\nan imagePullSecrets  to pass a Secret that contains a Docker (or other) image registry password\\nto the kubelet. The kubelet uses this information to pull a private image on behalf of your Pod.\\nSee the PodSpec API  for more information about the imagePullSecrets  field.\\nManually specifying an imagePullSecret\\nYou can learn how to specify imagePullSecrets  from the container images  documentation.1. \\n2.\", metadata={'source': './PDFS/Concepts.pdf', 'page': 376}),\n",
       " Document(page_content='Arranging for imagePullSecrets to be automatically attached\\nYou can manually create imagePullSecrets , and reference these from a ServiceAccount. Any\\nPods created with that ServiceAccount or created with that ServiceAccount by default, will get\\ntheir imagePullSecrets  field set to that of the service account. See Add ImagePullSecrets to a\\nservice account  for a detailed explanation of that process.\\nUsing Secrets with static Pods\\nYou cannot use ConfigMaps or Secrets with static Pods .\\nImmutable Secrets\\nFEATURE STATE:  Kubernetes v1.21 [stable]\\nKubernetes lets you mark specific Secrets (and ConfigMaps) as immutable . Preventing changes\\nto the data of an existing Secret has the following benefits:\\nprotects you from accidental (or unwanted) updates that could cause applications outages\\n(for clusters that extensively use Secrets - at least tens of thousands of unique Secret to\\nPod mounts), switching to immutable Secrets improves the performance of your cluster\\nby significantly reducing load on kube-apiserver. The kubelet does not need to maintain a\\n[watch] on any Secrets that are marked as immutable.\\nMarking a Secret as immutable\\nYou can create an immutable Secret by setting the immutable  field to true. For example,\\napiVersion : v1\\nkind: Secret\\nmetadata : ...\\ndata: ...\\nimmutable : true\\nYou can also update any existing mutable Secret to make it immutable.\\nNote:  Once a Secret or ConfigMap is marked as immutable, it is not possible to revert this\\nchange nor to mutate the contents of the data field. You can only delete and recreate the Secret.\\nExisting Pods maintain a mount point to the deleted Secret - it is recommended to recreate\\nthese pods.\\nInformation security for Secrets\\nAlthough ConfigMap and Secret work similarly, Kubernetes applies some additional protection\\nfor Secret objects.\\nSecrets often hold values that span a spectrum of importance, many of which can cause\\nescalations within Kubernetes (e.g. service account tokens) and to external systems. Even if an\\nindividual app can reason about the power of the Secrets it expects to interact with, other apps\\nwithin the same namespace can render those assumptions invalid.• \\n•', metadata={'source': './PDFS/Concepts.pdf', 'page': 377}),\n",
       " Document(page_content='A Secret is only sent to a node if a Pod on that node requires it. For mounting Secrets into Pods,\\nthe kubelet stores a copy of the data into a tmpfs  so that the confidential data is not written to\\ndurable storage. Once the Pod that depends on the Secret is deleted, the kubelet deletes its local\\ncopy of the confidential data from the Secret.\\nThere may be several containers in a Pod. By default, containers you define only have access to\\nthe default ServiceAccount and its related Secret. You must explicitly define environment\\nvariables or map a volume into a container in order to provide access to any other Secret.\\nThere may be Secrets for several Pods on the same node. However, only the Secrets that a Pod\\nrequests are potentially visible within its containers. Therefore, one Pod does not have access to\\nthe Secrets of another Pod.\\nConfigure least-privilege access to Secrets\\nTo enhance the security measures around Secrets, Kubernetes provides a mechanism: you can\\nannotate a ServiceAccount as kubernetes.io/enforce-mountable-secrets: \"true\" .\\nFor more information, you can refer to the documentation about this annotation .\\nWarning:  Any containers that run with privileged: true  on a node can access all Secrets used\\non that node.\\nWhat\\'s next\\nFor guidelines to manage and improve the security of your Secrets, refer to Good\\npractices for Kubernetes Secrets .\\nLearn how to manage Secrets using kubectl\\nLearn how to manage Secrets using config file\\nLearn how to manage Secrets using kustomize\\nRead the API reference  for Secret\\nResource Management for Pods and\\nContainers\\nWhen you specify a Pod, you can optionally specify how much of each resource a container\\nneeds. The most common resources to specify are CPU and memory (RAM); there are others.\\nWhen you specify the resource request  for containers in a Pod, the kube-scheduler  uses this\\ninformation to decide which node to place the Pod on. When you specify a resource limit  for a\\ncontainer, the kubelet  enforces those limits so that the running container is not allowed to use\\nmore of that resource than the limit you set. The kubelet also reserves at least the request\\namount of that system resource specifically for that container to use.\\nRequests and limits\\nIf the node where a Pod is running has enough of a resource available, it\\'s possible (and\\nallowed) for a container to use more resource than its request  for that resource specifies.\\nHowever, a container is not allowed to use more than its resource limit .• \\n• \\n• \\n• \\n•', metadata={'source': './PDFS/Concepts.pdf', 'page': 378}),\n",
       " Document(page_content='For example, if you set a memory  request of 256 MiB for a container, and that container is in a\\nPod scheduled to a Node with 8GiB of memory and no other Pods, then the container can try to\\nuse more RAM.\\nIf you set a memory  limit of 4GiB for that container, the kubelet (and container runtime )\\nenforce the limit. The runtime prevents the container from using more than the configured\\nresource limit. For example: when a process in the container tries to consume more than the\\nallowed amount of memory, the system kernel terminates the process that attempted the\\nallocation, with an out of memory (OOM) error.\\nLimits can be implemented either reactively (the system intervenes once it sees a violation) or\\nby enforcement (the system prevents the container from ever exceeding the limit). Different\\nruntimes can have different ways to implement the same restrictions.\\nNote:  If you specify a limit for a resource, but do not specify any request, and no admission-\\ntime mechanism has applied a default request for that resource, then Kubernetes copies the\\nlimit you specified and uses it as the requested value for the resource.\\nResource types\\nCPU and memory  are each a resource type . A resource type has a base unit. CPU represents\\ncompute processing and is specified in units of Kubernetes CPUs . Memory is specified in units\\nof bytes. For Linux workloads, you can specify huge page  resources. Huge pages are a Linux-\\nspecific feature where the node kernel allocates blocks of memory that are much larger than the\\ndefault page size.\\nFor example, on a system where the default page size is 4KiB, you could specify a limit, \\nhugepages-2Mi: 80Mi . If the container tries allocating over 40 2MiB huge pages (a total of 80\\nMiB), that allocation fails.\\nNote:  You cannot overcommit hugepages-*  resources. This is different from the memory  and \\ncpu resources.\\nCPU and memory are collectively referred to as compute resources , or resources . Compute\\nresources are measurable quantities that can be requested, allocated, and consumed. They are\\ndistinct from API resources . API resources, such as Pods and Services  are objects that can be\\nread and modified through the Kubernetes API server.\\nResource requests and limits of Pod and container\\nFor each container, you can specify resource limits and requests, including the following:\\nspec.containers[].resources.limits.cpu\\nspec.containers[].resources.limits.memory\\nspec.containers[].resources.limits.hugepages-<size>\\nspec.containers[].resources.requests.cpu\\nspec.containers[].resources.requests.memory\\nspec.containers[].resources.requests.hugepages-<size>\\nAlthough you can only specify requests and limits for individual containers, it is also useful to\\nthink about the overall resource requests and limits for a Pod. For a particular resource, a Pod• \\n• \\n• \\n• \\n• \\n•', metadata={'source': './PDFS/Concepts.pdf', 'page': 379}),\n",
       " Document(page_content='resource request/limit  is the sum of the resource requests/limits of that type for each container\\nin the Pod.\\nResource units in Kubernetes\\nCPU resource units\\nLimits and requests for CPU resources are measured in cpu units. In Kubernetes, 1 CPU unit is\\nequivalent to 1 physical CPU core , or 1 virtual core , depending on whether the node is a\\nphysical host or a virtual machine running inside a physical machine.\\nFractional requests are allowed. When you define a container with \\nspec.containers[].resources.requests.cpu  set to 0.5, you are requesting half as much CPU time\\ncompared to if you asked for 1.0 CPU. For CPU resource units, the quantity  expression 0.1 is\\nequivalent to the expression 100m , which can be read as \"one hundred millicpu\". Some people\\nsay \"one hundred millicores\", and this is understood to mean the same thing.\\nCPU resource is always specified as an absolute amount of resource, never as a relative amount.\\nFor example, 500m  CPU represents the roughly same amount of computing power whether that\\ncontainer runs on a single-core, dual-core, or 48-core machine.\\nNote:  Kubernetes doesn\\'t allow you to specify CPU resources with a precision finer than 1m.\\nBecause of this, it\\'s useful to specify CPU units less than 1.0 or 1000m  using the milliCPU form;\\nfor example, 5m rather than 0.005 .\\nMemory resource units\\nLimits and requests for memory  are measured in bytes. You can express memory as a plain\\ninteger or as a fixed-point number using one of these quantity  suffixes: E, P, T, G, M, k. You can\\nalso use the power-of-two equivalents: Ei, Pi, Ti, Gi, Mi, Ki. For example, the following\\nrepresent roughly the same value:\\n128974848, 129e6, 129M,  128974848000m, 123Mi\\nPay attention to the case of the suffixes. If you request 400m  of memory, this is a request for 0.4\\nbytes. Someone who types that probably meant to ask for 400 mebibytes ( 400Mi ) or 400\\nmegabytes ( 400M ).\\nContainer resources example\\nThe following Pod has two containers. Both containers are defined with a request for 0.25 CPU\\nand 64MiB (226 bytes) of memory. Each container has a limit of 0.5 CPU and 128MiB of\\nmemory. You can say the Pod has a request of 0.5 CPU and 128 MiB of memory, and a limit of 1\\nCPU and 256MiB of memory.\\n---\\napiVersion : v1\\nkind: Pod\\nmetadata :\\n  name : frontend\\nspec:', metadata={'source': './PDFS/Concepts.pdf', 'page': 380}),\n",
       " Document(page_content='containers :\\n  - name : app\\n    image : images.my-company.example/app:v4\\n    resources :\\n      requests :\\n        memory : \"64Mi\"\\n        cpu: \"250m\"\\n      limits :\\n        memory : \"128Mi\"\\n        cpu: \"500m\"\\n  - name : log-aggregator\\n    image : images.my-company.example/log-aggregator:v6\\n    resources :\\n      requests :\\n        memory : \"64Mi\"\\n        cpu: \"250m\"\\n      limits :\\n        memory : \"128Mi\"\\n        cpu: \"500m\"\\nHow Pods with resource requests are scheduled\\nWhen you create a Pod, the Kubernetes scheduler selects a node for the Pod to run on. Each\\nnode has a maximum capacity for each of the resource types: the amount of CPU and memory\\nit can provide for Pods. The scheduler ensures that, for each resource type, the sum of the\\nresource requests of the scheduled containers is less than the capacity of the node. Note that\\nalthough actual memory or CPU resource usage on nodes is very low, the scheduler still refuses\\nto place a Pod on a node if the capacity check fails. This protects against a resource shortage on\\na node when resource usage later increases, for example, during a daily peak in request rate.\\nHow Kubernetes applies resource requests and limits\\nWhen the kubelet starts a container as part of a Pod, the kubelet passes that container\\'s\\nrequests and limits for memory and CPU to the container runtime.\\nOn Linux, the container runtime typically configures kernel cgroups  that apply and enforce the\\nlimits you defined.\\nThe CPU limit defines a hard ceiling on how much CPU time that the container can use.\\nDuring each scheduling interval (time slice), the Linux kernel checks to see if this limit is\\nexceeded; if so, the kernel waits before allowing that cgroup to resume execution.\\nThe CPU request typically defines a weighting. If several different containers (cgroups)\\nwant to run on a contended system, workloads with larger CPU requests are allocated\\nmore CPU time than workloads with small requests.\\nThe memory request is mainly used during (Kubernetes) Pod scheduling. On a node that\\nuses cgroups v2, the container runtime might use the memory request as a hint to set \\nmemory.min  and memory.low .\\nThe memory limit defines a memory limit for that cgroup. If the container tries to allocate\\nmore memory than this limit, the Linux kernel out-of-memory subsystem activates and,\\ntypically, intervenes by stopping one of the processes in the container that tried to\\nallocate memory. If that process is the container\\'s PID 1, and the container is marked as\\nrestartable, Kubernetes restarts the container.• \\n• \\n• \\n•', metadata={'source': './PDFS/Concepts.pdf', 'page': 381}),\n",
       " Document(page_content='The memory limit for the Pod or container can also apply to pages in memory backed\\nvolumes, such as an emptyDir . The kubelet tracks tmpfs  emptyDir volumes as container\\nmemory use, rather than as local ephemeral storage.\\nIf a container exceeds its memory request and the node that it runs on becomes short of\\nmemory overall, it is likely that the Pod the container belongs to will be evicted .\\nA container might or might not be allowed to exceed its CPU limit for extended periods of time.\\nHowever, container runtimes don\\'t terminate Pods or containers for excessive CPU usage.\\nTo determine whether a container cannot be scheduled or is being killed due to resource limits,\\nsee the Troubleshooting  section.\\nMonitoring compute & memory resource usage\\nThe kubelet reports the resource usage of a Pod as part of the Pod status .\\nIf optional tools for monitoring  are available in your cluster, then Pod resource usage can be\\nretrieved either from the Metrics API  directly or from your monitoring tools.\\nLocal ephemeral storage\\nFEATURE STATE:  Kubernetes v1.25 [stable]\\nNodes have local ephemeral storage, backed by locally-attached writeable devices or,\\nsometimes, by RAM. \"Ephemeral\" means that there is no long-term guarantee about durability.\\nPods use ephemeral local storage for scratch space, caching, and for logs. The kubelet can\\nprovide scratch space to Pods using local ephemeral storage to mount emptyDir  volumes  into\\ncontainers.\\nThe kubelet also uses this kind of storage to hold node-level container logs , container images,\\nand the writable layers of running containers.\\nCaution:  If a node fails, the data in its ephemeral storage can be lost. Your applications cannot\\nexpect any performance SLAs (disk IOPS for example) from local ephemeral storage.\\nNote:\\nTo make the resource quota work on ephemeral-storage, two things need to be done:\\nAn admin sets the resource quota for ephemeral-storage in a namespace.\\nA user needs to specify limits for the ephemeral-storage resource in the Pod spec.\\nIf the user doesn\\'t specify the ephemeral-storage resource limit in the Pod spec, the resource\\nquota is not enforced on ephemeral-storage.\\nKubernetes lets you track, reserve and limit the amount of ephemeral local storage a Pod can\\nconsume.• \\n• \\n•', metadata={'source': './PDFS/Concepts.pdf', 'page': 382}),\n",
       " Document(page_content=\"Configurations for local ephemeral storage\\nKubernetes supports two ways to configure local ephemeral storage on a node:\\nSingle filesystem\\nTwo filesystems\\nIn this configuration, you place all different kinds of ephemeral local data ( emptyDir  volumes,\\nwriteable layers, container images, logs) into one filesystem. The most effective way to\\nconfigure the kubelet means dedicating this filesystem to Kubernetes (kubelet) data.\\nThe kubelet also writes node-level container logs  and treats these similarly to ephemeral local\\nstorage.\\nThe kubelet writes logs to files inside its configured log directory ( /var/log  by default); and has\\na base directory for other locally stored data ( /var/lib/kubelet  by default).\\nTypically, both /var/lib/kubelet  and /var/log  are on the system root filesystem, and the kubelet\\nis designed with that layout in mind.\\nYour node can have as many other filesystems, not used for Kubernetes, as you like.\\nYou have a filesystem on the node that you're using for ephemeral data that comes from\\nrunning Pods: logs, and emptyDir  volumes. You can use this filesystem for other data (for\\nexample: system logs not related to Kubernetes); it can even be the root filesystem.\\nThe kubelet also writes node-level container logs  into the first filesystem, and treats these\\nsimilarly to ephemeral local storage.\\nYou also use a separate filesystem, backed by a different logical storage device. In this\\nconfiguration, the directory where you tell the kubelet to place container image layers and\\nwriteable layers is on this second filesystem.\\nThe first filesystem does not hold any image layers or writeable layers.\\nYour node can have as many other filesystems, not used for Kubernetes, as you like.\\nThe kubelet can measure how much local storage it is using. It does this provided that you have\\nset up the node using one of the supported configurations for local ephemeral storage.\\nIf you have a different configuration, then the kubelet does not apply resource limits for\\nephemeral local storage.\\nNote:  The kubelet tracks tmpfs  emptyDir volumes as container memory use, rather than as\\nlocal ephemeral storage.\\nNote:  The kubelet will only track the root filesystem for ephemeral storage. OS layouts that\\nmount a separate disk to /var/lib/kubelet  or /var/lib/containers  will not report ephemeral\\nstorage correctly.• \\n•\", metadata={'source': './PDFS/Concepts.pdf', 'page': 383}),\n",
       " Document(page_content='Setting requests and limits for local ephemeral storage\\nYou can specify ephemeral-storage  for managing local ephemeral storage. Each container of a\\nPod can specify either or both of the following:\\nspec.containers[].resources.limits.ephemeral-storage\\nspec.containers[].resources.requests.ephemeral-storage\\nLimits and requests for ephemeral-storage  are measured in byte quantities. You can express\\nstorage as a plain integer or as a fixed-point number using one of these suffixes: E, P, T, G, M, k.\\nYou can also use the power-of-two equivalents: Ei, Pi, Ti, Gi, Mi, Ki. For example, the following\\nquantities all represent roughly the same value:\\n128974848\\n129e6\\n129M\\n123Mi\\nPay attention to the case of the suffixes. If you request 400m  of ephemeral-storage, this is a\\nrequest for 0.4 bytes. Someone who types that probably meant to ask for 400 mebibytes ( 400Mi )\\nor 400 megabytes ( 400M ).\\nIn the following example, the Pod has two containers. Each container has a request of 2GiB of\\nlocal ephemeral storage. Each container has a limit of 4GiB of local ephemeral storage.\\nTherefore, the Pod has a request of 4GiB of local ephemeral storage, and a limit of 8GiB of local\\nephemeral storage. 500Mi of that limit could be consumed by the emptyDir  volume.\\napiVersion : v1\\nkind: Pod\\nmetadata :\\n  name : frontend\\nspec:\\n  containers :\\n  - name : app\\n    image : images.my-company.example/app:v4\\n    resources :\\n      requests :\\n        ephemeral-storage : \"2Gi\"\\n      limits :\\n        ephemeral-storage : \"4Gi\"\\n    volumeMounts :\\n    - name : ephemeral\\n      mountPath : \"/tmp\"\\n  - name : log-aggregator\\n    image : images.my-company.example/log-aggregator:v6\\n    resources :\\n      requests :\\n        ephemeral-storage : \"2Gi\"\\n      limits :\\n        ephemeral-storage : \"4Gi\"\\n    volumeMounts :\\n    - name : ephemeral\\n      mountPath : \"/tmp\"• \\n• \\n• \\n• \\n• \\n•', metadata={'source': './PDFS/Concepts.pdf', 'page': 384}),\n",
       " Document(page_content=\"volumes :\\n    - name : ephemeral\\n      emptyDir :\\n        sizeLimit : 500Mi\\nHow Pods with ephemeral-storage requests are scheduled\\nWhen you create a Pod, the Kubernetes scheduler selects a node for the Pod to run on. Each\\nnode has a maximum amount of local ephemeral storage it can provide for Pods. For more\\ninformation, see Node Allocatable .\\nThe scheduler ensures that the sum of the resource requests of the scheduled containers is less\\nthan the capacity of the node.\\nEphemeral storage consumption management\\nIf the kubelet is managing local ephemeral storage as a resource, then the kubelet measures\\nstorage use in:\\nemptyDir  volumes, except tmpfs  emptyDir  volumes\\ndirectories holding node-level logs\\nwriteable container layers\\nIf a Pod is using more ephemeral storage than you allow it to, the kubelet sets an eviction signal\\nthat triggers Pod eviction.\\nFor container-level isolation, if a container's writable layer and log usage exceeds its storage\\nlimit, the kubelet marks the Pod for eviction.\\nFor pod-level isolation the kubelet works out an overall Pod storage limit by summing the\\nlimits for the containers in that Pod. In this case, if the sum of the local ephemeral storage\\nusage from all containers and also the Pod's emptyDir  volumes exceeds the overall Pod storage\\nlimit, then the kubelet also marks the Pod for eviction.\\nCaution:\\nIf the kubelet is not measuring local ephemeral storage, then a Pod that exceeds its local storage\\nlimit will not be evicted for breaching local storage resource limits.\\nHowever, if the filesystem space for writeable container layers, node-level logs, or emptyDir\\nvolumes falls low, the node taints  itself as short on local storage and this taint triggers eviction\\nfor any Pods that don't specifically tolerate the taint.\\nSee the supported configurations  for ephemeral local storage.\\nThe kubelet supports different ways to measure Pod storage use:\\nPeriodic scanning\\nFilesystem project quota\\nThe kubelet performs regular, scheduled checks that scan each emptyDir  volume, container log\\ndirectory, and writeable container layer.\\nThe scan measures how much space is used.• \\n• \\n• \\n• \\n•\", metadata={'source': './PDFS/Concepts.pdf', 'page': 385}),\n",
       " Document(page_content='Note:\\nIn this mode, the kubelet does not track open file descriptors for deleted files.\\nIf you (or a container) create a file inside an emptyDir  volume, something then opens that file,\\nand you delete the file while it is still open, then the inode for the deleted file stays until you\\nclose that file but the kubelet does not categorize the space as in use.\\nFEATURE STATE:  Kubernetes v1.15 [alpha]\\nProject quotas are an operating-system level feature for managing storage use on filesystems.\\nWith Kubernetes, you can enable project quotas for monitoring storage use. Make sure that the\\nfilesystem backing the emptyDir  volumes, on the node, provides project quota support. For\\nexample, XFS and ext4fs offer project quotas.\\nNote:  Project quotas let you monitor storage use; they do not enforce limits.\\nKubernetes uses project IDs starting from 1048576 . The IDs in use are registered in /etc/projects\\nand /etc/projid . If project IDs in this range are used for other purposes on the system, those\\nproject IDs must be registered in /etc/projects  and /etc/projid  so that Kubernetes does not use\\nthem.\\nQuotas are faster and more accurate than directory scanning. When a directory is assigned to a\\nproject, all files created under a directory are created in that project, and the kernel merely has\\nto keep track of how many blocks are in use by files in that project. If a file is created and\\ndeleted, but has an open file descriptor, it continues to consume space. Quota tracking records\\nthat space accurately whereas directory scans overlook the storage used by deleted files.\\nIf you want to use project quotas, you should:\\nEnable the LocalStorageCapacityIsolationFSQuotaMonitoring=true  feature gate  using the \\nfeatureGates  field in the kubelet configuration  or the --feature-gates  command line flag.\\nEnsure that the root filesystem (or optional runtime filesystem) has project quotas\\nenabled. All XFS filesystems support project quotas. For ext4 filesystems, you need to\\nenable the project quota tracking feature while the filesystem is not mounted.\\n# For ext4, with /dev/block-device not mounted\\nsudo tune2fs -O project -Q prjquota /dev/block-device\\nEnsure that the root filesystem (or optional runtime filesystem) is mounted with project\\nquotas enabled. For both XFS and ext4fs, the mount option is named prjquota .\\nExtended resources\\nExtended resources are fully-qualified resource names outside the kubernetes.io  domain. They\\nallow cluster operators to advertise and users to consume the non-Kubernetes-built-in\\nresources.\\nThere are two steps required to use Extended Resources. First, the cluster operator must\\nadvertise an Extended Resource. Second, users must request the Extended Resource in Pods.• \\n• \\n•', metadata={'source': './PDFS/Concepts.pdf', 'page': 386}),\n",
       " Document(page_content='Managing extended resources\\nNode-level extended resources\\nNode-level extended resources are tied to nodes.\\nDevice plugin managed resources\\nSee Device Plugin  for how to advertise device plugin managed resources on each node.\\nOther resources\\nTo advertise a new node-level extended resource, the cluster operator can submit a PATCH\\nHTTP request to the API server to specify the available quantity in the status.capacity  for a\\nnode in the cluster. After this operation, the node\\'s status.capacity  will include a new resource.\\nThe status.allocatable  field is updated automatically with the new resource asynchronously by\\nthe kubelet.\\nBecause the scheduler uses the node\\'s status.allocatable  value when evaluating Pod fitness, the\\nscheduler only takes account of the new value after that asynchronous update. There may be a\\nshort delay between patching the node capacity with a new resource and the time when the\\nfirst Pod that requests the resource can be scheduled on that node.\\nExample:\\nHere is an example showing how to use curl to form an HTTP request that advertises five\\n\"example.com/foo\" resources on node k8s-node-1  whose master is k8s-master .\\ncurl --header \"Content-Type: application/json-patch+json\"  \\\\\\n--request PATCH \\\\\\n--data \\'[{\"op\": \"add\", \"path\": \"/status/capacity/example.com~1foo\", \"value\": \"5\"}]\\'  \\\\\\nhttp://k8s-master:8080/api/v1/nodes/k8s-node-1/status\\nNote:  In the preceding request, ~1 is the encoding for the character / in the patch path. The\\noperation path value in JSON-Patch is interpreted as a JSON-Pointer. For more details, see IETF\\nRFC 6901, section 3 .\\nCluster-level extended resources\\nCluster-level extended resources are not tied to nodes. They are usually managed by scheduler\\nextenders, which handle the resource consumption and resource quota.\\nYou can specify the extended resources that are handled by scheduler extenders in scheduler\\nconfiguration\\nExample:\\nThe following configuration for a scheduler policy indicates that the cluster-level extended\\nresource \"example.com/foo\" is handled by the scheduler extender.\\nThe scheduler sends a Pod to the scheduler extender only if the Pod requests\\n\"example.com/foo\".•', metadata={'source': './PDFS/Concepts.pdf', 'page': 387}),\n",
       " Document(page_content='The ignoredByScheduler  field specifies that the scheduler does not check the\\n\"example.com/foo\" resource in its PodFitsResources  predicate.\\n{\\n  \"kind\" : \"Policy\" ,\\n  \"apiVersion\" : \"v1\",\\n  \"extenders\" : [\\n    {\\n      \"urlPrefix\" :\"<extender-endpoint>\" ,\\n      \"bindVerb\" : \"bind\" ,\\n      \"managedResources\" : [\\n        {\\n          \"name\" : \"example.com/foo\" ,\\n          \"ignoredByScheduler\" : true\\n        }\\n      ]\\n    }\\n  ]\\n}\\nConsuming extended resources\\nUsers can consume extended resources in Pod specs like CPU and memory. The scheduler takes\\ncare of the resource accounting so that no more than the available amount is simultaneously\\nallocated to Pods.\\nThe API server restricts quantities of extended resources to whole numbers. Examples of valid\\nquantities are 3, 3000m  and 3Ki. Examples of invalid  quantities are 0.5 and 1500m .\\nNote:  Extended resources replace Opaque Integer Resources. Users can use any domain name\\nprefix other than kubernetes.io  which is reserved.\\nTo consume an extended resource in a Pod, include the resource name as a key in the \\nspec.containers[].resources.limits  map in the container spec.\\nNote:  Extended resources cannot be overcommitted, so request and limit must be equal if both\\nare present in a container spec.\\nA Pod is scheduled only if all of the resource requests are satisfied, including CPU, memory and\\nany extended resources. The Pod remains in the PENDING  state as long as the resource request\\ncannot be satisfied.\\nExample:\\nThe Pod below requests 2 CPUs and 1 \"example.com/foo\" (an extended resource).\\napiVersion : v1\\nkind: Pod\\nmetadata :\\n  name : my-pod\\nspec:\\n  containers :\\n  - name : my-container\\n    image : myimage•', metadata={'source': './PDFS/Concepts.pdf', 'page': 388}),\n",
       " Document(page_content='resources :\\n      requests :\\n        cpu: 2\\n        example.com/foo : 1\\n      limits :\\n        example.com/foo : 1\\nPID limiting\\nProcess ID (PID) limits allow for the configuration of a kubelet to limit the number of PIDs that\\na given Pod can consume. See PID Limiting  for information.\\nTroubleshooting\\nMy Pods are pending with event message FailedScheduling\\nIf the scheduler cannot find any node where a Pod can fit, the Pod remains unscheduled until a\\nplace can be found. An Event  is produced each time the scheduler fails to find a place for the\\nPod. You can use kubectl  to view the events for a Pod; for example:\\nkubectl describe pod frontend | grep -A 9999999999  Events\\nEvents:\\n  Type     Reason            Age   From               Message\\n  ----     ------            ----  ----               -------\\n  Warning  FailedScheduling  23s   default-scheduler  0/42 nodes available: insufficient cpu\\nIn the preceding example, the Pod named \"frontend\" fails to be scheduled due to insufficient\\nCPU resource on any node. Similar error messages can also suggest failure due to insufficient\\nmemory (PodExceedsFreeMemory). In general, if a Pod is pending with a message of this type,\\nthere are several things to try:\\nAdd more nodes to the cluster.\\nTerminate unneeded Pods to make room for pending Pods.\\nCheck that the Pod is not larger than all the nodes. For example, if all the nodes have a\\ncapacity of cpu: 1 , then a Pod with a request of cpu: 1.1  will never be scheduled.\\nCheck for node taints. If most of your nodes are tainted, and the new Pod does not\\ntolerate that taint, the scheduler only considers placements onto the remaining nodes\\nthat don\\'t have that taint.\\nYou can check node capacities and amounts allocated with the kubectl describe nodes\\ncommand. For example:\\nkubectl describe nodes e2e-test-node-pool-4lw4\\nName:            e2e-test-node-pool-4lw4\\n[ ... lines removed for clarity ...]\\nCapacity:\\n cpu:                               2\\n memory:                            7679792Ki\\n pods:                              110\\nAllocatable:• \\n• \\n• \\n•', metadata={'source': './PDFS/Concepts.pdf', 'page': 389}),\n",
       " Document(page_content='cpu:                               1800m\\n memory:                            7474992Ki\\n pods:                              110\\n[ ... lines removed for clarity ...]\\nNon-terminated Pods:        (5 in total)\\n  Namespace    Name                                  CPU Requests  CPU Limits  Memory Requests  \\nMemory Limits\\n  ---------    ----                                  ------------  ----------  ---------------  -------------\\n  kube-system  fluentd-gcp-v1.38-28bv1               100m (5%)     0 (0%)      200Mi (2%)       200Mi \\n(2%)\\n  kube-system  kube-dns-3297075139-61lj3             260m (13%)    0 (0%)      100Mi (1%)       170Mi \\n(2%)\\n  kube-system  kube-proxy-e2e-test-...               100m (5%)     0 (0%)      0 (0%)           0 (0%)\\n  kube-system  monitoring-influxdb-grafana-v4-z1m12  200m (10%)    200m (10%)  600Mi \\n(8%)       600Mi (8%)\\n  kube-system  node-problem-detector-v0.1-fj7m3      20m (1%)      200m (10%)  20Mi (0%)        \\n100Mi (1%)\\nAllocated resources:\\n  (Total limits may be over 100 percent, i.e., overcommitted.)\\n  CPU Requests    CPU Limits    Memory Requests    Memory Limits\\n  ------------    ----------    ---------------    -------------\\n  680m (34%)      400m (20%)    920Mi (11%)        1070Mi (13%)\\nIn the preceding output, you can see that if a Pod requests more than 1.120 CPUs or more than\\n6.23Gi of memory, that Pod will not fit on the node.\\nBy looking at the “Pods” section, you can see which Pods are taking up space on the node.\\nThe amount of resources available to Pods is less than the node capacity because system\\ndaemons use a portion of the available resources. Within the Kubernetes API, each Node has a\\n.status.allocatable  field (see NodeStatus  for details).\\nThe .status.allocatable  field describes the amount of resources that are available to Pods on that\\nnode (for example: 15 virtual CPUs and 7538 MiB of memory). For more information on node\\nallocatable resources in Kubernetes, see Reserve Compute Resources for System Daemons .\\nYou can configure resource quotas  to limit the total amount of resources that a namespace can\\nconsume. Kubernetes enforces quotas for objects in particular namespace when there is a\\nResourceQuota in that namespace. For example, if you assign specific namespaces to different\\nteams, you can add ResourceQuotas into those namespaces. Setting resource quotas helps to\\nprevent one team from using so much of any resource that this over-use affects other teams.\\nYou should also consider what access you grant to that namespace: full write access to a\\nnamespace allows someone with that access to remove any resource, including a configured\\nResourceQuota.\\nMy container is terminated\\nYour container might get terminated because it is resource-starved. To check whether a\\ncontainer is being killed because it is hitting a resource limit, call kubectl describe pod  on the\\nPod of interest:\\nkubectl describe pod simmemleak-hra99', metadata={'source': './PDFS/Concepts.pdf', 'page': 390}),\n",
       " Document(page_content='The output is similar to:\\nName:                           simmemleak-hra99\\nNamespace:                      default\\nImage(s):                       saadali/simmemleak\\nNode:                           kubernetes-node-tf0f/10.240.216.66\\nLabels:                         name=simmemleak\\nStatus:                         Running\\nReason:\\nMessage:\\nIP:                             10.244.2.75\\nContainers:\\n  simmemleak:\\n    Image:  saadali/simmemleak:latest\\n    Limits:\\n      cpu:          100m\\n      memory:       50Mi\\n    State:          Running\\n      Started:      Tue, 07 Jul 2019 12:54:41 -0700\\n    Last State:     Terminated\\n      Reason:       OOMKilled\\n      Exit Code:    137\\n      Started:      Fri, 07 Jul 2019 12:54:30 -0700\\n      Finished:     Fri, 07 Jul 2019 12:54:33 -0700\\n    Ready:          False\\n    Restart Count:  5\\nConditions:\\n  Type      Status\\n  Ready     False\\nEvents:\\n  Type    Reason     Age   From               Message\\n  ----    ------     ----  ----               -------\\n  Normal  Scheduled  42s   default-scheduler  Successfully assigned simmemleak-hra99 to \\nkubernetes-node-tf0f\\n  Normal  Pulled     41s   kubelet            Container image \"saadali/simmemleak:latest\" already \\npresent on machine\\n  Normal  Created    41s   kubelet            Created container simmemleak\\n  Normal  Started    40s   kubelet            Started container simmemleak\\n  Normal  Killing    32s   kubelet            Killing container with id \\nead3fb35-5cf5-44ed-9ae1-488115be66c6: Need to kill Pod\\nIn the preceding example, the Restart Count: 5  indicates that the simmemleak  container in the\\nPod was terminated and restarted five times (so far). The OOMKilled  reason shows that the\\ncontainer tried to use more memory than its limit.\\nYour next step might be to check the application code for a memory leak. If you find that the\\napplication is behaving how you expect, consider setting a higher memory limit (and possibly\\nrequest) for that container.\\nWhat\\'s next\\nGet hands-on experience assigning Memory resources to containers and Pods . •', metadata={'source': './PDFS/Concepts.pdf', 'page': 391}),\n",
       " Document(page_content='Get hands-on experience assigning CPU resources to containers and Pods .\\nRead how the API reference defines a container  and its resource requirements\\nRead about project quotas  in XFS\\nRead more about the kube-scheduler configuration reference (v1beta3)\\nRead more about Quality of Service classes for Pods\\nOrganizing Cluster Access Using\\nkubeconfig Files\\nUse kubeconfig files to organize information about clusters, users, namespaces, and\\nauthentication mechanisms. The kubectl  command-line tool uses kubeconfig files to find the\\ninformation it needs to choose a cluster and communicate with the API server of a cluster.\\nNote:  A file that is used to configure access to clusters is called a kubeconfig file . This is a\\ngeneric way of referring to configuration files. It does not mean that there is a file named \\nkubeconfig .\\nWarning:  Only use kubeconfig files from trusted sources. Using a specially-crafted kubeconfig\\nfile could result in malicious code execution or file exposure. If you must use an untrusted\\nkubeconfig file, inspect it carefully first, much as you would a shell script.\\nBy default, kubectl  looks for a file named config  in the $HOME/.kube  directory. You can specify\\nother kubeconfig files by setting the KUBECONFIG  environment variable or by setting the --\\nkubeconfig  flag.\\nFor step-by-step instructions on creating and specifying kubeconfig files, see Configure Access\\nto Multiple Clusters .\\nSupporting multiple clusters, users, and authentication\\nmechanisms\\nSuppose you have several clusters, and your users and components authenticate in a variety of\\nways. For example:\\nA running kubelet might authenticate using certificates.\\nA user might authenticate using tokens.\\nAdministrators might have sets of certificates that they provide to individual users.\\nWith kubeconfig files, you can organize your clusters, users, and namespaces. You can also\\ndefine contexts to quickly and easily switch between clusters and namespaces.\\nContext\\nA context  element in a kubeconfig file is used to group access parameters under a convenient\\nname. Each context has three parameters: cluster, namespace, and user. By default, the kubectl\\ncommand-line tool uses parameters from the current context  to communicate with the cluster.\\nTo choose the current context:\\nkubectl config use-context• \\n• \\n• \\n• \\n• \\n• \\n• \\n•', metadata={'source': './PDFS/Concepts.pdf', 'page': 392}),\n",
       " Document(page_content=\"The KUBECONFIG environment variable\\nThe KUBECONFIG  environment variable holds a list of kubeconfig files. For Linux and Mac, the\\nlist is colon-delimited. For Windows, the list is semicolon-delimited. The KUBECONFIG\\nenvironment variable is not required. If the KUBECONFIG  environment variable doesn't exist, \\nkubectl  uses the default kubeconfig file, $HOME/.kube/config .\\nIf the KUBECONFIG  environment variable does exist, kubectl  uses an effective configuration\\nthat is the result of merging the files listed in the KUBECONFIG  environment variable.\\nMerging kubeconfig files\\nTo see your configuration, enter this command:\\nkubectl config view\\nAs described previously, the output might be from a single kubeconfig file, or it might be the\\nresult of merging several kubeconfig files.\\nHere are the rules that kubectl  uses when it merges kubeconfig files:\\nIf the --kubeconfig  flag is set, use only the specified file. Do not merge. Only one instance\\nof this flag is allowed.\\nOtherwise, if the KUBECONFIG  environment variable is set, use it as a list of files that\\nshould be merged. Merge the files listed in the KUBECONFIG  environment variable\\naccording to these rules:\\nIgnore empty filenames.\\nProduce errors for files with content that cannot be deserialized.\\nThe first file to set a particular value or map key wins.\\nNever change the value or map key. Example: Preserve the context of the first file\\nto set current-context . Example: If two files specify a red-user , use only values from\\nthe first file's red-user . Even if the second file has non-conflicting entries under red-\\nuser, discard them.\\nFor an example of setting the KUBECONFIG  environment variable, see Setting the\\nKUBECONFIG environment variable .\\nOtherwise, use the default kubeconfig file, $HOME/.kube/config , with no merging.\\nDetermine the context to use based on the first hit in this chain:\\nUse the --context  command-line flag if it exists.\\nUse the current-context  from the merged kubeconfig files.\\nAn empty context is allowed at this point.\\nDetermine the cluster and user. At this point, there might or might not be a context.\\nDetermine the cluster and user based on the first hit in this chain, which is run twice:\\nonce for user and once for cluster:\\nUse a command-line flag if it exists: --user  or --cluster .1. \\n◦ \\n◦ \\n◦ \\n◦ \\n2. \\n1. \\n2. \\n3. \\n1.\", metadata={'source': './PDFS/Concepts.pdf', 'page': 393}),\n",
       " Document(page_content='If the context is non-empty, take the user or cluster from the context.\\nThe user and cluster can be empty at this point.\\nDetermine the actual cluster information to use. At this point, there might or might not\\nbe cluster information. Build each piece of the cluster information based on this chain;\\nthe first hit wins:\\nUse command line flags if they exist: --server , --certificate-authority , --insecure-\\nskip-tls-verify .\\nIf any cluster information attributes exist from the merged kubeconfig files, use\\nthem.\\nIf there is no server location, fail.\\nDetermine the actual user information to use. Build user information using the same\\nrules as cluster information, except allow only one authentication technique per user:\\nUse command line flags if they exist: --client-certificate , --client-key , --username , --\\npassword , --token .\\nUse the user fields from the merged kubeconfig files.\\nIf there are two conflicting techniques, fail.\\nFor any information still missing, use default values and potentially prompt for\\nauthentication information.\\nFile references\\nFile and path references in a kubeconfig file are relative to the location of the kubeconfig file.\\nFile references on the command line are relative to the current working directory. In \\n$HOME/.kube/config , relative paths are stored relatively, and absolute paths are stored\\nabsolutely.\\nProxy\\nYou can configure kubectl  to use a proxy per cluster using proxy-url  in your kubeconfig file,\\nlike this:\\napiVersion : v1\\nkind: Config\\nclusters :\\n- cluster :\\n    proxy-url : http://proxy.example.org:3128\\n    server : https://k8s.example.org/k8s/clusters/c-xxyyzz\\n  name : development\\nusers :\\n- name : developer\\ncontexts :\\n- context :\\n  name : development2. \\n4. \\n1. \\n2. \\n3. \\n5. \\n1. \\n2. \\n3. \\n6.', metadata={'source': './PDFS/Concepts.pdf', 'page': 394}),\n",
       " Document(page_content=\"What's next\\nConfigure Access to Multiple Clusters\\nkubectl config\\nResource Management for Windows nodes\\nThis page outlines the differences in how resources are managed between Linux and Windows.\\nOn Linux nodes, cgroups  are used as a pod boundary for resource control. Containers are\\ncreated within that boundary for network, process and file system isolation. The Linux cgroup\\nAPIs can be used to gather CPU, I/O, and memory use statistics.\\nIn contrast, Windows uses a job object  per container with a system namespace filter to contain\\nall processes in a container and provide logical isolation from the host. (Job objects are a\\nWindows process isolation mechanism and are different from what Kubernetes refers to as a \\nJob).\\nThere is no way to run a Windows container without the namespace filtering in place. This\\nmeans that system privileges cannot be asserted in the context of the host, and thus privileged\\ncontainers are not available on Windows. Containers cannot assume an identity from the host\\nbecause the Security Account Manager (SAM) is separate.\\nMemory management\\nWindows does not have an out-of-memory process killer as Linux does. Windows always treats\\nall user-mode memory allocations as virtual, and pagefiles are mandatory.\\nWindows nodes do not overcommit memory for processes. The net effect is that Windows\\nwon't reach out of memory conditions the same way Linux does, and processes page to disk\\ninstead of being subject to out of memory (OOM) termination. If memory is over-provisioned\\nand all physical memory is exhausted, then paging can slow down performance.\\nCPU management\\nWindows can limit the amount of CPU time allocated for different processes but cannot\\nguarantee a minimum amount of CPU time.\\nOn Windows, the kubelet supports a command-line flag to set the scheduling priority  of the\\nkubelet process: --windows-priorityclass . This flag allows the kubelet process to get more CPU\\ntime slices when compared to other processes running on the Windows host. More information\\non the allowable values and their meaning is available at Windows Priority Classes . To ensure\\nthat running Pods do not starve the kubelet of CPU cycles, set this flag to \\nABOVE_NORMAL_PRIORITY_CLASS  or above.\\nResource reservation\\nTo account for memory and CPU used by the operating system, the container runtime, and by\\nKubernetes host processes such as the kubelet, you can (and should) reserve memory and CPU• \\n•\", metadata={'source': './PDFS/Concepts.pdf', 'page': 395}),\n",
       " Document(page_content=\"resources with the --kube-reserved  and/or --system-reserved  kubelet flags. On Windows these\\nvalues are only used to calculate the node's allocatable  resources.\\nCaution:\\nAs you deploy workloads, set resource memory and CPU limits on containers. This also\\nsubtracts from NodeAllocatable  and helps the cluster-wide scheduler in determining which\\npods to place on which nodes.\\nScheduling pods without limits may over-provision the Windows nodes and in extreme cases\\ncan cause the nodes to become unhealthy.\\nOn Windows, a good practice is to reserve at least 2GiB of memory.\\nTo determine how much CPU to reserve, identify the maximum pod density for each node and\\nmonitor the CPU usage of the system services running there, then choose a value that meets\\nyour workload needs.\\nSecurity\\nConcepts for keeping your cloud-native workload secure.\\nOverview of Cloud Native Security\\nA model for thinking about Kubernetes security in the context of Cloud Native security.\\nPod Security Standards\\nA detailed look at the different policy levels defined in the Pod Security Standards.\\nService Accounts\\nLearn about ServiceAccount objects in Kubernetes.\\nPod Security Admission\\nAn overview of the Pod Security Admission Controller, which can enforce the Pod Security\\nStandards.\\nPod Security Policies\\nSecurity For Windows Nodes\\nControlling Access to the Kubernetes API\\nRole Based Access Control Good Practices\\nPrinciples and practices for good RBAC design for cluster operators.\", metadata={'source': './PDFS/Concepts.pdf', 'page': 396}),\n",
       " Document(page_content=\"Good practices for Kubernetes Secrets\\nPrinciples and practices for good Secret management for cluster administrators and application\\ndevelopers.\\nMulti-tenancy\\nHardening Guide - Authentication Mechanisms\\nInformation on authentication options in Kubernetes and their security properties.\\nKubernetes API Server Bypass Risks\\nSecurity architecture information relating to the API server and other components\\nSecurity Checklist\\nBaseline checklist for ensuring security in Kubernetes clusters.\\nOverview of Cloud Native Security\\nA model for thinking about Kubernetes security in the context of Cloud Native security.\\nThis overview defines a model for thinking about Kubernetes security in the context of Cloud\\nNative security.\\nWarning:  This container security model provides suggestions, not proven information security\\npolicies.\\nThe 4C's of Cloud Native security\\nYou can think about security in layers. The 4C's of Cloud Native security are Cloud, Clusters,\\nContainers, and Code.\\nNote:  This layered approach augments the defense in depth  computing approach to security,\\nwhich is widely regarded as a best practice for securing software systems.\\nThe 4C's of Cloud Native Security\\nEach layer of the Cloud Native security model builds upon the next outermost layer. The Code\\nlayer benefits from strong base (Cloud, Cluster, Container) security layers. You cannot\\nsafeguard against poor security standards in the base layers by addressing security at the Code\\nlevel.\\nCloud\\nIn many ways, the Cloud (or co-located servers, or the corporate datacenter) is the trusted\\ncomputing base  of a Kubernetes cluster. If the Cloud layer is vulnerable (or configured in a\\nvulnerable way) then there is no guarantee that the components built on top of this base are\", metadata={'source': './PDFS/Concepts.pdf', 'page': 397}),\n",
       " Document(page_content=\"secure. Each cloud provider makes security recommendations for running workloads securely\\nin their environment.\\nCloud provider security\\nIf you are running a Kubernetes cluster on your own hardware or a different cloud provider,\\nconsult your documentation for security best practices. Here are links to some of the popular\\ncloud providers' security documentation:\\nCloud provider security\\nIaaS Provider Link\\nAlibaba Cloud https://www.alibabacloud.com/trust-center\\nAmazon Web Services https://aws.amazon.com/security\\nGoogle Cloud Platform https://cloud.google.com/security\\nHuawei Cloudhttps://www.huaweicloud.com/intl/en-us/securecenter/\\noverallsafety\\nIBM Cloud https://www.ibm.com/cloud/security\\nMicrosoft Azure https://docs.microsoft.com/en-us/azure/security/azure-security\\nOracle Cloud\\nInfrastructurehttps://www.oracle.com/security\\nVMware vSphere https://www.vmware.com/security/hardening-guides\\nInfrastructure security\\nSuggestions for securing your infrastructure in a Kubernetes cluster:\\nInfrastructure security\\nArea of Concern\\nfor Kubernetes\\nInfrastructureRecommendation\\nNetwork access to\\nAPI Server (Control\\nplane)All access to the Kubernetes control plane is not allowed publicly on the\\ninternet and is controlled by network access control lists restricted to\\nthe set of IP addresses needed to administer the cluster.\\nNetwork access to\\nNodes (nodes)Nodes should be configured to only accept connections (via network\\naccess control lists) from the control plane on the specified ports, and\\naccept connections for services in Kubernetes of type NodePort and\\nLoadBalancer. If possible, these nodes should not be exposed on the\\npublic internet entirely.\\nKubernetes access to\\nCloud Provider APIEach cloud provider needs to grant a different set of permissions to the\\nKubernetes control plane and nodes. It is best to provide the cluster with\\ncloud provider access that follows the principle of least privilege  for the\\nresources it needs to administer. The Kops documentation  provides\\ninformation about IAM policies and roles.\\nAccess to etcdAccess to etcd (the datastore of Kubernetes) should be limited to the\\ncontrol plane only. Depending on your configuration, you should\\nattempt to use etcd over TLS. More information can be found in the etcd\\ndocumentation .\\netcd EncryptionWherever possible it's a good practice to encrypt all storage at rest, and\\nsince etcd holds the state of the entire cluster (including Secrets) its disk\\nshould especially be encrypted at rest.\", metadata={'source': './PDFS/Concepts.pdf', 'page': 398}),\n",
       " Document(page_content='Cluster\\nThere are two areas of concern for securing Kubernetes:\\nSecuring the cluster components that are configurable\\nSecuring the applications which run in the cluster\\nComponents of the Cluster\\nIf you want to protect your cluster from accidental or malicious access and adopt good\\ninformation practices, read and follow the advice about securing your cluster .\\nComponents in the cluster (your application)\\nDepending on the attack surface of your application, you may want to focus on specific aspects\\nof security. For example: If you are running a service (Service A) that is critical in a chain of\\nother resources and a separate workload (Service B) which is vulnerable to a resource\\nexhaustion attack, then the risk of compromising Service A is high if you do not limit the\\nresources of Service B. The following table lists areas of security concerns and\\nrecommendations for securing workloads running in Kubernetes:\\nArea of Concern for Workload\\nSecurityRecommendation\\nRBAC Authorization (Access to the\\nKubernetes API)https://kubernetes.io/docs/reference/access-authn-\\nauthz/rbac/\\nAuthenticationhttps://kubernetes.io/docs/concepts/security/\\ncontrolling-access/\\nApplication secrets management (and\\nencrypting them in etcd at rest)https://kubernetes.io/docs/concepts/configuration/\\nsecret/\\nhttps://kubernetes.io/docs/tasks/administer-cluster/\\nencrypt-data/\\nEnsuring that pods meet defined Pod\\nSecurity Standardshttps://kubernetes.io/docs/concepts/security/pod-\\nsecurity-standards/#policy-instantiation\\nQuality of Service (and Cluster resource\\nmanagement)https://kubernetes.io/docs/tasks/configure-pod-\\ncontainer/quality-service-pod/\\nNetwork Policieshttps://kubernetes.io/docs/concepts/services-\\nnetworking/network-policies/\\nTLS for Kubernetes Ingresshttps://kubernetes.io/docs/concepts/services-\\nnetworking/ingress/#tls\\nContainer\\nContainer security is outside the scope of this guide. Here are general recommendations and\\nlinks to explore this topic:\\nArea of Concern for\\nContainersRecommendation\\nContainer Vulnerability\\nScanning and OS\\nDependency SecurityAs part of an image build step, you should scan your containers for\\nknown vulnerabilities.• \\n•', metadata={'source': './PDFS/Concepts.pdf', 'page': 399}),\n",
       " Document(page_content=\"Area of Concern for\\nContainersRecommendation\\nImage Signing and\\nEnforcementSign container images to maintain a system of trust for the content\\nof your containers.\\nDisallow privileged usersWhen constructing containers, consult your documentation for how\\nto create users inside of the containers that have the least level of\\noperating system privilege necessary in order to carry out the goal of\\nthe container.\\nUse container runtime\\nwith stronger isolationSelect container runtime classes  that provide stronger isolation.\\nCode\\nApplication code is one of the primary attack surfaces over which you have the most control.\\nWhile securing application code is outside of the Kubernetes security topic, here are\\nrecommendations to protect application code:\\nCode security\\nCode security\\nArea of Concern\\nfor CodeRecommendation\\nAccess over TLS\\nonlyIf your code needs to communicate by TCP, perform a TLS handshake with\\nthe client ahead of time. With the exception of a few cases, encrypt\\neverything in transit. Going one step further, it's a good idea to encrypt\\nnetwork traffic between services. This can be done through a process known\\nas mutual TLS authentication or mTLS  which performs a two sided\\nverification of communication between two certificate holding services.\\nLimiting port\\nranges of\\ncommunicationThis recommendation may be a bit self-explanatory, but wherever possible\\nyou should only expose the ports on your service that are absolutely\\nessential for communication or metric gathering.\\n3rd Party\\nDependency\\nSecurityIt is a good practice to regularly scan your application's third party libraries\\nfor known security vulnerabilities. Each programming language has a tool\\nfor performing this check automatically.\\nStatic Code\\nAnalysisMost languages provide a way for a snippet of code to be analyzed for any\\npotentially unsafe coding practices. Whenever possible you should perform\\nchecks using automated tooling that can scan codebases for common\\nsecurity errors. Some of the tools can be found at: https://owasp.org/www-\\ncommunity/Source_Code_Analysis_Tools\\nDynamic probing\\nattacksThere are a few automated tools that you can run against your service to try\\nsome of the well known service attacks. These include SQL injection, CSRF,\\nand XSS. One of the most popular dynamic analysis tools is the OWASP Zed\\nAttack proxy  tool.\\nWhat's next\\nLearn about related Kubernetes security topics:\\nPod security standards\\nNetwork policies for Pods• \\n•\", metadata={'source': './PDFS/Concepts.pdf', 'page': 400}),\n",
       " Document(page_content='Controlling Access to the Kubernetes API\\nSecuring your cluster\\nData encryption in transit  for the control plane\\nData encryption at rest\\nSecrets in Kubernetes\\nRuntime class\\nPod Security Standards\\nA detailed look at the different policy levels defined in the Pod Security Standards.\\nThe Pod Security Standards define three different policies  to broadly cover the security\\nspectrum. These policies are cumulative  and range from highly-permissive to highly-restrictive.\\nThis guide outlines the requirements of each policy.\\nProfile Description\\nPrivilegedUnrestricted policy, providing the widest possible level of permissions. This policy\\nallows for known privilege escalations.\\nBaselineMinimally restrictive policy which prevents known privilege escalations. Allows\\nthe default (minimally specified) Pod configuration.\\nRestricted Heavily restricted policy, following current Pod hardening best practices.\\nProfile Details\\nPrivileged\\nThe Privileged  policy is purposely-open, and entirely unrestricted.  This type of policy is\\ntypically aimed at system- and infrastructure-level workloads managed by privileged, trusted\\nusers.\\nThe Privileged policy is defined by an absence of restrictions. Allow-by-default mechanisms\\n(such as gatekeeper) may be Privileged by default. In contrast, for a deny-by-default mechanism\\n(such as Pod Security Policy) the Privileged policy should disable all restrictions.\\nBaseline\\nThe Baseline  policy is aimed at ease of adoption for common containerized workloads\\nwhile preventing known privilege escalations.  This policy is targeted at application\\noperators and developers of non-critical applications. The following listed controls should be\\nenforced/disallowed:\\nNote:  In this table, wildcards ( *) indicate all elements in a list. For example, \\nspec.containers[*].securityContext  refers to the Security Context object for all defined\\ncontainers . If any of the listed containers fails to meet the requirements, the entire pod will fail\\nvalidation.\\nBaseline policy specification\\nControl Policy\\nHostProcess• \\n• \\n• \\n• \\n• \\n•', metadata={'source': './PDFS/Concepts.pdf', 'page': 401}),\n",
       " Document(page_content='Windows pods offer the ability to run HostProcess containers  which enables\\nprivileged access to the Windows node. Privileged access to the host is disallowed\\nin the baseline policy.\\nFEATURE STATE:  Kubernetes v1.26 [stable]\\nRestricted Fields\\nspec.securityContext.windowsOptions.hostProcess\\nspec.containers[*].securityContext.windowsOptions.hostProcess\\nspec.initContainers[*].securityContext.windowsOptions.hostProcess\\nspec.ephemeralContainers[*].securityContext.windowsOptions.hostProcess\\nAllowed Values\\nUndefined/nil\\nfalse\\nHost\\nNamespacesSharing the host namespaces must be disallowed.\\nRestricted Fields\\nspec.hostNetwork\\nspec.hostPID\\nspec.hostIPC\\nAllowed Values\\nUndefined/nil\\nfalse\\nPrivileged\\nContainersPrivileged Pods disable most security mechanisms and must be disallowed.\\nRestricted Fields\\nspec.containers[*].securityContext.privileged\\nspec.initContainers[*].securityContext.privileged\\nspec.ephemeralContainers[*].securityContext.privileged\\nAllowed Values\\nUndefined/nil\\nfalse\\nCapabilitiesAdding additional capabilities beyond those listed below must be disallowed.\\nRestricted Fields\\nspec.containers[*].securityContext.capabilities.add\\nspec.initContainers[*].securityContext.capabilities.add\\nspec.ephemeralContainers[*].securityContext.capabilities.add• \\n• \\n• \\n• \\n• \\n• \\n• \\n• \\n• \\n• \\n• \\n• \\n• \\n• \\n• \\n• \\n• \\n• \\n•', metadata={'source': './PDFS/Concepts.pdf', 'page': 402}),\n",
       " Document(page_content='Allowed Values\\nUndefined/nil\\nAUDIT_WRITE\\nCHOWN\\nDAC_OVERRIDE\\nFOWNER\\nFSETID\\nKILL\\nMKNOD\\nNET_BIND_SERVICE\\nSETFCAP\\nSETGID\\nSETPCAP\\nSETUID\\nSYS_CHROOT\\nHostPath\\nVolumesHostPath volumes must be forbidden.\\nRestricted Fields\\nspec.volumes[*].hostPath\\nAllowed Values\\nUndefined/nil\\nHost PortsHostPorts should be disallowed entirely (recommended) or restricted to a known\\nlist\\nRestricted Fields\\nspec.containers[*].ports[*].hostPort\\nspec.initContainers[*].ports[*].hostPort\\nspec.ephemeralContainers[*].ports[*].hostPort\\nAllowed Values\\nUndefined/nil\\nKnown list (not supported by the built-in Pod Security Admission\\ncontroller )\\n0\\nAppArmorOn supported hosts, the runtime/default  AppArmor profile is applied by default.\\nThe baseline policy should prevent overriding or disabling the default AppArmor\\nprofile, or restrict overrides to an allowed set of profiles.\\nRestricted Fields\\nmetadata.annotations[\"container.apparmor.security.beta.kubernetes.io/*\"]• \\n• \\n• \\n• \\n• \\n• \\n• \\n• \\n• \\n• \\n• \\n• \\n• \\n• \\n• \\n• \\n• \\n• \\n• \\n• \\n• \\n• \\n•', metadata={'source': './PDFS/Concepts.pdf', 'page': 403}),\n",
       " Document(page_content='Allowed Values\\nUndefined/nil\\nruntime/default\\nlocalhost/*\\nSELinuxSetting the SELinux type is restricted, and setting a custom SELinux user or role\\noption is forbidden.\\nRestricted Fields\\nspec.securityContext.seLinuxOptions.type\\nspec.containers[*].securityContext.seLinuxOptions.type\\nspec.initContainers[*].securityContext.seLinuxOptions.type\\nspec.ephemeralContainers[*].securityContext.seLinuxOptions.type\\nAllowed Values\\nUndefined/\"\"\\ncontainer_t\\ncontainer_init_t\\ncontainer_kvm_t\\nRestricted Fields\\nspec.securityContext.seLinuxOptions.user\\nspec.containers[*].securityContext.seLinuxOptions.user\\nspec.initContainers[*].securityContext.seLinuxOptions.user\\nspec.ephemeralContainers[*].securityContext.seLinuxOptions.user\\nspec.securityContext.seLinuxOptions.role\\nspec.containers[*].securityContext.seLinuxOptions.role\\nspec.initContainers[*].securityContext.seLinuxOptions.role\\nspec.ephemeralContainers[*].securityContext.seLinuxOptions.role\\nAllowed Values\\nUndefined/\"\"• \\n• \\n• \\n• \\n• \\n• \\n• \\n• \\n• \\n• \\n• \\n• \\n• \\n• \\n• \\n• \\n• \\n• \\n• \\n•', metadata={'source': './PDFS/Concepts.pdf', 'page': 404}),\n",
       " Document(page_content='/proc\\nMount TypeThe default /proc  masks are set up to reduce attack surface, and should be\\nrequired.\\nRestricted Fields\\nspec.containers[*].securityContext.procMount\\nspec.initContainers[*].securityContext.procMount\\nspec.ephemeralContainers[*].securityContext.procMount\\nAllowed Values\\nUndefined/nil\\nDefault\\nSeccompSeccomp profile must not be explicitly set to Unconfined .\\nRestricted Fields\\nspec.securityContext.seccompProfile.type\\nspec.containers[*].securityContext.seccompProfile.type\\nspec.initContainers[*].securityContext.seccompProfile.type\\nspec.ephemeralContainers[*].securityContext.seccompProfile.type\\nAllowed Values\\nUndefined/nil\\nRuntimeDefault\\nLocalhost\\nSysctlsSysctls can disable security mechanisms or affect all containers on a host, and\\nshould be disallowed except for an allowed \"safe\" subset. A sysctl is considered\\nsafe if it is namespaced in the container or the Pod, and it is isolated from other\\nPods or processes on the same Node.\\nRestricted Fields\\nspec.securityContext.sysctls[*].name\\nAllowed Values\\nUndefined/nil\\nkernel.shm_rmid_forced\\nnet.ipv4.ip_local_port_range\\nnet.ipv4.ip_unprivileged_port_start\\nnet.ipv4.tcp_syncookies\\nnet.ipv4.ping_group_range• \\n• \\n• \\n• \\n• \\n• \\n• \\n• \\n• \\n• \\n• \\n• \\n• \\n• \\n• \\n• \\n• \\n• \\n•', metadata={'source': './PDFS/Concepts.pdf', 'page': 405}),\n",
       " Document(page_content='Restricted\\nThe Restricted  policy is aimed at enforcing current Pod hardening best practices, at the\\nexpense of some compatibility.  It is targeted at operators and developers of security-critical\\napplications, as well as lower-trust users. The following listed controls should be enforced/\\ndisallowed:\\nNote:  In this table, wildcards ( *) indicate all elements in a list. For example, \\nspec.containers[*].securityContext  refers to the Security Context object for all defined\\ncontainers . If any of the listed containers fails to meet the requirements, the entire pod will fail\\nvalidation.\\nRestricted policy specification\\nControl Policy\\nEverything from the baseline profile.\\nVolume\\nTypesThe restricted policy only permits the following volume types.\\nRestricted Fields\\nspec.volumes[*]\\nAllowed Values\\nEvery item in the spec.volumes[*]  list must set one of the following fields to a\\nnon-null value:\\nspec.volumes[*].configMap\\nspec.volumes[*].csi\\nspec.volumes[*].downwardAPI\\nspec.volumes[*].emptyDir\\nspec.volumes[*].ephemeral\\nspec.volumes[*].persistentVolumeClaim\\nspec.volumes[*].projected\\nspec.volumes[*].secret\\nPrivilege\\nEscalation\\n(v1.8+)Privilege escalation (such as via set-user-ID or set-group-ID file mode) should not\\nbe allowed. This is Linux only policy  in v1.25+ (spec.os.name != windows)\\nRestricted Fields\\nspec.containers[*].securityContext.allowPrivilegeEscalation\\nspec.initContainers[*].securityContext.allowPrivilegeEscalation\\nspec.ephemeralContainers[*].securityContext.allowPrivilegeEscalation\\nAllowed Values\\nfalse\\nRunning as\\nNon-rootContainers must be required to run as non-root users.• \\n• \\n• \\n• \\n• \\n• \\n• \\n• \\n• \\n• \\n• \\n• \\n•', metadata={'source': './PDFS/Concepts.pdf', 'page': 406}),\n",
       " Document(page_content='Restricted Fields\\nspec.securityContext.runAsNonRoot\\nspec.containers[*].securityContext.runAsNonRoot\\nspec.initContainers[*].securityContext.runAsNonRoot\\nspec.ephemeralContainers[*].securityContext.runAsNonRoot\\nAllowed Values\\ntrue\\nThe container fields may be undefined/ nil if the pod-level spec.securityContext.runAsNonR\\noot is set to true.\\nRunning as\\nNon-root\\nuser\\n(v1.23+)Containers must not set runAsUser  to 0\\nRestricted Fields\\nspec.securityContext.runAsUser\\nspec.containers[*].securityContext.runAsUser\\nspec.initContainers[*].securityContext.runAsUser\\nspec.ephemeralContainers[*].securityContext.runAsUser\\nAllowed Values\\nany non-zero value\\nundefined/null\\nSeccomp\\n(v1.19+)Seccomp profile must be explicitly set to one of the allowed values. Both the \\nUnconfined  profile and the absence  of a profile are prohibited. This is Linux only\\npolicy  in v1.25+ (spec.os.name != windows)\\nRestricted Fields\\nspec.securityContext.seccompProfile.type\\nspec.containers[*].securityContext.seccompProfile.type\\nspec.initContainers[*].securityContext.seccompProfile.type\\nspec.ephemeralContainers[*].securityContext.seccompProfile.type\\nAllowed Values\\nRuntimeDefault\\nLocalhost\\nThe container fields may be undefined/ nil if the pod-level spec.securityContext.seccompProf\\nile.type  field is set appropriately. Conversely, the pod-level field may be undefined/ nil if\\n_all_ container- level fields are set.\\nCapabilities\\n(v1.22+)Containers must drop ALL capabilities, and are only permitted to add back the \\nNET_BIND_SERVICE  capability. This is Linux only policy  in v1.25+ (.spec.os.name !\\n= \"windows\")• \\n• \\n• \\n• \\n• \\n• \\n• \\n• \\n• \\n• \\n• \\n• \\n• \\n• \\n• \\n• \\n•', metadata={'source': './PDFS/Concepts.pdf', 'page': 407}),\n",
       " Document(page_content=\"Restricted Fields\\nspec.containers[*].securityContext.capabilities.drop\\nspec.initContainers[*].securityContext.capabilities.drop\\nspec.ephemeralContainers[*].securityContext.capabilities.drop\\nAllowed Values\\nAny list of capabilities that includes ALL\\nRestricted Fields\\nspec.containers[*].securityContext.capabilities.add\\nspec.initContainers[*].securityContext.capabilities.add\\nspec.ephemeralContainers[*].securityContext.capabilities.add\\nAllowed Values\\nUndefined/nil\\nNET_BIND_SERVICE\\nPolicy Instantiation\\nDecoupling policy definition from policy instantiation allows for a common understanding and\\nconsistent language of policies across clusters, independent of the underlying enforcement\\nmechanism.\\nAs mechanisms mature, they will be defined below on a per-policy basis. The methods of\\nenforcement of individual policies are not defined here.\\nPod Security Admission Controller\\nPrivileged namespace\\nBaseline namespace\\nRestricted namespace\\nAlternatives\\nNote:  This section links to third party projects that provide functionality required by\\nKubernetes. The Kubernetes project authors aren't responsible for these projects, which are\\nlisted alphabetically. To add a project to this list, read the content guide  before submitting a\\nchange. More information.\\nOther alternatives for enforcing policies are being developed in the Kubernetes ecosystem, such\\nas:\\nKubewarden\\nKyverno\\nOPA Gatekeeper• \\n• \\n• \\n• \\n• \\n• \\n• \\n• \\n• \\n• \\n• \\n• \\n• \\n• \\n•\", metadata={'source': './PDFS/Concepts.pdf', 'page': 408}),\n",
       " Document(page_content=\"Pod OS field\\nKubernetes lets you use nodes that run either Linux or Windows. You can mix both kinds of\\nnode in one cluster. Windows in Kubernetes has some limitations and differentiators from\\nLinux-based workloads. Specifically, many of the Pod securityContext  fields have no effect on\\nWindows .\\nNote:  Kubelets prior to v1.24 don't enforce the pod OS field, and if a cluster has nodes on\\nversions earlier than v1.24 the restricted policies should be pinned to a version prior to v1.25.\\nRestricted Pod Security Standard changes\\nAnother important change, made in Kubernetes v1.25 is that the restricted  Pod security has\\nbeen updated to use the pod.spec.os.name  field. Based on the OS name, certain policies that are\\nspecific to a particular OS can be relaxed for the other OS.\\nOS-specific policy controls\\nRestrictions on the following controls are only required if .spec.os.name  is not windows :\\nPrivilege Escalation\\nSeccomp\\nLinux Capabilities\\nFAQ\\nWhy isn't there a profile between privileged and baseline?\\nThe three profiles defined here have a clear linear progression from most secure (restricted) to\\nleast secure (privileged), and cover a broad set of workloads. Privileges required above the\\nbaseline policy are typically very application specific, so we do not offer a standard profile in\\nthis niche. This is not to say that the privileged profile should always be used in this case, but\\nthat policies in this space need to be defined on a case-by-case basis.\\nSIG Auth may reconsider this position in the future, should a clear need for other profiles arise.\\nWhat's the difference between a security profile and a security context?\\nSecurity Contexts  configure Pods and Containers at runtime. Security contexts are defined as\\npart of the Pod and container specifications in the Pod manifest, and represent parameters to\\nthe container runtime.\\nSecurity profiles are control plane mechanisms to enforce specific settings in the Security\\nContext, as well as other related parameters outside the Security Context. As of July 2021, Pod\\nSecurity Policies  are deprecated in favor of the built-in Pod Security Admission Controller .\\nWhat about sandboxed Pods?\\nThere is not currently an API standard that controls whether a Pod is considered sandboxed or\\nnot. Sandbox Pods may be identified by the use of a sandboxed runtime (such as gVisor or Kata\\nContainers), but there is no standard definition of what a sandboxed runtime is.• \\n• \\n•\", metadata={'source': './PDFS/Concepts.pdf', 'page': 409}),\n",
       " Document(page_content=\"The protections necessary for sandboxed workloads can differ from others. For example, the\\nneed to restrict privileged permissions is lessened when the workload is isolated from the\\nunderlying kernel. This allows for workloads requiring heightened permissions to still be\\nisolated.\\nAdditionally, the protection of sandboxed workloads is highly dependent on the method of\\nsandboxing. As such, no single recommended profile is recommended for all sandboxed\\nworkloads.\\nService Accounts\\nLearn about ServiceAccount objects in Kubernetes.\\nThis page introduces the ServiceAccount object in Kubernetes, providing information about\\nhow service accounts work, use cases, limitations, alternatives, and links to resources for\\nadditional guidance.\\nWhat are service accounts?\\nA service account is a type of non-human account that, in Kubernetes, provides a distinct\\nidentity in a Kubernetes cluster. Application Pods, system components, and entities inside and\\noutside the cluster can use a specific ServiceAccount's credentials to identify as that\\nServiceAccount. This identity is useful in various situations, including authenticating to the API\\nserver or implementing identity-based security policies.\\nService accounts exist as ServiceAccount objects in the API server. Service accounts have the\\nfollowing properties:\\nNamespaced:  Each service account is bound to a Kubernetes namespace . Every\\nnamespace gets a default  ServiceAccount  upon creation.\\nLightweight:  Service accounts exist in the cluster and are defined in the Kubernetes API.\\nYou can quickly create service accounts to enable specific tasks.\\nPortable:  A configuration bundle for a complex containerized workload might include\\nservice account definitions for the system's components. The lightweight nature of\\nservice accounts and the namespaced identities make the configurations portable.\\nService accounts are different from user accounts, which are authenticated human users in the\\ncluster. By default, user accounts don't exist in the Kubernetes API server; instead, the API\\nserver treats user identities as opaque data. You can authenticate as a user account using\\nmultiple methods. Some Kubernetes distributions might add custom extension APIs to\\nrepresent user accounts in the API server.\\nComparison between service accounts and users\\nDescription ServiceAccount User or group\\nLocationKubernetes API (ServiceAccount\\nobject)External\\nAccess\\ncontrolKubernetes RBAC or other \\nauthorization mechanismsKubernetes RBAC or other identity and\\naccess management mechanisms\\nIntended use Workloads, automation People• \\n• \\n•\", metadata={'source': './PDFS/Concepts.pdf', 'page': 410}),\n",
       " Document(page_content=\"Default service accounts\\nWhen you create a cluster, Kubernetes automatically creates a ServiceAccount object named \\ndefault  for every namespace in your cluster. The default  service accounts in each namespace get\\nno permissions by default other than the default API discovery permissions  that Kubernetes\\ngrants to all authenticated principals if role-based access control (RBAC) is enabled. If you\\ndelete the default  ServiceAccount object in a namespace, the control plane  replaces it with a\\nnew one.\\nIf you deploy a Pod in a namespace, and you don't manually assign a ServiceAccount to the\\nPod, Kubernetes assigns the default  ServiceAccount for that namespace to the Pod.\\nUse cases for Kubernetes service accounts\\nAs a general guideline, you can use service accounts to provide identities in the following\\nscenarios:\\nYour Pods need to communicate with the Kubernetes API server, for example in\\nsituations such as the following:\\nProviding read-only access to sensitive information stored in Secrets.\\nGranting cross-namespace access , such as allowing a Pod in namespace example  to\\nread, list, and watch for Lease objects in the kube-node-lease  namespace.\\nYour Pods need to communicate with an external service. For example, a workload Pod\\nrequires an identity for a commercially available cloud API, and the commercial provider\\nallows configuring a suitable trust relationship.\\nAuthenticating to a private image registry using an imagePullSecret .\\nAn external service needs to communicate with the Kubernetes API server. For example,\\nauthenticating to the cluster as part of a CI/CD pipeline.\\nYou use third-party security software in your cluster that relies on the ServiceAccount\\nidentity of different Pods to group those Pods into different contexts.\\nHow to use service accounts\\nTo use a Kubernetes service account, you do the following:\\nCreate a ServiceAccount object using a Kubernetes client like kubectl  or a manifest that\\ndefines the object.\\nGrant permissions to the ServiceAccount object using an authorization mechanism such\\nas RBAC .\\nAssign the ServiceAccount object to Pods during Pod creation.\\nIf you're using the identity from an external service, retrieve the ServiceAccount token\\nand use it from that service instead.\\nFor instructions, refer to Configure Service Accounts for Pods .\\nGrant permissions to a ServiceAccount\\nYou can use the built-in Kubernetes role-based access control (RBAC)  mechanism to grant the\\nminimum permissions required by each service account. You create a role, which grants access,• \\n◦ \\n◦ \\n• \\n• \\n• \\n• \\n1. \\n2. \\n3.\", metadata={'source': './PDFS/Concepts.pdf', 'page': 411}),\n",
       " Document(page_content=\"and then bind the role to your ServiceAccount. RBAC lets you define a minimum set of\\npermissions so that the service account permissions follow the principle of least privilege. Pods\\nthat use that service account don't get more permissions than are required to function correctly.\\nFor instructions, refer to ServiceAccount permissions .\\nCross-namespace access using a ServiceAccount\\nYou can use RBAC to allow service accounts in one namespace to perform actions on resources\\nin a different namespace in the cluster. For example, consider a scenario where you have a\\nservice account and Pod in the dev namespace and you want your Pod to see Jobs running in\\nthe maintenance  namespace. You could create a Role object that grants permissions to list Job\\nobjects. Then, you'd create a RoleBinding object in the maintenance  namespace to bind the Role\\nto the ServiceAccount object. Now, Pods in the dev namespace can list Job objects in the \\nmaintenance  namespace using that service account.\\nAssign a ServiceAccount to a Pod\\nTo assign a ServiceAccount to a Pod, you set the spec.serviceAccountName  field in the Pod\\nspecification. Kubernetes then automatically provides the credentials for that ServiceAccount to\\nthe Pod. In v1.22 and later, Kubernetes gets a short-lived, automatically rotating  token using\\nthe TokenRequest  API and mounts the token as a projected volume .\\nBy default, Kubernetes provides the Pod with the credentials for an assigned ServiceAccount,\\nwhether that is the default  ServiceAccount or a custom ServiceAccount that you specify.\\nTo prevent Kubernetes from automatically injecting credentials for a specified ServiceAccount\\nor the default  ServiceAccount, set the automountServiceAccountToken  field in your Pod\\nspecification to false.\\nIn versions earlier than 1.22, Kubernetes provides a long-lived, static token to the Pod as a\\nSecret.\\nManually retrieve ServiceAccount credentials\\nIf you need the credentials for a ServiceAccount to mount in a non-standard location, or for an\\naudience that isn't the API server, use one of the following methods:\\nTokenRequest API  (recommended): Request a short-lived service account token from\\nwithin your own application code . The token expires automatically and can rotate upon\\nexpiration. If you have a legacy application that is not aware of Kubernetes, you could\\nuse a sidecar container within the same pod to fetch these tokens and make them\\navailable to the application workload.\\nToken Volume Projection  (also recommended): In Kubernetes v1.20 and later, use the Pod\\nspecification to tell the kubelet to add the service account token to the Pod as a projected\\nvolume . Projected tokens expire automatically, and the kubelet rotates the token before it\\nexpires.\\nService Account Token Secrets  (not recommended): You can mount service account\\ntokens as Kubernetes Secrets in Pods. These tokens don't expire and don't rotate. This\\nmethod is not recommended, especially at scale, because of the risks associated with\\nstatic, long-lived credentials. In Kubernetes v1.24 and later, the \\nLegacyServiceAccountTokenNoAutoGeneration feature gate  prevents Kubernetes from\\nautomatically creating these tokens for ServiceAccounts. • \\n• \\n•\", metadata={'source': './PDFS/Concepts.pdf', 'page': 412}),\n",
       " Document(page_content='LegacyServiceAccountTokenNoAutoGeneration  is enabled by default; in other words,\\nKubernetes does not create these tokens.\\nNote:\\nFor applications running outside your Kubernetes cluster, you might be considering creating a\\nlong-lived ServiceAccount token that is stored in a Secret. This allows authentication, but the\\nKubernetes project recommends you avoid this approach. Long-lived bearer tokens represent a\\nsecurity risk as, once disclosed, the token can be misused. Instead, consider using an alternative.\\nFor example, your external application can authenticate using a well-protected private key and\\na certificate, or using a custom mechanism such as an authentication webhook  that you\\nimplement yourself.\\nYou can also use TokenRequest to obtain short-lived tokens for your external application.\\nRestricting access to Secrets\\nKubernetes provides an annotation called kubernetes.io/enforce-mountable-secrets  that you can\\nadd to your ServiceAccounts. When this annotation is applied, the ServiceAccount\\'s secrets can\\nonly be mounted on specified types of resources, enhancing the security posture of your cluster.\\nYou can add the annotation to a ServiceAccount using a manifest:\\napiVersion : v1\\nkind: ServiceAccount\\nmetadata :\\n  annotations :\\n    kubernetes.io/enforce-mountable-secrets : \"true\"\\n  name : my-serviceaccount\\n  namespace : my-namespace\\nWhen this annotation is set to \"true\", the Kubernetes control plane ensures that the Secrets\\nfrom this ServiceAccount are subject to certain mounting restrictions.\\nThe name of each Secret that is mounted as a volume in a Pod must appear in the secrets\\nfield of the Pod\\'s ServiceAccount.\\nThe name of each Secret referenced using envFrom  in a Pod must also appear in the \\nsecrets  field of the Pod\\'s ServiceAccount.\\nThe name of each Secret referenced using imagePullSecrets  in a Pod must also appear in\\nthe secrets  field of the Pod\\'s ServiceAccount.\\nBy understanding and enforcing these restrictions, cluster administrators can maintain a tighter\\nsecurity profile and ensure that secrets are accessed only by the appropriate resources.\\nAuthenticating service account credentials\\nServiceAccounts use signed JSON Web Tokens  (JWTs) to authenticate to the Kubernetes API\\nserver, and to any other system where a trust relationship exists. Depending on how the token\\nwas issued (either time-limited using a TokenRequest  or using a legacy mechanism with a\\nSecret), a ServiceAccount token might also have an expiry time, an audience, and a time after\\nwhich the token starts  being valid. When a client that is acting as a ServiceAccount tries to\\ncommunicate with the Kubernetes API server, the client includes an Authorization: Bearer 1. \\n2. \\n3.', metadata={'source': './PDFS/Concepts.pdf', 'page': 413}),\n",
       " Document(page_content=\"<token>  header with the HTTP request. The API server checks the validity of that bearer token\\nas follows:\\nChecks the token signature.\\nChecks whether the token has expired.\\nChecks whether object references in the token claims are currently valid.\\nChecks whether the token is currently valid.\\nChecks the audience claims.\\nThe TokenRequest API produces bound tokens  for a ServiceAccount. This binding is linked to\\nthe lifetime of the client, such as a Pod, that is acting as that ServiceAccount.\\nFor tokens issued using the TokenRequest  API, the API server also checks that the specific\\nobject reference that is using the ServiceAccount still exists, matching by the unique ID  of that\\nobject. For legacy tokens that are mounted as Secrets in Pods, the API server checks the token\\nagainst the Secret.\\nFor more information about the authentication process, refer to Authentication .\\nAuthenticating service account credentials in your own code\\nIf you have services of your own that need to validate Kubernetes service account credentials,\\nyou can use the following methods:\\nTokenReview API  (recommended)\\nOIDC discovery\\nThe Kubernetes project recommends that you use the TokenReview API, because this method\\ninvalidates tokens that are bound to API objects such as Secrets, ServiceAccounts, and Pods\\nwhen those objects are deleted. For example, if you delete the Pod that contains a projected\\nServiceAccount token, the cluster invalidates that token immediately and a TokenReview\\nimmediately fails. If you use OIDC validation instead, your clients continue to treat the token as\\nvalid until the token reaches its expiration timestamp.\\nYour application should always define the audience that it accepts, and should check that the\\ntoken's audiences match the audiences that the application expects. This helps to minimize the\\nscope of the token so that it can only be used in your application and nowhere else.\\nAlternatives\\nIssue your own tokens using another mechanism, and then use Webhook Token\\nAuthentication  to validate bearer tokens using your own validation service.\\nProvide your own identities to Pods.\\nUse the SPIFFE CSI driver plugin to provide SPIFFE SVIDs as X.509 certificate pairs\\nto Pods .\\n This item links to a third party project or product that is not part of Kubernetes\\nitself. More information\\nUse a service mesh such as Istio to provide certificates to Pods .1. \\n2. \\n3. \\n4. \\n5. \\n• \\n• \\n• \\n• \\n◦ \\n◦\", metadata={'source': './PDFS/Concepts.pdf', 'page': 414}),\n",
       " Document(page_content=\"Authenticate from outside the cluster to the API server without using service account\\ntokens:\\nConfigure the API server to accept OpenID Connect (OIDC) tokens from your\\nidentity provider .\\nUse service accounts or user accounts created using an external Identity and Access\\nManagement (IAM) service, such as from a cloud provider, to authenticate to your\\ncluster.\\nUse the CertificateSigningRequest API with client certificates .\\nConfigure the kubelet to retrieve credentials from an image registry .\\nUse a Device Plugin to access a virtual Trusted Platform Module (TPM), which then\\nallows authentication using a private key.\\nWhat's next\\nLearn how to manage your ServiceAccounts as a cluster administrator .\\nLearn how to assign a ServiceAccount to a Pod .\\nRead the ServiceAccount API reference .\\nPod Security Admission\\nAn overview of the Pod Security Admission Controller, which can enforce the Pod Security\\nStandards.\\nFEATURE STATE:  Kubernetes v1.25 [stable]\\nThe Kubernetes Pod Security Standards  define different isolation levels for Pods. These\\nstandards let you define how you want to restrict the behavior of pods in a clear, consistent\\nfashion.\\nKubernetes offers a built-in Pod Security  admission controller  to enforce the Pod Security\\nStandards. Pod security restrictions are applied at the namespace  level when pods are created.\\nBuilt-in Pod Security admission enforcement\\nThis page is part of the documentation for Kubernetes v1.28. If you are running a different\\nversion of Kubernetes, consult the documentation for that release.\\nPod Security levels\\nPod Security admission places requirements on a Pod's Security Context  and other related fields\\naccording to the three levels defined by the Pod Security Standards : privileged , baseline , and \\nrestricted . Refer to the Pod Security Standards  page for an in-depth look at those requirements.\\nPod Security Admission labels for namespaces\\nOnce the feature is enabled or the webhook is installed, you can configure namespaces to define\\nthe admission control mode you want to use for pod security in each namespace. Kubernetes\\ndefines a set of labels  that you can set to define which of the predefined Pod Security Standard\\nlevels you want to use for a namespace. The label you select defines what action the control\\nplane  takes if a potential violation is detected:• \\n◦ \\n◦ \\n◦ \\n• \\n• \\n• \\n• \\n•\", metadata={'source': './PDFS/Concepts.pdf', 'page': 415}),\n",
       " Document(page_content='Pod Security Admission modes\\nMode Description\\nenforce Policy violations will cause the pod to be rejected.\\nauditPolicy violations will trigger the addition of an audit annotation to the event recorded\\nin the audit log , but are otherwise allowed.\\nwarn Policy violations will trigger a user-facing warning, but are otherwise allowed.\\nA namespace can configure any or all modes, or even set a different level for different modes.\\nFor each mode, there are two labels that determine the policy used:\\n# The per-mode level label indicates which policy level to apply for the mode.\\n#\\n# MODE must be one of `enforce`, `audit`, or `warn`.\\n# LEVEL must be one of `privileged`, `baseline`, or `restricted`.\\npod-security.kubernetes.io/<MODE> : <LEVEL>\\n# Optional: per-mode version label that can be used to pin the policy to the\\n# version that shipped with a given Kubernetes minor version (for example v1.28).\\n#\\n# MODE must be one of `enforce`, `audit`, or `warn`.\\n# VERSION must be a valid Kubernetes minor version, or `latest`.\\npod-security.kubernetes.io/<MODE>-version : <VERSION>\\nCheck out Enforce Pod Security Standards with Namespace Labels  to see example usage.\\nWorkload resources and Pod templates\\nPods are often created indirectly, by creating a workload object  such as a Deployment  or Job.\\nThe workload object defines a Pod template  and a controller  for the workload resource creates\\nPods based on that template. To help catch violations early, both the audit and warning modes\\nare applied to the workload resources. However, enforce mode is not applied to workload\\nresources, only to the resulting pod objects.\\nExemptions\\nYou can define exemptions  from pod security enforcement in order to allow the creation of pods\\nthat would have otherwise been prohibited due to the policy associated with a given\\nnamespace. Exemptions can be statically configured in the Admission Controller configuration .\\nExemptions must be explicitly enumerated. Requests meeting exemption criteria are ignored  by\\nthe Admission Controller (all enforce , audit  and warn  behaviors are skipped). Exemption\\ndimensions include:\\nUsernames:  requests from users with an exempt authenticated (or impersonated)\\nusername are ignored.\\nRuntimeClassNames:  pods and workload resources  specifying an exempt runtime class\\nname are ignored.\\nNamespaces:  pods and workload resources  in an exempt namespace are ignored.\\nCaution:  Most pods are created by a controller in response to a workload resource , meaning\\nthat exempting an end user will only exempt them from enforcement when creating pods• \\n• \\n•', metadata={'source': './PDFS/Concepts.pdf', 'page': 416}),\n",
       " Document(page_content=\"directly, but not when creating a workload resource. Controller service accounts (such as \\nsystem:serviceaccount:kube-system:replicaset-controller ) should generally not be exempted, as\\ndoing so would implicitly exempt any user that can create the corresponding workload\\nresource.\\nUpdates to the following pod fields are exempt from policy checks, meaning that if a pod\\nupdate request only changes these fields, it will not be denied even if the pod is in violation of\\nthe current policy level:\\nAny metadata updates except  changes to the seccomp or AppArmor annotations:\\nseccomp.security.alpha.kubernetes.io/pod  (deprecated)\\ncontainer.seccomp.security.alpha.kubernetes.io/*  (deprecated)\\ncontainer.apparmor.security.beta.kubernetes.io/*\\nValid updates to .spec.activeDeadlineSeconds\\nValid updates to .spec.tolerations\\nMetrics\\nHere are the Prometheus metrics exposed by kube-apiserver:\\npod_security_errors_total : This metric indicates the number of errors preventing normal\\nevaluation. Non-fatal errors may result in the latest restricted profile being used for\\nenforcement.\\npod_security_evaluations_total : This metric indicates the number of policy evaluations\\nthat have occurred, not counting ignored or exempt requests during exporting.\\npod_security_exemptions_total : This metric indicates the number of exempt requests, not\\ncounting ignored or out of scope requests.\\nWhat's next\\nPod Security Standards\\nEnforcing Pod Security Standards\\nEnforce Pod Security Standards by Configuring the Built-in Admission Controller\\nEnforce Pod Security Standards with Namespace Labels\\nIf you are running an older version of Kubernetes and want to upgrade to a version of\\nKubernetes that does not include PodSecurityPolicies, read migrate from PodSecurityPolicy to\\nthe Built-In PodSecurity Admission Controller .\\nPod Security Policies\\nRemoved feature\\nPodSecurityPolicy was deprecated  in Kubernetes v1.21, and removed from Kubernetes in v1.25.\\nInstead of using PodSecurityPolicy, you can enforce similar restrictions on Pods using either or\\nboth:\\nPod Security Admission\\na 3rd party admission plugin, that you deploy and configure yourself• \\n◦ \\n◦ \\n◦ \\n• \\n• \\n• \\n• \\n• \\n• \\n• \\n• \\n• \\n• \\n•\", metadata={'source': './PDFS/Concepts.pdf', 'page': 417}),\n",
       " Document(page_content=\"For a migration guide, see Migrate from PodSecurityPolicy to the Built-In PodSecurity\\nAdmission Controller . For more information on the removal of this API, see PodSecurityPolicy\\nDeprecation: Past, Present, and Future .\\nIf you are not running Kubernetes v1.28, check the documentation for your version of\\nKubernetes.\\nSecurity For Windows Nodes\\nThis page describes security considerations and best practices specific to the Windows\\noperating system.\\nProtection for Secret data on nodes\\nOn Windows, data from Secrets are written out in clear text onto the node's local storage (as\\ncompared to using tmpfs / in-memory filesystems on Linux). As a cluster operator, you should\\ntake both of the following additional measures:\\nUse file ACLs to secure the Secrets' file location.\\nApply volume-level encryption using BitLocker .\\nContainer users\\nRunAsUsername  can be specified for Windows Pods or containers to execute the container\\nprocesses as specific user. This is roughly equivalent to RunAsUser .\\nWindows containers offer two default user accounts, ContainerUser and\\nContainerAdministrator. The differences between these two user accounts are covered in When\\nto use ContainerAdmin and ContainerUser user accounts  within Microsoft's Secure Windows\\ncontainers  documentation.\\nLocal users can be added to container images during the container build process.\\nNote:\\nNano Server  based images run as ContainerUser  by default\\nServer Core  based images run as ContainerAdministrator  by default\\nWindows containers can also run as Active Directory identities by utilizing Group Managed\\nService Accounts\\nPod-level security isolation\\nLinux-specific pod security context mechanisms (such as SELinux, AppArmor, Seccomp, or\\ncustom POSIX capabilities) are not supported on Windows nodes.\\nPrivileged containers are not supported  on Windows. Instead HostProcess containers  can be\\nused on Windows to perform many of the tasks performed by privileged containers on Linux.1. \\n2. \\n• \\n•\", metadata={'source': './PDFS/Concepts.pdf', 'page': 418}),\n",
       " Document(page_content='Controlling Access to the Kubernetes API\\nThis page provides an overview of controlling access to the Kubernetes API.\\nUsers access the Kubernetes API  using kubectl , client libraries, or by making REST requests.\\nBoth human users and Kubernetes service accounts  can be authorized for API access. When a\\nrequest reaches the API, it goes through several stages, illustrated in the following diagram:\\nDiagram of request handling steps for Kubernetes API request\\nTransport security\\nBy default, the Kubernetes API server listens on port 6443 on the first non-localhost network\\ninterface, protected by TLS. In a typical production Kubernetes cluster, the API serves on port\\n443. The port can be changed with the --secure-port , and the listening IP address with the --\\nbind-address  flag.\\nThe API server presents a certificate. This certificate may be signed using a private certificate\\nauthority (CA), or based on a public key infrastructure linked to a generally recognized CA. The\\ncertificate and corresponding private key can be set by using the --tls-cert-file  and --tls-private-\\nkey-file  flags.\\nIf your cluster uses a private certificate authority, you need a copy of that CA certificate\\nconfigured into your ~/.kube/config  on the client, so that you can trust the connection and be\\nconfident it was not intercepted.\\nYour client can present a TLS client certificate at this stage.\\nAuthentication\\nOnce TLS is established, the HTTP request moves to the Authentication step. This is shown as\\nstep 1 in the diagram. The cluster creation script or cluster admin configures the API server to\\nrun one or more Authenticator modules. Authenticators are described in more detail in \\nAuthentication .\\nThe input to the authentication step is the entire HTTP request; however, it typically examines\\nthe headers and/or client certificate.\\nAuthentication modules include client certificates, password, and plain tokens, bootstrap\\ntokens, and JSON Web Tokens (used for service accounts).\\nMultiple authentication modules can be specified, in which case each one is tried in sequence,\\nuntil one of them succeeds.\\nIf the request cannot be authenticated, it is rejected with HTTP status code 401. Otherwise, the\\nuser is authenticated as a specific username , and the user name is available to subsequent steps\\nto use in their decisions. Some authenticators also provide the group memberships of the user,\\nwhile other authenticators do not.\\nWhile Kubernetes uses usernames for access control decisions and in request logging, it does\\nnot have a User object nor does it store usernames or other information about users in its API.', metadata={'source': './PDFS/Concepts.pdf', 'page': 419}),\n",
       " Document(page_content='Authorization\\nAfter the request is authenticated as coming from a specific user, the request must be\\nauthorized. This is shown as step\\xa0 2\\xa0in the diagram.\\nA request must include the username of the requester, the requested action, and the object\\naffected by the action. The request is authorized if an existing policy declares that the user has\\npermissions to complete the requested action.\\nFor example, if Bob has the policy below, then he can read pods only in the namespace \\nprojectCaribou :\\n{\\n    \"apiVersion\" : \"abac.authorization.kubernetes.io/v1beta1\" ,\\n    \"kind\" : \"Policy\" ,\\n    \"spec\" : {\\n        \"user\" : \"bob\" ,\\n        \"namespace\" : \"projectCaribou\" ,\\n        \"resource\" : \"pods\" ,\\n        \"readonly\" : true\\n    }\\n}\\nIf Bob makes the following request, the request is authorized because he is allowed to read\\nobjects in the projectCaribou  namespace:\\n{\\n  \"apiVersion\" : \"authorization.k8s.io/v1beta1\" ,\\n  \"kind\" : \"SubjectAccessReview\" ,\\n  \"spec\" : {\\n    \"resourceAttributes\" : {\\n      \"namespace\" : \"projectCaribou\" ,\\n      \"verb\" : \"get\" ,\\n      \"group\" : \"unicorn.example.org\" ,\\n      \"resource\" : \"pods\"\\n    }\\n  }\\n}\\nIf Bob makes a request to write ( create  or update ) to the objects in the projectCaribou\\nnamespace, his authorization is denied. If Bob makes a request to read ( get) objects in a\\ndifferent namespace such as projectFish , then his authorization is denied.\\nKubernetes authorization requires that you use common REST attributes to interact with\\nexisting organization-wide or cloud-provider-wide access control systems. It is important to use\\nREST formatting because these control systems might interact with other APIs besides the\\nKubernetes API.\\nKubernetes supports multiple authorization modules, such as ABAC mode, RBAC Mode, and\\nWebhook mode. When an administrator creates a cluster, they configure the authorization\\nmodules that should be used in the API server. If more than one authorization modules are\\nconfigured, Kubernetes checks each module, and if any module authorizes the request, then the', metadata={'source': './PDFS/Concepts.pdf', 'page': 420}),\n",
       " Document(page_content=\"request can proceed. If all of the modules deny the request, then the request is denied (HTTP\\nstatus code 403).\\nTo learn more about Kubernetes authorization, including details about creating policies using\\nthe supported authorization modules, see Authorization .\\nAdmission control\\nAdmission Control modules are software modules that can modify or reject requests. In\\naddition to the attributes available to Authorization modules, Admission Control modules can\\naccess the contents of the object that is being created or modified.\\nAdmission controllers act on requests that create, modify, delete, or connect to (proxy) an\\nobject. Admission controllers do not act on requests that merely read objects. When multiple\\nadmission controllers are configured, they are called in order.\\nThis is shown as step 3 in the diagram.\\nUnlike Authentication and Authorization modules, if any admission controller module rejects,\\nthen the request is immediately rejected.\\nIn addition to rejecting objects, admission controllers can also set complex defaults for fields.\\nThe available Admission Control modules are described in Admission Controllers .\\nOnce a request passes all admission controllers, it is validated using the validation routines for\\nthe corresponding API object, and then written to the object store (shown as step 4).\\nAuditing\\nKubernetes auditing provides a security-relevant, chronological set of records documenting the\\nsequence of actions in a cluster. The cluster audits the activities generated by users, by\\napplications that use the Kubernetes API, and by the control plane itself.\\nFor more information, see Auditing .\\nWhat's next\\nRead more documentation on authentication, authorization and API access control:\\nAuthenticating\\nAuthenticating with Bootstrap Tokens\\nAdmission Controllers\\nDynamic Admission Control\\nAuthorization\\nRole Based Access Control\\nAttribute Based Access Control\\nNode Authorization\\nWebhook Authorization\\nCertificate Signing Requests\\nincluding CSR approval  and certificate signing• \\n◦ \\n• \\n◦ \\n• \\n◦ \\n◦ \\n◦ \\n◦ \\n• \\n◦\", metadata={'source': './PDFS/Concepts.pdf', 'page': 421}),\n",
       " Document(page_content='Service accounts\\nDeveloper guide\\nAdministration\\nYou can learn about:\\nhow Pods can use Secrets  to obtain API credentials.\\nRole Based Access Control Good Practices\\nPrinciples and practices for good RBAC design for cluster operators.\\nKubernetes RBAC  is a key security control to ensure that cluster users and workloads have only\\nthe access to resources required to execute their roles. It is important to ensure that, when\\ndesigning permissions for cluster users, the cluster administrator understands the areas where\\nprivilege escalation could occur, to reduce the risk of excessive access leading to security\\nincidents.\\nThe good practices laid out here should be read in conjunction with the general RBAC\\ndocumentation .\\nGeneral good practice\\nLeast privilege\\nIdeally, minimal RBAC rights should be assigned to users and service accounts. Only\\npermissions explicitly required for their operation should be used. While each cluster will be\\ndifferent, some general rules that can be applied are :\\nAssign permissions at the namespace level where possible. Use RoleBindings as opposed\\nto ClusterRoleBindings to give users rights only within a specific namespace.\\nAvoid providing wildcard permissions when possible, especially to all resources. As\\nKubernetes is an extensible system, providing wildcard access gives rights not just to all\\nobject types that currently exist in the cluster, but also to all object types which are\\ncreated in the future.\\nAdministrators should not use cluster-admin  accounts except where specifically needed.\\nProviding a low privileged account with impersonation rights  can avoid accidental\\nmodification of cluster resources.\\nAvoid adding users to the system:masters  group. Any user who is a member of this group\\nbypasses all RBAC rights checks and will always have unrestricted superuser access,\\nwhich cannot be revoked by removing RoleBindings or ClusterRoleBindings. As an aside,\\nif a cluster is using an authorization webhook, membership of this group also bypasses\\nthat webhook (requests from users who are members of that group are never sent to the\\nwebhook)• \\n◦ \\n◦ \\n• \\n• \\n• \\n• \\n•', metadata={'source': './PDFS/Concepts.pdf', 'page': 422}),\n",
       " Document(page_content=\"Minimize distribution of privileged tokens\\nIdeally, pods shouldn't be assigned service accounts that have been granted powerful\\npermissions (for example, any of the rights listed under privilege escalation risks ). In cases\\nwhere a workload requires powerful permissions, consider the following practices:\\nLimit the number of nodes running powerful pods. Ensure that any DaemonSets you run\\nare necessary and are run with least privilege to limit the blast radius of container\\nescapes.\\nAvoid running powerful pods alongside untrusted or publicly-exposed ones. Consider\\nusing Taints and Toleration , NodeAffinity , or PodAntiAffinity  to ensure pods don't run\\nalongside untrusted or less-trusted Pods. Pay especial attention to situations where less-\\ntrustworthy Pods are not meeting the Restricted  Pod Security Standard.\\nHardening\\nKubernetes defaults to providing access which may not be required in every cluster. Reviewing\\nthe RBAC rights provided by default can provide opportunities for security hardening. In\\ngeneral, changes should not be made to rights provided to system:  accounts some options to\\nharden cluster rights exist:\\nReview bindings for the system:unauthenticated  group and remove them where possible,\\nas this gives access to anyone who can contact the API server at a network level.\\nAvoid the default auto-mounting of service account tokens by setting \\nautomountServiceAccountToken: false . For more details, see using default service account\\ntoken . Setting this value for a Pod will overwrite the service account setting, workloads\\nwhich require service account tokens can still mount them.\\nPeriodic review\\nIt is vital to periodically review the Kubernetes RBAC settings for redundant entries and\\npossible privilege escalations. If an attacker is able to create a user account with the same name\\nas a deleted user, they can automatically inherit all the rights of the deleted user, especially the\\nrights assigned to that user.\\nKubernetes RBAC - privilege escalation risks\\nWithin Kubernetes RBAC there are a number of privileges which, if granted, can allow a user or\\na service account to escalate their privileges in the cluster or affect systems outside the cluster.\\nThis section is intended to provide visibility of the areas where cluster operators should take\\ncare, to ensure that they do not inadvertently allow for more access to clusters than intended.\\nListing secrets\\nIt is generally clear that allowing get access on Secrets will allow a user to read their contents.\\nIt is also important to note that list and watch  access also effectively allow for users to reveal\\nthe Secret contents. For example, when a List response is returned (for example, via kubectl get \\nsecrets -A -o yaml ), the response includes the contents of all Secrets.• \\n• \\n• \\n•\", metadata={'source': './PDFS/Concepts.pdf', 'page': 423}),\n",
       " Document(page_content='Workload creation\\nPermission to create workloads (either Pods, or workload resources  that manage Pods) in a\\nnamespace implicitly grants access to many other resources in that namespace, such as Secrets,\\nConfigMaps, and PersistentVolumes that can be mounted in Pods. Additionally, since Pods can\\nrun as any ServiceAccount , granting permission to create workloads also implicitly grants the\\nAPI access levels of any service account in that namespace.\\nUsers who can run privileged Pods can use that access to gain node access and potentially to\\nfurther elevate their privileges. Where you do not fully trust a user or other principal with the\\nability to create suitably secure and isolated Pods, you should enforce either the Baseline  or \\nRestricted  Pod Security Standard. You can use Pod Security admission  or other (third party)\\nmechanisms to implement that enforcement.\\nFor these reasons, namespaces should be used to separate resources requiring different levels of\\ntrust or tenancy. It is still considered best practice to follow least privilege  principles and assign\\nthe minimum set of permissions, but boundaries within a namespace should be considered\\nweak.\\nPersistent volume creation\\nIf someone - or some application - is allowed to create arbitrary PersistentVolumes, that access\\nincludes the creation of hostPath  volumes, which then means that a Pod would get access to the\\nunderlying host filesystem(s) on the associated node. Granting that ability is a security risk.\\nThere are many ways a container with unrestricted access to the host filesystem can escalate\\nprivileges, including reading data from other containers, and abusing the credentials of system\\nservices, such as Kubelet.\\nYou should only allow access to create PersistentVolume objects for:\\nusers (cluster operators) that need this access for their work, and who you trust,\\nthe Kubernetes control plane components which creates PersistentVolumes based on\\nPersistentVolumeClaims that are configured for automatic provisioning. This is usually\\nsetup by the Kubernetes provider or by the operator when installing a CSI driver.\\nWhere access to persistent storage is required trusted administrators should create\\nPersistentVolumes, and constrained users should use PersistentVolumeClaims to access that\\nstorage.\\nAccess to proxy  subresource of Nodes\\nUsers with access to the proxy sub-resource of node objects have rights to the Kubelet API,\\nwhich allows for command execution on every pod on the node(s) to which they have rights.\\nThis access bypasses audit logging and admission control, so care should be taken before\\ngranting rights to this resource.\\nEscalate verb\\nGenerally, the RBAC system prevents users from creating clusterroles with more rights than the\\nuser possesses. The exception to this is the escalate  verb. As noted in the RBAC documentation ,\\nusers with this right can effectively escalate their privileges.• \\n•', metadata={'source': './PDFS/Concepts.pdf', 'page': 424}),\n",
       " Document(page_content=\"Bind verb\\nSimilar to the escalate  verb, granting users this right allows for the bypass of Kubernetes in-\\nbuilt protections against privilege escalation, allowing users to create bindings to roles with\\nrights they do not already have.\\nImpersonate verb\\nThis verb allows users to impersonate and gain the rights of other users in the cluster. Care\\nshould be taken when granting it, to ensure that excessive permissions cannot be gained via\\none of the impersonated accounts.\\nCSRs and certificate issuing\\nThe CSR API allows for users with create  rights to CSRs and update  rights on \\ncertificatesigningrequests/approval  where the signer is kubernetes.io/kube-apiserver-client  to\\ncreate new client certificates which allow users to authenticate to the cluster. Those client\\ncertificates can have arbitrary names including duplicates of Kubernetes system components.\\nThis will effectively allow for privilege escalation.\\nToken request\\nUsers with create  rights on serviceaccounts/token  can create TokenRequests to issue tokens for\\nexisting service accounts.\\nControl admission webhooks\\nUsers with control over validatingwebhookconfigurations  or mutatingwebhookconfigurations\\ncan control webhooks that can read any object admitted to the cluster, and in the case of\\nmutating webhooks, also mutate admitted objects.\\nKubernetes RBAC - denial of service risks\\nObject creation denial-of-service\\nUsers who have rights to create objects in a cluster may be able to create sufficient large objects\\nto create a denial of service condition either based on the size or number of objects, as\\ndiscussed in etcd used by Kubernetes is vulnerable to OOM attack . This may be specifically\\nrelevant in multi-tenant clusters if semi-trusted or untrusted users are allowed limited access to\\na system.\\nOne option for mitigation of this issue would be to use resource quotas  to limit the quantity of\\nobjects which can be created.\\nWhat's next\\nTo learn more about RBAC, see the RBAC documentation . •\", metadata={'source': './PDFS/Concepts.pdf', 'page': 425}),\n",
       " Document(page_content=\"Good practices for Kubernetes Secrets\\nPrinciples and practices for good Secret management for cluster administrators and application\\ndevelopers.\\nIn Kubernetes, a Secret is an object that stores sensitive information, such as passwords, OAuth\\ntokens, and SSH keys.\\nSecrets give you more control over how sensitive information is used and reduces the risk of\\naccidental exposure. Secret values are encoded as base64 strings and are stored unencrypted by\\ndefault, but can be configured to be encrypted at rest .\\nA Pod can reference the Secret in a variety of ways, such as in a volume mount or as an\\nenvironment variable. Secrets are designed for confidential data and ConfigMaps  are designed\\nfor non-confidential data.\\nThe following good practices are intended for both cluster administrators and application\\ndevelopers. Use these guidelines to improve the security of your sensitive information in Secret\\nobjects, as well as to more effectively manage your Secrets.\\nCluster administrators\\nThis section provides good practices that cluster administrators can use to improve the security\\nof confidential information in the cluster.\\nConfigure encryption at rest\\nBy default, Secret objects are stored unencrypted in etcd. You should configure encryption of\\nyour Secret data in etcd. For instructions, refer to Encrypt Secret Data at Rest .\\nConfigure least-privilege access to Secrets\\nWhen planning your access control mechanism, such as Kubernetes Role-based Access Control  \\n(RBAC) , consider the following guidelines for access to Secret  objects. You should also follow\\nthe other guidelines in RBAC good practices .\\nComponents : Restrict watch  or list access to only the most privileged, system-level\\ncomponents. Only grant get access for Secrets if the component's normal behavior\\nrequires it.\\nHumans : Restrict get, watch , or list access to Secrets. Only allow cluster administrators\\nto access etcd. This includes read-only access. For more complex access control, such as\\nrestricting access to Secrets with specific annotations, consider using third-party\\nauthorization mechanisms.\\nCaution:  Granting list access to Secrets implicitly lets the subject fetch the contents of the\\nSecrets.\\nA user who can create a Pod that uses a Secret can also see the value of that Secret. Even if\\ncluster policies do not allow a user to read the Secret directly, the same user could have access\\nto run a Pod that then exposes the Secret. You can detect or limit the impact caused by Secret• \\n•\", metadata={'source': './PDFS/Concepts.pdf', 'page': 426}),\n",
       " Document(page_content=\"data being exposed, either intentionally or unintentionally, by a user with this access. Some\\nrecommendations include:\\nUse short-lived Secrets\\nImplement audit rules that alert on specific events, such as concurrent reading of multiple\\nSecrets by a single user\\nAdditional ServiceAccount annotations for Secret management\\nYou can also use the kubernetes.io/enforce-mountable-secrets  annotation on a ServiceAccount\\nto enforce specific rules on how Secrets are used in a Pod. For more details, see the \\ndocumentation on this annotation .\\nImprove etcd management policies\\nConsider wiping or shredding the durable storage used by etcd once it is no longer in use.\\nIf there are multiple etcd instances, configure encrypted SSL/TLS communication between the\\ninstances to protect the Secret data in transit.\\nConfigure access to external Secrets\\nNote:  This section links to third party projects that provide functionality required by\\nKubernetes. The Kubernetes project authors aren't responsible for these projects, which are\\nlisted alphabetically. To add a project to this list, read the content guide  before submitting a\\nchange. More information.\\nYou can use third-party Secrets store providers to keep your confidential data outside your\\ncluster and then configure Pods to access that information. The Kubernetes Secrets Store CSI\\nDriver  is a DaemonSet that lets the kubelet retrieve Secrets from external stores, and mount the\\nSecrets as a volume into specific Pods that you authorize to access the data.\\nFor a list of supported providers, refer to Providers for the Secret Store CSI Driver .\\nDevelopers\\nThis section provides good practices for developers to use to improve the security of\\nconfidential data when building and deploying Kubernetes resources.\\nRestrict Secret access to specific containers\\nIf you are defining multiple containers in a Pod, and only one of those containers needs access\\nto a Secret, define the volume mount or environment variable configuration so that the other\\ncontainers do not have access to that Secret.\\nProtect Secret data after reading\\nApplications still need to protect the value of confidential information after reading it from an\\nenvironment variable or volume. For example, your application must avoid logging the secret\\ndata in the clear or transmitting it to an untrusted party.• \\n•\", metadata={'source': './PDFS/Concepts.pdf', 'page': 427}),\n",
       " Document(page_content='Avoid sharing Secret manifests\\nIf you configure a Secret through a manifest , with the secret data encoded as base64, sharing\\nthis file or checking it in to a source repository means the secret is available to everyone who\\ncan read the manifest.\\nCaution:  Base64 encoding is not an encryption method, it provides no additional\\nconfidentiality over plain text.\\nMulti-tenancy\\nThis page provides an overview of available configuration options and best practices for cluster\\nmulti-tenancy.\\nSharing clusters saves costs and simplifies administration. However, sharing clusters also\\npresents challenges such as security, fairness, and managing noisy neighbors .\\nClusters can be shared in many ways. In some cases, different applications may run in the same\\ncluster. In other cases, multiple instances of the same application may run in the same cluster,\\none for each end user. All these types of sharing are frequently described using the umbrella\\nterm multi-tenancy .\\nWhile Kubernetes does not have first-class concepts of end users or tenants, it provides several\\nfeatures to help manage different tenancy requirements. These are discussed below.\\nUse cases\\nThe first step to determining how to share your cluster is understanding your use case, so you\\ncan evaluate the patterns and tools available. In general, multi-tenancy in Kubernetes clusters\\nfalls into two broad categories, though many variations and hybrids are also possible.\\nMultiple teams\\nA common form of multi-tenancy is to share a cluster between multiple teams within an\\norganization, each of whom may operate one or more workloads. These workloads frequently\\nneed to communicate with each other, and with other workloads located on the same or\\ndifferent clusters.\\nIn this scenario, members of the teams often have direct access to Kubernetes resources via\\ntools such as kubectl , or indirect access through GitOps controllers or other types of release\\nautomation tools. There is often some level of trust between members of different teams, but\\nKubernetes policies such as RBAC, quotas, and network policies are essential to safely and\\nfairly share clusters.\\nMultiple customers\\nThe other major form of multi-tenancy frequently involves a Software-as-a-Service (SaaS)\\nvendor running multiple instances of a workload for customers. This business model is so\\nstrongly associated with this deployment style that many people call it \"SaaS tenancy.\"\\nHowever, a better term might be \"multi-customer tenancy,\" since SaaS vendors may also use\\nother deployment models, and this deployment model can also be used outside of SaaS.', metadata={'source': './PDFS/Concepts.pdf', 'page': 428}),\n",
       " Document(page_content='In this scenario, the customers do not have access to the cluster; Kubernetes is invisible from\\ntheir perspective and is only used by the vendor to manage the workloads. Cost optimization is\\nfrequently a critical concern, and Kubernetes policies are used to ensure that the workloads are\\nstrongly isolated from each other.\\nTerminology\\nTenants\\nWhen discussing multi-tenancy in Kubernetes, there is no single definition for a \"tenant\".\\nRather, the definition of a tenant will vary depending on whether multi-team or multi-customer\\ntenancy is being discussed.\\nIn multi-team usage, a tenant is typically a team, where each team typically deploys a small\\nnumber of workloads that scales with the complexity of the service. However, the definition of\\n\"team\" may itself be fuzzy, as teams may be organized into higher-level divisions or subdivided\\ninto smaller teams.\\nBy contrast, if each team deploys dedicated workloads for each new client, they are using a\\nmulti-customer model of tenancy. In this case, a \"tenant\" is simply a group of users who share a\\nsingle workload. This may be as large as an entire company, or as small as a single team at that\\ncompany.\\nIn many cases, the same organization may use both definitions of \"tenants\" in different\\ncontexts. For example, a platform team may offer shared services such as security tools and\\ndatabases to multiple internal “customers” and a SaaS vendor may also have multiple teams\\nsharing a development cluster. Finally, hybrid architectures are also possible, such as a SaaS\\nprovider using a combination of per-customer workloads for sensitive data, combined with\\nmulti-tenant shared services.\\nA cluster showing coexisting tenancy models\\nIsolation\\nThere are several ways to design and build multi-tenant solutions with Kubernetes. Each of\\nthese methods comes with its own set of tradeoffs that impact the isolation level,\\nimplementation effort, operational complexity, and cost of service.\\nA Kubernetes cluster consists of a control plane which runs Kubernetes software, and a data\\nplane consisting of worker nodes where tenant workloads are executed as pods. Tenant\\nisolation can be applied in both the control plane and the data plane based on organizational\\nrequirements.\\nThe level of isolation offered is sometimes described using terms like “hard” multi-tenancy,\\nwhich implies strong isolation, and “soft” multi-tenancy, which implies weaker isolation. In\\nparticular, \"hard\" multi-tenancy is often used to describe cases where the tenants do not trust\\neach other, often from security and resource sharing perspectives (e.g. guarding against attacks\\nsuch as data exfiltration or DoS). Since data planes typically have much larger attack surfaces,\\n\"hard\" multi-tenancy often requires extra attention to isolating the data-plane, though control\\nplane isolation also remains critical.', metadata={'source': './PDFS/Concepts.pdf', 'page': 429}),\n",
       " Document(page_content='However, the terms \"hard\" and \"soft\" can often be confusing, as there is no single definition that\\nwill apply to all users. Rather, \"hardness\" or \"softness\" is better understood as a broad spectrum,\\nwith many different techniques that can be used to maintain different types of isolation in your\\nclusters, based on your requirements.\\nIn more extreme cases, it may be easier or necessary to forgo any cluster-level sharing at all and\\nassign each tenant their dedicated cluster, possibly even running on dedicated hardware if VMs\\nare not considered an adequate security boundary. This may be easier with managed\\nKubernetes clusters, where the overhead of creating and operating clusters is at least somewhat\\ntaken on by a cloud provider. The benefit of stronger tenant isolation must be evaluated against\\nthe cost and complexity of managing multiple clusters. The Multi-cluster SIG  is responsible for\\naddressing these types of use cases.\\nThe remainder of this page focuses on isolation techniques used for shared Kubernetes clusters.\\nHowever, even if you are considering dedicated clusters, it may be valuable to review these\\nrecommendations, as it will give you the flexibility to shift to shared clusters in the future if\\nyour needs or capabilities change.\\nControl plane isolation\\nControl plane isolation ensures that different tenants cannot access or affect each others\\'\\nKubernetes API resources.\\nNamespaces\\nIn Kubernetes, a Namespace  provides a mechanism for isolating groups of API resources within\\na single cluster. This isolation has two key dimensions:\\nObject names within a namespace can overlap with names in other namespaces, similar\\nto files in folders. This allows tenants to name their resources without having to consider\\nwhat other tenants are doing.\\nMany Kubernetes security policies are scoped to namespaces. For example, RBAC Roles\\nand Network Policies are namespace-scoped resources. Using RBAC, Users and Service\\nAccounts can be restricted to a namespace.\\nIn a multi-tenant environment, a Namespace helps segment a tenant\\'s workload into a logical\\nand distinct management unit. In fact, a common practice is to isolate every workload in its\\nown namespace, even if multiple workloads are operated by the same tenant. This ensures that\\neach workload has its own identity and can be configured with an appropriate security policy.\\nThe namespace isolation model requires configuration of several other Kubernetes resources,\\nnetworking plugins, and adherence to security best practices to properly isolate tenant\\nworkloads. These considerations are discussed below.\\nAccess controls\\nThe most important type of isolation for the control plane is authorization. If teams or their\\nworkloads can access or modify each others\\' API resources, they can change or disable all other\\ntypes of policies thereby negating any protection those policies may offer. As a result, it is\\ncritical to ensure that each tenant has the appropriate access to only the namespaces they need,\\nand no more. This is known as the \"Principle of Least Privilege.\"1. \\n2.', metadata={'source': './PDFS/Concepts.pdf', 'page': 430}),\n",
       " Document(page_content=\"Role-based access control (RBAC) is commonly used to enforce authorization in the Kubernetes\\ncontrol plane, for both users and workloads (service accounts). Roles  and RoleBindings  are\\nKubernetes objects that are used at a namespace level to enforce access control in your\\napplication; similar objects exist for authorizing access to cluster-level objects, though these are\\nless useful for multi-tenant clusters.\\nIn a multi-team environment, RBAC must be used to restrict tenants' access to the appropriate\\nnamespaces, and ensure that cluster-wide resources can only be accessed or modified by\\nprivileged users such as cluster administrators.\\nIf a policy ends up granting a user more permissions than they need, this is likely a signal that\\nthe namespace containing the affected resources should be refactored into finer-grained\\nnamespaces. Namespace management tools may simplify the management of these finer-\\ngrained namespaces by applying common RBAC policies to different namespaces, while still\\nallowing fine-grained policies where necessary.\\nQuotas\\nKubernetes workloads consume node resources, like CPU and memory. In a multi-tenant\\nenvironment, you can use Resource Quotas  to manage resource usage of tenant workloads. For\\nthe multiple teams use case, where tenants have access to the Kubernetes API, you can use\\nresource quotas to limit the number of API resources (for example: the number of Pods, or the\\nnumber of ConfigMaps) that a tenant can create. Limits on object count ensure fairness and aim\\nto avoid noisy neighbor  issues from affecting other tenants that share a control plane.\\nResource quotas are namespaced objects. By mapping tenants to namespaces, cluster admins\\ncan use quotas to ensure that a tenant cannot monopolize a cluster's resources or overwhelm its\\ncontrol plane. Namespace management tools simplify the administration of quotas. In addition,\\nwhile Kubernetes quotas only apply within a single namespace, some namespace management\\ntools allow groups of namespaces to share quotas, giving administrators far more flexibility\\nwith less effort than built-in quotas.\\nQuotas prevent a single tenant from consuming greater than their allocated share of resources\\nhence minimizing the “noisy neighbor” issue, where one tenant negatively impacts the\\nperformance of other tenants' workloads.\\nWhen you apply a quota to namespace, Kubernetes requires you to also specify resource\\nrequests and limits for each container. Limits are the upper bound for the amount of resources\\nthat a container can consume. Containers that attempt to consume resources that exceed the\\nconfigured limits will either be throttled or killed, based on the resource type. When resource\\nrequests are set lower than limits, each container is guaranteed the requested amount but there\\nmay still be some potential for impact across workloads.\\nQuotas cannot protect against all kinds of resource sharing, such as network traffic. Node\\nisolation (described below) may be a better solution for this problem.\\nData Plane Isolation\\nData plane isolation ensures that pods and workloads for different tenants are sufficiently\\nisolated.\", metadata={'source': './PDFS/Concepts.pdf', 'page': 431}),\n",
       " Document(page_content='Network isolation\\nBy default, all pods in a Kubernetes cluster are allowed to communicate with each other, and all\\nnetwork traffic is unencrypted. This can lead to security vulnerabilities where traffic is\\naccidentally or maliciously sent to an unintended destination, or is intercepted by a workload\\non a compromised node.\\nPod-to-pod communication can be controlled using Network Policies , which restrict\\ncommunication between pods using namespace labels or IP address ranges. In a multi-tenant\\nenvironment where strict network isolation between tenants is required, starting with a default\\npolicy that denies communication between pods is recommended with another rule that allows\\nall pods to query the DNS server for name resolution. With such a default policy in place, you\\ncan begin adding more permissive rules that allow for communication within a namespace. It is\\nalso recommended not to use empty label selector \\'{}\\' for namespaceSelector field in network\\npolicy definition, in case traffic need to be allowed between namespaces. This scheme can be\\nfurther refined as required. Note that this only applies to pods within a single control plane;\\npods that belong to different virtual control planes cannot talk to each other via Kubernetes\\nnetworking.\\nNamespace management tools may simplify the creation of default or common network\\npolicies. In addition, some of these tools allow you to enforce a consistent set of namespace\\nlabels across your cluster, ensuring that they are a trusted basis for your policies.\\nWarning:  Network policies require a CNI plugin  that supports the implementation of network\\npolicies. Otherwise, NetworkPolicy resources will be ignored.\\nMore advanced network isolation may be provided by service meshes, which provide OSI Layer\\n7 policies based on workload identity, in addition to namespaces. These higher-level policies can\\nmake it easier to manage namespace-based multi-tenancy, especially when multiple\\nnamespaces are dedicated to a single tenant. They frequently also offer encryption using mutual\\nTLS, protecting your data even in the presence of a compromised node, and work across\\ndedicated or virtual clusters. However, they can be significantly more complex to manage and\\nmay not be appropriate for all users.\\nStorage isolation\\nKubernetes offers several types of volumes that can be used as persistent storage for workloads.\\nFor security and data-isolation, dynamic volume provisioning  is recommended and volume\\ntypes that use node resources should be avoided.\\nStorageClasses  allow you to describe custom \"classes\" of storage offered by your cluster, based\\non quality-of-service levels, backup policies, or custom policies determined by the cluster\\nadministrators.\\nPods can request storage using a PersistentVolumeClaim . A PersistentVolumeClaim is a\\nnamespaced resource, which enables isolating portions of the storage system and dedicating it\\nto tenants within the shared Kubernetes cluster. However, it is important to note that a\\nPersistentVolume is a cluster-wide resource and has a lifecycle independent of workloads and\\nnamespaces.\\nFor example, you can configure a separate StorageClass for each tenant and use this to\\nstrengthen isolation. If a StorageClass is shared, you should set a reclaim policy of Delete  to\\nensure that a PersistentVolume cannot be reused across different namespaces.', metadata={'source': './PDFS/Concepts.pdf', 'page': 432}),\n",
       " Document(page_content=\"Sandboxing containers\\nNote:  This section links to third party projects that provide functionality required by\\nKubernetes. The Kubernetes project authors aren't responsible for these projects, which are\\nlisted alphabetically. To add a project to this list, read the content guide  before submitting a\\nchange. More information.\\nKubernetes pods are composed of one or more containers that execute on worker nodes.\\nContainers utilize OS-level virtualization and hence offer a weaker isolation boundary than\\nvirtual machines that utilize hardware-based virtualization.\\nIn a shared environment, unpatched vulnerabilities in the application and system layers can be\\nexploited by attackers for container breakouts and remote code execution that allow access to\\nhost resources. In some applications, like a Content Management System (CMS), customers may\\nbe allowed the ability to upload and execute untrusted scripts or code. In either case,\\nmechanisms to further isolate and protect workloads using strong isolation are desirable.\\nSandboxing provides a way to isolate workloads running in a shared cluster. It typically\\ninvolves running each pod in a separate execution environment such as a virtual machine or a\\nuserspace kernel. Sandboxing is often recommended when you are running untrusted code,\\nwhere workloads are assumed to be malicious. Part of the reason this type of isolation is\\nnecessary is because containers are processes running on a shared kernel; they mount file\\nsystems like /sys and /proc  from the underlying host, making them less secure than an\\napplication that runs on a virtual machine which has its own kernel. While controls such as\\nseccomp, AppArmor, and SELinux can be used to strengthen the security of containers, it is\\nhard to apply a universal set of rules to all workloads running in a shared cluster. Running\\nworkloads in a sandbox environment helps to insulate the host from container escapes, where\\nan attacker exploits a vulnerability to gain access to the host system and all the processes/files\\nrunning on that host.\\nVirtual machines and userspace kernels are 2 popular approaches to sandboxing. The following\\nsandboxing implementations are available:\\ngVisor  intercepts syscalls from containers and runs them through a userspace kernel,\\nwritten in Go, with limited access to the underlying host.\\nKata Containers  is an OCI compliant runtime that allows you to run containers in a VM.\\nThe hardware virtualization available in Kata offers an added layer of security for\\ncontainers running untrusted code.\\nNode Isolation\\nNode isolation is another technique that you can use to isolate tenant workloads from each\\nother. With node isolation, a set of nodes is dedicated to running pods from a particular tenant\\nand co-mingling of tenant pods is prohibited. This configuration reduces the noisy tenant issue,\\nas all pods running on a node will belong to a single tenant. The risk of information disclosure\\nis slightly lower with node isolation because an attacker that manages to escape from a\\ncontainer will only have access to the containers and volumes mounted to that node.\\nAlthough workloads from different tenants are running on different nodes, it is important to be\\naware that the kubelet and (unless using virtual control planes) the API service are still shared\\nservices. A skilled attacker could use the permissions assigned to the kubelet or other pods\\nrunning on the node to move laterally within the cluster and gain access to tenant workloads\\nrunning on other nodes. If this is a major concern, consider implementing compensating• \\n•\", metadata={'source': './PDFS/Concepts.pdf', 'page': 433}),\n",
       " Document(page_content='controls such as seccomp, AppArmor or SELinux or explore using sandboxed containers or\\ncreating separate clusters for each tenant.\\nNode isolation is a little easier to reason about from a billing standpoint than sandboxing\\ncontainers since you can charge back per node rather than per pod. It also has fewer\\ncompatibility and performance issues and may be easier to implement than sandboxing\\ncontainers. For example, nodes for each tenant can be configured with taints so that only pods\\nwith the corresponding toleration can run on them. A mutating webhook could then be used to\\nautomatically add tolerations and node affinities to pods deployed into tenant namespaces so\\nthat they run on a specific set of nodes designated for that tenant.\\nNode isolation can be implemented using an pod node selectors  or a Virtual Kubelet .\\nAdditional Considerations\\nThis section discusses other Kubernetes constructs and patterns that are relevant for multi-\\ntenancy.\\nAPI Priority and Fairness\\nAPI priority and fairness  is a Kubernetes feature that allows you to assign a priority to certain\\npods running within the cluster. When an application calls the Kubernetes API, the API server\\nevaluates the priority assigned to pod. Calls from pods with higher priority are fulfilled before\\nthose with a lower priority. When contention is high, lower priority calls can be queued until\\nthe server is less busy or you can reject the requests.\\nUsing API priority and fairness will not be very common in SaaS environments unless you are\\nallowing customers to run applications that interface with the Kubernetes API, for example, a\\ncontroller.\\nQuality-of-Service (QoS)\\nWhen you’re running a SaaS application, you may want the ability to offer different Quality-of-\\nService (QoS) tiers of service to different tenants. For example, you may have freemium service\\nthat comes with fewer performance guarantees and features and a for-fee service tier with\\nspecific performance guarantees. Fortunately, there are several Kubernetes constructs that can\\nhelp you accomplish this within a shared cluster, including network QoS, storage classes, and\\npod priority and preemption. The idea with each of these is to provide tenants with the quality\\nof service that they paid for. Let’s start by looking at networking QoS.\\nTypically, all pods on a node share a network interface. Without network QoS, some pods may\\nconsume an unfair share of the available bandwidth at the expense of other pods. The\\nKubernetes bandwidth plugin  creates an extended resource  for networking that allows you to\\nuse Kubernetes resources constructs, i.e. requests/limits, to apply rate limits to pods by using\\nLinux tc queues. Be aware that the plugin is considered experimental as per the Network\\nPlugins  documentation and should be thoroughly tested before use in production\\nenvironments.\\nFor storage QoS, you will likely want to create different storage classes or profiles with different\\nperformance characteristics. Each storage profile can be associated with a different tier of\\nservice that is optimized for different workloads such IO, redundancy, or throughput.', metadata={'source': './PDFS/Concepts.pdf', 'page': 434}),\n",
       " Document(page_content=\"Additional logic might be necessary to allow the tenant to associate the appropriate storage\\nprofile with their workload.\\nFinally, there’s pod priority and preemption  where you can assign priority values to pods.\\nWhen scheduling pods, the scheduler will try evicting pods with lower priority when there are\\ninsufficient resources to schedule pods that are assigned a higher priority. If you have a use case\\nwhere tenants have different service tiers in a shared cluster e.g. free and paid, you may want to\\ngive higher priority to certain tiers using this feature.\\nDNS\\nKubernetes clusters include a Domain Name System (DNS) service to provide translations from\\nnames to IP addresses, for all Services and Pods. By default, the Kubernetes DNS service allows\\nlookups across all namespaces in the cluster.\\nIn multi-tenant environments where tenants can access pods and other Kubernetes resources,\\nor where stronger isolation is required, it may be necessary to prevent pods from looking up\\nservices in other Namespaces. You can restrict cross-namespace DNS lookups by configuring\\nsecurity rules for the DNS service. For example, CoreDNS (the default DNS service for\\nKubernetes) can leverage Kubernetes metadata to restrict queries to Pods and Services within a\\nnamespace. For more information, read an example  of configuring this within the CoreDNS\\ndocumentation.\\nWhen a Virtual Control Plane per tenant  model is used, a DNS service must be configured per\\ntenant or a multi-tenant DNS service must be used. Here is an example of a customized version\\nof CoreDNS  that supports multiple tenants.\\nOperators\\nOperators  are Kubernetes controllers that manage applications. Operators can simplify the\\nmanagement of multiple instances of an application, like a database service, which makes them\\na common building block in the multi-consumer (SaaS) multi-tenancy use case.\\nOperators used in a multi-tenant environment should follow a stricter set of guidelines.\\nSpecifically, the Operator should:\\nSupport creating resources within different tenant namespaces, rather than just in the\\nnamespace in which the Operator is deployed.\\nEnsure that the Pods are configured with resource requests and limits, to ensure\\nscheduling and fairness.\\nSupport configuration of Pods for data-plane isolation techniques such as node isolation\\nand sandboxed containers.\\nImplementations\\nNote:  This section links to third party projects that provide functionality required by\\nKubernetes. The Kubernetes project authors aren't responsible for these projects, which are\\nlisted alphabetically. To add a project to this list, read the content guide  before submitting a\\nchange. More information.• \\n• \\n•\", metadata={'source': './PDFS/Concepts.pdf', 'page': 435}),\n",
       " Document(page_content='There are two primary ways to share a Kubernetes cluster for multi-tenancy: using Namespaces\\n(that is, a Namespace per tenant) or by virtualizing the control plane (that is, virtual control\\nplane per tenant).\\nIn both cases, data plane isolation, and management of additional considerations such as API\\nPriority and Fairness, is also recommended.\\nNamespace isolation is well-supported by Kubernetes, has a negligible resource cost, and\\nprovides mechanisms to allow tenants to interact appropriately, such as by allowing service-to-\\nservice communication. However, it can be difficult to configure, and doesn\\'t apply to\\nKubernetes resources that can\\'t be namespaced, such as Custom Resource Definitions, Storage\\nClasses, and Webhooks.\\nControl plane virtualization allows for isolation of non-namespaced resources at the cost of\\nsomewhat higher resource usage and more difficult cross-tenant sharing. It is a good option\\nwhen namespace isolation is insufficient but dedicated clusters are undesirable, due to the high\\ncost of maintaining them (especially on-prem) or due to their higher overhead and lack of\\nresource sharing. However, even within a virtualized control plane, you will likely see benefits\\nby using namespaces as well.\\nThe two options are discussed in more detail in the following sections.\\nNamespace per tenant\\nAs previously mentioned, you should consider isolating each workload in its own namespace,\\neven if you are using dedicated clusters or virtualized control planes. This ensures that each\\nworkload only has access to its own resources, such as ConfigMaps and Secrets, and allows you\\nto tailor dedicated security policies for each workload. In addition, it is a best practice to give\\neach namespace names that are unique across your entire fleet (that is, even if they are in\\nseparate clusters), as this gives you the flexibility to switch between dedicated and shared\\nclusters in the future, or to use multi-cluster tooling such as service meshes.\\nConversely, there are also advantages to assigning namespaces at the tenant level, not just the\\nworkload level, since there are often policies that apply to all workloads owned by a single\\ntenant. However, this raises its own problems. Firstly, this makes it difficult or impossible to\\ncustomize policies to individual workloads, and secondly, it may be challenging to come up\\nwith a single level of \"tenancy\" that should be given a namespace. For example, an organization\\nmay have divisions, teams, and subteams - which should be assigned a namespace?\\nTo solve this, Kubernetes provides the Hierarchical Namespace Controller (HNC) , which allows\\nyou to organize your namespaces into hierarchies, and share certain policies and resources\\nbetween them. It also helps you manage namespace labels, namespace lifecycles, and delegated\\nmanagement, and share resource quotas across related namespaces. These capabilities can be\\nuseful in both multi-team and multi-customer scenarios.\\nOther projects that provide similar capabilities and aid in managing namespaced resources are\\nlisted below.\\nMulti-team tenancy\\nCapsule\\nKiosk• \\n•', metadata={'source': './PDFS/Concepts.pdf', 'page': 436}),\n",
       " Document(page_content='Multi-customer tenancy\\nKubeplus\\nPolicy engines\\nPolicy engines provide features to validate and generate tenant configurations:\\nKyverno\\nOPA/Gatekeeper\\nVirtual control plane per tenant\\nAnother form of control-plane isolation is to use Kubernetes extensions to provide each tenant\\na virtual control-plane that enables segmentation of cluster-wide API resources. Data plane\\nisolation  techniques can be used with this model to securely manage worker nodes across\\ntenants.\\nThe virtual control plane based multi-tenancy model extends namespace-based multi-tenancy\\nby providing each tenant with dedicated control plane components, and hence complete control\\nover cluster-wide resources and add-on services. Worker nodes are shared across all tenants,\\nand are managed by a Kubernetes cluster that is normally inaccessible to tenants. This cluster is\\noften referred to as a super-cluster  (or sometimes as a host-cluster ). Since a tenant’s control-\\nplane is not directly associated with underlying compute resources it is referred to as a virtual\\ncontrol plane .\\nA virtual control plane typically consists of the Kubernetes API server, the controller manager,\\nand the etcd data store. It interacts with the super cluster via a metadata synchronization\\ncontroller which coordinates changes across tenant control planes and the control plane of the\\nsuper-cluster.\\nBy using per-tenant dedicated control planes, most of the isolation problems due to sharing one\\nAPI server among all tenants are solved. Examples include noisy neighbors in the control plane,\\nuncontrollable blast radius of policy misconfigurations, and conflicts between cluster scope\\nobjects such as webhooks and CRDs. Hence, the virtual control plane model is particularly\\nsuitable for cases where each tenant requires access to a Kubernetes API server and expects the\\nfull cluster manageability.\\nThe improved isolation comes at the cost of running and maintaining an individual virtual\\ncontrol plane per tenant. In addition, per-tenant control planes do not solve isolation problems\\nin the data plane, such as node-level noisy neighbors or security threats. These must still be\\naddressed separately.\\nThe Kubernetes Cluster API - Nested (CAPN)  project provides an implementation of virtual\\ncontrol planes.\\nOther implementations\\nKamaji\\nvcluster• \\n• \\n• \\n• \\n•', metadata={'source': './PDFS/Concepts.pdf', 'page': 437}),\n",
       " Document(page_content=\"Hardening Guide - Authentication\\nMechanisms\\nInformation on authentication options in Kubernetes and their security properties.\\nSelecting the appropriate authentication mechanism(s) is a crucial aspect of securing your\\ncluster. Kubernetes provides several built-in mechanisms, each with its own strengths and\\nweaknesses that should be carefully considered when choosing the best authentication\\nmechanism for your cluster.\\nIn general, it is recommended to enable as few authentication mechanisms as possible to\\nsimplify user management and prevent cases where users retain access to a cluster that is no\\nlonger required.\\nIt is important to note that Kubernetes does not have an in-built user database within the\\ncluster. Instead, it takes user information from the configured authentication system and uses\\nthat to make authorization decisions. Therefore, to audit user access, you need to review\\ncredentials from every configured authentication source.\\nFor production clusters with multiple users directly accessing the Kubernetes API, it is\\nrecommended to use external authentication sources such as OIDC. The internal authentication\\nmechanisms, such as client certificates and service account tokens, described below, are not\\nsuitable for this use-case.\\nX.509 client certificate authentication\\nKubernetes leverages X.509 client certificate  authentication for system components, such as\\nwhen the Kubelet authenticates to the API Server. While this mechanism can also be used for\\nuser authentication, it might not be suitable for production use due to several restrictions:\\nClient certificates cannot be individually revoked. Once compromised, a certificate can be\\nused by an attacker until it expires. To mitigate this risk, it is recommended to configure\\nshort lifetimes for user authentication credentials created using client certificates.\\nIf a certificate needs to be invalidated, the certificate authority must be re-keyed, which\\ncan introduce availability risks to the cluster.\\nThere is no permanent record of client certificates created in the cluster. Therefore, all\\nissued certificates must be recorded if you need to keep track of them.\\nPrivate keys used for client certificate authentication cannot be password-protected.\\nAnyone who can read the file containing the key will be able to make use of it.\\nUsing client certificate authentication requires a direct connection from the client to the\\nAPI server with no intervening TLS termination points, which can complicate network\\narchitectures.\\nGroup data is embedded in the O value of the client certificate, which means the user's\\ngroup memberships cannot be changed for the lifetime of the certificate.• \\n• \\n• \\n• \\n• \\n•\", metadata={'source': './PDFS/Concepts.pdf', 'page': 438}),\n",
       " Document(page_content='Static token file\\nAlthough Kubernetes allows you to load credentials from a static token file  located on the\\ncontrol plane node disks, this approach is not recommended for production servers due to\\nseveral reasons:\\nCredentials are stored in clear text on control plane node disks, which can be a security\\nrisk.\\nChanging any credential requires a restart of the API server process to take effect, which\\ncan impact availability.\\nThere is no mechanism available to allow users to rotate their credentials. To rotate a\\ncredential, a cluster administrator must modify the token on disk and distribute it to the\\nusers.\\nThere is no lockout mechanism available to prevent brute-force attacks.\\nBootstrap tokens\\nBootstrap tokens  are used for joining nodes to clusters and are not recommended for user\\nauthentication due to several reasons:\\nThey have hard-coded group memberships that are not suitable for general use, making\\nthem unsuitable for authentication purposes.\\nManually generating bootstrap tokens can lead to weak tokens that can be guessed by an\\nattacker, which can be a security risk.\\nThere is no lockout mechanism available to prevent brute-force attacks, making it easier\\nfor attackers to guess or crack the token.\\nServiceAccount secret tokens\\nService account secrets  are available as an option to allow workloads running in the cluster to\\nauthenticate to the API server. In Kubernetes < 1.23, these were the default option, however,\\nthey are being replaced with TokenRequest API tokens. While these secrets could be used for\\nuser authentication, they are generally unsuitable for a number of reasons:\\nThey cannot be set with an expiry and will remain valid until the associated service\\naccount is deleted.\\nThe authentication tokens are visible to any cluster user who can read secrets in the\\nnamespace that they are defined in.\\nService accounts cannot be added to arbitrary groups complicating RBAC management\\nwhere they are used.\\nTokenRequest API tokens\\nThe TokenRequest API is a useful tool for generating short-lived credentials for service\\nauthentication to the API server or third-party systems. However, it is not generally\\nrecommended for user authentication as there is no revocation method available, and\\ndistributing credentials to users in a secure manner can be challenging.\\nWhen using TokenRequest tokens for service authentication, it is recommended to implement a\\nshort lifespan to reduce the impact of compromised tokens.• \\n• \\n• \\n• \\n• \\n• \\n• \\n• \\n• \\n•', metadata={'source': './PDFS/Concepts.pdf', 'page': 439}),\n",
       " Document(page_content='OpenID Connect token authentication\\nKubernetes supports integrating external authentication services with the Kubernetes API\\nusing OpenID Connect (OIDC) . There is a wide variety of software that can be used to integrate\\nKubernetes with an identity provider. However, when using OIDC authentication for\\nKubernetes, it is important to consider the following hardening measures:\\nThe software installed in the cluster to support OIDC authentication should be isolated\\nfrom general workloads as it will run with high privileges.\\nSome Kubernetes managed services are limited in the OIDC providers that can be used.\\nAs with TokenRequest tokens, OIDC tokens should have a short lifespan to reduce the\\nimpact of compromised tokens.\\nWebhook token authentication\\nWebhook token authentication  is another option for integrating external authentication\\nproviders into Kubernetes. This mechanism allows for an authentication service, either running\\ninside the cluster or externally, to be contacted for an authentication decision over a webhook.\\nIt is important to note that the suitability of this mechanism will likely depend on the software\\nused for the authentication service, and there are some Kubernetes-specific considerations to\\ntake into account.\\nTo configure Webhook authentication, access to control plane server filesystems is required.\\nThis means that it will not be possible with Managed Kubernetes unless the provider\\nspecifically makes it available. Additionally, any software installed in the cluster to support this\\naccess should be isolated from general workloads, as it will run with high privileges.\\nAuthenticating proxy\\nAnother option for integrating external authentication systems into Kubernetes is to use an \\nauthenticating proxy . With this mechanism, Kubernetes expects to receive requests from the\\nproxy with specific header values set, indicating the username and group memberships to\\nassign for authorization purposes. It is important to note that there are specific considerations\\nto take into account when using this mechanism.\\nFirstly, securely configured TLS must be used between the proxy and Kubernetes API server to\\nmitigate the risk of traffic interception or sniffing attacks. This ensures that the communication\\nbetween the proxy and Kubernetes API server is secure.\\nSecondly, it is important to be aware that an attacker who is able to modify the headers of the\\nrequest may be able to gain unauthorized access to Kubernetes resources. As such, it is\\nimportant to ensure that the headers are properly secured and cannot be tampered with.\\nKubernetes API Server Bypass Risks\\nSecurity architecture information relating to the API server and other components\\nThe Kubernetes API server is the main point of entry to a cluster for external parties (users and\\nservices) interacting with it.• \\n• \\n•', metadata={'source': './PDFS/Concepts.pdf', 'page': 440}),\n",
       " Document(page_content=\"As part of this role, the API server has several key built-in security controls, such as audit\\nlogging and admission controllers . However, there are ways to modify the configuration or\\ncontent of the cluster that bypass these controls.\\nThis page describes the ways in which the security controls built into the Kubernetes API\\nserver can be bypassed, so that cluster operators and security architects can ensure that these\\nbypasses are appropriately restricted.\\nStatic Pods\\nThe kubelet  on each node loads and directly manages any manifests that are stored in a named\\ndirectory or fetched from a specific URL as static Pods  in your cluster. The API server doesn't\\nmanage these static Pods. An attacker with write access to this location could modify the\\nconfiguration of static pods loaded from that source, or could introduce new static Pods.\\nStatic Pods are restricted from accessing other objects in the Kubernetes API. For example, you\\ncan't configure a static Pod to mount a Secret from the cluster. However, these Pods can take\\nother security sensitive actions, such as using hostPath  mounts from the underlying node.\\nBy default, the kubelet creates a mirror pod  so that the static Pods are visible in the Kubernetes\\nAPI. However, if the attacker uses an invalid namespace name when creating the Pod, it will not\\nbe visible in the Kubernetes API and can only be discovered by tooling that has access to the\\naffected host(s).\\nIf a static Pod fails admission control, the kubelet won't register the Pod with the API server.\\nHowever, the Pod still runs on the node. For more information, refer to kubeadm issue #1541 .\\nMitigations\\nOnly enable the kubelet static Pod manifest functionality  if required by the node.\\nIf a node uses the static Pod functionality, restrict filesystem access to the static Pod\\nmanifest directory or URL to users who need the access.\\nRestrict access to kubelet configuration parameters and files to prevent an attacker\\nsetting a static Pod path or URL.\\nRegularly audit and centrally report all access to directories or web storage locations that\\nhost static Pod manifests and kubelet configuration files.\\nThe kubelet API\\nThe kubelet provides an HTTP API that is typically exposed on TCP port 10250 on cluster\\nworker nodes. The API might also be exposed on control plane nodes depending on the\\nKubernetes distribution in use. Direct access to the API allows for disclosure of information\\nabout the pods running on a node, the logs from those pods, and execution of commands in\\nevery container running on the node.\\nWhen Kubernetes cluster users have RBAC access to Node  object sub-resources, that access\\nserves as authorization to interact with the kubelet API. The exact access depends on which\\nsub-resource access has been granted, as detailed in kubelet authorization .\\nDirect access to the kubelet API is not subject to admission control and is not logged by\\nKubernetes audit logging. An attacker with direct access to this API may be able to bypass\\ncontrols that detect or prevent certain actions.• \\n• \\n• \\n•\", metadata={'source': './PDFS/Concepts.pdf', 'page': 441}),\n",
       " Document(page_content='The kubelet API can be configured to authenticate requests in a number of ways. By default, the\\nkubelet configuration allows anonymous access. Most Kubernetes providers change the default\\nto use webhook and certificate authentication. This lets the control plane ensure that the caller\\nis authorized to access the nodes  API resource or sub-resources. The default anonymous access\\ndoesn\\'t make this assertion with the control plane.\\nMitigations\\nRestrict access to sub-resources of the nodes  API object using mechanisms such as RBAC .\\nOnly grant this access when required, such as by monitoring services.\\nRestrict access to the kubelet port. Only allow specified and trusted IP address ranges to\\naccess the port.\\nEnsure that kubelet authentication . is set to webhook or certificate mode.\\nEnsure that the unauthenticated \"read-only\" Kubelet port is not enabled on the cluster.\\nThe etcd API\\nKubernetes clusters use etcd as a datastore. The etcd service listens on TCP port 2379. The only\\nclients that need access are the Kubernetes API server and any backup tooling that you use.\\nDirect access to this API allows for disclosure or modification of any data held in the cluster.\\nAccess to the etcd API is typically managed by client certificate authentication. Any certificate\\nissued by a certificate authority that etcd trusts allows full access to the data stored inside etcd.\\nDirect access to etcd is not subject to Kubernetes admission control and is not logged by\\nKubernetes audit logging. An attacker who has read access to the API server\\'s etcd client\\ncertificate private key (or can create a new trusted client certificate) can gain cluster admin\\nrights by accessing cluster secrets or modifying access rules. Even without elevating their\\nKubernetes RBAC privileges, an attacker who can modify etcd can retrieve any API object or\\ncreate new workloads inside the cluster.\\nMany Kubernetes providers configure etcd to use mutual TLS (both client and server verify\\neach other\\'s certificate for authentication). There is no widely accepted implementation of\\nauthorization for the etcd API, although the feature exists. Since there is no authorization\\nmodel, any certificate with client access to etcd can be used to gain full access to etcd. Typically,\\netcd client certificates that are only used for health checking can also grant full read and write\\naccess.\\nMitigations\\nEnsure that the certificate authority trusted by etcd is used only for the purposes of\\nauthentication to that service.\\nControl access to the private key for the etcd server certificate, and to the API server\\'s\\nclient certificate and key.\\nConsider restricting access to the etcd port at a network level, to only allow access from\\nspecified and trusted IP address ranges.\\nContainer runtime socket\\nOn each node in a Kubernetes cluster, access to interact with containers is controlled by the\\ncontainer runtime (or runtimes, if you have configured more than one). Typically, the container• \\n• \\n• \\n• \\n• \\n• \\n•', metadata={'source': './PDFS/Concepts.pdf', 'page': 442}),\n",
       " Document(page_content='runtime exposes a Unix socket that the kubelet can access. An attacker with access to this\\nsocket can launch new containers or interact with running containers.\\nAt the cluster level, the impact of this access depends on whether the containers that run on the\\ncompromised node have access to Secrets or other confidential data that an attacker could use\\nto escalate privileges to other worker nodes or to control plane components.\\nMitigations\\nEnsure that you tightly control filesystem access to container runtime sockets. When\\npossible, restrict this access to the root user.\\nIsolate the kubelet from other components running on the node, using mechanisms such\\nas Linux kernel namespaces.\\nEnsure that you restrict or forbid the use of hostPath  mounts  that include the container\\nruntime socket, either directly or by mounting a parent directory. Also hostPath  mounts\\nmust be set as read-only to mitigate risks of attackers bypassing directory restrictions.\\nRestrict user access to nodes, and especially restrict superuser access to nodes.\\nSecurity Checklist\\nBaseline checklist for ensuring security in Kubernetes clusters.\\nThis checklist aims at providing a basic list of guidance with links to more comprehensive\\ndocumentation on each topic. It does not claim to be exhaustive and is meant to evolve.\\nOn how to read and use this document:\\nThe order of topics does not reflect an order of priority.\\nSome checklist items are detailed in the paragraph below the list of each section.\\nCaution:  Checklists are not sufficient for attaining a good security posture on their own. A\\ngood security posture requires constant attention and improvement, but a checklist can be the\\nfirst step on the never-ending journey towards security preparedness. Some of the\\nrecommendations in this checklist may be too restrictive or too lax for your specific security\\nneeds. Since Kubernetes security is not \"one size fits all\", each category of checklist items\\nshould be evaluated on its merits.\\nAuthentication & Authorization\\nsystem:masters  group is not used for user or component authentication after\\nbootstrapping.\\nThe kube-controller-manager is running with --use-service-account-credentials  enabled.\\nThe root certificate is protected (either an offline CA, or a managed online CA with\\neffective access controls).\\nIntermediate and leaf certificates have an expiry date no more than 3 years in the future.\\nA process exists for periodic access review, and reviews occur no more than 24 months\\napart.\\nThe Role Based Access Control Good Practices  is followed for guidance related to\\nauthentication and authorization.• \\n• \\n• \\n• \\n• \\n• \\n• \\n• \\n• \\n• \\n• \\n•', metadata={'source': './PDFS/Concepts.pdf', 'page': 443}),\n",
       " Document(page_content='After bootstrapping, neither users nor components should authenticate to the Kubernetes API\\nas system:masters . Similarly, running all of kube-controller-manager as system:masters  should\\nbe avoided. In fact, system:masters  should only be used as a break-glass mechanism, as opposed\\nto an admin user.\\nNetwork security\\nCNI plugins in-use supports network policies.\\nIngress and egress network policies are applied to all workloads in the cluster.\\nDefault network policies within each namespace, selecting all pods, denying everything,\\nare in place.\\nIf appropriate, a service mesh is used to encrypt all communications inside of the cluster.\\nThe Kubernetes API, kubelet API and etcd are not exposed publicly on Internet.\\nAccess from the workloads to the cloud metadata API is filtered.\\nUse of LoadBalancer and ExternalIPs is restricted.\\nA number of Container Network Interface (CNI) plugins  plugins provide the functionality to\\nrestrict network resources that pods may communicate with. This is most commonly done\\nthrough Network Policies  which provide a namespaced resource to define rules. Default\\nnetwork policies blocking everything egress and ingress, in each namespace, selecting all the\\npods, can be useful to adopt an allow list approach, ensuring that no workloads is missed.\\nNot all CNI plugins provide encryption in transit. If the chosen plugin lacks this feature, an\\nalternative solution could be to use a service mesh to provide that functionality.\\nThe etcd datastore of the control plane should have controls to limit access and not be publicly\\nexposed on the Internet. Furthermore, mutual TLS (mTLS) should be used to communicate\\nsecurely with it. The certificate authority for this should be unique to etcd.\\nExternal Internet access to the Kubernetes API server should be restricted to not expose the API\\npublicly. Be careful as many managed Kubernetes distribution are publicly exposing the API\\nserver by default. You can then use a bastion host to access the server.\\nThe kubelet  API access should be restricted and not publicly exposed, the defaults\\nauthentication and authorization settings, when no configuration file specified with the --config\\nflag, are overly permissive.\\nIf a cloud provider is used for hosting Kubernetes, the access from pods to the cloud metadata\\nAPI 169.254.169.254  should also be restricted or blocked if not needed because it may leak\\ninformation.\\nFor restricted LoadBalancer and ExternalIPs use, see CVE-2020-8554: Man in the middle using\\nLoadBalancer or ExternalIPs  and the DenyServiceExternalIPs admission controller  for further\\ninformation.\\nPod security\\nRBAC rights to create , update , patch , delete  workloads is only granted if necessary.\\nAppropriate Pod Security Standards policy is applied for all namespaces and enforced.\\nMemory limit is set for the workloads with a limit equal or inferior to the request.\\nCPU limit might be set on sensitive workloads.• \\n• \\n• \\n• \\n• \\n• \\n• \\n• \\n• \\n• \\n•', metadata={'source': './PDFS/Concepts.pdf', 'page': 444}),\n",
       " Document(page_content=\"For nodes that support it, Seccomp is enabled with appropriate syscalls profile for\\nprograms.\\nFor nodes that support it, AppArmor or SELinux is enabled with appropriate profile for\\nprograms.\\nRBAC authorization is crucial but cannot be granular enough to have authorization on the\\nPods' resources  (or on any resource that manages Pods). The only granularity is the API verbs\\non the resource itself, for example, create  on Pods. Without additional admission, the\\nauthorization to create these resources allows direct unrestricted access to the schedulable\\nnodes of a cluster.\\nThe Pod Security Standards  define three different policies, privileged, baseline and restricted\\nthat limit how fields can be set in the PodSpec  regarding security. These standards can be\\nenforced at the namespace level with the new Pod Security  admission, enabled by default, or by\\nthird-party admission webhook. Please note that, contrary to the removed PodSecurityPolicy\\nadmission it replaces, Pod Security  admission can be easily combined with admission webhooks\\nand external services.\\nPod Security admission restricted  policy, the most restrictive policy of the Pod Security\\nStandards  set, can operate in several modes , warn , audit  or enforce  to gradually apply the most\\nappropriate security context  according to security best practices. Nevertheless, pods' security\\ncontext  should be separately investigated to limit the privileges and access pods may have on\\ntop of the predefined security standards, for specific use cases.\\nFor a hands-on tutorial on Pod Security , see the blog post Kubernetes 1.23: Pod Security\\nGraduates to Beta .\\nMemory and CPU limits  should be set in order to restrict the memory and CPU resources a pod\\ncan consume on a node, and therefore prevent potential DoS attacks from malicious or\\nbreached workloads. Such policy can be enforced by an admission controller. Please note that\\nCPU limits will throttle usage and thus can have unintended effects on auto-scaling features or\\nefficiency i.e. running the process in best effort with the CPU resource available.\\nCaution:  Memory limit superior to request can expose the whole node to OOM issues.\\nEnabling Seccomp\\nSeccomp stands for secure computing mode and has been a feature of the Linux kernel since\\nversion 2.6.12. It can be used to sandbox the privileges of a process, restricting the calls it is able\\nto make from userspace into the kernel. Kubernetes lets you automatically apply seccomp\\nprofiles loaded onto a node to your Pods and containers.\\nSeccomp can improve the security of your workloads by reducing the Linux kernel syscall\\nattack surface available inside containers. The seccomp filter mode leverages BPF to create an\\nallow or deny list of specific syscalls, named profiles.\\nSince Kubernetes 1.27, you can enable the use of RuntimeDefault  as the default seccomp profile\\nfor all workloads. A security tutorial  is available on this topic. In addition, the Kubernetes\\nSecurity Profiles Operator  is a project that facilitates the management and use of seccomp in\\nclusters.\\nNote:  Seccomp is only available on Linux nodes.• \\n•\", metadata={'source': './PDFS/Concepts.pdf', 'page': 445}),\n",
       " Document(page_content=\"Enabling AppArmor or SELinux\\nAppArmor\\nAppArmor  is a Linux kernel security module that can provide an easy way to implement\\nMandatory Access Control (MAC) and better auditing through system logs. To enable\\nAppArmor in Kubernetes , at least version 1.4 is required. Like seccomp, AppArmor is also\\nconfigured through profiles, where each profile is either running in enforcing mode, which\\nblocks access to disallowed resources or complain mode, which only reports violations.\\nAppArmor profiles are enforced on a per-container basis, with an annotation, allowing for\\nprocesses to gain just the right privileges.\\nNote:  AppArmor is only available on Linux nodes, and enabled in some Linux distributions .\\nSELinux\\nSELinux  is also a Linux kernel security module that can provide a mechanism for supporting\\naccess control security policies, including Mandatory Access Controls (MAC). SELinux labels\\ncan be assigned to containers or pods via their securityContext  section .\\nNote:  SELinux is only available on Linux nodes, and enabled in some Linux distributions .\\nLogs and auditing\\nAudit logs, if enabled, are protected from general access.\\nThe /logs  API is disabled (you are running kube-apiserver with --enable-logs-\\nhandler=false ).\\nKubernetes includes a /logs  API endpoint, enabled by default, that lets users request the\\ncontents of the API server's /var/log  directory over HTTP. Accessing that endpoint\\nrequires authentication.\\nAllowing broad access to Kubernetes logs can make security information available to a\\npotential attacker.\\nAs a good practice, set up a separate means to collect and aggregate control plane logs, and do\\nnot use the /logs  API endpoint. Alternatively, if you run your control plane with the /logs  API\\nendpoint and limit the content of /var/log  (within the host or container where the API server is\\nrunning) to Kubernetes API server logs only.\\nPod placement\\nPod placement is done in accordance with the tiers of sensitivity of the application.\\nSensitive applications are running isolated on nodes or with specific sandboxed runtimes.\\nPods that are on different tiers of sensitivity, for example, an application pod and the\\nKubernetes API server, should be deployed onto separate nodes. The purpose of node isolation\\nis to prevent an application container breakout to directly providing access to applications with\\nhigher level of sensitivity to easily pivot within the cluster. This separation should be enforced• \\n• \\n• \\n•\", metadata={'source': './PDFS/Concepts.pdf', 'page': 446}),\n",
       " Document(page_content=\"to prevent pods accidentally being deployed onto the same node. This could be enforced with\\nthe following features:\\nNode Selectors\\nKey-value pairs, as part of the pod specification, that specify which nodes to deploy onto.\\nThese can be enforced at the namespace and cluster level with the PodNodeSelector\\nadmission controller.\\nPodTolerationRestriction\\nAn admission controller that allows administrators to restrict permitted tolerations\\nwithin a namespace. Pods within a namespace may only utilize the tolerations specified\\non the namespace object annotation keys that provide a set of default and allowed\\ntolerations.\\nRuntimeClass\\nRuntimeClass is a feature for selecting the container runtime configuration. The container\\nruntime configuration is used to run a Pod's containers and can provide more or less\\nisolation from the host at the cost of performance overhead.\\nSecrets\\nConfigMaps are not used to hold confidential data.\\nEncryption at rest is configured for the Secret API.\\nIf appropriate, a mechanism to inject secrets stored in third-party storage is deployed and\\navailable.\\nService account tokens are not mounted in pods that don't require them.\\nBound service account token volume  is in-use instead of non-expiring tokens.\\nSecrets required for pods should be stored within Kubernetes Secrets as opposed to alternatives\\nsuch as ConfigMap. Secret resources stored within etcd should be encrypted at rest .\\nPods needing secrets should have these automatically mounted through volumes, preferably\\nstored in memory like with the emptyDir.medium  option . Mechanism can be used to also inject\\nsecrets from third-party storages as volume, like the Secrets Store CSI Driver . This should be\\ndone preferentially as compared to providing the pods service account RBAC access to secrets.\\nThis would allow adding secrets into the pod as environment variables or files. Please note that\\nthe environment variable method might be more prone to leakage due to crash dumps in logs\\nand the non-confidential nature of environment variable in Linux, as opposed to the permission\\nmechanism on files.\\nService account tokens should not be mounted into pods that do not require them. This can be\\nconfigured by setting automountServiceAccountToken  to false either within the service account\\nto apply throughout the namespace or specifically for a pod. For Kubernetes v1.22 and above,\\nuse Bound Service Accounts  for time-bound service account credentials.\\nImages\\nMinimize unnecessary content in container images.\\nContainer images are configured to be run as unprivileged user.\\nReferences to container images are made by sha256 digests (rather than tags) or the\\nprovenance of the image is validated by verifying the image's digital signature at deploy\\ntime via admission control .\\nContainer images are regularly scanned during creation and in deployment, and known\\nvulnerable software is patched.• \\n• \\n• \\n• \\n• \\n• \\n• \\n• \\n•\", metadata={'source': './PDFS/Concepts.pdf', 'page': 447}),\n",
       " Document(page_content=\"Container image should contain the bare minimum to run the program they package.\\nPreferably, only the program and its dependencies, building the image from the minimal\\npossible base. In particular, image used in production should not contain shells or debugging\\nutilities, as an ephemeral debug container  can be used for troubleshooting.\\nBuild images to directly start with an unprivileged user by using the USER  instruction in\\nDockerfile . The Security Context  allows a container image to be started with a specific user and\\ngroup with runAsUser  and runAsGroup , even if not specified in the image manifest. However,\\nthe file permissions in the image layers might make it impossible to just start the process with a\\nnew unprivileged user without image modification.\\nAvoid using image tags to reference an image, especially the latest  tag, the image behind a tag\\ncan be easily modified in a registry. Prefer using the complete sha256  digest which is unique to\\nthe image manifest. This policy can be enforced via an ImagePolicyWebhook . Image signatures\\ncan also be automatically verified with an admission controller  at deploy time to validate their\\nauthenticity and integrity.\\nScanning a container image can prevent critical vulnerabilities from being deployed to the\\ncluster alongside the container image. Image scanning should be completed before deploying a\\ncontainer image to a cluster and is usually done as part of the deployment process in a CI/CD\\npipeline. The purpose of an image scan is to obtain information about possible vulnerabilities\\nand their prevention in the container image, such as a Common Vulnerability Scoring System\\n(CVSS)  score. If the result of the image scans is combined with the pipeline compliance rules,\\nonly properly patched container images will end up in Production.\\nAdmission controllers\\nAn appropriate selection of admission controllers is enabled.\\nA pod security policy is enforced by the Pod Security Admission or/and a webhook\\nadmission controller.\\nThe admission chain plugins and webhooks are securely configured.\\nAdmission controllers can help to improve the security of the cluster. However, they can\\npresent risks themselves as they extend the API server and should be properly secured .\\nThe following lists present a number of admission controllers that could be considered to\\nenhance the security posture of your cluster and application. It includes controllers that may be\\nreferenced in other parts of this document.\\nThis first group of admission controllers includes plugins enabled by default , consider to leave\\nthem enabled unless you know what you are doing:\\nCertificateApproval\\nPerforms additional authorization checks to ensure the approving user has permission to\\napprove certificate request.\\nCertificateSigning\\nPerforms additional authorization checks to ensure the signing user has permission to\\nsign certificate requests.\\nCertificateSubjectRestriction\\nRejects any certificate request that specifies a 'group' (or 'organization attribute') of \\nsystem:masters .\\nLimitRanger\\nEnforce the LimitRange API constraints.• \\n• \\n•\", metadata={'source': './PDFS/Concepts.pdf', 'page': 448}),\n",
       " Document(page_content='MutatingAdmissionWebhook\\nAllows the use of custom controllers through webhooks, these controllers may mutate\\nrequests that it reviews.\\nPodSecurity\\nReplacement for Pod Security Policy, restricts security contexts of deployed Pods.\\nResourceQuota\\nEnforces resource quotas to prevent over-usage of resources.\\nValidatingAdmissionWebhook\\nAllows the use of custom controllers through webhooks, these controllers do not mutate\\nrequests that it reviews.\\nThe second group includes plugin that are not enabled by default but in general availability\\nstate and recommended to improve your security posture:\\nDenyServiceExternalIPs\\nRejects all net-new usage of the Service.spec.externalIPs  field. This is a mitigation for \\nCVE-2020-8554: Man in the middle using LoadBalancer or ExternalIPs .\\nNodeRestriction\\nRestricts kubelet\\'s permissions to only modify the pods API resources they own or the\\nnode API ressource that represent themselves. It also prevents kubelet from using the \\nnode-restriction.kubernetes.io/  annotation, which can be used by an attacker with access\\nto the kubelet\\'s credentials to influence pod placement to the controlled node.\\nThe third group includes plugins that are not enabled by default but could be considered for\\ncertain use cases:\\nAlwaysPullImages\\nEnforces the usage of the latest version of a tagged image and ensures that the deployer\\nhas permissions to use the image.\\nImagePolicyWebhook\\nAllows enforcing additional controls for images through webhooks.\\nWhat\\'s next\\nRBAC Good Practices  for further information on authorization.\\nSecuring a Cluster  for information on protecting a cluster from accidental or malicious\\naccess.\\nCluster Multi-tenancy guide  for configuration options recommendations and best\\npractices on multi-tenancy.\\nBlog post \"A Closer Look at NSA/CISA Kubernetes Hardening Guidance\"  for\\ncomplementary resource on hardening Kubernetes clusters.\\nPolicies\\nManage security and best-practices with policies.\\nKubernetes policies are configurations that manage other configurations or runtime behaviors.\\nKubernetes offers various forms of policies, described below:• \\n• \\n• \\n•', metadata={'source': './PDFS/Concepts.pdf', 'page': 449}),\n",
       " Document(page_content='Apply policies using API objects\\nSome API objects act as policies. Here are some examples:\\nNetworkPolicies  can be used to restrict ingress and egress traffic for a workload.\\nLimitRanges  manage resource allocation constraints across different object kinds.\\nResourceQuotas  limit resource consumption for a namespace .\\nApply policies using admission controllers\\nAn admission controller  runs in the API server and can validate or mutate API requests. Some\\nadmission controllers act to apply policies. For example, the AlwaysPullImages  admission\\ncontroller modifies a new Pod to set the image pull policy to Always .\\nKubernetes has several built-in admission controllers that are configurable via the API server --\\nenable-admission-plugins  flag.\\nDetails on admission controllers, with the complete list of available admission controllers, are\\ndocumented in a dedicated section:\\nAdmission Controllers\\nApply policies using ValidatingAdmissionPolicy\\nValidating admission policies allow configurable validation checks to be executed in the API\\nserver using the Common Expression Language (CEL). For example, a \\nValidatingAdmissionPolicy  can be used to disallow use of the latest  image tag.\\nA ValidatingAdmissionPolicy  operates on an API request and can be used to block, audit, and\\nwarn users about non-compliant configurations.\\nDetails on the ValidatingAdmissionPolicy  API, with examples, are documented in a dedicated\\nsection:\\nValidating Admission Policy\\nApply policies using dynamic admission control\\nDynamic admission controllers (or admission webhooks) run outside the API server as separate\\napplications that register to receive webhooks requests to perform validation or mutation of\\nAPI requests.\\nDynamic admission controllers can be used to apply policies on API requests and trigger other\\npolicy-based workflows. A dynamic admission controller can perform complex checks\\nincluding those that require retrieval of other cluster resources and external data. For example,\\nan image verification check can lookup data from OCI registries to validate the container image\\nsignatures and attestations.\\nDetails on dynamic admission control are documented in a dedicated section:\\nDynamic Admission Control• \\n• \\n• \\n• \\n• \\n•', metadata={'source': './PDFS/Concepts.pdf', 'page': 450}),\n",
       " Document(page_content=\"Implementations\\nNote:  This section links to third party projects that provide functionality required by\\nKubernetes. The Kubernetes project authors aren't responsible for these projects, which are\\nlisted alphabetically. To add a project to this list, read the content guide  before submitting a\\nchange. More information.\\nDynamic Admission Controllers that act as flexible policy engines are being developed in the\\nKubernetes ecosystem, such as:\\nKubewarden\\nKyverno\\nOPA Gatekeeper\\nPolaris\\nApply policies using Kubelet configurations\\nKubernetes allows configuring the Kubelet on each worker node. Some Kubelet configurations\\nact as policies:\\nProcess ID limts and reservations  are used to limit and reserve allocatable PIDs.\\nNode Resource Managers  can manage compute, memory, and device resources for\\nlatency-critical and high-throughput workloads.\\nLimit Ranges\\nBy default, containers run with unbounded compute resources  on a Kubernetes cluster. Using\\nKubernetes resource quotas , administrators (also termed cluster operators ) can restrict\\nconsumption and creation of cluster resources (such as CPU time, memory, and persistent\\nstorage) within a specified namespace . Within a namespace, a Pod can consume as much CPU\\nand memory as is allowed by the ResourceQuotas that apply to that namespace. As a cluster\\noperator, or as a namespace-level administrator, you might also be concerned about making\\nsure that a single object cannot monopolize all available resources within a namespace.\\nA LimitRange is a policy to constrain the resource allocations (limits and requests) that you can\\nspecify for each applicable object kind (such as Pod or PersistentVolumeClaim ) in a namespace.\\nA LimitRange  provides constraints that can:\\nEnforce minimum and maximum compute resources usage per Pod or Container in a\\nnamespace.\\nEnforce minimum and maximum storage request per PersistentVolumeClaim  in a\\nnamespace.\\nEnforce a ratio between request and limit for a resource in a namespace.\\nSet default request/limit for compute resources in a namespace and automatically inject\\nthem to Containers at runtime.\\nA LimitRange is enforced in a particular namespace when there is a LimitRange object in that\\nnamespace.\\nThe name of a LimitRange object must be a valid DNS subdomain name .• \\n• \\n• \\n• \\n• \\n• \\n• \\n• \\n• \\n•\", metadata={'source': './PDFS/Concepts.pdf', 'page': 451}),\n",
       " Document(page_content='Constraints on resource limits and requests\\nThe administrator creates a LimitRange in a namespace.\\nUsers create (or try to create) objects in that namespace, such as Pods or\\nPersistentVolumeClaims.\\nFirst, the LimitRange  admission controller applies default request and limit values for all\\nPods (and their containers) that do not set compute resource requirements.\\nSecond, the LimitRange  tracks usage to ensure it does not exceed resource minimum,\\nmaximum and ratio defined in any LimitRange  present in the namespace.\\nIf you attempt to create or update an object (Pod or PersistentVolumeClaim) that violates\\na LimitRange  constraint, your request to the API server will fail with an HTTP status\\ncode 403 Forbidden  and a message explaining the constraint that has been violated.\\nIf you add a LimitRange  in a namespace that applies to compute-related resources such as \\ncpu and memory , you must specify requests or limits for those values. Otherwise, the\\nsystem may reject Pod creation.\\nLimitRange  validations occur only at Pod admission stage, not on running Pods. If you\\nadd or modify a LimitRange, the Pods that already exist in that namespace continue\\nunchanged.\\nIf two or more LimitRange  objects exist in the namespace, it is not deterministic which\\ndefault value will be applied.\\nLimitRange and admission checks for Pods\\nA LimitRange  does not check the consistency of the default values it applies. This means that a\\ndefault value for the limit  that is set by LimitRange  may be less than the request  value specified\\nfor the container in the spec that a client submits to the API server. If that happens, the final\\nPod will not be schedulable.\\nFor example, you define a LimitRange  with this manifest:\\nconcepts/policy/limit-range/problematic-limit-range.yaml  \\napiVersion : v1\\nkind: LimitRange\\nmetadata :\\n  name : cpu-resource-constraint\\nspec:\\n  limits :\\n  - default : # this section defines default limits\\n      cpu: 500m\\n    defaultRequest : # this section defines default requests\\n      cpu: 500m\\n    max: # max and min define the limit range\\n      cpu: \"1\"\\n    min:\\n      cpu: 100m\\n    type: Container\\nalong with a Pod that declares a CPU resource request of 700m , but not a limit:\\nconcepts/policy/limit-range/example-conflict-with-limitrange-cpu.yaml  • \\n• \\n• \\n• \\n• \\n• \\n• \\n•', metadata={'source': './PDFS/Concepts.pdf', 'page': 452}),\n",
       " Document(page_content='apiVersion : v1\\nkind: Pod\\nmetadata :\\n  name : example-conflict-with-limitrange-cpu\\nspec:\\n  containers :\\n  - name : demo\\n    image : registry.k8s.io/pause:2.0\\n    resources :\\n      requests :\\n        cpu: 700m\\nthen that Pod will not be scheduled, failing with an error similar to:\\nPod \"example-conflict-with-limitrange-cpu\" is invalid: spec.containers[0].resources.requests: \\nInvalid value: \"700m\": must be less than or equal to cpu limit\\nIf you set both request  and limit , then that new Pod will be scheduled successfully even with\\nthe same LimitRange  in place:\\nconcepts/policy/limit-range/example-no-conflict-with-limitrange-cpu.yaml  \\napiVersion : v1\\nkind: Pod\\nmetadata :\\n  name : example-no-conflict-with-limitrange-cpu\\nspec:\\n  containers :\\n  - name : demo\\n    image : registry.k8s.io/pause:2.0\\n    resources :\\n      requests :\\n        cpu: 700m\\n      limits :\\n        cpu: 700m\\nExample resource constraints\\nExamples of policies that could be created using LimitRange  are:\\nIn a 2 node cluster with a capacity of 8 GiB RAM and 16 cores, constrain Pods in a\\nnamespace to request 100m of CPU with a max limit of 500m for CPU and request 200Mi\\nfor Memory with a max limit of 600Mi for Memory.\\nDefine default CPU limit and request to 150m and memory default request to 300Mi for\\nContainers started with no cpu and memory requests in their specs.\\nIn the case where the total limits of the namespace is less than the sum of the limits of the\\nPods/Containers, there may be contention for resources. In this case, the Containers or Pods\\nwill not be created.\\nNeither contention nor changes to a LimitRange will affect already created resources.• \\n•', metadata={'source': './PDFS/Concepts.pdf', 'page': 453}),\n",
       " Document(page_content=\"What's next\\nFor examples on using limits, see:\\nhow to configure minimum and maximum CPU constraints per namespace .\\nhow to configure minimum and maximum Memory constraints per namespace .\\nhow to configure default CPU Requests and Limits per namespace .\\nhow to configure default Memory Requests and Limits per namespace .\\nhow to configure minimum and maximum Storage consumption per namespace .\\na detailed example on configuring quota per namespace .\\nRefer to the LimitRanger design document  for context and historical information.\\nResource Quotas\\nWhen several users or teams share a cluster with a fixed number of nodes, there is a concern\\nthat one team could use more than its fair share of resources.\\nResource quotas are a tool for administrators to address this concern.\\nA resource quota, defined by a ResourceQuota  object, provides constraints that limit aggregate\\nresource consumption per namespace. It can limit the quantity of objects that can be created in\\na namespace by type, as well as the total amount of compute resources that may be consumed\\nby resources in that namespace.\\nResource quotas work like this:\\nDifferent teams work in different namespaces. This can be enforced with RBAC .\\nThe administrator creates one ResourceQuota for each namespace.\\nUsers create resources (pods, services, etc.) in the namespace, and the quota system tracks\\nusage to ensure it does not exceed hard resource limits defined in a ResourceQuota.\\nIf creating or updating a resource violates a quota constraint, the request will fail with\\nHTTP status code 403 FORBIDDEN  with a message explaining the constraint that would\\nhave been violated.\\nIf quota is enabled in a namespace for compute resources like cpu and memory , users\\nmust specify requests or limits for those values; otherwise, the quota system may reject\\npod creation. Hint: Use the LimitRanger  admission controller to force defaults for pods\\nthat make no compute resource requirements.\\nSee the walkthrough  for an example of how to avoid this problem.\\nNote:\\nFor cpu and memory  resources, ResourceQuotas enforce that every  (new) pod in that\\nnamespace sets a limit for that resource. If you enforce a resource quota in a namespace\\nfor either cpu or memory , you, and other clients, must  specify either requests  or limits\\nfor that resource, for every new Pod you submit. If you don't, the control plane may reject\\nadmission for that Pod.• \\n• \\n• \\n• \\n• \\n• \\n• \\n• \\n• \\n• \\n• \\n•\", metadata={'source': './PDFS/Concepts.pdf', 'page': 454}),\n",
       " Document(page_content='For other resources: ResourceQuota works and will ignore pods in the namespace\\nwithout setting a limit or request for that resource. It means that you can create a new\\npod without limit/request ephemeral storage if the resource quota limits the ephemeral\\nstorage of this namespace. You can use a LimitRange  to automatically set a default\\nrequest for these resources.\\nThe name of a ResourceQuota object must be a valid DNS subdomain name .\\nExamples of policies that could be created using namespaces and quotas are:\\nIn a cluster with a capacity of 32 GiB RAM, and 16 cores, let team A use 20 GiB and 10\\ncores, let B use 10GiB and 4 cores, and hold 2GiB and 2 cores in reserve for future\\nallocation.\\nLimit the \"testing\" namespace to using 1 core and 1GiB RAM. Let the \"production\"\\nnamespace use any amount.\\nIn the case where the total capacity of the cluster is less than the sum of the quotas of the\\nnamespaces, there may be contention for resources. This is handled on a first-come-first-served\\nbasis.\\nNeither contention nor changes to quota will affect already created resources.\\nEnabling Resource Quota\\nResource Quota support is enabled by default for many Kubernetes distributions. It is enabled\\nwhen the API server  --enable-admission-plugins=  flag has ResourceQuota  as one of its\\narguments.\\nA resource quota is enforced in a particular namespace when there is a ResourceQuota in that\\nnamespace.\\nCompute Resource Quota\\nYou can limit the total sum of compute resources  that can be requested in a given namespace.\\nThe following resource types are supported:\\nResource Name Description\\nlimits.cpuAcross all pods in a non-terminal state, the sum of CPU limits cannot exceed\\nthis value.\\nlimits.memoryAcross all pods in a non-terminal state, the sum of memory limits cannot\\nexceed this value.\\nrequests.cpuAcross all pods in a non-terminal state, the sum of CPU requests cannot\\nexceed this value.\\nrequests.memoryAcross all pods in a non-terminal state, the sum of memory requests cannot\\nexceed this value.\\nhugepages-\\n<size>Across all pods in a non-terminal state, the number of huge page requests of\\nthe specified size cannot exceed this value.\\ncpu Same as requests.cpu\\nmemory Same as requests.memory• \\n• \\n•', metadata={'source': './PDFS/Concepts.pdf', 'page': 455}),\n",
       " Document(page_content='Resource Quota For Extended Resources\\nIn addition to the resources mentioned above, in release 1.10, quota support for extended\\nresources  is added.\\nAs overcommit is not allowed for extended resources, it makes no sense to specify both \\nrequests  and limits  for the same extended resource in a quota. So for extended resources, only\\nquota items with prefix requests.  is allowed for now.\\nTake the GPU resource as an example, if the resource name is nvidia.com/gpu , and you want to\\nlimit the total number of GPUs requested in a namespace to 4, you can define a quota as\\nfollows:\\nrequests.nvidia.com/gpu: 4\\nSee Viewing and Setting Quotas  for more detail information.\\nStorage Resource Quota\\nYou can limit the total sum of storage resources  that can be requested in a given namespace.\\nIn addition, you can limit consumption of storage resources based on associated storage-class.\\nResource Name Description\\nrequests.storageAcross all persistent volume claims, the sum of\\nstorage requests cannot exceed this value.\\npersistentvolumeclaimsThe total number of PersistentVolumeClaims  that\\ncan exist in the namespace.\\n<storage-class-\\nname>.storageclass.storage.k8s.io/\\nrequests.storageAcross all persistent volume claims associated with\\nthe <storage-class-name> , the sum of storage\\nrequests cannot exceed this value.\\n<storage-class-\\nname>.storageclass.storage.k8s.io/\\npersistentvolumeclaimsAcross all persistent volume claims associated with\\nthe <storage-class-name> , the total number of \\npersistent volume claims  that can exist in the\\nnamespace.\\nFor example, if an operator wants to quota storage with gold storage class separate from bronze\\nstorage class, the operator can define a quota as follows:\\ngold.storageclass.storage.k8s.io/requests.storage: 500Gi\\nbronze.storageclass.storage.k8s.io/requests.storage: 100Gi\\nIn release 1.8, quota support for local ephemeral storage is added as an alpha feature:\\nResource Name Description\\nrequests.ephemeral-\\nstorageAcross all pods in the namespace, the sum of local ephemeral storage\\nrequests cannot exceed this value.\\nlimits.ephemeral-\\nstorageAcross all pods in the namespace, the sum of local ephemeral storage\\nlimits cannot exceed this value.\\nephemeral-storage Same as requests.ephemeral-storage .• \\n• \\n•', metadata={'source': './PDFS/Concepts.pdf', 'page': 456}),\n",
       " Document(page_content='Note:  When using a CRI container runtime, container logs will count against the ephemeral\\nstorage quota. This can result in the unexpected eviction of pods that have exhausted their\\nstorage quotas. Refer to Logging Architecture  for details.\\nObject Count Quota\\nYou can set quota for the total number of certain resources of all standard, namespaced resource\\ntypes using the following syntax:\\ncount/<resource>.<group>  for resources from non-core groups\\ncount/<resource>  for resources from the core group\\nHere is an example set of resources users may want to put under object count quota:\\ncount/persistentvolumeclaims\\ncount/services\\ncount/secrets\\ncount/configmaps\\ncount/replicationcontrollers\\ncount/deployments.apps\\ncount/replicasets.apps\\ncount/statefulsets.apps\\ncount/jobs.batch\\ncount/cronjobs.batch\\nThe same syntax can be used for custom resources. For example, to create a quota on a widgets\\ncustom resource in the example.com  API group, use count/widgets.example.com .\\nWhen using count/*  resource quota, an object is charged against the quota if it exists in server\\nstorage. These types of quotas are useful to protect against exhaustion of storage resources. For\\nexample, you may want to limit the number of Secrets in a server given their large size. Too\\nmany Secrets in a cluster can actually prevent servers and controllers from starting. You can set\\na quota for Jobs to protect against a poorly configured CronJob. CronJobs that create too many\\nJobs in a namespace can lead to a denial of service.\\nIt is also possible to do generic object count quota on a limited set of resources. The following\\ntypes are supported:\\nResource Name Description\\nconfigmaps The total number of ConfigMaps that can exist in the namespace.\\npersistentvolumeclaimsThe total number of PersistentVolumeClaims  that can exist in the\\nnamespace.\\npodsThe total number of Pods in a non-terminal state that can exist in the\\nnamespace. A pod is in a terminal state if .status.phase in (Failed, \\nSucceeded)  is true.\\nreplicationcontrollersThe total number of ReplicationControllers that can exist in the\\nnamespace.\\nresourcequotas The total number of ResourceQuotas that can exist in the namespace.\\nservices The total number of Services that can exist in the namespace.\\nservices.loadbalancersThe total number of Services of type LoadBalancer  that can exist in the\\nnamespace.• \\n• \\n• \\n• \\n• \\n• \\n• \\n• \\n• \\n• \\n• \\n•', metadata={'source': './PDFS/Concepts.pdf', 'page': 457}),\n",
       " Document(page_content=\"Resource Name Description\\nservices.nodeportsThe total number of Services of type NodePort  that can exist in the\\nnamespace.\\nsecrets The total number of Secrets that can exist in the namespace.\\nFor example, pods  quota counts and enforces a maximum on the number of pods  created in a\\nsingle namespace that are not terminal. You might want to set a pods  quota on a namespace to\\navoid the case where a user creates many small pods and exhausts the cluster's supply of Pod\\nIPs.\\nQuota Scopes\\nEach quota can have an associated set of scopes . A quota will only measure usage for a resource\\nif it matches the intersection of enumerated scopes.\\nWhen a scope is added to the quota, it limits the number of resources it supports to those that\\npertain to the scope. Resources specified on the quota outside of the allowed set results in a\\nvalidation error.\\nScope Description\\nTerminating Match pods where .spec.activeDeadlineSeconds >= 0\\nNotTerminating Match pods where .spec.activeDeadlineSeconds is nil\\nBestEffort Match pods that have best effort quality of service.\\nNotBestEffort Match pods that do not have best effort quality of service.\\nPriorityClass Match pods that references the specified priority class .\\nCrossNamespacePodAffinity Match pods that have cross-namespace pod (anti)affinity terms .\\nThe BestEffort  scope restricts a quota to tracking the following resource:\\npods\\nThe Terminating , NotTerminating , NotBestEffort  and PriorityClass  scopes restrict a quota to\\ntracking the following resources:\\npods\\ncpu\\nmemory\\nrequests.cpu\\nrequests.memory\\nlimits.cpu\\nlimits.memory\\nNote that you cannot specify both the Terminating  and the NotTerminating  scopes in the same\\nquota, and you cannot specify both the BestEffort  and NotBestEffort  scopes in the same quota\\neither.\\nThe scopeSelector  supports the following values in the operator  field:\\nIn\\nNotIn\\nExists\\nDoesNotExist• \\n• \\n• \\n• \\n• \\n• \\n• \\n• \\n• \\n• \\n• \\n•\", metadata={'source': './PDFS/Concepts.pdf', 'page': 458}),\n",
       " Document(page_content='When using one of the following values as the scopeName  when defining the scopeSelector ,\\nthe operator  must be Exists .\\nTerminating\\nNotTerminating\\nBestEffort\\nNotBestEffort\\nIf the operator  is In or NotIn , the values  field must have at least one value. For example:\\n  scopeSelector :\\n    matchExpressions :\\n      - scopeName : PriorityClass\\n        operator : In\\n        values :\\n          - middle\\nIf the operator  is Exists  or DoesNotExist , the values  field must NOT be specified.\\nResource Quota Per PriorityClass\\nFEATURE STATE:  Kubernetes v1.17 [stable]\\nPods can be created at a specific priority . You can control a pod\\'s consumption of system\\nresources based on a pod\\'s priority, by using the scopeSelector  field in the quota spec.\\nA quota is matched and consumed only if scopeSelector  in the quota spec selects the pod.\\nWhen quota is scoped for priority class using scopeSelector  field, quota object is restricted to\\ntrack only following resources:\\npods\\ncpu\\nmemory\\nephemeral-storage\\nlimits.cpu\\nlimits.memory\\nlimits.ephemeral-storage\\nrequests.cpu\\nrequests.memory\\nrequests.ephemeral-storage\\nThis example creates a quota object and matches it with pods at specific priorities. The example\\nworks as follows:\\nPods in the cluster have one of the three priority classes, \"low\", \"medium\", \"high\".\\nOne quota object is created for each priority.\\nSave the following YAML to a file quota.yml .\\napiVersion : v1\\nkind: List\\nitems :\\n- apiVersion : v1• \\n• \\n• \\n• \\n• \\n• \\n• \\n• \\n• \\n• \\n• \\n• \\n• \\n• \\n• \\n•', metadata={'source': './PDFS/Concepts.pdf', 'page': 459}),\n",
       " Document(page_content='kind: ResourceQuota\\n  metadata :\\n    name : pods-high\\n  spec:\\n    hard:\\n      cpu: \"1000\"\\n      memory : 200Gi\\n      pods : \"10\"\\n    scopeSelector :\\n      matchExpressions :\\n      - operator : In\\n        scopeName : PriorityClass\\n        values : [\"high\" ]\\n- apiVersion : v1\\n  kind: ResourceQuota\\n  metadata :\\n    name : pods-medium\\n  spec:\\n    hard:\\n      cpu: \"10\"\\n      memory : 20Gi\\n      pods : \"10\"\\n    scopeSelector :\\n      matchExpressions :\\n      - operator : In\\n        scopeName : PriorityClass\\n        values : [\"medium\" ]\\n- apiVersion : v1\\n  kind: ResourceQuota\\n  metadata :\\n    name : pods-low\\n  spec:\\n    hard:\\n      cpu: \"5\"\\n      memory : 10Gi\\n      pods : \"10\"\\n    scopeSelector :\\n      matchExpressions :\\n      - operator : In\\n        scopeName : PriorityClass\\n        values : [\"low\" ]\\nApply the YAML using kubectl create .\\nkubectl create -f ./quota.yml\\nresourcequota/pods-high created\\nresourcequota/pods-medium created\\nresourcequota/pods-low created\\nVerify that Used  quota is 0 using kubectl describe quota .\\nkubectl describe quota', metadata={'source': './PDFS/Concepts.pdf', 'page': 460}),\n",
       " Document(page_content='Name:       pods-high\\nNamespace:  default\\nResource    Used  Hard\\n--------    ----  ----\\ncpu         0     1k\\nmemory      0     200Gi\\npods        0     10\\nName:       pods-low\\nNamespace:  default\\nResource    Used  Hard\\n--------    ----  ----\\ncpu         0     5\\nmemory      0     10Gi\\npods        0     10\\nName:       pods-medium\\nNamespace:  default\\nResource    Used  Hard\\n--------    ----  ----\\ncpu         0     10\\nmemory      0     20Gi\\npods        0     10\\nCreate a pod with priority \"high\". Save the following YAML to a file high-priority-pod.yml .\\napiVersion : v1\\nkind: Pod\\nmetadata :\\n  name : high-priority\\nspec:\\n  containers :\\n  - name : high-priority\\n    image : ubuntu\\n    command : [\"/bin/sh\" ]\\n    args: [\"-c\", \"while true; do echo hello; sleep 10;done\" ]\\n    resources :\\n      requests :\\n        memory : \"10Gi\"\\n        cpu: \"500m\"\\n      limits :\\n        memory : \"10Gi\"\\n        cpu: \"500m\"\\n  priorityClassName : high\\nApply it with kubectl create .\\nkubectl create -f ./high-priority-pod.yml\\nVerify that \"Used\" stats for \"high\" priority quota, pods-high , has changed and that the other two\\nquotas are unchanged.', metadata={'source': './PDFS/Concepts.pdf', 'page': 461}),\n",
       " Document(page_content='kubectl describe quota\\nName:       pods-high\\nNamespace:  default\\nResource    Used  Hard\\n--------    ----  ----\\ncpu         500m  1k\\nmemory      10Gi  200Gi\\npods        1     10\\nName:       pods-low\\nNamespace:  default\\nResource    Used  Hard\\n--------    ----  ----\\ncpu         0     5\\nmemory      0     10Gi\\npods        0     10\\nName:       pods-medium\\nNamespace:  default\\nResource    Used  Hard\\n--------    ----  ----\\ncpu         0     10\\nmemory      0     20Gi\\npods        0     10\\nCross-namespace Pod Affinity Quota\\nFEATURE STATE:  Kubernetes v1.24 [stable]\\nOperators can use CrossNamespacePodAffinity  quota scope to limit which namespaces are\\nallowed to have pods with affinity terms that cross namespaces. Specifically, it controls which\\npods are allowed to set namespaces  or namespaceSelector  fields in pod affinity terms.\\nPreventing users from using cross-namespace affinity terms might be desired since a pod with\\nanti-affinity constraints can block pods from all other namespaces from getting scheduled in a\\nfailure domain.\\nUsing this scope operators can prevent certain namespaces ( foo-ns  in the example below) from\\nhaving pods that use cross-namespace pod affinity by creating a resource quota object in that\\nnamespace with CrossNamespacePodAffinity  scope and hard limit of 0:\\napiVersion : v1\\nkind: ResourceQuota\\nmetadata :\\n  name : disable-cross-namespace-affinity\\n  namespace : foo-ns\\nspec:\\n  hard:\\n    pods : \"0\"\\n  scopeSelector :', metadata={'source': './PDFS/Concepts.pdf', 'page': 462}),\n",
       " Document(page_content='matchExpressions :\\n    - scopeName : CrossNamespacePodAffinity\\n      operator : Exists\\nIf operators want to disallow using namespaces  and namespaceSelector  by default, and only\\nallow it for specific namespaces, they could configure CrossNamespacePodAffinity  as a limited\\nresource by setting the kube-apiserver flag --admission-control-config-file to the path of the\\nfollowing configuration file:\\napiVersion : apiserver.config.k8s.io/v1\\nkind: AdmissionConfiguration\\nplugins :\\n- name : \"ResourceQuota\"\\n  configuration :\\n    apiVersion : apiserver.config.k8s.io/v1\\n    kind: ResourceQuotaConfiguration\\n    limitedResources :\\n    - resource : pods\\n      matchScopes :\\n      - scopeName : CrossNamespacePodAffinity\\n        operator : Exists\\nWith the above configuration, pods can use namespaces  and namespaceSelector  in pod affinity\\nonly if the namespace where they are created have a resource quota object with \\nCrossNamespacePodAffinity  scope and a hard limit greater than or equal to the number of pods\\nusing those fields.\\nRequests compared to Limits\\nWhen allocating compute resources, each container may specify a request and a limit value for\\neither CPU or memory. The quota can be configured to quota either value.\\nIf the quota has a value specified for requests.cpu  or requests.memory , then it requires that\\nevery incoming container makes an explicit request for those resources. If the quota has a value\\nspecified for limits.cpu  or limits.memory , then it requires that every incoming container\\nspecifies an explicit limit for those resources.\\nViewing and Setting Quotas\\nKubectl supports creating, updating, and viewing quotas:\\nkubectl create namespace myspace\\ncat <<EOF > compute-resources.yaml\\napiVersion: v1\\nkind: ResourceQuota\\nmetadata:\\n  name: compute-resources\\nspec:\\n  hard:\\n    requests.cpu: \"1\"\\n    requests.memory: 1Gi', metadata={'source': './PDFS/Concepts.pdf', 'page': 463}),\n",
       " Document(page_content='limits.cpu: \"2\"\\n    limits.memory: 2Gi\\n    requests.nvidia.com/gpu: 4\\nEOF\\nkubectl create -f ./compute-resources.yaml --namespace =myspace\\ncat <<EOF > object-counts.yaml\\napiVersion: v1\\nkind: ResourceQuota\\nmetadata:\\n  name: object-counts\\nspec:\\n  hard:\\n    configmaps: \"10\"\\n    persistentvolumeclaims: \"4\"\\n    pods: \"4\"\\n    replicationcontrollers: \"20\"\\n    secrets: \"10\"\\n    services: \"10\"\\n    services.loadbalancers: \"2\"\\nEOF\\nkubectl create -f ./object-counts.yaml --namespace =myspace\\nkubectl get quota --namespace =myspace\\nNAME                    AGE\\ncompute-resources       30s\\nobject-counts           32s\\nkubectl describe quota compute-resources --namespace =myspace\\nName:                    compute-resources\\nNamespace:               myspace\\nResource                 Used  Hard\\n--------                 ----  ----\\nlimits.cpu               0     2\\nlimits.memory            0     2Gi\\nrequests.cpu             0     1\\nrequests.memory          0     1Gi\\nrequests.nvidia.com/gpu  0     4\\nkubectl describe quota object-counts --namespace =myspace\\nName:                   object-counts\\nNamespace:              myspace\\nResource                Used    Hard\\n--------                ----    ----\\nconfigmaps              0       10\\npersistentvolumeclaims  0       4\\npods                    0       4\\nreplicationcontrollers  0       20\\nsecrets                 1       10', metadata={'source': './PDFS/Concepts.pdf', 'page': 464}),\n",
       " Document(page_content='services                0       10\\nservices.loadbalancers  0       2\\nKubectl also supports object count quota for all standard namespaced resources using the\\nsyntax count/<resource>.<group> :\\nkubectl create namespace myspace\\nkubectl create quota test --hard =count/deployments.apps =2,count/replicasets.apps =4,count/\\npods =3,count/secrets =4 --namespace =myspace\\nkubectl create deployment nginx --image =nginx --namespace =myspace --replicas =2\\nkubectl describe quota --namespace =myspace\\nName:                         test\\nNamespace:                    myspace\\nResource                      Used  Hard\\n--------                      ----  ----\\ncount/deployments.apps        1     2\\ncount/pods                    2     3\\ncount/replicasets.apps        1     4\\ncount/secrets                 1     4\\nQuota and Cluster Capacity\\nResourceQuotas are independent of the cluster capacity. They are expressed in absolute units.\\nSo, if you add nodes to your cluster, this does not automatically give each namespace the ability\\nto consume more resources.\\nSometimes more complex policies may be desired, such as:\\nProportionally divide total cluster resources among several teams.\\nAllow each tenant to grow resource usage as needed, but have a generous limit to prevent\\naccidental resource exhaustion.\\nDetect demand from one namespace, add nodes, and increase quota.\\nSuch policies could be implemented using ResourceQuotas  as building blocks, by writing a\\n\"controller\" that watches the quota usage and adjusts the quota hard limits of each namespace\\naccording to other signals.\\nNote that resource quota divides up aggregate cluster resources, but it creates no restrictions\\naround nodes: pods from several namespaces may run on the same node.\\nLimit Priority Class consumption by default\\nIt may be desired that pods at a particular priority, eg. \"cluster-services\", should be allowed in a\\nnamespace, if and only if, a matching quota object exists.\\nWith this mechanism, operators are able to restrict usage of certain high priority classes to a\\nlimited number of namespaces and not every namespace will be able to consume these priority\\nclasses by default.• \\n• \\n•', metadata={'source': './PDFS/Concepts.pdf', 'page': 465}),\n",
       " Document(page_content='To enforce this, kube-apiserver  flag --admission-control-config-file  should be used to pass path\\nto the following configuration file:\\napiVersion : apiserver.config.k8s.io/v1\\nkind: AdmissionConfiguration\\nplugins :\\n- name : \"ResourceQuota\"\\n  configuration :\\n    apiVersion : apiserver.config.k8s.io/v1\\n    kind: ResourceQuotaConfiguration\\n    limitedResources :\\n    - resource : pods\\n      matchScopes :\\n      - scopeName : PriorityClass\\n        operator : In\\n        values : [\"cluster-services\" ]\\nThen, create a resource quota object in the kube-system  namespace:\\npolicy/priority-class-resourcequota.yaml  \\napiVersion : v1\\nkind: ResourceQuota\\nmetadata :\\n  name : pods-cluster-services\\nspec:\\n  scopeSelector :\\n    matchExpressions :\\n      - operator : In\\n        scopeName : PriorityClass\\n        values : [\"cluster-services\" ]\\nkubectl apply -f https://k8s.io/examples/policy/priority-class-resourcequota.yaml -n kube-\\nsystem\\nresourcequota/pods-cluster-services created\\nIn this case, a pod creation will be allowed if:\\nthe Pod\\'s priorityClassName  is not specified.\\nthe Pod\\'s priorityClassName  is specified to a value other than cluster-services .\\nthe Pod\\'s priorityClassName  is set to cluster-services , it is to be created in the kube-\\nsystem  namespace, and it has passed the resource quota check.\\nA Pod creation request is rejected if its priorityClassName  is set to cluster-services  and it is to\\nbe created in a namespace other than kube-system .\\nWhat\\'s next\\nSee ResourceQuota design doc  for more information.\\nSee a detailed example for how to use resource quota .\\nRead Quota support for priority class design doc .\\nSee LimitedResources1. \\n2. \\n3. \\n• \\n• \\n• \\n•', metadata={'source': './PDFS/Concepts.pdf', 'page': 466}),\n",
       " Document(page_content=\"Process ID Limits And Reservations\\nFEATURE STATE:  Kubernetes v1.20 [stable]\\nKubernetes allow you to limit the number of process IDs (PIDs) that a Pod can use. You can also\\nreserve a number of allocatable PIDs for each node  for use by the operating system and\\ndaemons (rather than by Pods).\\nProcess IDs (PIDs) are a fundamental resource on nodes. It is trivial to hit the task limit without\\nhitting any other resource limits, which can then cause instability to a host machine.\\nCluster administrators require mechanisms to ensure that Pods running in the cluster cannot\\ninduce PID exhaustion that prevents host daemons (such as the kubelet  or kube-proxy , and\\npotentially also the container runtime) from running. In addition, it is important to ensure that\\nPIDs are limited among Pods in order to ensure they have limited impact on other workloads\\non the same node.\\nNote:  On certain Linux installations, the operating system sets the PIDs limit to a low default,\\nsuch as 32768 . Consider raising the value of /proc/sys/kernel/pid_max .\\nYou can configure a kubelet to limit the number of PIDs a given Pod can consume. For example,\\nif your node's host OS is set to use a maximum of 262144  PIDs and expect to host less than 250\\nPods, one can give each Pod a budget of 1000 PIDs to prevent using up that node's overall\\nnumber of available PIDs. If the admin wants to overcommit PIDs similar to CPU or memory,\\nthey may do so as well with some additional risks. Either way, a single Pod will not be able to\\nbring the whole machine down. This kind of resource limiting helps to prevent simple fork\\nbombs from affecting operation of an entire cluster.\\nPer-Pod PID limiting allows administrators to protect one Pod from another, but does not\\nensure that all Pods scheduled onto that host are unable to impact the node overall. Per-Pod\\nlimiting also does not protect the node agents themselves from PID exhaustion.\\nYou can also reserve an amount of PIDs for node overhead, separate from the allocation to\\nPods. This is similar to how you can reserve CPU, memory, or other resources for use by the\\noperating system and other facilities outside of Pods and their containers.\\nPID limiting is a an important sibling to compute resource  requests and limits. However, you\\nspecify it in a different way: rather than defining a Pod's resource limit in the .spec  for a Pod,\\nyou configure the limit as a setting on the kubelet. Pod-defined PID limits are not currently\\nsupported.\\nCaution:  This means that the limit that applies to a Pod may be different depending on where\\nthe Pod is scheduled. To make things simple, it's easiest if all Nodes use the same PID resource\\nlimits and reservations.\\nNode PID limits\\nKubernetes allows you to reserve a number of process IDs for the system use. To configure the\\nreservation, use the parameter pid=<number>  in the --system-reserved  and --kube-reserved\\ncommand line options to the kubelet. The value you specified declares that the specified\\nnumber of process IDs will be reserved for the system as a whole and for Kubernetes system\\ndaemons respectively.\", metadata={'source': './PDFS/Concepts.pdf', 'page': 467}),\n",
       " Document(page_content=\"Pod PID limits\\nKubernetes allows you to limit the number of processes running in a Pod. You specify this limit\\nat the node level, rather than configuring it as a resource limit for a particular Pod. Each Node\\ncan have a different PID limit.\\nTo configure the limit, you can specify the command line parameter --pod-max-pids  to the\\nkubelet, or set PodPidsLimit  in the kubelet configuration file .\\nPID based eviction\\nYou can configure kubelet to start terminating a Pod when it is misbehaving and consuming\\nabnormal amount of resources. This feature is called eviction. You can Configure Out of\\nResource Handling  for various eviction signals. Use pid.available  eviction signal to configure\\nthe threshold for number of PIDs used by Pod. You can set soft and hard eviction policies.\\nHowever, even with the hard eviction policy, if the number of PIDs growing very fast, node can\\nstill get into unstable state by hitting the node PIDs limit. Eviction signal value is calculated\\nperiodically and does NOT enforce the limit.\\nPID limiting - per Pod and per Node sets the hard limit. Once the limit is hit, workload will start\\nexperiencing failures when trying to get a new PID. It may or may not lead to rescheduling of a\\nPod, depending on how workload reacts on these failures and how liveness and readiness\\nprobes are configured for the Pod. However, if limits were set correctly, you can guarantee that\\nother Pods workload and system processes will not run out of PIDs when one Pod is\\nmisbehaving.\\nWhat's next\\nRefer to the PID Limiting enhancement document  for more information.\\nFor historical context, read Process ID Limiting for Stability Improvements in Kubernetes\\n1.14.\\nRead Managing Resources for Containers .\\nLearn how to Configure Out of Resource Handling .\\nNode Resource Managers\\nIn order to support latency-critical and high-throughput workloads, Kubernetes offers a suite of\\nResource Managers. The managers aim to co-ordinate and optimise node's resources alignment\\nfor pods configured with a specific requirement for CPUs, devices, and memory (hugepages)\\nresources.\\nThe main manager, the Topology Manager, is a Kubelet component that co-ordinates the overall\\nresource management process through its policy .\\nThe configuration of individual managers is elaborated in dedicated documents:\\nCPU Manager Policies\\nDevice Manager\\nMemory Manager Policies• \\n• \\n• \\n• \\n• \\n• \\n•\", metadata={'source': './PDFS/Concepts.pdf', 'page': 468}),\n",
       " Document(page_content='Scheduling, Preemption and Eviction\\nIn Kubernetes, scheduling refers to making sure that Pods are matched to Nodes so that the\\nkubelet can run them. Preemption is the process of terminating Pods with lower Priority so that\\nPods with higher Priority can schedule on Nodes. Eviction is the process of proactively\\nterminating one or more Pods on resource-starved Nodes.\\nIn Kubernetes, scheduling refers to making sure that Pods  are matched to Nodes  so that the \\nkubelet  can run them. Preemption is the process of terminating Pods with lower Priority  so that\\nPods with higher Priority can schedule on Nodes. Eviction is the process of terminating one or\\nmore Pods on Nodes.\\nScheduling\\nKubernetes Scheduler\\nAssigning Pods to Nodes\\nPod Overhead\\nPod Topology Spread Constraints\\nTaints and Tolerations\\nScheduling Framework\\nDynamic Resource Allocation\\nScheduler Performance Tuning\\nResource Bin Packing for Extended Resources\\nPod Scheduling Readiness\\nDescheduler\\nPod Disruption\\nPod disruption  is the process by which Pods on Nodes are terminated either voluntarily or\\ninvoluntarily.\\nVoluntary disruptions are started intentionally by application owners or cluster administrators.\\nInvoluntary disruptions are unintentional and can be triggered by unavoidable issues like\\nNodes running out of resources, or by accidental deletions.\\nPod Priority and Preemption\\nNode-pressure Eviction\\nAPI-initiated Eviction\\nKubernetes Scheduler\\nIn Kubernetes, scheduling  refers to making sure that Pods  are matched to Nodes  so that Kubelet\\ncan run them.\\nScheduling overview\\nA scheduler watches for newly created Pods that have no Node assigned. For every Pod that the\\nscheduler discovers, the scheduler becomes responsible for finding the best Node for that Pod to• \\n• \\n• \\n• \\n• \\n• \\n• \\n• \\n• \\n• \\n• \\n• \\n• \\n•', metadata={'source': './PDFS/Concepts.pdf', 'page': 469}),\n",
       " Document(page_content=\"run on. The scheduler reaches this placement decision taking into account the scheduling\\nprinciples described below.\\nIf you want to understand why Pods are placed onto a particular Node, or if you're planning to\\nimplement a custom scheduler yourself, this page will help you learn about scheduling.\\nkube-scheduler\\nkube-scheduler  is the default scheduler for Kubernetes and runs as part of the control plane .\\nkube-scheduler is designed so that, if you want and need to, you can write your own scheduling\\ncomponent and use that instead.\\nKube-scheduler selects an optimal node to run newly created or not yet scheduled\\n(unscheduled) pods. Since containers in pods - and pods themselves - can have different\\nrequirements, the scheduler filters out any nodes that don't meet a Pod's specific scheduling\\nneeds. Alternatively, the API lets you specify a node for a Pod when you create it, but this is\\nunusual and is only done in special cases.\\nIn a cluster, Nodes that meet the scheduling requirements for a Pod are called feasible  nodes. If\\nnone of the nodes are suitable, the pod remains unscheduled until the scheduler is able to place\\nit.\\nThe scheduler finds feasible Nodes for a Pod and then runs a set of functions to score the\\nfeasible Nodes and picks a Node with the highest score among the feasible ones to run the Pod.\\nThe scheduler then notifies the API server about this decision in a process called binding .\\nFactors that need to be taken into account for scheduling decisions include individual and\\ncollective resource requirements, hardware / software / policy constraints, affinity and anti-\\naffinity specifications, data locality, inter-workload interference, and so on.\\nNode selection in kube-scheduler\\nkube-scheduler selects a node for the pod in a 2-step operation:\\nFiltering\\nScoring\\nThe filtering  step finds the set of Nodes where it's feasible to schedule the Pod. For example, the\\nPodFitsResources filter checks whether a candidate Node has enough available resource to meet\\na Pod's specific resource requests. After this step, the node list contains any suitable Nodes;\\noften, there will be more than one. If the list is empty, that Pod isn't (yet) schedulable.\\nIn the scoring  step, the scheduler ranks the remaining nodes to choose the most suitable Pod\\nplacement. The scheduler assigns a score to each Node that survived filtering, basing this score\\non the active scoring rules.\\nFinally, kube-scheduler assigns the Pod to the Node with the highest ranking. If there is more\\nthan one node with equal scores, kube-scheduler selects one of these at random.\\nThere are two supported ways to configure the filtering and scoring behavior of the scheduler:\\nScheduling Policies  allow you to configure Predicates  for filtering and Priorities  for\\nscoring.1. \\n2. \\n1.\", metadata={'source': './PDFS/Concepts.pdf', 'page': 470}),\n",
       " Document(page_content=\"Scheduling Profiles  allow you to configure Plugins that implement different scheduling\\nstages, including: QueueSort , Filter , Score , Bind , Reserve , Permit , and others. You can also\\nconfigure the kube-scheduler to run different profiles.\\nWhat's next\\nRead about scheduler performance tuning\\nRead about Pod topology spread constraints\\nRead the reference documentation  for kube-scheduler\\nRead the kube-scheduler config (v1beta3)  reference\\nLearn about configuring multiple schedulers\\nLearn about topology management policies\\nLearn about Pod Overhead\\nLearn about scheduling of Pods that use volumes in:\\nVolume Topology Support\\nStorage Capacity Tracking\\nNode-specific Volume Limits\\nAssigning Pods to Nodes\\nYou can constrain a Pod so that it is restricted  to run on particular node(s) , or to prefer  to run on\\nparticular nodes. There are several ways to do this and the recommended approaches all use \\nlabel selectors  to facilitate the selection. Often, you do not need to set any such constraints; the \\nscheduler  will automatically do a reasonable placement (for example, spreading your Pods\\nacross nodes so as not place Pods on a node with insufficient free resources). However, there\\nare some circumstances where you may want to control which node the Pod deploys to, for\\nexample, to ensure that a Pod ends up on a node with an SSD attached to it, or to co-locate Pods\\nfrom two different services that communicate a lot into the same availability zone.\\nYou can use any of the following methods to choose where Kubernetes schedules specific Pods:\\nnodeSelector  field matching against node labels\\nAffinity and anti-affinity\\nnodeName  field\\nPod topology spread constraints\\nNode labels\\nLike many other Kubernetes objects, nodes have labels . You can attach labels manually .\\nKubernetes also populates a standard set of labels  on all nodes in a cluster.\\nNote:  The value of these labels is cloud provider specific and is not guaranteed to be reliable.\\nFor example, the value of kubernetes.io/hostname  may be the same as the node name in some\\nenvironments and a different value in other environments.\\nNode isolation/restriction\\nAdding labels to nodes allows you to target Pods for scheduling on specific nodes or groups of\\nnodes. You can use this functionality to ensure that specific Pods only run on nodes with\\ncertain isolation, security, or regulatory properties.2. \\n• \\n• \\n• \\n• \\n• \\n• \\n• \\n• \\n◦ \\n◦ \\n◦ \\n• \\n• \\n• \\n•\", metadata={'source': './PDFS/Concepts.pdf', 'page': 471}),\n",
       " Document(page_content=\"If you use labels for node isolation, choose label keys that the kubelet  cannot modify. This\\nprevents a compromised node from setting those labels on itself so that the scheduler schedules\\nworkloads onto the compromised node.\\nThe NodeRestriction  admission plugin  prevents the kubelet from setting or modifying labels\\nwith a node-restriction.kubernetes.io/  prefix.\\nTo make use of that label prefix for node isolation:\\nEnsure you are using the Node authorizer  and have enabled  the NodeRestriction\\nadmission plugin.\\nAdd labels with the node-restriction.kubernetes.io/  prefix to your nodes, and use those\\nlabels in your node selectors . For example, example.com.node-restriction.kubernetes.io/\\nfips=true  or example.com.node-restriction.kubernetes.io/pci-dss=true .\\nnodeSelector\\nnodeSelector  is the simplest recommended form of node selection constraint. You can add the \\nnodeSelector  field to your Pod specification and specify the node labels  you want the target\\nnode to have. Kubernetes only schedules the Pod onto nodes that have each of the labels you\\nspecify.\\nSee Assign Pods to Nodes  for more information.\\nAffinity and anti-affinity\\nnodeSelector  is the simplest way to constrain Pods to nodes with specific labels. Affinity and\\nanti-affinity expands the types of constraints you can define. Some of the benefits of affinity\\nand anti-affinity include:\\nThe affinity/anti-affinity language is more expressive. nodeSelector  only selects nodes\\nwith all the specified labels. Affinity/anti-affinity gives you more control over the\\nselection logic.\\nYou can indicate that a rule is soft or preferred , so that the scheduler still schedules the\\nPod even if it can't find a matching node.\\nYou can constrain a Pod using labels on other Pods running on the node (or other\\ntopological domain), instead of just node labels, which allows you to define rules for\\nwhich Pods can be co-located on a node.\\nThe affinity feature consists of two types of affinity:\\nNode affinity  functions like the nodeSelector  field but is more expressive and allows you\\nto specify soft rules.\\nInter-pod affinity/anti-affinity  allows you to constrain Pods against labels on other Pods.1. \\n2. \\n• \\n• \\n• \\n• \\n•\", metadata={'source': './PDFS/Concepts.pdf', 'page': 472}),\n",
       " Document(page_content=\"Node affinity\\nNode affinity is conceptually similar to nodeSelector , allowing you to constrain which nodes\\nyour Pod can be scheduled on based on node labels. There are two types of node affinity:\\nrequiredDuringSchedulingIgnoredDuringExecution : The scheduler can't schedule the Pod\\nunless the rule is met. This functions like nodeSelector , but with a more expressive\\nsyntax.\\npreferredDuringSchedulingIgnoredDuringExecution : The scheduler tries to find a node\\nthat meets the rule. If a matching node is not available, the scheduler still schedules the\\nPod.\\nNote:  In the preceding types, IgnoredDuringExecution  means that if the node labels change\\nafter Kubernetes schedules the Pod, the Pod continues to run.\\nYou can specify node affinities using the .spec.affinity.nodeAffinity  field in your Pod spec.\\nFor example, consider the following Pod spec:\\npods/pod-with-node-affinity.yaml  \\napiVersion : v1\\nkind: Pod\\nmetadata :\\n  name : with-node-affinity\\nspec:\\n  affinity :\\n    nodeAffinity :\\n      requiredDuringSchedulingIgnoredDuringExecution :\\n        nodeSelectorTerms :\\n        - matchExpressions :\\n          - key: topology.kubernetes.io/zone\\n            operator : In\\n            values :\\n            - antarctica-east1\\n            - antarctica-west1\\n      preferredDuringSchedulingIgnoredDuringExecution :\\n      - weight : 1\\n        preference :\\n          matchExpressions :\\n          - key: another-node-label-key\\n            operator : In\\n            values :\\n            - another-node-label-value\\n  containers :\\n  - name : with-node-affinity\\n    image : registry.k8s.io/pause:2.0\\nIn this example, the following rules apply:\\nThe node must  have a label with the key topology.kubernetes.io/zone  and the value of\\nthat label must  be either antarctica-east1  or antarctica-west1 .\\nThe node preferably  has a label with the key another-node-label-key  and the value \\nanother-node-label-value .• \\n• \\n• \\n•\", metadata={'source': './PDFS/Concepts.pdf', 'page': 473}),\n",
       " Document(page_content='You can use the operator  field to specify a logical operator for Kubernetes to use when\\ninterpreting the rules. You can use In, NotIn , Exists , DoesNotExist , Gt and Lt.\\nRead Operators  to learn more about how these work.\\nNotIn  and DoesNotExist  allow you to define node anti-affinity behavior. Alternatively, you can\\nuse node taints  to repel Pods from specific nodes.\\nNote:\\nIf you specify both nodeSelector  and nodeAffinity , both must be satisfied for the Pod to be\\nscheduled onto a node.\\nIf you specify multiple terms in nodeSelectorTerms  associated with nodeAffinity  types, then the\\nPod can be scheduled onto a node if one of the specified terms can be satisfied (terms are\\nORed).\\nIf you specify multiple expressions in a single matchExpressions  field associated with a term in \\nnodeSelectorTerms , then the Pod can be scheduled onto a node only if all the expressions are\\nsatisfied (expressions are ANDed).\\nSee Assign Pods to Nodes using Node Affinity  for more information.\\nNode affinity weight\\nYou can specify a weight  between 1 and 100 for each instance of the \\npreferredDuringSchedulingIgnoredDuringExecution  affinity type. When the scheduler finds\\nnodes that meet all the other scheduling requirements of the Pod, the scheduler iterates\\nthrough every preferred rule that the node satisfies and adds the value of the weight  for that\\nexpression to a sum.\\nThe final sum is added to the score of other priority functions for the node. Nodes with the\\nhighest total score are prioritized when the scheduler makes a scheduling decision for the Pod.\\nFor example, consider the following Pod spec:\\npods/pod-with-affinity-anti-affinity.yaml  \\napiVersion : v1\\nkind: Pod\\nmetadata :\\n  name : with-affinity-anti-affinity\\nspec:\\n  affinity :\\n    nodeAffinity :\\n      requiredDuringSchedulingIgnoredDuringExecution :\\n        nodeSelectorTerms :\\n        - matchExpressions :\\n          - key: kubernetes.io/os\\n            operator : In\\n            values :\\n            - linux\\n      preferredDuringSchedulingIgnoredDuringExecution :\\n      - weight : 1', metadata={'source': './PDFS/Concepts.pdf', 'page': 474}),\n",
       " Document(page_content='preference :\\n          matchExpressions :\\n          - key: label-1\\n            operator : In\\n            values :\\n            - key-1\\n      - weight : 50\\n        preference :\\n          matchExpressions :\\n          - key: label-2\\n            operator : In\\n            values :\\n            - key-2\\n  containers :\\n  - name : with-node-affinity\\n    image : registry.k8s.io/pause:2.0\\nIf there are two possible nodes that match the \\npreferredDuringSchedulingIgnoredDuringExecution  rule, one with the label-1:key-1  label and\\nanother with the label-2:key-2  label, the scheduler considers the weight  of each node and adds\\nthe weight to the other scores for that node, and schedules the Pod onto the node with the\\nhighest final score.\\nNote:  If you want Kubernetes to successfully schedule the Pods in this example, you must have\\nexisting nodes with the kubernetes.io/os=linux  label.\\nNode affinity per scheduling profile\\nFEATURE STATE:  Kubernetes v1.20 [beta]\\nWhen configuring multiple scheduling profiles , you can associate a profile with a node affinity,\\nwhich is useful if a profile only applies to a specific set of nodes. To do so, add an addedAffinity\\nto the args field of the NodeAffinity  plugin  in the scheduler configuration . For example:\\napiVersion : kubescheduler.config.k8s.io/v1beta3\\nkind: KubeSchedulerConfiguration\\nprofiles :\\n  - schedulerName : default-scheduler\\n  - schedulerName : foo-scheduler\\n    pluginConfig :\\n      - name : NodeAffinity\\n        args:\\n          addedAffinity :\\n            requiredDuringSchedulingIgnoredDuringExecution :\\n              nodeSelectorTerms :\\n              - matchExpressions :\\n                - key: scheduler-profile\\n                  operator : In\\n                  values :\\n                  - foo', metadata={'source': './PDFS/Concepts.pdf', 'page': 475}),\n",
       " Document(page_content='The addedAffinity  is applied to all Pods that set .spec.schedulerName  to foo-scheduler , in\\naddition to the NodeAffinity specified in the PodSpec. That is, in order to match the Pod, nodes\\nneed to satisfy addedAffinity  and the Pod\\'s .spec.NodeAffinity .\\nSince the addedAffinity  is not visible to end users, its behavior might be unexpected to them.\\nUse node labels that have a clear correlation to the scheduler profile name.\\nNote:  The DaemonSet controller, which creates Pods for DaemonSets , does not support\\nscheduling profiles. When the DaemonSet controller creates Pods, the default Kubernetes\\nscheduler places those Pods and honors any nodeAffinity  rules in the DaemonSet controller.\\nInter-pod affinity and anti-affinity\\nInter-pod affinity and anti-affinity allow you to constrain which nodes your Pods can be\\nscheduled on based on the labels of Pods  already running on that node, instead of the node\\nlabels.\\nInter-pod affinity and anti-affinity rules take the form \"this Pod should (or, in the case of anti-\\naffinity, should not) run in an X if that X is already running one or more Pods that meet rule Y\",\\nwhere X is a topology domain like node, rack, cloud provider zone or region, or similar and Y is\\nthe rule Kubernetes tries to satisfy.\\nYou express these rules (Y) as label selectors  with an optional associated list of namespaces.\\nPods are namespaced objects in Kubernetes, so Pod labels also implicitly have namespaces. Any\\nlabel selectors for Pod labels should specify the namespaces in which Kubernetes should look\\nfor those labels.\\nYou express the topology domain (X) using a topologyKey , which is the key for the node label\\nthat the system uses to denote the domain. For examples, see Well-Known Labels, Annotations\\nand Taints .\\nNote:  Inter-pod affinity and anti-affinity require substantial amount of processing which can\\nslow down scheduling in large clusters significantly. We do not recommend using them in\\nclusters larger than several hundred nodes.\\nNote:  Pod anti-affinity requires nodes to be consistently labelled, in other words, every node in\\nthe cluster must have an appropriate label matching topologyKey . If some or all nodes are\\nmissing the specified topologyKey  label, it can lead to unintended behavior.\\nTypes of inter-pod affinity and anti-affinity\\nSimilar to node affinity  are two types of Pod affinity and anti-affinity as follows:\\nrequiredDuringSchedulingIgnoredDuringExecution\\npreferredDuringSchedulingIgnoredDuringExecution\\nFor example, you could use requiredDuringSchedulingIgnoredDuringExecution  affinity to tell\\nthe scheduler to co-locate Pods of two services in the same cloud provider zone because they\\ncommunicate with each other a lot. Similarly, you could use \\npreferredDuringSchedulingIgnoredDuringExecution  anti-affinity to spread Pods from a service\\nacross multiple cloud provider zones.\\nTo use inter-pod affinity, use the affinity.podAffinity  field in the Pod spec. For inter-pod anti-\\naffinity, use the affinity.podAntiAffinity  field in the Pod spec.• \\n•', metadata={'source': './PDFS/Concepts.pdf', 'page': 476}),\n",
       " Document(page_content='Scheduling a group of pods with inter-pod affinity to themselves\\nIf the current Pod being scheduled is the first in a series that have affinity to themselves, it is\\nallowed to be scheduled if it passes all other affinity checks. This is determined by verifying that\\nno other pod in the cluster matches the namespace and selector of this pod, that the pod\\nmatches its own terms, and the chosen node matches all requested topologies. This ensures that\\nthere will not be a deadlock even if all the pods have inter-pod affinity specified.\\nPod affinity example\\nConsider the following Pod spec:\\npods/pod-with-pod-affinity.yaml  \\napiVersion : v1\\nkind: Pod\\nmetadata :\\n  name : with-pod-affinity\\nspec:\\n  affinity :\\n    podAffinity :\\n      requiredDuringSchedulingIgnoredDuringExecution :\\n      - labelSelector :\\n          matchExpressions :\\n          - key: security\\n            operator : In\\n            values :\\n            - S1\\n        topologyKey : topology.kubernetes.io/zone\\n    podAntiAffinity :\\n      preferredDuringSchedulingIgnoredDuringExecution :\\n      - weight : 100\\n        podAffinityTerm :\\n          labelSelector :\\n            matchExpressions :\\n            - key: security\\n              operator : In\\n              values :\\n              - S2\\n          topologyKey : topology.kubernetes.io/zone\\n  containers :\\n  - name : with-pod-affinity\\n    image : registry.k8s.io/pause:2.0\\nThis example defines one Pod affinity rule and one Pod anti-affinity rule. The Pod affinity rule\\nuses the \"hard\" requiredDuringSchedulingIgnoredDuringExecution , while the anti-affinity rule\\nuses the \"soft\" preferredDuringSchedulingIgnoredDuringExecution .\\nThe affinity rule specifies that the scheduler is allowed to place the example Pod on a node only\\nif that node belongs to a specific zone  where other Pods have been labeled with security=S1 .\\nFor instance, if we have a cluster with a designated zone, let\\'s call it \"Zone V,\" consisting of\\nnodes labeled with topology.kubernetes.io/zone=V , the scheduler can assign the Pod to any\\nnode within Zone V, as long as there is at least one Pod within Zone V already labeled with', metadata={'source': './PDFS/Concepts.pdf', 'page': 477}),\n",
       " Document(page_content='security=S1 . Conversely, if there are no Pods with security=S1  labels in Zone V, the scheduler\\nwill not assign the example Pod to any node in that zone.\\nThe anti-affinity rule specifies that the scheduler should try to avoid scheduling the Pod on a\\nnode if that node belongs to a specific zone  where other Pods have been labeled with \\nsecurity=S2 . For instance, if we have a cluster with a designated zone, let\\'s call it \"Zone R,\"\\nconsisting of nodes labeled with topology.kubernetes.io/zone=R , the scheduler should avoid\\nassigning the Pod to any node within Zone R, as long as there is at least one Pod within Zone R\\nalready labeled with security=S2 . Conversely, the anti-affinity rule does not impact scheduling\\ninto Zone R if there are no Pods with security=S2  labels.\\nTo get yourself more familiar with the examples of Pod affinity and anti-affinity, refer to the \\ndesign proposal .\\nYou can use the In, NotIn , Exists  and DoesNotExist  values in the operator  field for Pod affinity\\nand anti-affinity.\\nRead Operators  to learn more about how these work.\\nIn principle, the topologyKey  can be any allowed label key with the following exceptions for\\nperformance and security reasons:\\nFor Pod affinity and anti-affinity, an empty topologyKey  field is not allowed in both \\nrequiredDuringSchedulingIgnoredDuringExecution  and \\npreferredDuringSchedulingIgnoredDuringExecution .\\nFor requiredDuringSchedulingIgnoredDuringExecution  Pod anti-affinity rules, the\\nadmission controller LimitPodHardAntiAffinityTopology  limits topologyKey  to \\nkubernetes.io/hostname . You can modify or disable the admission controller if you want\\nto allow custom topologies.\\nIn addition to labelSelector  and topologyKey , you can optionally specify a list of namespaces\\nwhich the labelSelector  should match against using the namespaces  field at the same level as \\nlabelSelector  and topologyKey . If omitted or empty, namespaces  defaults to the namespace of\\nthe Pod where the affinity/anti-affinity definition appears.\\nNamespace selector\\nFEATURE STATE:  Kubernetes v1.24 [stable]\\nYou can also select matching namespaces using namespaceSelector , which is a label query over\\nthe set of namespaces. The affinity term is applied to namespaces selected by both \\nnamespaceSelector  and the namespaces  field. Note that an empty namespaceSelector  ({})\\nmatches all namespaces, while a null or empty namespaces  list and null namespaceSelector\\nmatches the namespace of the Pod where the rule is defined.\\nMore practical use-cases\\nInter-pod affinity and anti-affinity can be even more useful when they are used with higher\\nlevel collections such as ReplicaSets, StatefulSets, Deployments, etc. These rules allow you to\\nconfigure that a set of workloads should be co-located in the same defined topology; for\\nexample, preferring to place two related Pods onto the same node.• \\n•', metadata={'source': './PDFS/Concepts.pdf', 'page': 478}),\n",
       " Document(page_content='For example: imagine a three-node cluster. You use the cluster to run a web application and also\\nan in-memory cache (such as Redis). For this example, also assume that latency between the\\nweb application and the memory cache should be as low as is practical. You could use inter-pod\\naffinity and anti-affinity to co-locate the web servers with the cache as much as possible.\\nIn the following example Deployment for the Redis cache, the replicas get the label app=store .\\nThe podAntiAffinity  rule tells the scheduler to avoid placing multiple replicas with the \\napp=store  label on a single node. This creates each cache in a separate node.\\napiVersion : apps/v1\\nkind: Deployment\\nmetadata :\\n  name : redis-cache\\nspec:\\n  selector :\\n    matchLabels :\\n      app: store\\n  replicas : 3\\n  template :\\n    metadata :\\n      labels :\\n        app: store\\n    spec:\\n      affinity :\\n        podAntiAffinity :\\n          requiredDuringSchedulingIgnoredDuringExecution :\\n          - labelSelector :\\n              matchExpressions :\\n              - key: app\\n                operator : In\\n                values :\\n                - store\\n            topologyKey : \"kubernetes.io/hostname\"\\n      containers :\\n      - name : redis-server\\n        image : redis:3.2-alpine\\nThe following example Deployment for the web servers creates replicas with the label \\napp=web-store . The Pod affinity rule tells the scheduler to place each replica on a node that has\\na Pod with the label app=store . The Pod anti-affinity rule tells the scheduler never to place\\nmultiple app=web-store  servers on a single node.\\napiVersion : apps/v1\\nkind: Deployment\\nmetadata :\\n  name : web-server\\nspec:\\n  selector :\\n    matchLabels :\\n      app: web-store\\n  replicas : 3\\n  template :\\n    metadata :', metadata={'source': './PDFS/Concepts.pdf', 'page': 479}),\n",
       " Document(page_content='labels :\\n        app: web-store\\n    spec:\\n      affinity :\\n        podAntiAffinity :\\n          requiredDuringSchedulingIgnoredDuringExecution :\\n          - labelSelector :\\n              matchExpressions :\\n              - key: app\\n                operator : In\\n                values :\\n                - web-store\\n            topologyKey : \"kubernetes.io/hostname\"\\n        podAffinity :\\n          requiredDuringSchedulingIgnoredDuringExecution :\\n          - labelSelector :\\n              matchExpressions :\\n              - key: app\\n                operator : In\\n                values :\\n                - store\\n            topologyKey : \"kubernetes.io/hostname\"\\n      containers :\\n      - name : web-app\\n        image : nginx:1.16-alpine\\nCreating the two preceding Deployments results in the following cluster layout, where each\\nweb server is co-located with a cache, on three separate nodes.\\nnode-1 node-2 node-3\\nwebserver-1 webserver-2 webserver-3\\ncache-1 cache-2 cache-3\\nThe overall effect is that each cache instance is likely to be accessed by a single client, that is\\nrunning on the same node. This approach aims to minimize both skew (imbalanced load) and\\nlatency.\\nYou might have other reasons to use Pod anti-affinity. See the ZooKeeper tutorial  for an\\nexample of a StatefulSet configured with anti-affinity for high availability, using the same\\ntechnique as this example.\\nnodeName\\nnodeName  is a more direct form of node selection than affinity or nodeSelector . nodeName  is a\\nfield in the Pod spec. If the nodeName  field is not empty, the scheduler ignores the Pod and the\\nkubelet on the named node tries to place the Pod on that node. Using nodeName  overrules\\nusing nodeSelector  or affinity and anti-affinity rules.\\nSome of the limitations of using nodeName  to select nodes are:\\nIf the named node does not exist, the Pod will not run, and in some cases may be\\nautomatically deleted.•', metadata={'source': './PDFS/Concepts.pdf', 'page': 480}),\n",
       " Document(page_content='If the named node does not have the resources to accommodate the Pod, the Pod will fail\\nand its reason will indicate why, for example OutOfmemory or OutOfcpu.\\nNode names in cloud environments are not always predictable or stable.\\nNote:  nodeName  is intended for use by custom schedulers or advanced use cases where you\\nneed to bypass any configured schedulers. Bypassing the schedulers might lead to failed Pods if\\nthe assigned Nodes get oversubscribed. You can use node affinity  or a the nodeselector  field  to\\nassign a Pod to a specific Node without bypassing the schedulers.\\nHere is an example of a Pod spec using the nodeName  field:\\napiVersion : v1\\nkind: Pod\\nmetadata :\\n  name : nginx\\nspec:\\n  containers :\\n  - name : nginx\\n    image : nginx\\n  nodeName : kube-01\\nThe above Pod will only run on the node kube-01 .\\nPod topology spread constraints\\nYou can use topology spread constraints  to control how Pods  are spread across your cluster\\namong failure-domains such as regions, zones, nodes, or among any other topology domains\\nthat you define. You might do this to improve performance, expected availability, or overall\\nutilization.\\nRead Pod topology spread constraints  to learn more about how these work.\\nOperators\\nThe following are all the logical operators that you can use in the operator  field for nodeAffinity\\nand podAffinity  mentioned above.\\nOperator Behavior\\nIn The label value is present in the supplied set of strings\\nNotIn The label value is not contained in the supplied set of strings\\nExists A label with this key exists on the object\\nDoesNotExist No label with this key exists on the object\\nThe following operators can only be used with nodeAffinity .\\nOperator Behaviour\\nGtThe supplied value will be parsed as an integer, and that integer is less than the\\ninteger that results from parsing the value of a label named by this selector\\nLtThe supplied value will be parsed as an integer, and that integer is greater than the\\ninteger that results from parsing the value of a label named by this selector• \\n•', metadata={'source': './PDFS/Concepts.pdf', 'page': 481}),\n",
       " Document(page_content='Note:  Gt and Lt operators will not work with non-integer values. If the given value doesn\\'t\\nparse as an integer, the pod will fail to get scheduled. Also, Gt and Lt are not available for \\npodAffinity .\\nWhat\\'s next\\nRead more about taints and tolerations  .\\nRead the design docs for node affinity  and for inter-pod affinity/anti-affinity .\\nLearn about how the topology manager  takes part in node-level resource allocation\\ndecisions.\\nLearn how to use nodeSelector .\\nLearn how to use affinity and anti-affinity .\\nPod Overhead\\nFEATURE STATE:  Kubernetes v1.24 [stable]\\nWhen you run a Pod on a Node, the Pod itself takes an amount of system resources. These\\nresources are additional to the resources needed to run the container(s) inside the Pod. In\\nKubernetes, Pod Overhead  is a way to account for the resources consumed by the Pod\\ninfrastructure on top of the container requests & limits.\\nIn Kubernetes, the Pod\\'s overhead is set at admission  time according to the overhead associated\\nwith the Pod\\'s RuntimeClass .\\nA pod\\'s overhead is considered in addition to the sum of container resource requests when\\nscheduling a Pod. Similarly, the kubelet will include the Pod overhead when sizing the Pod\\ncgroup, and when carrying out Pod eviction ranking.\\nConfiguring Pod overhead\\nYou need to make sure a RuntimeClass  is utilized which defines the overhead  field.\\nUsage example\\nTo work with Pod overhead, you need a RuntimeClass that defines the overhead  field. As an\\nexample, you could use the following RuntimeClass definition with a virtualization container\\nruntime that uses around 120MiB per Pod for the virtual machine and the guest OS:\\napiVersion : node.k8s.io/v1\\nkind: RuntimeClass\\nmetadata :\\n  name : kata-fc\\nhandler : kata-fc\\noverhead :\\n  podFixed :\\n    memory : \"120Mi\"\\n    cpu: \"250m\"• \\n• \\n• \\n• \\n•', metadata={'source': './PDFS/Concepts.pdf', 'page': 482}),\n",
       " Document(page_content=\"Workloads which are created which specify the kata-fc  RuntimeClass handler will take the\\nmemory and cpu overheads into account for resource quota calculations, node scheduling, as\\nwell as Pod cgroup sizing.\\nConsider running the given example workload, test-pod:\\napiVersion : v1\\nkind: Pod\\nmetadata :\\n  name : test-pod\\nspec:\\n  runtimeClassName : kata-fc\\n  containers :\\n  - name : busybox-ctr\\n    image : busybox:1.28\\n    stdin : true\\n    tty: true\\n    resources :\\n      limits :\\n        cpu: 500m\\n        memory : 100Mi\\n  - name : nginx-ctr\\n    image : nginx\\n    resources :\\n      limits :\\n        cpu: 1500m\\n        memory : 100Mi\\nAt admission time the RuntimeClass admission controller  updates the workload's PodSpec to\\ninclude the overhead  as described in the RuntimeClass. If the PodSpec already has this field\\ndefined, the Pod will be rejected. In the given example, since only the RuntimeClass name is\\nspecified, the admission controller mutates the Pod to include an overhead .\\nAfter the RuntimeClass admission controller has made modifications, you can check the\\nupdated Pod overhead value:\\nkubectl get pod test-pod -o jsonpath ='{.spec.overhead}'\\nThe output is:\\nmap[cpu:250m memory:120Mi]\\nIf a ResourceQuota  is defined, the sum of container requests as well as the overhead  field are\\ncounted.\\nWhen the kube-scheduler is deciding which node should run a new Pod, the scheduler\\nconsiders that Pod's overhead  as well as the sum of container requests for that Pod. For this\\nexample, the scheduler adds the requests and the overhead, then looks for a node that has 2.25\\nCPU and 320 MiB of memory available.\\nOnce a Pod is scheduled to a node, the kubelet on that node creates a new cgroup  for the Pod. It\\nis within this pod that the underlying container runtime will create containers.\", metadata={'source': './PDFS/Concepts.pdf', 'page': 483}),\n",
       " Document(page_content='If the resource has a limit defined for each container (Guaranteed QoS or Burstable QoS with\\nlimits defined), the kubelet will set an upper limit for the pod cgroup associated with that\\nresource (cpu.cfs_quota_us for CPU and memory.limit_in_bytes memory). This upper limit is\\nbased on the sum of the container limits plus the overhead  defined in the PodSpec.\\nFor CPU, if the Pod is Guaranteed or Burstable QoS, the kubelet will set cpu.shares  based on the\\nsum of container requests plus the overhead  defined in the PodSpec.\\nLooking at our example, verify the container requests for the workload:\\nkubectl get pod test-pod -o jsonpath =\\'{.spec.containers[*].resources.limits}\\'\\nThe total container requests are 2000m CPU and 200MiB of memory:\\nmap[cpu: 500m memory:100Mi] map[cpu:1500m memory:100Mi]\\nCheck this against what is observed by the node:\\nkubectl describe node | grep test-pod -B2\\nThe output shows requests for 2250m CPU, and for 320MiB of memory. The requests include\\nPod overhead:\\n  Namespace    Name       CPU Requests  CPU Limits   Memory Requests  Memory Limits  AGE\\n  ---------    ----       ------------  ----------   ---------------  -------------  ---\\n  default      test-pod   2250m (56%)   2250m (56%)  320Mi (1%)       320Mi (1%)     36m\\nVerify Pod cgroup limits\\nCheck the Pod\\'s memory cgroups on the node where the workload is running. In the following\\nexample, crictl  is used on the node, which provides a CLI for CRI-compatible container\\nruntimes. This is an advanced example to show Pod overhead behavior, and it is not expected\\nthat users should need to check cgroups directly on the node.\\nFirst, on the particular node, determine the Pod identifier:\\n# Run this on the node where the Pod is scheduled\\nPOD_ID =\"$(sudo crictl pods --name test-pod -q )\"\\nFrom this, you can determine the cgroup path for the Pod:\\n# Run this on the node where the Pod is scheduled\\nsudo crictl inspectp -o =json $POD_ID  | grep cgroupsPath\\nThe resulting cgroup path includes the Pod\\'s pause  container. The Pod level cgroup is one\\ndirectory above.\\n  \"cgroupsPath\": \"/kubepods/podd7f4b509-cf94-4951-9417-\\nd1087c92a5b2/7ccf55aee35dd16aca4189c952d83487297f3cd760f1bbf09620e206e7d0c27a\"\\nIn this specific case, the pod cgroup path is kubepods/podd7f4b509-cf94-4951-9417-\\nd1087c92a5b2 . Verify the Pod level cgroup setting for memory:', metadata={'source': './PDFS/Concepts.pdf', 'page': 484}),\n",
       " Document(page_content='# Run this on the node where the Pod is scheduled.\\n# Also, change the name of the cgroup to match the cgroup allocated for your pod.\\n cat /sys/fs/cgroup/memory/kubepods/podd7f4b509-cf94-4951-9417-d1087c92a5b2/\\nmemory.limit_in_bytes\\nThis is 320 MiB, as expected:\\n335544320\\nObservability\\nSome kube_pod_overhead_*  metrics are available in kube-state-metrics  to help identify when\\nPod overhead is being utilized and to help observe stability of workloads running with a\\ndefined overhead.\\nWhat\\'s next\\nLearn more about RuntimeClass\\nRead the PodOverhead Design  enhancement proposal for extra context\\nPod Scheduling Readiness\\nFEATURE STATE:  Kubernetes v1.27 [beta]\\nPods were considered ready for scheduling once created. Kubernetes scheduler does its due\\ndiligence to find nodes to place all pending Pods. However, in a real-world case, some Pods may\\nstay in a \"miss-essential-resources\" state for a long period. These Pods actually churn the\\nscheduler (and downstream integrators like Cluster AutoScaler) in an unnecessary manner.\\nBy specifying/removing a Pod\\'s .spec.schedulingGates , you can control when a Pod is ready to\\nbe considered for scheduling.\\nConfiguring Pod schedulingGates\\nThe schedulingGates  field contains a list of strings, and each string literal is perceived as a\\ncriteria that Pod should be satisfied before considered schedulable. This field can be initialized\\nonly when a Pod is created (either by the client, or mutated during admission). After creation,\\neach schedulingGate can be removed in arbitrary order, but addition of a new scheduling gate is\\ndisallowed.\\npod-scheduling-gates-diagram\\nFigure. Pod SchedulingGates\\nUsage example\\nTo mark a Pod not-ready for scheduling, you can create it with one or more scheduling gates\\nlike this:\\npods/pod-with-scheduling-gates.yaml  • \\n•', metadata={'source': './PDFS/Concepts.pdf', 'page': 485}),\n",
       " Document(page_content='apiVersion : v1\\nkind: Pod\\nmetadata :\\n  name : test-pod\\nspec:\\n  schedulingGates :\\n  - name : example.com/foo\\n  - name : example.com/bar\\n  containers :\\n  - name : pause\\n    image : registry.k8s.io/pause:3.6\\nAfter the Pod\\'s creation, you can check its state using:\\nkubectl get pod test-pod\\nThe output reveals it\\'s in SchedulingGated  state:\\nNAME       READY   STATUS            RESTARTS   AGE\\ntest-pod   0/1     SchedulingGated   0          7s\\nYou can also check its schedulingGates  field by running:\\nkubectl get pod test-pod -o jsonpath =\\'{.spec.schedulingGates}\\'\\nThe output is:\\n[{\"name\":\"example.com/foo\"},{\"name\":\"example.com/bar\"}]\\nTo inform scheduler this Pod is ready for scheduling, you can remove its schedulingGates\\nentirely by re-applying a modified manifest:\\npods/pod-without-scheduling-gates.yaml  \\napiVersion : v1\\nkind: Pod\\nmetadata :\\n  name : test-pod\\nspec:\\n  containers :\\n  - name : pause\\n    image : registry.k8s.io/pause:3.6\\nYou can check if the schedulingGates  is cleared by running:\\nkubectl get pod test-pod -o jsonpath =\\'{.spec.schedulingGates}\\'\\nThe output is expected to be empty. And you can check its latest status by running:\\nkubectl get pod test-pod -o wide\\nGiven the test-pod doesn\\'t request any CPU/memory resources, it\\'s expected that this Pod\\'s\\nstate get transited from previous SchedulingGated  to Running :', metadata={'source': './PDFS/Concepts.pdf', 'page': 486}),\n",
       " Document(page_content='NAME       READY   STATUS    RESTARTS   AGE   IP         NODE  \\ntest-pod   1/1     Running   0          15s   10.0.0.4   node-2\\nObservability\\nThe metric scheduler_pending_pods  comes with a new label \"gated\"  to distinguish whether a\\nPod has been tried scheduling but claimed as unschedulable, or explicitly marked as not ready\\nfor scheduling. You can use scheduler_pending_pods{queue=\"gated\"}  to check the metric result.\\nMutable Pod Scheduling Directives\\nFEATURE STATE:  Kubernetes v1.27 [beta]\\nYou can mutate scheduling directives of Pods while they have scheduling gates, with certain\\nconstraints. At a high level, you can only tighten the scheduling directives of a Pod. In other\\nwords, the updated directives would cause the Pods to only be able to be scheduled on a subset\\nof the nodes that it would previously match. More concretely, the rules for updating a Pod\\'s\\nscheduling directives are as follows:\\nFor .spec.nodeSelector , only additions are allowed. If absent, it will be allowed to be set.\\nFor spec.affinity.nodeAffinity , if nil, then setting anything is allowed.\\nIf NodeSelectorTerms  was empty, it will be allowed to be set. If not empty, then only\\nadditions of NodeSelectorRequirements  to matchExpressions  or fieldExpressions  are\\nallowed, and no changes to existing matchExpressions  and fieldExpressions  will be\\nallowed. This is because the terms in\\n.requiredDuringSchedulingIgnoredDuringExecution.NodeSelectorTerms , are ORed while\\nthe expressions in nodeSelectorTerms[].matchExpressions  and \\nnodeSelectorTerms[].fieldExpressions  are ANDed.\\nFor .preferredDuringSchedulingIgnoredDuringExecution , all updates are allowed. This is\\nbecause preferred terms are not authoritative, and so policy controllers don\\'t validate\\nthose terms.\\nWhat\\'s next\\nRead the PodSchedulingReadiness KEP  for more details\\nPod Topology Spread Constraints\\nYou can use topology spread constraints  to control how Pods  are spread across your cluster\\namong failure-domains such as regions, zones, nodes, and other user-defined topology domains.\\nThis can help to achieve high availability as well as efficient resource utilization.\\nYou can set cluster-level constraints  as a default, or configure topology spread constraints for\\nindividual workloads.1. \\n2. \\n3. \\n4. \\n•', metadata={'source': './PDFS/Concepts.pdf', 'page': 487}),\n",
       " Document(page_content=\"Motivation\\nImagine that you have a cluster of up to twenty nodes, and you want to run a workload  that\\nautomatically scales how many replicas it uses. There could be as few as two Pods or as many\\nas fifteen. When there are only two Pods, you'd prefer not to have both of those Pods run on the\\nsame node: you would run the risk that a single node failure takes your workload offline.\\nIn addition to this basic usage, there are some advanced usage examples that enable your\\nworkloads to benefit on high availability and cluster utilization.\\nAs you scale up and run more Pods, a different concern becomes important. Imagine that you\\nhave three nodes running five Pods each. The nodes have enough capacity to run that many\\nreplicas; however, the clients that interact with this workload are split across three different\\ndatacenters (or infrastructure zones). Now you have less concern about a single node failure,\\nbut you notice that latency is higher than you'd like, and you are paying for network costs\\nassociated with sending network traffic between the different zones.\\nYou decide that under normal operation you'd prefer to have a similar number of replicas \\nscheduled  into each infrastructure zone, and you'd like the cluster to self-heal in the case that\\nthere is a problem.\\nPod topology spread constraints offer you a declarative way to configure that.\\ntopologySpreadConstraints  field\\nThe Pod API includes a field, spec.topologySpreadConstraints . The usage of this field looks like\\nthe following:\\n---\\napiVersion : v1\\nkind: Pod\\nmetadata :\\n  name : example-pod\\nspec:\\n  # Configure a topology spread constraint\\n  topologySpreadConstraints :\\n    - maxSkew : <integer>\\n      minDomains : <integer>  # optional; beta since v1.25\\n      topologyKey : <string>\\n      whenUnsatisfiable : <string>\\n      labelSelector : <object>\\n      matchLabelKeys : <list>  # optional; beta since v1.27\\n      nodeAffinityPolicy : [Honor|Ignore]  # optional; beta since v1.26\\n      nodeTaintsPolicy : [Honor|Ignore]  # optional; beta since v1.26\\n  ### other Pod fields go here\\nYou can read more about this field by running kubectl explain \\nPod.spec.topologySpreadConstraints  or refer to scheduling  section of the API reference for Pod.\", metadata={'source': './PDFS/Concepts.pdf', 'page': 488}),\n",
       " Document(page_content=\"Spread constraint definition\\nYou can define one or multiple topologySpreadConstraints  entries to instruct the kube-\\nscheduler how to place each incoming Pod in relation to the existing Pods across your cluster.\\nThose fields are:\\nmaxSkew  describes the degree to which Pods may be unevenly distributed. You must\\nspecify this field and the number must be greater than zero. Its semantics differ according\\nto the value of whenUnsatisfiable :\\nif you select whenUnsatisfiable: DoNotSchedule , then maxSkew  defines the\\nmaximum permitted difference between the number of matching pods in the target\\ntopology and the global minimum  (the minimum number of matching pods in an\\neligible domain or zero if the number of eligible domains is less than MinDomains).\\nFor example, if you have 3 zones with 2, 2 and 1 matching pods respectively, \\nMaxSkew  is set to 1 then the global minimum is 1.\\nif you select whenUnsatisfiable: ScheduleAnyway , the scheduler gives higher\\nprecedence to topologies that would help reduce the skew.\\nminDomains  indicates a minimum number of eligible domains. This field is optional. A\\ndomain is a particular instance of a topology. An eligible domain is a domain whose\\nnodes match the node selector.\\nNote:  The MinDomainsInPodTopologySpread  feature gate  enables minDomains  for pod\\ntopology spread. Starting from v1.28, the MinDomainsInPodTopologySpread  gate is\\nenabled by default. In older Kubernetes clusters it might be explicitly disabled or the field\\nmight not be available.\\nThe value of minDomains  must be greater than 0, when specified. You can only\\nspecify minDomains  in conjunction with whenUnsatisfiable: DoNotSchedule .\\nWhen the number of eligible domains with match topology keys is less than \\nminDomains , Pod topology spread treats global minimum as 0, and then the\\ncalculation of skew  is performed. The global minimum is the minimum number of\\nmatching Pods in an eligible domain, or zero if the number of eligible domains is\\nless than minDomains .\\nWhen the number of eligible domains with matching topology keys equals or is\\ngreater than minDomains , this value has no effect on scheduling.\\nIf you do not specify minDomains , the constraint behaves as if minDomains  is 1.\\ntopologyKey  is the key of node labels . Nodes that have a label with this key and\\nidentical values are considered to be in the same topology. We call each instance of a\\ntopology (in other words, a <key, value> pair) a domain. The scheduler will try to put a\\nbalanced number of pods into each domain. Also, we define an eligible domain as a\\ndomain whose nodes meet the requirements of nodeAffinityPolicy and nodeTaintsPolicy.\\nwhenUnsatisfiable  indicates how to deal with a Pod if it doesn't satisfy the spread\\nconstraint:\\nDoNotSchedule  (default) tells the scheduler not to schedule it.\\nScheduleAnyway  tells the scheduler to still schedule it while prioritizing nodes that\\nminimize the skew.\\nlabelSelector  is used to find matching Pods. Pods that match this label selector are\\ncounted to determine the number of Pods in their corresponding topology domain. See \\nLabel Selectors  for more details.• \\n◦ \\n◦ \\n• \\n◦ \\n◦ \\n◦ \\n◦ \\n• \\n• \\n◦ \\n◦ \\n•\", metadata={'source': './PDFS/Concepts.pdf', 'page': 489}),\n",
       " Document(page_content=\"matchLabelKeys  is a list of pod label keys to select the pods over which spreading will\\nbe calculated. The keys are used to lookup values from the pod labels, those key-value\\nlabels are ANDed with labelSelector  to select the group of existing pods over which\\nspreading will be calculated for the incoming pod. The same key is forbidden to exist in\\nboth matchLabelKeys  and labelSelector . matchLabelKeys  cannot be set when \\nlabelSelector  isn't set. Keys that don't exist in the pod labels will be ignored. A null or\\nempty list means only match against the labelSelector .\\nWith matchLabelKeys , you don't need to update the pod.spec  between different revisions.\\nThe controller/operator just needs to set different values to the same label key for\\ndifferent revisions. The scheduler will assume the values automatically based on \\nmatchLabelKeys . For example, if you are configuring a Deployment, you can use the label\\nkeyed with pod-template-hash , which is added automatically by the Deployment\\ncontroller, to distinguish between different revisions in a single Deployment.\\n    topologySpreadConstraints :\\n        - maxSkew : 1\\n          topologyKey : kubernetes.io/hostname\\n          whenUnsatisfiable : DoNotSchedule\\n          labelSelector :\\n            matchLabels :\\n              app: foo\\n          matchLabelKeys :\\n            - pod-template-hash\\nNote:  The matchLabelKeys  field is a beta-level field and enabled by default in 1.27. You\\ncan disable it by disabling the MatchLabelKeysInPodTopologySpread  feature gate .\\nnodeAffinityPolicy  indicates how we will treat Pod's nodeAffinity/nodeSelector when\\ncalculating pod topology spread skew. Options are:\\nHonor: only nodes matching nodeAffinity/nodeSelector are included in the\\ncalculations.\\nIgnore: nodeAffinity/nodeSelector are ignored. All nodes are included in the\\ncalculations.\\nIf this value is null, the behavior is equivalent to the Honor policy.\\nNote:  The nodeAffinityPolicy  is a beta-level field and enabled by default in 1.26. You can\\ndisable it by disabling the NodeInclusionPolicyInPodTopologySpread  feature gate .\\nnodeTaintsPolicy  indicates how we will treat node taints when calculating pod\\ntopology spread skew. Options are:\\nHonor: nodes without taints, along with tainted nodes for which the incoming pod\\nhas a toleration, are included.\\nIgnore: node taints are ignored. All nodes are included.\\nIf this value is null, the behavior is equivalent to the Ignore policy.\\nNote:  The nodeTaintsPolicy  is a beta-level field and enabled by default in 1.26. You can\\ndisable it by disabling the NodeInclusionPolicyInPodTopologySpread  feature gate .• \\n• \\n◦ \\n◦ \\n• \\n◦ \\n◦\", metadata={'source': './PDFS/Concepts.pdf', 'page': 490}),\n",
       " Document(page_content='When a Pod defines more than one topologySpreadConstraint , those constraints are combined\\nusing a logical AND operation: the kube-scheduler looks for a node for the incoming Pod that\\nsatisfies all the configured constraints.\\nNode labels\\nTopology spread constraints rely on node labels to identify the topology domain(s) that each \\nnode  is in. For example, a node might have labels:\\n  region : us-east-1\\n  zone : us-east-1a\\nNote:\\nFor brevity, this example doesn\\'t use the well-known  label keys topology.kubernetes.io/zone\\nand topology.kubernetes.io/region . However, those registered label keys are nonetheless\\nrecommended rather than the private (unqualified) label keys region  and zone  that are used\\nhere.\\nYou can\\'t make a reliable assumption about the meaning of a private label key between different\\ncontexts.\\nSuppose you have a 4-node cluster with the following labels:\\nNAME    STATUS   ROLES    AGE     VERSION   LABELS\\nnode1   Ready    <none>   4m26s   v1.16.0   node=node1,zone=zoneA\\nnode2   Ready    <none>   3m58s   v1.16.0   node=node2,zone=zoneA\\nnode3   Ready    <none>   3m17s   v1.16.0   node=node3,zone=zoneB\\nnode4   Ready    <none>   2m43s   v1.16.0   node=node4,zone=zoneB\\nThen the cluster is logically viewed as below:\\ngraph TB subgraph \"zoneB\" n3(Node3) n4(Node4) end subgraph \"zoneA\" n1(Node1)\\nn2(Node2) end classDef plain fill:#ddd,stroke:#fff,stroke-width:4px,color:#000;\\nclassDef k8s fill:#326ce5,stroke:#fff,stroke-width:4px,color:#fff; classDef cluster\\nfill:#fff,stroke:#bbb,stroke-width:2px,color:#326ce5; class n1,n2,n3,n4 k8s; class\\nzoneA,zoneB cluster;\\nJavaScript must be enabled  to view this content\\nConsistency\\nYou should set the same Pod topology spread constraints on all pods in a group.\\nUsually, if you are using a workload controller such as a Deployment, the pod template takes\\ncare of this for you. If you mix different spread constraints then Kubernetes follows the API\\ndefinition of the field; however, the behavior is more likely to become confusing and\\ntroubleshooting is less straightforward.\\nYou need a mechanism to ensure that all the nodes in a topology domain (such as a cloud\\nprovider region) are labelled consistently. To avoid you needing to manually label nodes, most\\nclusters automatically populate well-known labels such as kubernetes.io/hostname . Check\\nwhether your cluster supports this.', metadata={'source': './PDFS/Concepts.pdf', 'page': 491}),\n",
       " Document(page_content='Topology spread constraint examples\\nExample: one topology spread constraint\\nSuppose you have a 4-node cluster where 3 Pods labelled foo: bar  are located in node1, node2\\nand node3 respectively:\\ngraph BT subgraph \"zoneB\" p3(Pod) --> n3(Node3) n4(Node4) end subgraph\\n\"zoneA\" p1(Pod) --> n1(Node1) p2(Pod) --> n2(Node2) end classDef plain\\nfill:#ddd,stroke:#fff,stroke-width:4px,color:#000; classDef k8s\\nfill:#326ce5,stroke:#fff,stroke-width:4px,color:#fff; classDef cluster\\nfill:#fff,stroke:#bbb,stroke-width:2px,color:#326ce5; class n1,n2,n3,n4,p1,p2,p3 k8s;\\nclass zoneA,zoneB cluster;\\nJavaScript must be enabled  to view this content\\nIf you want an incoming Pod to be evenly spread with existing Pods across zones, you can use a\\nmanifest similar to:\\npods/topology-spread-constraints/one-constraint.yaml  \\nkind: Pod\\napiVersion : v1\\nmetadata :\\n  name : mypod\\n  labels :\\n    foo: bar\\nspec:\\n  topologySpreadConstraints :\\n  - maxSkew : 1\\n    topologyKey : zone\\n    whenUnsatisfiable : DoNotSchedule\\n    labelSelector :\\n      matchLabels :\\n        foo: bar\\n  containers :\\n  - name : pause\\n    image : registry.k8s.io/pause:3.1\\nFrom that manifest, topologyKey: zone  implies the even distribution will only be applied to\\nnodes that are labelled zone: <any value>  (nodes that don\\'t have a zone  label are skipped). The\\nfield whenUnsatisfiable: DoNotSchedule  tells the scheduler to let the incoming Pod stay\\npending if the scheduler can\\'t find a way to satisfy the constraint.\\nIf the scheduler placed this incoming Pod into zone A, the distribution of Pods would become \\n[3, 1] . That means the actual skew is then 2 (calculated as 3 - 1), which violates maxSkew: 1 . To\\nsatisfy the constraints and context for this example, the incoming Pod can only be placed onto a\\nnode in zone B:\\ngraph BT subgraph \"zoneB\" p3(Pod) --> n3(Node3) p4(mypod) --> n4(Node4) end\\nsubgraph \"zoneA\" p1(Pod) --> n1(Node1) p2(Pod) --> n2(Node2) end classDef plain\\nfill:#ddd,stroke:#fff,stroke-width:4px,color:#000; classDef k8s\\nfill:#326ce5,stroke:#fff,stroke-width:4px,color:#fff; classDef cluster', metadata={'source': './PDFS/Concepts.pdf', 'page': 492}),\n",
       " Document(page_content='fill:#fff,stroke:#bbb,stroke-width:2px,color:#326ce5; class n1,n2,n3,n4,p1,p2,p3 k8s;\\nclass p4 plain; class zoneA,zoneB cluster;\\nJavaScript must be enabled  to view this content\\nOR\\ngraph BT subgraph \"zoneB\" p3(Pod) --> n3(Node3) p4(mypod) --> n3 n4(Node4)\\nend subgraph \"zoneA\" p1(Pod) --> n1(Node1) p2(Pod) --> n2(Node2) end classDef\\nplain fill:#ddd,stroke:#fff,stroke-width:4px,color:#000; classDef k8s\\nfill:#326ce5,stroke:#fff,stroke-width:4px,color:#fff; classDef cluster\\nfill:#fff,stroke:#bbb,stroke-width:2px,color:#326ce5; class n1,n2,n3,n4,p1,p2,p3 k8s;\\nclass p4 plain; class zoneA,zoneB cluster;\\nJavaScript must be enabled  to view this content\\nYou can tweak the Pod spec to meet various kinds of requirements:\\nChange maxSkew  to a bigger value - such as 2 - so that the incoming Pod can be placed\\ninto zone A as well.\\nChange topologyKey  to node  so as to distribute the Pods evenly across nodes instead of\\nzones. In the above example, if maxSkew  remains 1, the incoming Pod can only be placed\\nonto the node node4 .\\nChange whenUnsatisfiable: DoNotSchedule  to whenUnsatisfiable: ScheduleAnyway  to\\nensure the incoming Pod to be always schedulable (suppose other scheduling APIs are\\nsatisfied). However, it\\'s preferred to be placed into the topology domain which has fewer\\nmatching Pods. (Be aware that this preference is jointly normalized with other internal\\nscheduling priorities such as resource usage ratio).\\nExample: multiple topology spread constraints\\nThis builds upon the previous example. Suppose you have a 4-node cluster where 3 existing\\nPods labeled foo: bar  are located on node1, node2 and node3 respectively:\\ngraph BT subgraph \"zoneB\" p3(Pod) --> n3(Node3) n4(Node4) end subgraph\\n\"zoneA\" p1(Pod) --> n1(Node1) p2(Pod) --> n2(Node2) end classDef plain\\nfill:#ddd,stroke:#fff,stroke-width:4px,color:#000; classDef k8s\\nfill:#326ce5,stroke:#fff,stroke-width:4px,color:#fff; classDef cluster\\nfill:#fff,stroke:#bbb,stroke-width:2px,color:#326ce5; class n1,n2,n3,n4,p1,p2,p3 k8s;\\nclass p4 plain; class zoneA,zoneB cluster;\\nJavaScript must be enabled  to view this content\\nYou can combine two topology spread constraints to control the spread of Pods both by node\\nand by zone:\\npods/topology-spread-constraints/two-constraints.yaml  \\nkind: Pod\\napiVersion : v1\\nmetadata :\\n  name : mypod\\n  labels :\\n    foo: bar• \\n• \\n•', metadata={'source': './PDFS/Concepts.pdf', 'page': 493}),\n",
       " Document(page_content='spec:\\n  topologySpreadConstraints :\\n  - maxSkew : 1\\n    topologyKey : zone\\n    whenUnsatisfiable : DoNotSchedule\\n    labelSelector :\\n      matchLabels :\\n        foo: bar\\n  - maxSkew : 1\\n    topologyKey : node\\n    whenUnsatisfiable : DoNotSchedule\\n    labelSelector :\\n      matchLabels :\\n        foo: bar\\n  containers :\\n  - name : pause\\n    image : registry.k8s.io/pause:3.1\\nIn this case, to match the first constraint, the incoming Pod can only be placed onto nodes in\\nzone B; while in terms of the second constraint, the incoming Pod can only be scheduled to the\\nnode node4 . The scheduler only considers options that satisfy all defined constraints, so the\\nonly valid placement is onto node node4 .\\nExample: conflicting topology spread constraints\\nMultiple constraints can lead to conflicts. Suppose you have a 3-node cluster across 2 zones:\\ngraph BT subgraph \"zoneB\" p4(Pod) --> n3(Node3) p5(Pod) --> n3 end subgraph\\n\"zoneA\" p1(Pod) --> n1(Node1) p2(Pod) --> n1 p3(Pod) --> n2(Node2) end classDef\\nplain fill:#ddd,stroke:#fff,stroke-width:4px,color:#000; classDef k8s\\nfill:#326ce5,stroke:#fff,stroke-width:4px,color:#fff; classDef cluster\\nfill:#fff,stroke:#bbb,stroke-width:2px,color:#326ce5; class n1,n2,n3,n4,p1,p2,p3,p4,p5\\nk8s; class zoneA,zoneB cluster;\\nJavaScript must be enabled  to view this content\\nIf you were to apply two-constraints.yaml  (the manifest from the previous example) to this\\ncluster, you would see that the Pod mypod  stays in the Pending  state. This happens because: to\\nsatisfy the first constraint, the Pod mypod  can only be placed into zone B; while in terms of the\\nsecond constraint, the Pod mypod  can only schedule to node node2 . The intersection of the two\\nconstraints returns an empty set, and the scheduler cannot place the Pod.\\nTo overcome this situation, you can either increase the value of maxSkew  or modify one of the\\nconstraints to use whenUnsatisfiable: ScheduleAnyway . Depending on circumstances, you\\nmight also decide to delete an existing Pod manually - for example, if you are troubleshooting\\nwhy a bug-fix rollout is not making progress.\\nInteraction with node affinity and node selectors\\nThe scheduler will skip the non-matching nodes from the skew calculations if the incoming Pod\\nhas spec.nodeSelector  or spec.affinity.nodeAffinity  defined.', metadata={'source': './PDFS/Concepts.pdf', 'page': 494}),\n",
       " Document(page_content='Example: topology spread constraints with node affinity\\nSuppose you have a 5-node cluster ranging across zones A to C:\\ngraph BT subgraph \"zoneB\" p3(Pod) --> n3(Node3) n4(Node4) end subgraph\\n\"zoneA\" p1(Pod) --> n1(Node1) p2(Pod) --> n2(Node2) end classDef plain\\nfill:#ddd,stroke:#fff,stroke-width:4px,color:#000; classDef k8s\\nfill:#326ce5,stroke:#fff,stroke-width:4px,color:#fff; classDef cluster\\nfill:#fff,stroke:#bbb,stroke-width:2px,color:#326ce5; class n1,n2,n3,n4,p1,p2,p3 k8s;\\nclass p4 plain; class zoneA,zoneB cluster;\\nJavaScript must be enabled  to view this content\\ngraph BT subgraph \"zoneC\" n5(Node5) end classDef plain\\nfill:#ddd,stroke:#fff,stroke-width:4px,color:#000; classDef k8s\\nfill:#326ce5,stroke:#fff,stroke-width:4px,color:#fff; classDef cluster\\nfill:#fff,stroke:#bbb,stroke-width:2px,color:#326ce5; class n5 k8s; class zoneC cluster;\\nJavaScript must be enabled  to view this content\\nand you know that zone C must be excluded. In this case, you can compose a manifest as below,\\nso that Pod mypod  will be placed into zone B instead of zone C. Similarly, Kubernetes also\\nrespects spec.nodeSelector .\\npods/topology-spread-constraints/one-constraint-with-nodeaffinity.yaml  \\nkind: Pod\\napiVersion : v1\\nmetadata :\\n  name : mypod\\n  labels :\\n    foo: bar\\nspec:\\n  topologySpreadConstraints :\\n  - maxSkew : 1\\n    topologyKey : zone\\n    whenUnsatisfiable : DoNotSchedule\\n    labelSelector :\\n      matchLabels :\\n        foo: bar\\n  affinity :\\n    nodeAffinity :\\n      requiredDuringSchedulingIgnoredDuringExecution :\\n        nodeSelectorTerms :\\n        - matchExpressions :\\n          - key: zone\\n            operator : NotIn\\n            values :\\n            - zoneC\\n  containers :\\n  - name : pause\\n    image : registry.k8s.io/pause:3.1', metadata={'source': './PDFS/Concepts.pdf', 'page': 495}),\n",
       " Document(page_content='Implicit conventions\\nThere are some implicit conventions worth noting here:\\nOnly the Pods holding the same namespace as the incoming Pod can be matching\\ncandidates.\\nThe scheduler bypasses any nodes that don\\'t have any \\ntopologySpreadConstraints[*].topologyKey  present. This implies that:\\nany Pods located on those bypassed nodes do not impact maxSkew  calculation - in\\nthe above example, suppose the node node1  does not have a label \"zone\", then the 2\\nPods will be disregarded, hence the incoming Pod will be scheduled into zone A.\\nthe incoming Pod has no chances to be scheduled onto this kind of nodes - in the\\nabove example, suppose a node node5  has the mistyped  label zone-typo: zoneC\\n(and no zone  label set). After node node5  joins the cluster, it will be bypassed and\\nPods for this workload aren\\'t scheduled there.\\nBe aware of what will happen if the incoming Pod\\'s \\ntopologySpreadConstraints[*].labelSelector  doesn\\'t match its own labels. In the above\\nexample, if you remove the incoming Pod\\'s labels, it can still be placed onto nodes in zone\\nB, since the constraints are still satisfied. However, after that placement, the degree of\\nimbalance of the cluster remains unchanged - it\\'s still zone A having 2 Pods labelled as \\nfoo: bar , and zone B having 1 Pod labelled as foo: bar . If this is not what you expect,\\nupdate the workload\\'s topologySpreadConstraints[*].labelSelector  to match the labels in\\nthe pod template.\\nCluster-level default constraints\\nIt is possible to set default topology spread constraints for a cluster. Default topology spread\\nconstraints are applied to a Pod if, and only if:\\nIt doesn\\'t define any constraints in its .spec.topologySpreadConstraints .\\nIt belongs to a Service, ReplicaSet, StatefulSet or ReplicationController.\\nDefault constraints can be set as part of the PodTopologySpread  plugin arguments in a \\nscheduling profile . The constraints are specified with the same API above , except that \\nlabelSelector  must be empty. The selectors are calculated from the Services, ReplicaSets,\\nStatefulSets or ReplicationControllers that the Pod belongs to.\\nAn example configuration might look like follows:\\napiVersion : kubescheduler.config.k8s.io/v1beta3\\nkind: KubeSchedulerConfiguration\\nprofiles :\\n  - schedulerName : default-scheduler\\n    pluginConfig :\\n      - name : PodTopologySpread\\n        args:\\n          defaultConstraints :\\n            - maxSkew : 1\\n              topologyKey : topology.kubernetes.io/zone• \\n• \\n1. \\n2. \\n• \\n• \\n•', metadata={'source': './PDFS/Concepts.pdf', 'page': 496}),\n",
       " Document(page_content='whenUnsatisfiable : ScheduleAnyway\\n          defaultingType : List\\nBuilt-in default constraints\\nFEATURE STATE:  Kubernetes v1.24 [stable]\\nIf you don\\'t configure any cluster-level default constraints for pod topology spreading, then\\nkube-scheduler acts as if you specified the following default topology constraints:\\ndefaultConstraints :\\n  - maxSkew : 3\\n    topologyKey : \"kubernetes.io/hostname\"\\n    whenUnsatisfiable : ScheduleAnyway\\n  - maxSkew : 5\\n    topologyKey : \"topology.kubernetes.io/zone\"\\n    whenUnsatisfiable : ScheduleAnyway\\nAlso, the legacy SelectorSpread  plugin, which provides an equivalent behavior, is disabled by\\ndefault.\\nNote:\\nThe PodTopologySpread  plugin does not score the nodes that don\\'t have the topology keys\\nspecified in the spreading constraints. This might result in a different default behavior\\ncompared to the legacy SelectorSpread  plugin when using the default topology constraints.\\nIf your nodes are not expected to have both  kubernetes.io/hostname  and \\ntopology.kubernetes.io/zone  labels set, define your own constraints instead of using the\\nKubernetes defaults.\\nIf you don\\'t want to use the default Pod spreading constraints for your cluster, you can disable\\nthose defaults by setting defaultingType  to List and leaving empty defaultConstraints  in the \\nPodTopologySpread  plugin configuration:\\napiVersion : kubescheduler.config.k8s.io/v1beta3\\nkind: KubeSchedulerConfiguration\\nprofiles :\\n  - schedulerName : default-scheduler\\n    pluginConfig :\\n      - name : PodTopologySpread\\n        args:\\n          defaultConstraints : []\\n          defaultingType : List\\nComparison with podAffinity and podAntiAffinity\\nIn Kubernetes, inter-Pod affinity and anti-affinity  control how Pods are scheduled in relation to\\none another - either more packed or more scattered.\\npodAffinity', metadata={'source': './PDFS/Concepts.pdf', 'page': 497}),\n",
       " Document(page_content=\"attracts Pods; you can try to pack any number of Pods into qualifying topology\\ndomain(s).\\npodAntiAffinity\\nrepels Pods. If you set this to requiredDuringSchedulingIgnoredDuringExecution  mode\\nthen only a single Pod can be scheduled into a single topology domain; if you choose \\npreferredDuringSchedulingIgnoredDuringExecution  then you lose the ability to enforce\\nthe constraint.\\nFor finer control, you can specify topology spread constraints to distribute Pods across different\\ntopology domains - to achieve either high availability or cost-saving. This can also help on\\nrolling update workloads and scaling out replicas smoothly.\\nFor more context, see the Motivation  section of the enhancement proposal about Pod topology\\nspread constraints.\\nKnown limitations\\nThere's no guarantee that the constraints remain satisfied when Pods are removed. For\\nexample, scaling down a Deployment may result in imbalanced Pods distribution.\\nYou can use a tool such as the Descheduler  to rebalance the Pods distribution.\\nPods matched on tainted nodes are respected. See Issue 80921 .\\nThe scheduler doesn't have prior knowledge of all the zones or other topology domains\\nthat a cluster has. They are determined from the existing nodes in the cluster. This could\\nlead to a problem in autoscaled clusters, when a node pool (or node group) is scaled to\\nzero nodes, and you're expecting the cluster to scale up, because, in this case, those\\ntopology domains won't be considered until there is at least one node in them.\\nYou can work around this by using an cluster autoscaling tool that is aware of Pod\\ntopology spread constraints and is also aware of the overall set of topology domains.\\nWhat's next\\nThe blog article Introducing PodTopologySpread  explains maxSkew  in some detail, as\\nwell as covering some advanced usage examples.\\nRead the scheduling  section of the API reference for Pod.\\nTaints and Tolerations\\nNode affinity  is a property of Pods  that attracts  them to a set of nodes  (either as a preference or\\na hard requirement). Taints  are the opposite -- they allow a node to repel a set of pods.\\nTolerations  are applied to pods. Tolerations allow the scheduler to schedule pods with matching\\ntaints. Tolerations allow scheduling but don't guarantee scheduling: the scheduler also \\nevaluates other parameters  as part of its function.\\nTaints and tolerations work together to ensure that pods are not scheduled onto inappropriate\\nnodes. One or more taints are applied to a node; this marks that the node should not accept any\\npods that do not tolerate the taints.• \\n• \\n• \\n• \\n•\", metadata={'source': './PDFS/Concepts.pdf', 'page': 498}),\n",
       " Document(page_content='Concepts\\nYou add a taint to a node using kubectl taint . For example,\\nkubectl taint nodes node1 key1 =value1:NoSchedule\\nplaces a taint on node node1 . The taint has key key1 , value value1 , and taint effect NoSchedule .\\nThis means that no pod will be able to schedule onto node1  unless it has a matching toleration.\\nTo remove the taint added by the command above, you can run:\\nkubectl taint nodes node1 key1 =value1:NoSchedule-\\nYou specify a toleration for a pod in the PodSpec. Both of the following tolerations \"match\" the\\ntaint created by the kubectl taint  line above, and thus a pod with either toleration would be able\\nto schedule onto node1 :\\ntolerations :\\n- key: \"key1\"\\n  operator : \"Equal\"\\n  value : \"value1\"\\n  effect : \"NoSchedule\"\\ntolerations :\\n- key: \"key1\"\\n  operator : \"Exists\"\\n  effect : \"NoSchedule\"\\nHere\\'s an example of a pod that uses tolerations:\\npods/pod-with-toleration.yaml  \\napiVersion : v1\\nkind: Pod\\nmetadata :\\n  name : nginx\\n  labels :\\n    env: test\\nspec:\\n  containers :\\n  - name : nginx\\n    image : nginx\\n    imagePullPolicy : IfNotPresent\\n  tolerations :\\n  - key: \"example-key\"\\n    operator : \"Exists\"\\n    effect : \"NoSchedule\"\\nThe default value for operator  is Equal .\\nA toleration \"matches\" a taint if the keys are the same and the effects are the same, and:\\nthe operator  is Exists  (in which case no value  should be specified), or\\nthe operator  is Equal  and the value s are equal.• \\n•', metadata={'source': './PDFS/Concepts.pdf', 'page': 499}),\n",
       " Document(page_content='Note:\\nThere are two special cases:\\nAn empty key with operator Exists  matches all keys, values and effects which means this will\\ntolerate everything.\\nAn empty effect  matches all effects with key key1 .\\nThe above example used effect  of NoSchedule . Alternatively, you can use effect  of \\nPreferNoSchedule .\\nThe allowed values for the effect  field are:\\nNoExecute\\nThis affects pods that are already running on the node as follows:\\nPods that do not tolerate the taint are evicted immediately\\nPods that tolerate the taint without specifying tolerationSeconds  in their toleration\\nspecification remain bound forever\\nPods that tolerate the taint with a specified tolerationSeconds  remain bound for the\\nspecified amount of time. After that time elapses, the node lifecycle controller\\nevicts the Pods from the node.\\nNoSchedule\\nNo new Pods will be scheduled on the tainted node unless they have a matching\\ntoleration. Pods currently running on the node are not evicted.\\nPreferNoSchedule\\nPreferNoSchedule  is a \"preference\" or \"soft\" version of NoSchedule . The control plane\\nwill try to avoid placing a Pod that does not tolerate the taint on the node, but it is not\\nguaranteed.\\nYou can put multiple taints on the same node and multiple tolerations on the same pod. The\\nway Kubernetes processes multiple taints and tolerations is like a filter: start with all of a node\\'s\\ntaints, then ignore the ones for which the pod has a matching toleration; the remaining un-\\nignored taints have the indicated effects on the pod. In particular,\\nif there is at least one un-ignored taint with effect NoSchedule  then Kubernetes will not\\nschedule the pod onto that node\\nif there is no un-ignored taint with effect NoSchedule  but there is at least one un-ignored\\ntaint with effect PreferNoSchedule  then Kubernetes will try to not schedule the pod onto\\nthe node\\nif there is at least one un-ignored taint with effect NoExecute  then the pod will be evicted\\nfrom the node (if it is already running on the node), and will not be scheduled onto the\\nnode (if it is not yet running on the node).\\nFor example, imagine you taint a node like this\\nkubectl taint nodes node1 key1 =value1:NoSchedule\\nkubectl taint nodes node1 key1 =value1:NoExecute\\nkubectl taint nodes node1 key2 =value2:NoSchedule\\nAnd a pod has two tolerations:\\ntolerations :\\n- key: \"key1\"• \\n• \\n• \\n• \\n• \\n•', metadata={'source': './PDFS/Concepts.pdf', 'page': 500}),\n",
       " Document(page_content='operator : \"Equal\"\\n  value : \"value1\"\\n  effect : \"NoSchedule\"\\n- key: \"key1\"\\n  operator : \"Equal\"\\n  value : \"value1\"\\n  effect : \"NoExecute\"\\nIn this case, the pod will not be able to schedule onto the node, because there is no toleration\\nmatching the third taint. But it will be able to continue running if it is already running on the\\nnode when the taint is added, because the third taint is the only one of the three that is not\\ntolerated by the pod.\\nNormally, if a taint with effect NoExecute  is added to a node, then any pods that do not tolerate\\nthe taint will be evicted immediately, and pods that do tolerate the taint will never be evicted.\\nHowever, a toleration with NoExecute  effect can specify an optional tolerationSeconds  field\\nthat dictates how long the pod will stay bound to the node after the taint is added. For example,\\ntolerations :\\n- key: \"key1\"\\n  operator : \"Equal\"\\n  value : \"value1\"\\n  effect : \"NoExecute\"\\n  tolerationSeconds : 3600\\nmeans that if this pod is running and a matching taint is added to the node, then the pod will\\nstay bound to the node for 3600 seconds, and then be evicted. If the taint is removed before that\\ntime, the pod will not be evicted.\\nExample Use Cases\\nTaints and tolerations are a flexible way to steer pods away  from nodes or evict pods that\\nshouldn\\'t be running. A few of the use cases are\\nDedicated Nodes : If you want to dedicate a set of nodes for exclusive use by a particular\\nset of users, you can add a taint to those nodes (say, kubectl taint nodes nodename \\ndedicated=groupName:NoSchedule ) and then add a corresponding toleration to their\\npods (this would be done most easily by writing a custom admission controller ). The pods\\nwith the tolerations will then be allowed to use the tainted (dedicated) nodes as well as\\nany other nodes in the cluster. If you want to dedicate the nodes to them and ensure they \\nonly use the dedicated nodes, then you should additionally add a label similar to the taint\\nto the same set of nodes (e.g. dedicated=groupName ), and the admission controller should\\nadditionally add a node affinity to require that the pods can only schedule onto nodes\\nlabeled with dedicated=groupName .\\nNodes with Special Hardware : In a cluster where a small subset of nodes have\\nspecialized hardware (for example GPUs), it is desirable to keep pods that don\\'t need the\\nspecialized hardware off of those nodes, thus leaving room for later-arriving pods that do\\nneed the specialized hardware. This can be done by tainting the nodes that have the\\nspecialized hardware (e.g. kubectl taint nodes nodename special=true:NoSchedule  or \\nkubectl taint nodes nodename special=true:PreferNoSchedule ) and adding a\\ncorresponding toleration to pods that use the special hardware. As in the dedicated nodes\\nuse case, it is probably easiest to apply the tolerations using a custom admission• \\n•', metadata={'source': './PDFS/Concepts.pdf', 'page': 501}),\n",
       " Document(page_content='controller . For example, it is recommended to use Extended Resources  to represent the\\nspecial hardware, taint your special hardware nodes with the extended resource name\\nand run the ExtendedResourceToleration  admission controller. Now, because the nodes\\nare tainted, no pods without the toleration will schedule on them. But when you submit a\\npod that requests the extended resource, the ExtendedResourceToleration  admission\\ncontroller will automatically add the correct toleration to the pod and that pod will\\nschedule on the special hardware nodes. This will make sure that these special hardware\\nnodes are dedicated for pods requesting such hardware and you don\\'t have to manually\\nadd tolerations to your pods.\\nTaint based Evictions : A per-pod-configurable eviction behavior when there are node\\nproblems, which is described in the next section.\\nTaint based Evictions\\nFEATURE STATE:  Kubernetes v1.18 [stable]\\nThe node controller automatically taints a Node when certain conditions are true. The following\\ntaints are built in:\\nnode.kubernetes.io/not-ready : Node is not ready. This corresponds to the NodeCondition \\nReady  being \" False \".\\nnode.kubernetes.io/unreachable : Node is unreachable from the node controller. This\\ncorresponds to the NodeCondition Ready  being \" Unknown \".\\nnode.kubernetes.io/memory-pressure : Node has memory pressure.\\nnode.kubernetes.io/disk-pressure : Node has disk pressure.\\nnode.kubernetes.io/pid-pressure : Node has PID pressure.\\nnode.kubernetes.io/network-unavailable : Node\\'s network is unavailable.\\nnode.kubernetes.io/unschedulable : Node is unschedulable.\\nnode.cloudprovider.kubernetes.io/uninitialized : When the kubelet is started with\\n\"external\" cloud provider, this taint is set on a node to mark it as unusable. After a\\ncontroller from the cloud-controller-manager initializes this node, the kubelet removes\\nthis taint.\\nIn case a node is to be drained, the node controller or the kubelet adds relevant taints with \\nNoExecute  effect. This effect is added by default for the node.kubernetes.io/not-ready  and \\nnode.kubernetes.io/unreachable  taints. If the fault condition returns to normal, the kubelet or\\nnode controller can remove the relevant taint(s).\\nIn some cases when the node is unreachable, the API server is unable to communicate with the\\nkubelet on the node. The decision to delete the pods cannot be communicated to the kubelet\\nuntil communication with the API server is re-established. In the meantime, the pods that are\\nscheduled for deletion may continue to run on the partitioned node.\\nNote:  The control plane limits the rate of adding new taints to nodes. This rate limiting\\nmanages the number of evictions that are triggered when many nodes become unreachable at\\nonce (for example: if there is a network disruption).\\nYou can specify tolerationSeconds  for a Pod to define how long that Pod stays bound to a\\nfailing or unresponsive Node.• \\n• \\n• \\n• \\n• \\n• \\n• \\n• \\n•', metadata={'source': './PDFS/Concepts.pdf', 'page': 502}),\n",
       " Document(page_content='For example, you might want to keep an application with a lot of local state bound to node for a\\nlong time in the event of network partition, hoping that the partition will recover and thus the\\npod eviction can be avoided. The toleration you set for that Pod might look like:\\ntolerations :\\n- key: \"node.kubernetes.io/unreachable\"\\n  operator : \"Exists\"\\n  effect : \"NoExecute\"\\n  tolerationSeconds : 6000\\nNote:\\nKubernetes automatically adds a toleration for node.kubernetes.io/not-ready  and \\nnode.kubernetes.io/unreachable  with tolerationSeconds=300 , unless you, or a controller, set\\nthose tolerations explicitly.\\nThese automatically-added tolerations mean that Pods remain bound to Nodes for 5 minutes\\nafter one of these problems is detected.\\nDaemonSet  pods are created with NoExecute  tolerations for the following taints with no \\ntolerationSeconds :\\nnode.kubernetes.io/unreachable\\nnode.kubernetes.io/not-ready\\nThis ensures that DaemonSet pods are never evicted due to these problems.\\nTaint Nodes by Condition\\nThe control plane, using the node controller , automatically creates taints with a NoSchedule\\neffect for node conditions .\\nThe scheduler checks taints, not node conditions, when it makes scheduling decisions. This\\nensures that node conditions don\\'t directly affect scheduling. For example, if the DiskPressure\\nnode condition is active, the control plane adds the node.kubernetes.io/disk-pressure  taint and\\ndoes not schedule new pods onto the affected node. If the MemoryPressure  node condition is\\nactive, the control plane adds the node.kubernetes.io/memory-pressure  taint.\\nYou can ignore node conditions for newly created pods by adding the corresponding Pod\\ntolerations. The control plane also adds the node.kubernetes.io/memory-pressure  toleration on\\npods that have a QoS class  other than BestEffort . This is because Kubernetes treats pods in the \\nGuaranteed  or Burstable  QoS classes (even pods with no memory request set) as if they are able\\nto cope with memory pressure, while new BestEffort  pods are not scheduled onto the affected\\nnode.\\nThe DaemonSet controller automatically adds the following NoSchedule  tolerations to all\\ndaemons, to prevent DaemonSets from breaking.\\nnode.kubernetes.io/memory-pressure\\nnode.kubernetes.io/disk-pressure\\nnode.kubernetes.io/pid-pressure  (1.14 or later)\\nnode.kubernetes.io/unschedulable  (1.10 or later)\\nnode.kubernetes.io/network-unavailable  (host network only )• \\n• \\n• \\n• \\n• \\n• \\n•', metadata={'source': './PDFS/Concepts.pdf', 'page': 503}),\n",
       " Document(page_content='Adding these tolerations ensures backward compatibility. You can also add arbitrary tolerations\\nto DaemonSets.\\nWhat\\'s next\\nRead about Node-pressure Eviction  and how you can configure it\\nRead about Pod Priority\\nScheduling Framework\\nFEATURE STATE:  Kubernetes v1.19 [stable]\\nThe scheduling framework is a pluggable architecture for the Kubernetes scheduler. It adds a\\nnew set of \"plugin\" APIs to the existing scheduler. Plugins are compiled into the scheduler. The\\nAPIs allow most scheduling features to be implemented as plugins, while keeping the\\nscheduling \"core\" lightweight and maintainable. Refer to the design proposal of the scheduling\\nframework  for more technical information on the design of the framework.\\nFramework workflow\\nThe Scheduling Framework defines a few extension points. Scheduler plugins register to be\\ninvoked at one or more extension points. Some of these plugins can change the scheduling\\ndecisions and some are informational only.\\nEach attempt to schedule one Pod is split into two phases, the scheduling cycle  and the \\nbinding cycle .\\nScheduling Cycle & Binding Cycle\\nThe scheduling cycle selects a node for the Pod, and the binding cycle applies that decision to\\nthe cluster. Together, a scheduling cycle and binding cycle are referred to as a \"scheduling\\ncontext\".\\nScheduling cycles are run serially, while binding cycles may run concurrently.\\nA scheduling or binding cycle can be aborted if the Pod is determined to be unschedulable or if\\nthere is an internal error. The Pod will be returned to the queue and retried.\\nInterfaces\\nThe following picture shows the scheduling context of a Pod and the interfaces that the\\nscheduling framework exposes.\\nOne plugin may implement multiple interfaces to perform more complex or stateful tasks.\\nSome interfaces match the scheduler extension points which can be configured through \\nScheduler Configuration .• \\n•', metadata={'source': './PDFS/Concepts.pdf', 'page': 504}),\n",
       " Document(page_content=\"Scheduling framework extension points\\nPreEnqueue\\nThese plugins are called prior to adding Pods to the internal active queue, where Pods are\\nmarked as ready for scheduling.\\nOnly when all PreEnqueue plugins return Success , the Pod is allowed to enter the active queue.\\nOtherwise, it's placed in the internal unschedulable Pods list, and doesn't get an Unschedulable\\ncondition.\\nFor more details about how internal scheduler queues work, read Scheduling queue in kube-\\nscheduler .\\nEnqueueExtension\\nEnqueueExtension is the interface where the plugin can control whether to retry scheduling of\\nPods rejected by this plugin, based on changes in the cluster. Plugins that implement\\nPreEnqueue, PreFilter, Filter, Reserve or Permit should implement this interface.\\nQueueingHint\\nFEATURE STATE:  Kubernetes v1.28 [beta]\\nQueueingHint is a callback function for deciding whether a Pod can be requeued to the active\\nqueue or backoff queue. It's executed every time a certain kind of event or change happens in\\nthe cluster. When the QueueingHint finds that the event might make the Pod schedulable, the\\nPod is put into the active queue or the backoff queue so that the scheduler will retry the\\nscheduling of the Pod.\\nNote:  QueueingHint evaluation during scheduling is a beta-level feature and is enabled by\\ndefault in 1.28. You can disable it via the SchedulerQueueingHints  feature gate .\\nQueueSort\\nThese plugins are used to sort Pods in the scheduling queue. A queue sort plugin essentially\\nprovides a Less(Pod1, Pod2)  function. Only one queue sort plugin may be enabled at a time.\\nPreFilter\\nThese plugins are used to pre-process info about the Pod, or to check certain conditions that the\\ncluster or the Pod must meet. If a PreFilter plugin returns an error, the scheduling cycle is\\naborted.\\nFilter\\nThese plugins are used to filter out nodes that cannot run the Pod. For each node, the scheduler\\nwill call filter plugins in their configured order. If any filter plugin marks the node as infeasible,\\nthe remaining plugins will not be called for that node. Nodes may be evaluated concurrently.\", metadata={'source': './PDFS/Concepts.pdf', 'page': 505}),\n",
       " Document(page_content='PostFilter\\nThese plugins are called after Filter phase, but only when no feasible nodes were found for the\\npod. Plugins are called in their configured order. If any postFilter plugin marks the node as \\nSchedulable , the remaining plugins will not be called. A typical PostFilter implementation is\\npreemption, which tries to make the pod schedulable by preempting other Pods.\\nPreScore\\nThese plugins are used to perform \"pre-scoring\" work, which generates a sharable state for\\nScore plugins to use. If a PreScore plugin returns an error, the scheduling cycle is aborted.\\nScore\\nThese plugins are used to rank nodes that have passed the filtering phase. The scheduler will\\ncall each scoring plugin for each node. There will be a well defined range of integers\\nrepresenting the minimum and maximum scores. After the NormalizeScore  phase, the scheduler\\nwill combine node scores from all plugins according to the configured plugin weights.\\nNormalizeScore\\nThese plugins are used to modify scores before the scheduler computes a final ranking of\\nNodes. A plugin that registers for this extension point will be called with the Score  results from\\nthe same plugin. This is called once per plugin per scheduling cycle.\\nFor example, suppose a plugin BlinkingLightScorer  ranks Nodes based on how many blinking\\nlights they have.\\nfunc ScoreNode (_ *v1.pod, n *v1.Node) ( int, error ) {\\n    return  getBlinkingLightCount (n)\\n}\\nHowever, the maximum count of blinking lights may be small compared to NodeScoreMax . To\\nfix this, BlinkingLightScorer  should also register for this extension point.\\nfunc NormalizeScores (scores map[string ]int) {\\n    highest := 0\\n    for _, score := range  scores {\\n        highest = max(highest, score)\\n    }\\n    for node, score := range  scores {\\n        scores[node] = score *NodeScoreMax /highest\\n    }\\n}\\nIf any NormalizeScore plugin returns an error, the scheduling cycle is aborted.\\nNote:  Plugins wishing to perform \"pre-reserve\" work should use the NormalizeScore extension\\npoint.', metadata={'source': './PDFS/Concepts.pdf', 'page': 506}),\n",
       " Document(page_content='Reserve\\nA plugin that implements the Reserve interface has two methods, namely Reserve  and \\nUnreserve , that back two informational scheduling phases called Reserve and Unreserve,\\nrespectively. Plugins which maintain runtime state (aka \"stateful plugins\") should use these\\nphases to be notified by the scheduler when resources on a node are being reserved and\\nunreserved for a given Pod.\\nThe Reserve phase happens before the scheduler actually binds a Pod to its designated node. It\\nexists to prevent race conditions while the scheduler waits for the bind to succeed. The Reserve\\nmethod of each Reserve plugin may succeed or fail; if one Reserve  method call fails, subsequent\\nplugins are not executed and the Reserve phase is considered to have failed. If the Reserve\\nmethod of all plugins succeed, the Reserve phase is considered to be successful and the rest of\\nthe scheduling cycle and the binding cycle are executed.\\nThe Unreserve phase is triggered if the Reserve phase or a later phase fails. When this happens,\\nthe Unreserve  method of all Reserve plugins will be executed in the reverse order of Reserve\\nmethod calls. This phase exists to clean up the state associated with the reserved Pod.\\nCaution:  The implementation of the Unreserve  method in Reserve plugins must be idempotent\\nand may not fail.\\nPermit\\nPermit  plugins are invoked at the end of the scheduling cycle for each Pod, to prevent or delay\\nthe binding to the candidate node. A permit plugin can do one of the three things:\\napprove\\nOnce all Permit plugins approve a Pod, it is sent for binding.\\ndeny\\nIf any Permit plugin denies a Pod, it is returned to the scheduling queue. This will trigger\\nthe Unreserve phase in Reserve plugins .\\nwait  (with a timeout)\\nIf a Permit plugin returns \"wait\", then the Pod is kept in an internal \"waiting\" Pods list,\\nand the binding cycle of this Pod starts but directly blocks until it gets approved. If a\\ntimeout occurs, wait  becomes deny  and the Pod is returned to the scheduling queue,\\ntriggering the Unreserve phase in Reserve plugins .\\nNote:  While any plugin can access the list of \"waiting\" Pods and approve them (see \\nFrameworkHandle ), we expect only the permit plugins to approve binding of reserved Pods that\\nare in \"waiting\" state. Once a Pod is approved, it is sent to the PreBind  phase.\\nPreBind\\nThese plugins are used to perform any work required before a Pod is bound. For example, a pre-\\nbind plugin may provision a network volume and mount it on the target node before allowing\\nthe Pod to run there.\\nIf any PreBind plugin returns an error, the Pod is rejected  and returned to the scheduling queue.1. \\n2. \\n3.', metadata={'source': './PDFS/Concepts.pdf', 'page': 507}),\n",
       " Document(page_content='Bind\\nThese plugins are used to bind a Pod to a Node. Bind plugins will not be called until all PreBind\\nplugins have completed. Each bind plugin is called in the configured order. A bind plugin may\\nchoose whether or not to handle the given Pod. If a bind plugin chooses to handle a Pod, the\\nremaining bind plugins are skipped .\\nPostBind\\nThis is an informational interface. Post-bind plugins are called after a Pod is successfully bound.\\nThis is the end of a binding cycle, and can be used to clean up associated resources.\\nPlugin API\\nThere are two steps to the plugin API. First, plugins must register and get configured, then they\\nuse the extension point interfaces. Extension point interfaces have the following form.\\ntype Plugin interface  {\\n    Name () string\\n}\\ntype QueueSortPlugin interface  {\\n    Plugin\\n    Less(*v1.pod, *v1.pod) bool\\n}\\ntype PreFilterPlugin interface  {\\n    Plugin\\n    PreFilter (context.Context, *framework.CycleState, *v1.pod) error\\n}\\n// ...\\nPlugin configuration\\nYou can enable or disable plugins in the scheduler configuration. If you are using Kubernetes\\nv1.18 or later, most scheduling plugins  are in use and enabled by default.\\nIn addition to default plugins, you can also implement your own scheduling plugins and get\\nthem configured along with default plugins. You can visit scheduler-plugins  for more details.\\nIf you are using Kubernetes v1.18 or later, you can configure a set of plugins as a scheduler\\nprofile and then define multiple profiles to fit various kinds of workload. Learn more at multiple\\nprofiles .\\nDynamic Resource Allocation\\nFEATURE STATE:  Kubernetes v1.27 [alpha]', metadata={'source': './PDFS/Concepts.pdf', 'page': 508}),\n",
       " Document(page_content='Dynamic resource allocation is an API for requesting and sharing resources between pods and\\ncontainers inside a pod. It is a generalization of the persistent volumes API for generic\\nresources. Third-party resource drivers are responsible for tracking and allocating resources.\\nDifferent kinds of resources support arbitrary parameters for defining requirements and\\ninitialization.\\nBefore you begin\\nKubernetes v1.28 includes cluster-level API support for dynamic resource allocation, but it \\nneeds to be enabled  explicitly. You also must install a resource driver for specific resources that\\nare meant to be managed using this API. If you are not running Kubernetes v1.28, check the\\ndocumentation for that version of Kubernetes.\\nAPI\\nThe resource.k8s.io/v1alpha2  API group  provides four types:\\nResourceClass\\nDefines which resource driver handles a certain kind of resource and provides common\\nparameters for it. ResourceClasses are created by a cluster administrator when installing\\na resource driver.\\nResourceClaim\\nDefines a particular resource instances that is required by a workload. Created by a user\\n(lifecycle managed manually, can be shared between different Pods) or for individual\\nPods by the control plane based on a ResourceClaimTemplate (automatic lifecycle,\\ntypically used by just one Pod).\\nResourceClaimTemplate\\nDefines the spec and some meta data for creating ResourceClaims. Created by a user\\nwhen deploying a workload.\\nPodSchedulingContext\\nUsed internally by the control plane and resource drivers to coordinate pod scheduling\\nwhen ResourceClaims need to be allocated for a Pod.\\nParameters for ResourceClass and ResourceClaim are stored in separate objects, typically using\\nthe type defined by a CRD  that was created when installing a resource driver.\\nThe core/v1  PodSpec  defines ResourceClaims that are needed for a Pod in a resourceClaims\\nfield. Entries in that list reference either a ResourceClaim or a ResourceClaimTemplate. When\\nreferencing a ResourceClaim, all Pods using this PodSpec (for example, inside a Deployment or\\nStatefulSet) share the same ResourceClaim instance. When referencing a\\nResourceClaimTemplate, each Pod gets its own instance.\\nThe resources.claims  list for container resources defines whether a container gets access to\\nthese resource instances, which makes it possible to share resources between one or more\\ncontainers.\\nHere is an example for a fictional resource driver. Two ResourceClaim objects will get created\\nfor this Pod and each container gets access to one of them.\\napiVersion : resource.k8s.io/v1alpha2\\nkind: ResourceClass\\nname : resource.example.com', metadata={'source': './PDFS/Concepts.pdf', 'page': 509}),\n",
       " Document(page_content='driverName : resource-driver.example.com\\n---\\napiVersion : cats.resource.example.com/v1\\nkind: ClaimParameters\\nname : large-black-cat-claim-parameters\\nspec:\\n  color : black\\n  size: large\\n---\\napiVersion : resource.k8s.io/v1alpha2\\nkind: ResourceClaimTemplate\\nmetadata :\\n  name : large-black-cat-claim-template\\nspec:\\n  spec:\\n    resourceClassName : resource.example.com\\n    parametersRef :\\n      apiGroup : cats.resource.example.com\\n      kind: ClaimParameters\\n      name : large-black-cat-claim-parameters\\n–--\\napiVersion : v1\\nkind: Pod\\nmetadata :\\n  name : pod-with-cats\\nspec:\\n  containers :\\n  - name : container0\\n    image : ubuntu:20.04\\n    command : [\"sleep\" , \"9999\" ]\\n    resources :\\n      claims :\\n      - name : cat-0\\n  - name : container1\\n    image : ubuntu:20.04\\n    command : [\"sleep\" , \"9999\" ]\\n    resources :\\n      claims :\\n      - name : cat-1\\n  resourceClaims :\\n  - name : cat-0\\n    source :\\n      resourceClaimTemplateName : large-black-cat-claim-template\\n  - name : cat-1\\n    source :\\n      resourceClaimTemplateName : large-black-cat-claim-template\\nScheduling\\nIn contrast to native resources (CPU, RAM) and extended resources (managed by a device\\nplugin, advertised by kubelet), the scheduler has no knowledge of what dynamic resources are\\navailable in a cluster or how they could be split up to satisfy the requirements of a specific', metadata={'source': './PDFS/Concepts.pdf', 'page': 510}),\n",
       " Document(page_content='ResourceClaim. Resource drivers are responsible for that. They mark ResourceClaims as\\n\"allocated\" once resources for it are reserved. This also then tells the scheduler where in the\\ncluster a ResourceClaim is available.\\nResourceClaims can get allocated as soon as they are created (\"immediate allocation\"), without\\nconsidering which Pods will use them. The default is to delay allocation until a Pod gets\\nscheduled which needs the ResourceClaim (i.e. \"wait for first consumer\").\\nIn that mode, the scheduler checks all ResourceClaims needed by a Pod and creates a\\nPodScheduling object where it informs the resource drivers responsible for those\\nResourceClaims about nodes that the scheduler considers suitable for the Pod. The resource\\ndrivers respond by excluding nodes that don\\'t have enough of the driver\\'s resources left. Once\\nthe scheduler has that information, it selects one node and stores that choice in the\\nPodScheduling object. The resource drivers then allocate their ResourceClaims so that the\\nresources will be available on that node. Once that is complete, the Pod gets scheduled.\\nAs part of this process, ResourceClaims also get reserved for the Pod. Currently ResourceClaims\\ncan either be used exclusively by a single Pod or an unlimited number of Pods.\\nOne key feature is that Pods do not get scheduled to a node unless all of their resources are\\nallocated and reserved. This avoids the scenario where a Pod gets scheduled onto one node and\\nthen cannot run there, which is bad because such a pending Pod also blocks all other resources\\nlike RAM or CPU that were set aside for it.\\nMonitoring resources\\nThe kubelet provides a gRPC service to enable discovery of dynamic resources of running Pods.\\nFor more information on the gRPC endpoints, see the resource allocation reporting .\\nPre-scheduled Pods\\nWhen you - or another API client - create a Pod with spec.nodeName  already set, the scheduler\\ngets bypassed. If some ResourceClaim needed by that Pod does not exist yet, is not allocated or\\nnot reserved for the Pod, then the kubelet will fail to run the Pod and re-check periodically\\nbecause those requirements might still get fulfilled later.\\nSuch a situation can also arise when support for dynamic resource allocation was not enabled\\nin the scheduler at the time when the Pod got scheduled (version skew, configuration, feature\\ngate, etc.). kube-controller-manager detects this and tries to make the Pod runnable by\\ntriggering allocation and/or reserving the required ResourceClaims.\\nHowever, it is better to avoid this because a Pod that is assigned to a node blocks normal\\nresources (RAM, CPU) that then cannot be used for other Pods while the Pod is stuck. To make\\na Pod run on a specific node while still going through the normal scheduling flow, create the\\nPod with a node selector that exactly matches the desired node:\\napiVersion : v1\\nkind: Pod\\nmetadata :\\n  name : pod-with-cats\\nspec:\\n  nodeSelector :', metadata={'source': './PDFS/Concepts.pdf', 'page': 511}),\n",
       " Document(page_content='kubernetes.io/hostname : name-of-the-intended-node\\n  ...\\nYou may also be able to mutate the incoming Pod, at admission time, to unset the\\n.spec.nodeName  field and to use a node selector instead.\\nEnabling dynamic resource allocation\\nDynamic resource allocation is an alpha feature  and only enabled when the \\nDynamicResourceAllocation  feature gate  and the resource.k8s.io/v1alpha2  API group  are\\nenabled. For details on that, see the --feature-gates  and --runtime-config  kube-apiserver\\nparameters . kube-scheduler, kube-controller-manager and kubelet also need the feature gate.\\nA quick check whether a Kubernetes cluster supports the feature is to list ResourceClass objects\\nwith:\\nkubectl get resourceclasses\\nIf your cluster supports dynamic resource allocation, the response is either a list of\\nResourceClass objects or:\\nNo resources found\\nIf not supported, this error is printed instead:\\nerror: the server doesn\\'t have a resource type \"resourceclasses\"\\nThe default configuration of kube-scheduler enables the \"DynamicResources\" plugin if and only\\nif the feature gate is enabled and when using the v1 configuration API. Custom configurations\\nmay have to be modified to include it.\\nIn addition to enabling the feature in the cluster, a resource driver also has to be installed.\\nPlease refer to the driver\\'s documentation for details.\\nWhat\\'s next\\nFor more information on the design, see the Dynamic Resource Allocation KEP .\\nScheduler Performance Tuning\\nFEATURE STATE:  Kubernetes v1.14 [beta]\\nkube-scheduler  is the Kubernetes default scheduler. It is responsible for placement of Pods on\\nNodes in a cluster.\\nNodes in a cluster that meet the scheduling requirements of a Pod are called feasible  Nodes for\\nthe Pod. The scheduler finds feasible Nodes for a Pod and then runs a set of functions to score\\nthe feasible Nodes, picking a Node with the highest score among the feasible ones to run the\\nPod. The scheduler then notifies the API server about this decision in a process called Binding .\\nThis page explains performance tuning optimizations that are relevant for large Kubernetes\\nclusters.•', metadata={'source': './PDFS/Concepts.pdf', 'page': 512}),\n",
       " Document(page_content=\"In large clusters, you can tune the scheduler's behaviour balancing scheduling outcomes\\nbetween latency (new Pods are placed quickly) and accuracy (the scheduler rarely makes poor\\nplacement decisions).\\nYou configure this tuning setting via kube-scheduler setting percentageOfNodesToScore . This\\nKubeSchedulerConfiguration setting determines a threshold for scheduling nodes in your\\ncluster.\\nSetting the threshold\\nThe percentageOfNodesToScore  option accepts whole numeric values between 0 and 100. The\\nvalue 0 is a special number which indicates that the kube-scheduler should use its compiled-in\\ndefault. If you set percentageOfNodesToScore  above 100, kube-scheduler acts as if you had set a\\nvalue of 100.\\nTo change the value, edit the kube-scheduler configuration file  and then restart the scheduler.\\nIn many cases, the configuration file can be found at /etc/kubernetes/config/kube-\\nscheduler.yaml .\\nAfter you have made this change, you can run\\nkubectl get pods -n kube-system | grep kube-scheduler\\nto verify that the kube-scheduler component is healthy.\\nNode scoring threshold\\nTo improve scheduling performance, the kube-scheduler can stop looking for feasible nodes\\nonce it has found enough of them. In large clusters, this saves time compared to a naive\\napproach that would consider every node.\\nYou specify a threshold for how many nodes are enough, as a whole number percentage of all\\nthe nodes in your cluster. The kube-scheduler converts this into an integer number of nodes.\\nDuring scheduling, if the kube-scheduler has identified enough feasible nodes to exceed the\\nconfigured percentage, the kube-scheduler stops searching for more feasible nodes and moves\\non to the scoring phase .\\nHow the scheduler iterates over Nodes  describes the process in detail.\\nDefault threshold\\nIf you don't specify a threshold, Kubernetes calculates a figure using a linear formula that yields\\n50% for a 100-node cluster and yields 10% for a 5000-node cluster. The lower bound for the\\nautomatic value is 5%.\\nThis means that, the kube-scheduler always scores at least 5% of your cluster no matter how\\nlarge the cluster is, unless you have explicitly set percentageOfNodesToScore  to be smaller than\\n5.\\nIf you want the scheduler to score all nodes in your cluster, set percentageOfNodesToScore  to\\n100.\", metadata={'source': './PDFS/Concepts.pdf', 'page': 513}),\n",
       " Document(page_content=\"Example\\nBelow is an example configuration that sets percentageOfNodesToScore  to 50%.\\napiVersion : kubescheduler.config.k8s.io/v1alpha1\\nkind: KubeSchedulerConfiguration\\nalgorithmSource :\\n  provider : DefaultProvider\\n...\\npercentageOfNodesToScore : 50\\nTuning percentageOfNodesToScore\\npercentageOfNodesToScore  must be a value between 1 and 100 with the default value being\\ncalculated based on the cluster size. There is also a hardcoded minimum value of 50 nodes.\\nNote:\\nIn clusters with less than 50 feasible nodes, the scheduler still checks all the nodes because\\nthere are not enough feasible nodes to stop the scheduler's search early.\\nIn a small cluster, if you set a low value for percentageOfNodesToScore , your change will have\\nno or little effect, for a similar reason.\\nIf your cluster has several hundred Nodes or fewer, leave this configuration option at its default\\nvalue. Making changes is unlikely to improve the scheduler's performance significantly.\\nAn important detail to consider when setting this value is that when a smaller number of nodes\\nin a cluster are checked for feasibility, some nodes are not sent to be scored for a given Pod. As\\na result, a Node which could possibly score a higher value for running the given Pod might not\\neven be passed to the scoring phase. This would result in a less than ideal placement of the Pod.\\nYou should avoid setting percentageOfNodesToScore  very low so that kube-scheduler does not\\nmake frequent, poor Pod placement decisions. Avoid setting the percentage to anything below\\n10%, unless the scheduler's throughput is critical for your application and the score of nodes is\\nnot important. In other words, you prefer to run the Pod on any Node as long as it is feasible.\\nHow the scheduler iterates over Nodes\\nThis section is intended for those who want to understand the internal details of this feature.\\nIn order to give all the Nodes in a cluster a fair chance of being considered for running Pods,\\nthe scheduler iterates over the nodes in a round robin fashion. You can imagine that Nodes are\\nin an array. The scheduler starts from the start of the array and checks feasibility of the nodes\\nuntil it finds enough Nodes as specified by percentageOfNodesToScore . For the next Pod, the\\nscheduler continues from the point in the Node array that it stopped at when checking\\nfeasibility of Nodes for the previous Pod.\", metadata={'source': './PDFS/Concepts.pdf', 'page': 514}),\n",
       " Document(page_content=\"If Nodes are in multiple zones, the scheduler iterates over Nodes in various zones to ensure that\\nNodes from different zones are considered in the feasibility checks. As an example, consider six\\nnodes in two zones:\\nZone 1: Node 1, Node 2, Node 3, Node 4\\nZone 2: Node 5, Node 6\\nThe Scheduler evaluates feasibility of the nodes in this order:\\nNode 1, Node 5, Node 2, Node 6, Node 3, Node 4\\nAfter going over all the Nodes, it goes back to Node 1.\\nWhat's next\\nCheck the kube-scheduler configuration reference (v1beta3)\\nResource Bin Packing\\nIn the scheduling-plugin  NodeResourcesFit  of kube-scheduler, there are two scoring strategies\\nthat support the bin packing of resources: MostAllocated  and RequestedToCapacityRatio .\\nEnabling bin packing using MostAllocated strategy\\nThe MostAllocated  strategy scores the nodes based on the utilization of resources, favoring the\\nones with higher allocation. For each resource type, you can set a weight to modify its influence\\nin the node score.\\nTo set the MostAllocated  strategy for the NodeResourcesFit  plugin, use a scheduler\\nconfiguration  similar to the following:\\napiVersion : kubescheduler.config.k8s.io/v1beta3\\nkind: KubeSchedulerConfiguration\\nprofiles :\\n- pluginConfig :\\n  - args:\\n      scoringStrategy :\\n        resources :\\n        - name : cpu\\n          weight : 1\\n        - name : memory\\n          weight : 1\\n        - name : intel.com/foo\\n          weight : 3\\n        - name : intel.com/bar\\n          weight : 3\\n        type: MostAllocated\\n    name : NodeResourcesFit\\nTo learn more about other parameters and their default configuration, see the API\\ndocumentation for NodeResourcesFitArgs .•\", metadata={'source': './PDFS/Concepts.pdf', 'page': 515}),\n",
       " Document(page_content='Enabling bin packing using RequestedToCapacityRatio\\nThe RequestedToCapacityRatio  strategy allows the users to specify the resources along with\\nweights for each resource to score nodes based on the request to capacity ratio. This allows\\nusers to bin pack extended resources by using appropriate parameters to improve the utilization\\nof scarce resources in large clusters. It favors nodes according to a configured function of the\\nallocated resources. The behavior of the RequestedToCapacityRatio  in the NodeResourcesFit\\nscore function can be controlled by the scoringStrategy  field. Within the scoringStrategy  field,\\nyou can configure two parameters: requestedToCapacityRatio  and resources . The shape  in the \\nrequestedToCapacityRatio  parameter allows the user to tune the function as least requested or\\nmost requested based on utilization  and score  values. The resources  parameter consists of name\\nof the resource to be considered during scoring and weight  specify the weight of each resource.\\nBelow is an example configuration that sets the bin packing behavior for extended resources \\nintel.com/foo  and intel.com/bar  using the requestedToCapacityRatio  field.\\napiVersion : kubescheduler.config.k8s.io/v1beta3\\nkind: KubeSchedulerConfiguration\\nprofiles :\\n- pluginConfig :\\n  - args:\\n      scoringStrategy :\\n        resources :\\n        - name : intel.com/foo\\n          weight : 3\\n        - name : intel.com/bar\\n          weight : 3\\n        requestedToCapacityRatio :\\n          shape :\\n          - utilization : 0\\n            score : 0\\n          - utilization : 100\\n            score : 10\\n        type: RequestedToCapacityRatio\\n    name : NodeResourcesFit\\nReferencing the KubeSchedulerConfiguration  file with the kube-scheduler flag --config=/path/\\nto/config/file  will pass the configuration to the scheduler.\\nTo learn more about other parameters and their default configuration, see the API\\ndocumentation for NodeResourcesFitArgs .\\nTuning the score function\\nshape  is used to specify the behavior of the RequestedToCapacityRatio  function.\\nshape :\\n  - utilization : 0\\n    score : 0\\n  - utilization : 100\\n    score : 10', metadata={'source': './PDFS/Concepts.pdf', 'page': 516}),\n",
       " Document(page_content='The above arguments give the node a score  of 0 if utilization  is 0% and 10 for utilization  100%,\\nthus enabling bin packing behavior. To enable least requested the score value must be reversed\\nas follows.\\nshape :\\n  - utilization : 0\\n    score : 10\\n  - utilization : 100\\n    score : 0\\nresources  is an optional parameter which defaults to:\\nresources :\\n  - name : cpu\\n    weight : 1\\n  - name : memory\\n    weight : 1\\nIt can be used to add extended resources as follows:\\nresources :\\n  - name : intel.com/foo\\n    weight : 5\\n  - name : cpu\\n    weight : 3\\n  - name : memory\\n    weight : 1\\nThe weight  parameter is optional and is set to 1 if not specified. Also, the weight  cannot be set\\nto a negative value.\\nNode scoring for capacity allocation\\nThis section is intended for those who want to understand the internal details of this feature.\\nBelow is an example of how the node score is calculated for a given set of values.\\nRequested resources:\\nintel.com/foo : 2\\nmemory: 256MB\\ncpu: 2\\nResource weights:\\nintel.com/foo : 5\\nmemory: 1\\ncpu: 3\\nFunctionShapePoint {{0, 0}, {100, 10}}\\nNode 1 spec:\\nAvailable:\\n  intel.com/foo: 4', metadata={'source': './PDFS/Concepts.pdf', 'page': 517}),\n",
       " Document(page_content='memory: 1 GB\\n  cpu: 8\\nUsed:\\n  intel.com/foo: 1\\n  memory: 256MB\\n  cpu: 1\\nNode score:\\nintel.com/foo  = resourceScoringFunction((2+1),4)\\n               = (100 - ((4-3)*100/4)\\n               = (100 - 25)\\n               = 75                       # requested + used = 75% * available\\n               = rawScoringFunction(75)\\n               = 7                        # floor(75/10)\\nmemory         = resourceScoringFunction((256+256),1024)\\n               = (100 -((1024-512)*100/1024))\\n               = 50                       # requested + used = 50% * available\\n               = rawScoringFunction(50)\\n               = 5                        # floor(50/10)\\ncpu            = resourceScoringFunction((2+1),8)\\n               = (100 -((8-3)*100/8))\\n               = 37.5                     # requested + used = 37.5% * available\\n               = rawScoringFunction(37.5)\\n               = 3                        # floor(37.5/10)\\nNodeScore   =  ((7 * 5) + (5 * 1) + (3 * 3)) / (5 + 1 + 3)\\n            =  5\\nNode 2 spec:\\nAvailable:\\n  intel.com/foo: 8\\n  memory: 1GB\\n  cpu: 8\\nUsed:\\n  intel.com/foo: 2\\n  memory: 512MB\\n  cpu: 6\\nNode score:\\nintel.com/foo  = resourceScoringFunction((2+2),8)\\n               =  (100 - ((8-4)*100/8)\\n               =  (100 - 50)\\n               =  50\\n               =  rawScoringFunction(50)\\n               = 5\\nmemory         = resourceScoringFunction((256+512),1024)\\n               = (100 -((1024-768)*100/1024))', metadata={'source': './PDFS/Concepts.pdf', 'page': 518}),\n",
       " Document(page_content=\"= 75\\n               = rawScoringFunction(75)\\n               = 7\\ncpu            = resourceScoringFunction((2+6),8)\\n               = (100 -((8-8)*100/8))\\n               = 100\\n               = rawScoringFunction(100)\\n               = 10\\nNodeScore   =  ((5 * 5) + (7 * 1) + (10 * 3)) / (5 + 1 + 3)\\n            =  7\\nWhat's next\\nRead more about the scheduling framework\\nRead more about scheduler configuration\\nPod Priority and Preemption\\nFEATURE STATE:  Kubernetes v1.14 [stable]\\nPods  can have priority . Priority indicates the importance of a Pod relative to other Pods. If a Pod\\ncannot be scheduled, the scheduler tries to preempt (evict) lower priority Pods to make\\nscheduling of the pending Pod possible.\\nWarning:\\nIn a cluster where not all users are trusted, a malicious user could create Pods at the highest\\npossible priorities, causing other Pods to be evicted/not get scheduled. An administrator can use\\nResourceQuota to prevent users from creating pods at high priorities.\\nSee limit Priority Class consumption by default  for details.\\nHow to use priority and preemption\\nTo use priority and preemption:\\nAdd one or more PriorityClasses .\\nCreate Pods with priorityClassName  set to one of the added PriorityClasses. Of course\\nyou do not need to create the Pods directly; normally you would add priorityClassName\\nto the Pod template of a collection object like a Deployment.\\nKeep reading for more information about these steps.\\nNote:  Kubernetes already ships with two PriorityClasses: system-cluster-critical  and system-\\nnode-critical . These are common classes and are used to ensure that critical components are\\nalways scheduled first .• \\n• \\n1. \\n2.\", metadata={'source': './PDFS/Concepts.pdf', 'page': 519}),\n",
       " Document(page_content='PriorityClass\\nA PriorityClass is a non-namespaced object that defines a mapping from a priority class name\\nto the integer value of the priority. The name is specified in the name  field of the PriorityClass\\nobject\\'s metadata. The value is specified in the required value  field. The higher the value, the\\nhigher the priority. The name of a PriorityClass object must be a valid DNS subdomain name ,\\nand it cannot be prefixed with system- .\\nA PriorityClass object can have any 32-bit integer value smaller than or equal to 1 billion. This\\nmeans that the range of values for a PriorityClass object is from -2147483648 to 1000000000\\ninclusive. Larger numbers are reserved for built-in PriorityClasses that represent critical system\\nPods. A cluster admin should create one PriorityClass object for each such mapping that they\\nwant.\\nPriorityClass also has two optional fields: globalDefault  and description . The globalDefault  field\\nindicates that the value of this PriorityClass should be used for Pods without a \\npriorityClassName . Only one PriorityClass with globalDefault  set to true can exist in the\\nsystem. If there is no PriorityClass with globalDefault  set, the priority of Pods with no \\npriorityClassName  is zero.\\nThe description  field is an arbitrary string. It is meant to tell users of the cluster when they\\nshould use this PriorityClass.\\nNotes about PodPriority and existing clusters\\nIf you upgrade an existing cluster without this feature, the priority of your existing Pods\\nis effectively zero.\\nAddition of a PriorityClass with globalDefault  set to true does not change the priorities of\\nexisting Pods. The value of such a PriorityClass is used only for Pods created after the\\nPriorityClass is added.\\nIf you delete a PriorityClass, existing Pods that use the name of the deleted PriorityClass\\nremain unchanged, but you cannot create more Pods that use the name of the deleted\\nPriorityClass.\\nExample PriorityClass\\napiVersion : scheduling.k8s.io/v1\\nkind: PriorityClass\\nmetadata :\\n  name : high-priority\\nvalue : 1000000\\nglobalDefault : false\\ndescription : \"This priority class should be used for XYZ service pods only.\"\\nNon-preempting PriorityClass\\nFEATURE STATE:  Kubernetes v1.24 [stable]\\nPods with preemptionPolicy: Never  will be placed in the scheduling queue ahead of lower-\\npriority pods, but they cannot preempt other pods. A non-preempting pod waiting to be• \\n• \\n•', metadata={'source': './PDFS/Concepts.pdf', 'page': 520}),\n",
       " Document(page_content='scheduled will stay in the scheduling queue, until sufficient resources are free, and it can be\\nscheduled. Non-preempting pods, like other pods, are subject to scheduler back-off. This means\\nthat if the scheduler tries these pods and they cannot be scheduled, they will be retried with\\nlower frequency, allowing other pods with lower priority to be scheduled before them.\\nNon-preempting pods may still be preempted by other, high-priority pods.\\npreemptionPolicy  defaults to PreemptLowerPriority , which will allow pods of that PriorityClass\\nto preempt lower-priority pods (as is existing default behavior). If preemptionPolicy  is set to \\nNever , pods in that PriorityClass will be non-preempting.\\nAn example use case is for data science workloads. A user may submit a job that they want to\\nbe prioritized above other workloads, but do not wish to discard existing work by preempting\\nrunning pods. The high priority job with preemptionPolicy: Never  will be scheduled ahead of\\nother queued pods, as soon as sufficient cluster resources \"naturally\" become free.\\nExample Non-preempting PriorityClass\\napiVersion : scheduling.k8s.io/v1\\nkind: PriorityClass\\nmetadata :\\n  name : high-priority-nonpreempting\\nvalue : 1000000\\npreemptionPolicy : Never\\nglobalDefault : false\\ndescription : \"This priority class will not cause other pods to be preempted.\"\\nPod priority\\nAfter you have one or more PriorityClasses, you can create Pods that specify one of those\\nPriorityClass names in their specifications. The priority admission controller uses the \\npriorityClassName  field and populates the integer value of the priority. If the priority class is\\nnot found, the Pod is rejected.\\nThe following YAML is an example of a Pod configuration that uses the PriorityClass created in\\nthe preceding example. The priority admission controller checks the specification and resolves\\nthe priority of the Pod to 1000000.\\napiVersion : v1\\nkind: Pod\\nmetadata :\\n  name : nginx\\n  labels :\\n    env: test\\nspec:\\n  containers :\\n  - name : nginx\\n    image : nginx\\n    imagePullPolicy : IfNotPresent\\n  priorityClassName : high-priority', metadata={'source': './PDFS/Concepts.pdf', 'page': 521}),\n",
       " Document(page_content='Effect of Pod priority on scheduling order\\nWhen Pod priority is enabled, the scheduler orders pending Pods by their priority and a\\npending Pod is placed ahead of other pending Pods with lower priority in the scheduling queue.\\nAs a result, the higher priority Pod may be scheduled sooner than Pods with lower priority if its\\nscheduling requirements are met. If such Pod cannot be scheduled, scheduler will continue and\\ntries to schedule other lower priority Pods.\\nPreemption\\nWhen Pods are created, they go to a queue and wait to be scheduled. The scheduler picks a Pod\\nfrom the queue and tries to schedule it on a Node. If no Node is found that satisfies all the\\nspecified requirements of the Pod, preemption logic is triggered for the pending Pod. Let\\'s call\\nthe pending Pod P. Preemption logic tries to find a Node where removal of one or more Pods\\nwith lower priority than P would enable P to be scheduled on that Node. If such a Node is\\nfound, one or more lower priority Pods get evicted from the Node. After the Pods are gone, P\\ncan be scheduled on the Node.\\nUser exposed information\\nWhen Pod P preempts one or more Pods on Node N, nominatedNodeName  field of Pod P\\'s\\nstatus is set to the name of Node N. This field helps scheduler track resources reserved for Pod\\nP and also gives users information about preemptions in their clusters.\\nPlease note that Pod P is not necessarily scheduled to the \"nominated Node\". The scheduler\\nalways tries the \"nominated Node\" before iterating over any other nodes. After victim Pods are\\npreempted, they get their graceful termination period. If another node becomes available while\\nscheduler is waiting for the victim Pods to terminate, scheduler may use the other node to\\nschedule Pod P. As a result nominatedNodeName  and nodeName  of Pod spec are not always the\\nsame. Also, if scheduler preempts Pods on Node N, but then a higher priority Pod than Pod P\\narrives, scheduler may give Node N to the new higher priority Pod. In such a case, scheduler\\nclears nominatedNodeName  of Pod P. By doing this, scheduler makes Pod P eligible to preempt\\nPods on another Node.\\nLimitations of preemption\\nGraceful termination of preemption victims\\nWhen Pods are preempted, the victims get their graceful termination period . They have that\\nmuch time to finish their work and exit. If they don\\'t, they are killed. This graceful termination\\nperiod creates a time gap between the point that the scheduler preempts Pods and the time\\nwhen the pending Pod (P) can be scheduled on the Node (N). In the meantime, the scheduler\\nkeeps scheduling other pending Pods. As victims exit or get terminated, the scheduler tries to\\nschedule Pods in the pending queue. Therefore, there is usually a time gap between the point\\nthat scheduler preempts victims and the time that Pod P is scheduled. In order to minimize this\\ngap, one can set graceful termination period of lower priority Pods to zero or a small number.\\nPodDisruptionBudget is supported, but not guaranteed\\nA PodDisruptionBudget  (PDB) allows application owners to limit the number of Pods of a\\nreplicated application that are down simultaneously from voluntary disruptions. Kubernetes', metadata={'source': './PDFS/Concepts.pdf', 'page': 522}),\n",
       " Document(page_content='supports PDB when preempting Pods, but respecting PDB is best effort. The scheduler tries to\\nfind victims whose PDB are not violated by preemption, but if no such victims are found,\\npreemption will still happen, and lower priority Pods will be removed despite their PDBs being\\nviolated.\\nInter-Pod affinity on lower-priority Pods\\nA Node is considered for preemption only when the answer to this question is yes: \"If all the\\nPods with lower priority than the pending Pod are removed from the Node, can the pending\\nPod be scheduled on the Node?\"\\nNote:  Preemption does not necessarily remove all lower-priority Pods. If the pending Pod can\\nbe scheduled by removing fewer than all lower-priority Pods, then only a portion of the lower-\\npriority Pods are removed. Even so, the answer to the preceding question must be yes. If the\\nanswer is no, the Node is not considered for preemption.\\nIf a pending Pod has inter-pod affinity  to one or more of the lower-priority Pods on the Node,\\nthe inter-Pod affinity rule cannot be satisfied in the absence of those lower-priority Pods. In this\\ncase, the scheduler does not preempt any Pods on the Node. Instead, it looks for another Node.\\nThe scheduler might find a suitable Node or it might not. There is no guarantee that the\\npending Pod can be scheduled.\\nOur recommended solution for this problem is to create inter-Pod affinity only towards equal or\\nhigher priority Pods.\\nCross node preemption\\nSuppose a Node N is being considered for preemption so that a pending Pod P can be scheduled\\non N. P might become feasible on N only if a Pod on another Node is preempted. Here\\'s an\\nexample:\\nPod P is being considered for Node N.\\nPod Q is running on another Node in the same Zone as Node N.\\nPod P has Zone-wide anti-affinity with Pod Q ( topologyKey: topology.kubernetes.io/\\nzone ).\\nThere are no other cases of anti-affinity between Pod P and other Pods in the Zone.\\nIn order to schedule Pod P on Node N, Pod Q can be preempted, but scheduler does not\\nperform cross-node preemption. So, Pod P will be deemed unschedulable on Node N.\\nIf Pod Q were removed from its Node, the Pod anti-affinity violation would be gone, and Pod P\\ncould possibly be scheduled on Node N.\\nWe may consider adding cross Node preemption in future versions if there is enough demand\\nand if we find an algorithm with reasonable performance.\\nTroubleshooting\\nPod priority and pre-emption can have unwanted side effects. Here are some examples of\\npotential problems and ways to deal with them.• \\n• \\n• \\n• \\n•', metadata={'source': './PDFS/Concepts.pdf', 'page': 523}),\n",
       " Document(page_content=\"Pods are preempted unnecessarily\\nPreemption removes existing Pods from a cluster under resource pressure to make room for\\nhigher priority pending Pods. If you give high priorities to certain Pods by mistake, these\\nunintentionally high priority Pods may cause preemption in your cluster. Pod priority is\\nspecified by setting the priorityClassName  field in the Pod's specification. The integer value for\\npriority is then resolved and populated to the priority  field of podSpec .\\nTo address the problem, you can change the priorityClassName  for those Pods to use lower\\npriority classes, or leave that field empty. An empty priorityClassName  is resolved to zero by\\ndefault.\\nWhen a Pod is preempted, there will be events recorded for the preempted Pod. Preemption\\nshould happen only when a cluster does not have enough resources for a Pod. In such cases,\\npreemption happens only when the priority of the pending Pod (preemptor) is higher than the\\nvictim Pods. Preemption must not happen when there is no pending Pod, or when the pending\\nPods have equal or lower priority than the victims. If preemption happens in such scenarios,\\nplease file an issue.\\nPods are preempted, but the preemptor is not scheduled\\nWhen pods are preempted, they receive their requested graceful termination period, which is\\nby default 30 seconds. If the victim Pods do not terminate within this period, they are forcibly\\nterminated. Once all the victims go away, the preemptor Pod can be scheduled.\\nWhile the preemptor Pod is waiting for the victims to go away, a higher priority Pod may be\\ncreated that fits on the same Node. In this case, the scheduler will schedule the higher priority\\nPod instead of the preemptor.\\nThis is expected behavior: the Pod with the higher priority should take the place of a Pod with a\\nlower priority.\\nHigher priority Pods are preempted before lower priority pods\\nThe scheduler tries to find nodes that can run a pending Pod. If no node is found, the scheduler\\ntries to remove Pods with lower priority from an arbitrary node in order to make room for the\\npending pod. If a node with low priority Pods is not feasible to run the pending Pod, the\\nscheduler may choose another node with higher priority Pods (compared to the Pods on the\\nother node) for preemption. The victims must still have lower priority than the preemptor Pod.\\nWhen there are multiple nodes available for preemption, the scheduler tries to choose the node\\nwith a set of Pods with lowest priority. However, if such Pods have PodDisruptionBudget that\\nwould be violated if they are preempted then the scheduler may choose another node with\\nhigher priority Pods.\\nWhen multiple nodes exist for preemption and none of the above scenarios apply, the scheduler\\nchooses a node with the lowest priority.\\nInteractions between Pod priority and quality of service\\nPod priority and QoS class  are two orthogonal features with few interactions and no default\\nrestrictions on setting the priority of a Pod based on its QoS classes. The scheduler's preemption\", metadata={'source': './PDFS/Concepts.pdf', 'page': 524}),\n",
       " Document(page_content=\"logic does not consider QoS when choosing preemption targets. Preemption considers Pod\\npriority and attempts to choose a set of targets with the lowest priority. Higher-priority Pods\\nare considered for preemption only if the removal of the lowest priority Pods is not sufficient to\\nallow the scheduler to schedule the preemptor Pod, or if the lowest priority Pods are protected\\nby PodDisruptionBudget .\\nThe kubelet uses Priority to determine pod order for node-pressure eviction . You can use the\\nQoS class to estimate the order in which pods are most likely to get evicted. The kubelet ranks\\npods for eviction based on the following factors:\\nWhether the starved resource usage exceeds requests\\nPod Priority\\nAmount of resource usage relative to requests\\nSee Pod selection for kubelet eviction  for more details.\\nkubelet node-pressure eviction does not evict Pods when their usage does not exceed their\\nrequests. If a Pod with lower priority is not exceeding its requests, it won't be evicted. Another\\nPod with higher priority that exceeds its requests may be evicted.\\nWhat's next\\nRead about using ResourceQuotas in connection with PriorityClasses: limit Priority Class\\nconsumption by default\\nLearn about Pod Disruption\\nLearn about API-initiated Eviction\\nLearn about Node-pressure Eviction\\nNode-pressure Eviction\\nNode-pressure eviction is the process by which the kubelet  proactively terminates pods to\\nreclaim resources on nodes.\\nThe kubelet  monitors resources like memory, disk space, and filesystem inodes on your cluster's\\nnodes. When one or more of these resources reach specific consumption levels, the kubelet can\\nproactively fail one or more pods on the node to reclaim resources and prevent starvation.\\nDuring a node-pressure eviction, the kubelet sets the phase  for the selected pods to Failed , and\\nterminates the Pod.\\nNode-pressure eviction is not the same as API-initiated eviction .\\nThe kubelet does not respect your configured PodDisruptionBudget  or the pod's \\nterminationGracePeriodSeconds . If you use soft eviction thresholds , the kubelet respects your\\nconfigured eviction-max-pod-grace-period . If you use hard eviction thresholds , the kubelet uses\\na 0s grace period (immediate shutdown) for termination.\\nSelf healing behavior\\nThe kubelet attempts to reclaim node-level resources  before it terminates end-user pods. For\\nexample, it removes unused container images when disk resources are starved.1. \\n2. \\n3. \\n• \\n• \\n• \\n•\", metadata={'source': './PDFS/Concepts.pdf', 'page': 525}),\n",
       " Document(page_content=\"If the pods are managed by a workload  management object (such as StatefulSet  or Deployment )\\nthat replaces failed pods, the control plane ( kube-controller-manager ) creates new pods in place\\nof the evicted pods.\\nSelf healing for static pods\\nIf you are running a static pod  on a node that is under resource pressure, the kubelet may evict\\nthat static Pod. The kubelet then tries to create a replacement, because static Pods always\\nrepresent an intent to run a Pod on that node.\\nThe kubelet takes the priority  of the static pod into account when creating a replacement. If the\\nstatic pod manifest specifies a low priority, and there are higher-priority Pods defined within\\nthe cluster's control plane, and the node is under resource pressure, the kubelet may not be able\\nto make room for that static pod. The kubelet continues to attempt to run all static pods even\\nwhen there is resource pressure on a node.\\nEviction signals and thresholds\\nThe kubelet uses various parameters to make eviction decisions, like the following:\\nEviction signals\\nEviction thresholds\\nMonitoring intervals\\nEviction signals\\nEviction signals are the current state of a particular resource at a specific point in time. Kubelet\\nuses eviction signals to make eviction decisions by comparing the signals to eviction thresholds,\\nwhich are the minimum amount of the resource that should be available on the node.\\nOn Linux, the kubelet uses the following eviction signals:\\nEviction Signal Description\\nmemory.availablememory.available  := node.status.capacity[memory]  - \\nnode.stats.memory.workingSet\\nnodefs.available nodefs.available  := node.stats.fs.available\\nnodefs.inodesFree nodefs.inodesFree  := node.stats.fs.inodesFree\\nimagefs.available imagefs.available  := node.stats.runtime.imagefs.available\\nimagefs.inodesFree imagefs.inodesFree  := node.stats.runtime.imagefs.inodesFree\\npid.available pid.available  := node.stats.rlimit.maxpid  - node.stats.rlimit.curproc\\nIn this table, the Description  column shows how kubelet gets the value of the signal. Each\\nsignal supports either a percentage or a literal value. Kubelet calculates the percentage value\\nrelative to the total capacity associated with the signal.\\nThe value for memory.available  is derived from the cgroupfs instead of tools like free -m . This is\\nimportant because free -m  does not work in a container, and if users use the node allocatable\\nfeature, out of resource decisions are made local to the end user Pod part of the cgroup\\nhierarchy as well as the root node. This script  or cgroupv2 script  reproduces the same set of\\nsteps that the kubelet performs to calculate memory.available . The kubelet excludes• \\n• \\n•\", metadata={'source': './PDFS/Concepts.pdf', 'page': 526}),\n",
       " Document(page_content=\"inactive_file (the number of bytes of file-backed memory on the inactive LRU list) from its\\ncalculation, as it assumes that memory is reclaimable under pressure.\\nThe kubelet recognizes two specific filesystem identifiers:\\nnodefs : The node's main filesystem, used for local disk volumes, emptyDir volumes not\\nbacked by memory, log storage, and more. For example, nodefs  contains /var/lib/kubelet/ .\\nimagefs : An optional filesystem that container runtimes use to store container images and\\ncontainer writable layers.\\nKubelet auto-discovers these filesystems and ignores other node local filesystems. Kubelet does\\nnot support other configurations.\\nSome kubelet garbage collection features are deprecated in favor of eviction:\\nExisting Flag Rationale\\n--maximum-dead-containersdeprecated once old logs are stored outside of\\ncontainer's context\\n--maximum-dead-containers-per-\\ncontainerdeprecated once old logs are stored outside of\\ncontainer's context\\n--minimum-container-ttl-durationdeprecated once old logs are stored outside of\\ncontainer's context\\nEviction thresholds\\nYou can specify custom eviction thresholds for the kubelet to use when it makes eviction\\ndecisions. You can configure soft and hard eviction thresholds.\\nEviction thresholds have the form [eviction-signal][operator][quantity] , where:\\neviction-signal  is the eviction signal  to use.\\noperator  is the relational operator  you want, such as < (less than).\\nquantity  is the eviction threshold amount, such as 1Gi. The value of quantity  must match\\nthe quantity representation used by Kubernetes. You can use either literal values or\\npercentages ( %).\\nFor example, if a node has 10GiB of total memory and you want trigger eviction if the available\\nmemory falls below 1GiB, you can define the eviction threshold as either \\nmemory.available<10%  or memory.available<1Gi  (you cannot use both).\\nSoft eviction thresholds\\nA soft eviction threshold pairs an eviction threshold with a required administrator-specified\\ngrace period. The kubelet does not evict pods until the grace period is exceeded. The kubelet\\nreturns an error on startup if you do not specify a grace period.\\nYou can specify both a soft eviction threshold grace period and a maximum allowed pod\\ntermination grace period for kubelet to use during evictions. If you specify a maximum allowed\\ngrace period and the soft eviction threshold is met, the kubelet uses the lesser of the two grace\\nperiods. If you do not specify a maximum allowed grace period, the kubelet kills evicted pods\\nimmediately without graceful termination.1. \\n2. \\n• \\n• \\n•\", metadata={'source': './PDFS/Concepts.pdf', 'page': 527}),\n",
       " Document(page_content=\"You can use the following flags to configure soft eviction thresholds:\\neviction-soft : A set of eviction thresholds like memory.available<1.5Gi  that can trigger\\npod eviction if held over the specified grace period.\\neviction-soft-grace-period : A set of eviction grace periods like memory.available=1m30s\\nthat define how long a soft eviction threshold must hold before triggering a Pod eviction.\\neviction-max-pod-grace-period : The maximum allowed grace period (in seconds) to use\\nwhen terminating pods in response to a soft eviction threshold being met.\\nHard eviction thresholds\\nA hard eviction threshold has no grace period. When a hard eviction threshold is met, the\\nkubelet kills pods immediately without graceful termination to reclaim the starved resource.\\nYou can use the eviction-hard  flag to configure a set of hard eviction thresholds like \\nmemory.available<1Gi .\\nThe kubelet has the following default hard eviction thresholds:\\nmemory.available<100Mi\\nnodefs.available<10%\\nimagefs.available<15%\\nnodefs.inodesFree<5%  (Linux nodes)\\nThese default values of hard eviction thresholds will only be set if none of the parameters is\\nchanged. If you changed the value of any parameter, then the values of other parameters will\\nnot be inherited as the default values and will be set to zero. In order to provide custom values,\\nyou should provide all the thresholds respectively.\\nEviction monitoring interval\\nThe kubelet evaluates eviction thresholds based on its configured housekeeping-interval , which\\ndefaults to 10s.\\nNode conditions\\nThe kubelet reports node conditions  to reflect that the node is under pressure because hard or\\nsoft eviction threshold is met, independent of configured grace periods.\\nThe kubelet maps eviction signals to node conditions as follows:\\nNode\\nConditionEviction Signal Description\\nMemoryPressure memory.availableAvailable memory on\\nthe node has satisfied\\nan eviction threshold\\nDiskPressurenodefs.available , nodefs.inodesFree , imagefs.available ,\\nor imagefs.inodesFreeAvailable disk space\\nand inodes on either\\nthe node's root\\nfilesystem or image\\nfilesystem has• \\n• \\n• \\n• \\n• \\n• \\n•\", metadata={'source': './PDFS/Concepts.pdf', 'page': 528}),\n",
       " Document(page_content='Node\\nConditionEviction Signal Description\\nsatisfied an eviction\\nthreshold\\nPIDPressure pid.availableAvailable processes\\nidentifiers on the\\n(Linux) node has\\nfallen below an\\neviction threshold\\nThe control plane also maps  these node conditions to taints.\\nThe kubelet updates the node conditions based on the configured --node-status-update-\\nfrequency , which defaults to 10s.\\nNode condition oscillation\\nIn some cases, nodes oscillate above and below soft eviction thresholds without holding for the\\ndefined grace periods. This causes the reported node condition to constantly switch between \\ntrue and false, leading to bad eviction decisions.\\nTo protect against oscillation, you can use the eviction-pressure-transition-period  flag, which\\ncontrols how long the kubelet must wait before transitioning a node condition to a different\\nstate. The transition period has a default value of 5m.\\nReclaiming node level resources\\nThe kubelet tries to reclaim node-level resources before it evicts end-user pods.\\nWhen a DiskPressure  node condition is reported, the kubelet reclaims node-level resources\\nbased on the filesystems on the node.\\nWith imagefs\\nIf the node has a dedicated imagefs  filesystem for container runtimes to use, the kubelet does\\nthe following:\\nIf the nodefs  filesystem meets the eviction thresholds, the kubelet garbage collects dead\\npods and containers.\\nIf the imagefs  filesystem meets the eviction thresholds, the kubelet deletes all unused\\nimages.\\nWithout imagefs\\nIf the node only has a nodefs  filesystem that meets eviction thresholds, the kubelet frees up disk\\nspace in the following order:\\nGarbage collect dead pods and containers\\nDelete unused images• \\n• \\n1. \\n2.', metadata={'source': './PDFS/Concepts.pdf', 'page': 529}),\n",
       " Document(page_content=\"Pod selection for kubelet eviction\\nIf the kubelet's attempts to reclaim node-level resources don't bring the eviction signal below\\nthe threshold, the kubelet begins to evict end-user pods.\\nThe kubelet uses the following parameters to determine the pod eviction order:\\nWhether the pod's resource usage exceeds requests\\nPod Priority\\nThe pod's resource usage relative to requests\\nAs a result, kubelet ranks and evicts pods in the following order:\\nBestEffort  or Burstable  pods where the usage exceeds requests. These pods are evicted\\nbased on their Priority and then by how much their usage level exceeds the request.\\nGuaranteed  pods and Burstable  pods where the usage is less than requests are evicted\\nlast, based on their Priority.\\nNote:  The kubelet does not use the pod's QoS class  to determine the eviction order. You can use\\nthe QoS class to estimate the most likely pod eviction order when reclaiming resources like\\nmemory. QoS classification does not apply to EphemeralStorage requests, so the above scenario\\nwill not apply if the node is, for example, under DiskPressure .\\nGuaranteed  pods are guaranteed only when requests and limits are specified for all the\\ncontainers and they are equal. These pods will never be evicted because of another pod's\\nresource consumption. If a system daemon (such as kubelet  and journald ) is consuming more\\nresources than were reserved via system-reserved  or kube-reserved  allocations, and the node\\nonly has Guaranteed  or Burstable  pods using less resources than requests left on it, then the\\nkubelet must choose to evict one of these pods to preserve node stability and to limit the impact\\nof resource starvation on other pods. In this case, it will choose to evict pods of lowest Priority\\nfirst.\\nIf you are running a static pod  and want to avoid having it evicted under resource pressure, set\\nthe priority  field for that Pod directly. Static pods do not support the priorityClassName  field.\\nWhen the kubelet evicts pods in response to inode or process ID starvation, it uses the Pods'\\nrelative priority to determine the eviction order, because inodes and PIDs have no requests.\\nThe kubelet sorts pods differently based on whether the node has a dedicated imagefs\\nfilesystem:\\nWith imagefs\\nIf nodefs  is triggering evictions, the kubelet sorts pods based on nodefs  usage ( local volumes + \\nlogs of all containers ).\\nIf imagefs  is triggering evictions, the kubelet sorts pods based on the writable layer usage of all\\ncontainers.\\nWithout imagefs\\nIf nodefs  is triggering evictions, the kubelet sorts pods based on their total disk usage ( local \\nvolumes + logs & writable layer of all containers )1. \\n2. \\n3. \\n1. \\n2.\", metadata={'source': './PDFS/Concepts.pdf', 'page': 530}),\n",
       " Document(page_content='Minimum eviction reclaim\\nIn some cases, pod eviction only reclaims a small amount of the starved resource. This can lead\\nto the kubelet repeatedly hitting the configured eviction thresholds and triggering multiple\\nevictions.\\nYou can use the --eviction-minimum-reclaim  flag or a kubelet config file  to configure a\\nminimum reclaim amount for each resource. When the kubelet notices that a resource is\\nstarved, it continues to reclaim that resource until it reclaims the quantity you specify.\\nFor example, the following configuration sets minimum reclaim amounts:\\napiVersion : kubelet.config.k8s.io/v1beta1\\nkind: KubeletConfiguration\\nevictionHard :\\n  memory.available : \"500Mi\"\\n  nodefs.available : \"1Gi\"\\n  imagefs.available : \"100Gi\"\\nevictionMinimumReclaim :\\n  memory.available : \"0Mi\"\\n  nodefs.available : \"500Mi\"\\n  imagefs.available : \"2Gi\"\\nIn this example, if the nodefs.available  signal meets the eviction threshold, the kubelet reclaims\\nthe resource until the signal reaches the threshold of 1GiB, and then continues to reclaim the\\nminimum amount of 500MiB, until the available nodefs storage value reaches 1.5GiB.\\nSimilarly, the kubelet tries to reclaim the imagefs  resource until the imagefs.available  value\\nreaches 102Gi , representing 102 GiB of available container image storage. If the amount of\\nstorage that the kubelet could reclaim is less than 2GiB, the kubelet doesn\\'t reclaim anything.\\nThe default eviction-minimum-reclaim  is 0 for all resources.\\nNode out of memory behavior\\nIf the node experiences an out of memory  (OOM) event prior to the kubelet being able to\\nreclaim memory, the node depends on the oom_killer  to respond.\\nThe kubelet sets an oom_score_adj  value for each container based on the QoS for the pod.\\nQuality of\\nServiceoom_score_adj\\nGuaranteed -997\\nBestEffort 1000\\nBurstablemin(max(2, 1000 - (1000 × memoryRequestBytes) /\\nmachineMemoryCapacityBytes), 999)\\nNote:  The kubelet also sets an oom_score_adj  value of -997 for any containers in Pods that have \\nsystem-node-critical  Priority .\\nIf the kubelet can\\'t reclaim memory before a node experiences OOM, the oom_killer  calculates\\nan oom_score  based on the percentage of memory it\\'s using on the node, and then adds the', metadata={'source': './PDFS/Concepts.pdf', 'page': 531}),\n",
       " Document(page_content='oom_score_adj  to get an effective oom_score  for each container. It then kills the container with\\nthe highest score.\\nThis means that containers in low QoS pods that consume a large amount of memory relative to\\ntheir scheduling requests are killed first.\\nUnlike pod eviction, if a container is OOM killed, the kubelet can restart it based on its \\nrestartPolicy .\\nGood practices\\nThe following sections describe good practice for eviction configuration.\\nSchedulable resources and eviction policies\\nWhen you configure the kubelet with an eviction policy, you should make sure that the\\nscheduler will not schedule pods if they will trigger eviction because they immediately induce\\nmemory pressure.\\nConsider the following scenario:\\nNode memory capacity: 10GiB\\nOperator wants to reserve 10% of memory capacity for system daemons (kernel, kubelet ,\\netc.)\\nOperator wants to evict Pods at 95% memory utilization to reduce incidence of system\\nOOM.\\nFor this to work, the kubelet is launched as follows:\\n--eviction-hard=memory.available<500Mi\\n--system-reserved=memory=1.5Gi\\nIn this configuration, the --system-reserved  flag reserves 1.5GiB of memory for the system,\\nwhich is 10% of the total memory + the eviction threshold amount .\\nThe node can reach the eviction threshold if a pod is using more than its request, or if the\\nsystem is using more than 1GiB of memory, which makes the memory.available  signal fall\\nbelow 500MiB and triggers the threshold.\\nDaemonSets and node-pressure eviction\\nPod priority is a major factor in making eviction decisions. If you do not want the kubelet to\\nevict pods that belong to a DaemonSet, give those pods a high enough priority by specifying a\\nsuitable priorityClassName  in the pod spec. You can also use a lower priority, or the default, to\\nonly allow pods from that DaemonSet to run when there are enough resources.\\nKnown issues\\nThe following sections describe known issues related to out of resource handling.• \\n• \\n•', metadata={'source': './PDFS/Concepts.pdf', 'page': 532}),\n",
       " Document(page_content=\"kubelet may not observe memory pressure right away\\nBy default, the kubelet polls cAdvisor to collect memory usage stats at a regular interval. If\\nmemory usage increases within that window rapidly, the kubelet may not observe \\nMemoryPressure  fast enough, and the OOM killer will still be invoked.\\nYou can use the --kernel-memcg-notification  flag to enable the memcg  notification API on the\\nkubelet to get notified immediately when a threshold is crossed.\\nIf you are not trying to achieve extreme utilization, but a sensible measure of overcommit, a\\nviable workaround for this issue is to use the --kube-reserved  and --system-reserved  flags to\\nallocate memory for the system.\\nactive_file memory is not considered as available memory\\nOn Linux, the kernel tracks the number of bytes of file-backed memory on active least recently\\nused (LRU) list as the active_file  statistic. The kubelet treats active_file  memory areas as not\\nreclaimable. For workloads that make intensive use of block-backed local storage, including\\nephemeral local storage, kernel-level caches of file and block data means that many recently\\naccessed cache pages are likely to be counted as active_file . If enough of these kernel block\\nbuffers are on the active LRU list, the kubelet is liable to observe this as high resource use and\\ntaint the node as experiencing memory pressure - triggering pod eviction.\\nFor more details, see https://github.com/kubernetes/kubernetes/issues/43916\\nYou can work around that behavior by setting the memory limit and memory request the same\\nfor containers likely to perform intensive I/O activity. You will need to estimate or measure an\\noptimal memory limit value for that container.\\nWhat's next\\nLearn about API-initiated Eviction\\nLearn about Pod Priority and Preemption\\nLearn about PodDisruptionBudgets\\nLearn about Quality of Service  (QoS)\\nCheck out the Eviction API\\nAPI-initiated Eviction\\nAPI-initiated eviction is the process by which you use the Eviction API  to create an Eviction\\nobject that triggers graceful pod termination.\\nYou can request eviction by calling the Eviction API directly, or programmatically using a client\\nof the API server , like the kubectl drain  command. This creates an Eviction  object, which causes\\nthe API server to terminate the Pod.\\nAPI-initiated evictions respect your configured PodDisruptionBudgets  and \\nterminationGracePeriodSeconds .\\nUsing the API to create an Eviction object for a Pod is like performing a policy-controlled \\nDELETE  operation  on the Pod.• \\n• \\n• \\n• \\n•\", metadata={'source': './PDFS/Concepts.pdf', 'page': 533}),\n",
       " Document(page_content='Calling the Eviction API\\nYou can use a Kubernetes language client  to access the Kubernetes API and create an Eviction\\nobject. To do this, you POST the attempted operation, similar to the following example:\\npolicy/v1\\npolicy/v1beta1\\nNote:  policy/v1  Eviction is available in v1.22+. Use policy/v1beta1  with prior releases.\\n{\\n  \"apiVersion\" : \"policy/v1\" ,\\n  \"kind\" : \"Eviction\" ,\\n  \"metadata\" : {\\n    \"name\" : \"quux\" ,\\n    \"namespace\" : \"default\"\\n  }\\n}\\nNote:  Deprecated in v1.22 in favor of policy/v1\\n{\\n  \"apiVersion\" : \"policy/v1beta1\" ,\\n  \"kind\" : \"Eviction\" ,\\n  \"metadata\" : {\\n    \"name\" : \"quux\" ,\\n    \"namespace\" : \"default\"\\n  }\\n}\\nAlternatively, you can attempt an eviction operation by accessing the API using curl or wget ,\\nsimilar to the following example:\\ncurl -v -H \\'Content-type: application/json\\'  https://your-cluster-api-endpoint.example/api/v1/\\nnamespaces/default/pods/quux/eviction -d @eviction.json\\nHow API-initiated eviction works\\nWhen you request an eviction using the API, the API server performs admission checks and\\nresponds in one of the following ways:\\n200 OK : the eviction is allowed, the Eviction  subresource is created, and the Pod is\\ndeleted, similar to sending a DELETE  request to the Pod URL.\\n429 Too Many Requests : the eviction is not currently allowed because of the configured \\nPodDisruptionBudget . You may be able to attempt the eviction again later. You might also\\nsee this response because of API rate limiting.\\n500 Internal Server Error : the eviction is not allowed because there is a misconfiguration,\\nlike if multiple PodDisruptionBudgets reference the same Pod.\\nIf the Pod you want to evict isn\\'t part of a workload that has a PodDisruptionBudget, the API\\nserver always returns 200 OK  and allows the eviction.• \\n• \\n• \\n• \\n•', metadata={'source': './PDFS/Concepts.pdf', 'page': 534}),\n",
       " Document(page_content=\"If the API server allows the eviction, the Pod is deleted as follows:\\nThe Pod resource in the API server is updated with a deletion timestamp, after which the\\nAPI server considers the Pod resource to be terminated. The Pod resource is also marked\\nwith the configured grace period.\\nThe kubelet  on the node where the local Pod is running notices that the Pod resource is\\nmarked for termination and starts to gracefully shut down the local Pod.\\nWhile the kubelet is shutting the Pod down, the control plane removes the Pod from \\nEndpoint  and EndpointSlice  objects. As a result, controllers no longer consider the Pod as\\na valid object.\\nAfter the grace period for the Pod expires, the kubelet forcefully terminates the local Pod.\\nThe kubelet tells the API server to remove the Pod resource.\\nThe API server deletes the Pod resource.\\nTroubleshooting stuck evictions\\nIn some cases, your applications may enter a broken state, where the Eviction API will only\\nreturn 429 or 500 responses until you intervene. This can happen if, for example, a ReplicaSet\\ncreates pods for your application but new pods do not enter a Ready  state. You may also notice\\nthis behavior in cases where the last evicted Pod had a long termination grace period.\\nIf you notice stuck evictions, try one of the following solutions:\\nAbort or pause the automated operation causing the issue. Investigate the stuck\\napplication before you restart the operation.\\nWait a while, then directly delete the Pod from your cluster control plane instead of using\\nthe Eviction API.\\nWhat's next\\nLearn how to protect your applications with a Pod Disruption Budget .\\nLearn about Node-pressure Eviction .\\nLearn about Pod Priority and Preemption .\\nCluster Administration\\nLower-level detail relevant to creating or administering a Kubernetes cluster.\\nThe cluster administration overview is for anyone creating or administering a Kubernetes\\ncluster. It assumes some familiarity with core Kubernetes concepts .\\nPlanning a cluster\\nSee the guides in Setup  for examples of how to plan, set up, and configure Kubernetes clusters.\\nThe solutions listed in this article are called distros .\\nNote:  Not all distros are actively maintained. Choose distros which have been tested with a\\nrecent version of Kubernetes.1. \\n2. \\n3. \\n4. \\n5. \\n6. \\n• \\n• \\n• \\n• \\n•\", metadata={'source': './PDFS/Concepts.pdf', 'page': 535}),\n",
       " Document(page_content='Before choosing a guide, here are some considerations:\\nDo you want to try out Kubernetes on your computer, or do you want to build a high-\\navailability, multi-node cluster? Choose distros best suited for your needs.\\nWill you be using a hosted Kubernetes cluster , such as Google Kubernetes Engine , or \\nhosting your own cluster ?\\nWill your cluster be on-premises , or in the cloud (IaaS) ? Kubernetes does not directly\\nsupport hybrid clusters. Instead, you can set up multiple clusters.\\nIf you are configuring Kubernetes on-premises , consider which networking model\\nfits best.\\nWill you be running Kubernetes on \"bare metal\" hardware  or on virtual machines\\n(VMs) ?\\nDo you want to run a cluster , or do you expect to do active development of\\nKubernetes project code ? If the latter, choose an actively-developed distro. Some\\ndistros only use binary releases, but offer a greater variety of choices.\\nFamiliarize yourself with the components  needed to run a cluster.\\nManaging a cluster\\nLearn how to manage nodes .\\nLearn how to set up and manage the resource quota  for shared clusters.\\nSecuring a cluster\\nGenerate Certificates  describes the steps to generate certificates using different tool\\nchains.\\nKubernetes Container Environment  describes the environment for Kubelet managed\\ncontainers on a Kubernetes node.\\nControlling Access to the Kubernetes API  describes how Kubernetes implements access\\ncontrol for its own API.\\nAuthenticating  explains authentication in Kubernetes, including the various\\nauthentication options.\\nAuthorization  is separate from authentication, and controls how HTTP calls are handled.\\nUsing Admission Controllers  explains plug-ins which intercepts requests to the\\nKubernetes API server after authentication and authorization.\\nUsing Sysctls in a Kubernetes Cluster  describes to an administrator how to use the sysctl\\ncommand-line tool to set kernel parameters .\\nAuditing  describes how to interact with Kubernetes\\' audit logs.\\nSecuring the kubelet\\nControl Plane-Node communication\\nTLS bootstrapping\\nKubelet authentication/authorization• \\n• \\n• \\n• \\n• \\n• \\n• \\n• \\n• \\n• \\n• \\n• \\n• \\n• \\n• \\n• \\n• \\n• \\n• \\n•', metadata={'source': './PDFS/Concepts.pdf', 'page': 536}),\n",
       " Document(page_content=\"Optional Cluster Services\\nDNS Integration  describes how to resolve a DNS name directly to a Kubernetes service.\\nLogging and Monitoring Cluster Activity  explains how logging in Kubernetes works and\\nhow to implement it.\\nCertificates\\nTo learn how to generate certificates for your cluster, see Certificates .\\nManaging Resources\\nYou've deployed your application and exposed it via a service. Now what? Kubernetes provides\\na number of tools to help you manage your application deployment, including scaling and\\nupdating.\\nOrganizing resource configurations\\nMany applications require multiple resources to be created, such as a Deployment and a\\nService. Management of multiple resources can be simplified by grouping them together in the\\nsame file (separated by --- in YAML). For example:\\napplication/nginx-app.yaml  \\napiVersion : v1\\nkind: Service\\nmetadata :\\n  name : my-nginx-svc\\n  labels :\\n    app: nginx\\nspec:\\n  type: LoadBalancer\\n  ports :\\n  - port: 80\\n  selector :\\n    app: nginx\\n---\\napiVersion : apps/v1\\nkind: Deployment\\nmetadata :\\n  name : my-nginx\\n  labels :\\n    app: nginx\\nspec:\\n  replicas : 3\\n  selector :\\n    matchLabels :\\n      app: nginx• \\n•\", metadata={'source': './PDFS/Concepts.pdf', 'page': 537}),\n",
       " Document(page_content='template :\\n    metadata :\\n      labels :\\n        app: nginx\\n    spec:\\n      containers :\\n      - name : nginx\\n        image : nginx:1.14.2\\n        ports :\\n        - containerPort : 80\\nMultiple resources can be created the same way as a single resource:\\nkubectl apply -f https://k8s.io/examples/application/nginx-app.yaml\\nservice/my-nginx-svc created\\ndeployment.apps/my-nginx created\\nThe resources will be created in the order they appear in the file. Therefore, it\\'s best to specify\\nthe service first, since that will ensure the scheduler can spread the pods associated with the\\nservice as they are created by the controller(s), such as Deployment.\\nkubectl apply  also accepts multiple -f arguments:\\nkubectl apply -f https://k8s.io/examples/application/nginx/nginx-svc.yaml \\\\\\n  -f https://k8s.io/examples/application/nginx/nginx-deployment.yaml\\nIt is a recommended practice to put resources related to the same microservice or application\\ntier into the same file, and to group all of the files associated with your application in the same\\ndirectory. If the tiers of your application bind to each other using DNS, you can deploy all of the\\ncomponents of your stack together.\\nA URL can also be specified as a configuration source, which is handy for deploying directly\\nfrom configuration files checked into GitHub:\\nkubectl apply -f https://k8s.io/examples/application/nginx/nginx-deployment.yaml\\ndeployment.apps/my-nginx created\\nBulk operations in kubectl\\nResource creation isn\\'t the only operation that kubectl  can perform in bulk. It can also extract\\nresource names from configuration files in order to perform other operations, in particular to\\ndelete the same resources you created:\\nkubectl delete -f https://k8s.io/examples/application/nginx-app.yaml\\ndeployment.apps \"my-nginx\" deleted\\nservice \"my-nginx-svc\" deleted\\nIn the case of two resources, you can specify both resources on the command line using the\\nresource/name syntax:\\nkubectl delete deployments/my-nginx services/my-nginx-svc', metadata={'source': './PDFS/Concepts.pdf', 'page': 538}),\n",
       " Document(page_content='For larger numbers of resources, you\\'ll find it easier to specify the selector (label query)\\nspecified using -l or --selector , to filter resources by their labels:\\nkubectl delete deployment,services -l app=nginx\\ndeployment.apps \"my-nginx\" deleted\\nservice \"my-nginx-svc\" deleted\\nBecause kubectl  outputs resource names in the same syntax it accepts, you can chain operations\\nusing $() or xargs :\\nkubectl get $(kubectl create -f docs/concepts/cluster-administration/nginx/ -o name | grep \\nservice )\\nkubectl create -f docs/concepts/cluster-administration/nginx/ -o name | grep service | xargs -i \\nkubectl get {}\\nNAME           TYPE           CLUSTER-IP   EXTERNAL-IP   PORT(S)      AGE\\nmy-nginx-svc   LoadBalancer   10.0.0.208   <pending>     80/TCP       0s\\nWith the above commands, we first create resources under examples/application/nginx/  and\\nprint the resources created with -o name  output format (print each resource as resource/name).\\nThen we grep only the \"service\", and then print it with kubectl get .\\nIf you happen to organize your resources across several subdirectories within a particular\\ndirectory, you can recursively perform the operations on the subdirectories also, by specifying \\n--recursive  or -R alongside the --filename,-f  flag.\\nFor instance, assume there is a directory project/k8s/development  that holds all of the manifests\\nneeded for the development environment, organized by resource type:\\nproject/k8s/development\\n├── configmap\\n│\\xa0\\xa0 └── my-configmap.yaml\\n├── deployment\\n│\\xa0\\xa0 └── my-deployment.yaml\\n└── pvc\\n    └── my-pvc.yaml\\nBy default, performing a bulk operation on project/k8s/development  will stop at the first level\\nof the directory, not processing any subdirectories. If we had tried to create the resources in this\\ndirectory using the following command, we would have encountered an error:\\nkubectl apply -f project/k8s/development\\nerror: you must provide one or more resources by argument or filename (.json|.yaml|.yml|stdin)\\nInstead, specify the --recursive  or -R flag with the --filename,-f  flag as such:\\nkubectl apply -f project/k8s/development --recursive\\nconfigmap/my-config created\\ndeployment.apps/my-deployment created\\npersistentvolumeclaim/my-pvc created', metadata={'source': './PDFS/Concepts.pdf', 'page': 539}),\n",
       " Document(page_content=\"The --recursive  flag works with any operation that accepts the --filename,-f  flag such as: \\nkubectl {create,get,delete,describe,rollout}  etc.\\nThe --recursive  flag also works when multiple -f arguments are provided:\\nkubectl apply -f project/k8s/namespaces -f project/k8s/development --recursive\\nnamespace/development created\\nnamespace/staging created\\nconfigmap/my-config created\\ndeployment.apps/my-deployment created\\npersistentvolumeclaim/my-pvc created\\nIf you're interested in learning more about kubectl , go ahead and read Command line tool\\n(kubectl) .\\nCanary deployments\\nAnother scenario where multiple labels are needed is to distinguish deployments of different\\nreleases or configurations of the same component. It is common practice to deploy a canary  of a\\nnew application release (specified via image tag in the pod template) side by side with the\\nprevious release so that the new release can receive live production traffic before fully rolling it\\nout.\\nFor instance, you can use a track  label to differentiate different releases.\\nThe primary, stable release would have a track  label with value as stable :\\nname: frontend\\nreplicas: 3\\n...\\nlabels:\\n   app: guestbook\\n   tier: frontend\\n   track: stable\\n...\\nimage: gb-frontend:v3\\nand then you can create a new release of the guestbook frontend that carries the track  label\\nwith different value (i.e. canary ), so that two sets of pods would not overlap:\\nname: frontend-canary\\nreplicas: 1\\n...\\nlabels:\\n   app: guestbook\\n   tier: frontend\\n   track: canary\\n...\\nimage: gb-frontend:v4\\nThe frontend service would span both sets of replicas by selecting the common subset of their\\nlabels (i.e. omitting the track  label), so that the traffic will be redirected to both applications:\", metadata={'source': './PDFS/Concepts.pdf', 'page': 540}),\n",
       " Document(page_content=\"selector :\\n   app: guestbook\\n   tier: frontend\\nYou can tweak the number of replicas of the stable and canary releases to determine the ratio of\\neach release that will receive live production traffic (in this case, 3:1). Once you're confident,\\nyou can update the stable track to the new application release and remove the canary one.\\nFor a more concrete example, check the tutorial of deploying Ghost .\\nUpdating annotations\\nSometimes you would want to attach annotations to resources. Annotations are arbitrary non-\\nidentifying metadata for retrieval by API clients such as tools, libraries, etc. This can be done\\nwith kubectl annotate . For example:\\nkubectl annotate pods my-nginx-v4-9gw19 description ='my frontend running nginx'\\nkubectl get pods my-nginx-v4-9gw19 -o yaml\\napiVersion: v1\\nkind: pod\\nmetadata:\\n  annotations:\\n    description: my frontend running nginx\\n...\\nFor more information, see annotations  and kubectl annotate  document.\\nScaling your application\\nWhen load on your application grows or shrinks, use kubectl  to scale your application. For\\ninstance, to decrease the number of nginx replicas from 3 to 1, do:\\nkubectl scale deployment/my-nginx --replicas =1\\ndeployment.apps/my-nginx scaled\\nNow you only have one pod managed by the deployment.\\nkubectl get pods -l app=nginx\\nNAME                        READY     STATUS    RESTARTS   AGE\\nmy-nginx-2035384211-j5fhi   1/1       Running   0          30m\\nTo have the system automatically choose the number of nginx replicas as needed, ranging from\\n1 to 3, do:\\nkubectl autoscale deployment/my-nginx --min =1 --max =3\\nhorizontalpodautoscaler.autoscaling/my-nginx autoscaled\\nNow your nginx replicas will be scaled up and down as needed, automatically.\", metadata={'source': './PDFS/Concepts.pdf', 'page': 541}),\n",
       " Document(page_content=\"For more information, please see kubectl scale , kubectl autoscale  and horizontal pod autoscaler\\ndocument.\\nIn-place updates of resources\\nSometimes it's necessary to make narrow, non-disruptive updates to resources you've created.\\nkubectl apply\\nIt is suggested to maintain a set of configuration files in source control (see configuration as\\ncode ), so that they can be maintained and versioned along with the code for the resources they\\nconfigure. Then, you can use kubectl apply  to push your configuration changes to the cluster.\\nThis command will compare the version of the configuration that you're pushing with the\\nprevious version and apply the changes you've made, without overwriting any automated\\nchanges to properties you haven't specified.\\nkubectl apply -f https://k8s.io/examples/application/nginx/nginx-deployment.yaml\\ndeployment.apps/my-nginx configured\\nNote that kubectl apply  attaches an annotation to the resource in order to determine the\\nchanges to the configuration since the previous invocation. When it's invoked, kubectl apply\\ndoes a three-way diff between the previous configuration, the provided input and the current\\nconfiguration of the resource, in order to determine how to modify the resource.\\nCurrently, resources are created without this annotation, so the first invocation of kubectl apply\\nwill fall back to a two-way diff between the provided input and the current configuration of the\\nresource. During this first invocation, it cannot detect the deletion of properties set when the\\nresource was created. For this reason, it will not remove them.\\nAll subsequent calls to kubectl apply , and other commands that modify the configuration, such\\nas kubectl replace  and kubectl edit , will update the annotation, allowing subsequent calls to \\nkubectl apply  to detect and perform deletions using a three-way diff.\\nkubectl edit\\nAlternatively, you may also update resources with kubectl edit :\\nkubectl edit deployment/my-nginx\\nThis is equivalent to first get the resource, edit it in text editor, and then apply  the resource with\\nthe updated version:\\nkubectl get deployment my-nginx -o yaml > /tmp/nginx.yaml\\nvi /tmp/nginx.yaml\\n# do some edit, and then save the file\\nkubectl apply -f /tmp/nginx.yaml\\ndeployment.apps/my-nginx configured\\nrm /tmp/nginx.yaml\", metadata={'source': './PDFS/Concepts.pdf', 'page': 542}),\n",
       " Document(page_content=\"This allows you to do more significant changes more easily. Note that you can specify the editor\\nwith your EDITOR  or KUBE_EDITOR  environment variables.\\nFor more information, please see kubectl edit  document.\\nkubectl patch\\nYou can use kubectl patch  to update API objects in place. This command supports JSON patch,\\nJSON merge patch, and strategic merge patch. See Update API Objects in Place Using kubectl\\npatch  and kubectl patch .\\nDisruptive updates\\nIn some cases, you may need to update resource fields that cannot be updated once initialized,\\nor you may want to make a recursive change immediately, such as to fix broken pods created\\nby a Deployment. To change such fields, use replace --force , which deletes and re-creates the\\nresource. In this case, you can modify your original configuration file:\\nkubectl replace -f https://k8s.io/examples/application/nginx/nginx-deployment.yaml --force\\ndeployment.apps/my-nginx deleted\\ndeployment.apps/my-nginx replaced\\nUpdating your application without a service outage\\nAt some point, you'll eventually need to update your deployed application, typically by\\nspecifying a new image or image tag, as in the canary deployment scenario above. kubectl\\nsupports several update operations, each of which is applicable to different scenarios.\\nWe'll guide you through how to create and update applications with Deployments.\\nLet's say you were running version 1.14.2 of nginx:\\nkubectl create deployment my-nginx --image =nginx:1.14.2\\ndeployment.apps/my-nginx created\\nwith 3 replicas (so the old and new revisions can coexist):\\nkubectl scale deployment my-nginx --current-replicas =1 --replicas =3\\ndeployment.apps/my-nginx scaled\\nTo update to version 1.16.1, change .spec.template.spec.containers[0].image  from nginx:1.14.2  to \\nnginx:1.16.1  using the previous kubectl commands.\\nkubectl edit deployment/my-nginx\\nThat's it! The Deployment will declaratively update the deployed nginx application\\nprogressively behind the scene. It ensures that only a certain number of old replicas may be\\ndown while they are being updated, and only a certain number of new replicas may be created\\nabove the desired number of pods. To learn more details about it, visit Deployment page .\", metadata={'source': './PDFS/Concepts.pdf', 'page': 543}),\n",
       " Document(page_content=\"What's next\\nLearn about how to use kubectl  for application introspection and debugging .\\nSee Configuration Best Practices and Tips .\\nCluster Networking\\nNetworking is a central part of Kubernetes, but it can be challenging to understand exactly how\\nit is expected to work. There are 4 distinct networking problems to address:\\nHighly-coupled container-to-container communications: this is solved by Pods  and \\nlocalhost  communications.\\nPod-to-Pod communications: this is the primary focus of this document.\\nPod-to-Service communications: this is covered by Services .\\nExternal-to-Service communications: this is also covered by Services.\\nKubernetes is all about sharing machines between applications. Typically, sharing machines\\nrequires ensuring that two applications do not try to use the same ports. Coordinating ports\\nacross multiple developers is very difficult to do at scale and exposes users to cluster-level\\nissues outside of their control.\\nDynamic port allocation brings a lot of complications to the system - every application has to\\ntake ports as flags, the API servers have to know how to insert dynamic port numbers into\\nconfiguration blocks, services have to know how to find each other, etc. Rather than deal with\\nthis, Kubernetes takes a different approach.\\nTo learn about the Kubernetes networking model, see here.\\nHow to implement the Kubernetes network model\\nThe network model is implemented by the container runtime on each node. The most common\\ncontainer runtimes use Container Network Interface  (CNI) plugins to manage their network\\nand security capabilities. Many different CNI plugins exist from many different vendors. Some\\nof these provide only basic features of adding and removing network interfaces, while others\\nprovide more sophisticated solutions, such as integration with other container orchestration\\nsystems, running multiple CNI plugins, advanced IPAM features etc.\\nSee this page  for a non-exhaustive list of networking addons supported by Kubernetes.\\nWhat's next\\nThe early design of the networking model and its rationale are described in more detail in the \\nnetworking design document . For future plans and some on-going efforts that aim to improve\\nKubernetes networking, please refer to the SIG-Network KEPs .\\nLogging Architecture\\nApplication logs can help you understand what is happening inside your application. The logs\\nare particularly useful for debugging problems and monitoring cluster activity. Most modern• \\n• \\n1. \\n2. \\n3. \\n4.\", metadata={'source': './PDFS/Concepts.pdf', 'page': 544}),\n",
       " Document(page_content='applications have some kind of logging mechanism. Likewise, container engines are designed to\\nsupport logging. The easiest and most adopted logging method for containerized applications is\\nwriting to standard output and standard error streams.\\nHowever, the native functionality provided by a container engine or runtime is usually not\\nenough for a complete logging solution.\\nFor example, you may want to access your application\\'s logs if a container crashes, a pod gets\\nevicted, or a node dies.\\nIn a cluster, logs should have a separate storage and lifecycle independent of nodes, pods, or\\ncontainers. This concept is called cluster-level logging .\\nCluster-level logging architectures require a separate backend to store, analyze, and query logs.\\nKubernetes does not provide a native storage solution for log data. Instead, there are many\\nlogging solutions that integrate with Kubernetes. The following sections describe how to handle\\nand store logs on nodes.\\nPod and container logs\\nKubernetes captures logs from each container in a running Pod.\\nThis example uses a manifest for a Pod with a container that writes text to the standard output\\nstream, once per second.\\ndebug/counter-pod.yaml  \\napiVersion : v1\\nkind: Pod\\nmetadata :\\n  name : counter\\nspec:\\n  containers :\\n  - name : count\\n    image : busybox:1.28\\n    args: [/bin/sh, -c,\\n            \\'i=0; while true; do echo \"$i: $(date)\"; i=$((i+1)); sleep 1; done\\' ]\\nTo run this pod, use the following command:\\nkubectl apply -f https://k8s.io/examples/debug/counter-pod.yaml\\nThe output is:\\npod/counter created\\nTo fetch the logs, use the kubectl logs  command, as follows:\\nkubectl logs counter\\nThe output is similar to:', metadata={'source': './PDFS/Concepts.pdf', 'page': 545}),\n",
       " Document(page_content=\"0: Fri Apr  1 11:42:23 UTC 2022\\n1: Fri Apr  1 11:42:24 UTC 2022\\n2: Fri Apr  1 11:42:25 UTC 2022\\nYou can use kubectl logs --previous  to retrieve logs from a previous instantiation of a container.\\nIf your pod has multiple containers, specify which container's logs you want to access by\\nappending a container name to the command, with a -c flag, like so:\\nkubectl logs counter -c count\\nSee the kubectl logs  documentation  for more details.\\nHow nodes handle container logs\\nNode level logging\\nA container runtime handles and redirects any output generated to a containerized\\napplication's stdout  and stderr  streams. Different container runtimes implement this in different\\nways; however, the integration with the kubelet is standardized as the CRI logging format .\\nBy default, if a container restarts, the kubelet keeps one terminated container with its logs. If a\\npod is evicted from the node, all corresponding containers are also evicted, along with their\\nlogs.\\nThe kubelet makes logs available to clients via a special feature of the Kubernetes API. The\\nusual way to access this is by running kubectl logs .\\nLog rotation\\nFEATURE STATE:  Kubernetes v1.21 [stable]\\nYou can configure the kubelet to rotate logs automatically.\\nIf you configure rotation, the kubelet is responsible for rotating container logs and managing\\nthe logging directory structure. The kubelet sends this information to the container runtime\\n(using CRI), and the runtime writes the container logs to the given location.\\nYou can configure two kubelet configuration settings , containerLogMaxSize  and \\ncontainerLogMaxFiles , using the kubelet configuration file . These settings let you configure the\\nmaximum size for each log file and the maximum number of files allowed for each container\\nrespectively.\\nWhen you run kubectl logs  as in the basic logging example, the kubelet on the node handles the\\nrequest and reads directly from the log file. The kubelet returns the content of the log file.\\nNote:\\nOnly the contents of the latest log file are available through kubectl logs .\\nFor example, if a Pod writes 40 MiB of logs and the kubelet rotates logs after 10 MiB, running \\nkubectl logs  returns at most 10MiB of data.\", metadata={'source': './PDFS/Concepts.pdf', 'page': 546}),\n",
       " Document(page_content=\"System component logs\\nThere are two types of system components: those that typically run in a container, and those\\ncomponents directly involved in running containers. For example:\\nThe kubelet and container runtime do not run in containers. The kubelet runs your\\ncontainers (grouped together in pods )\\nThe Kubernetes scheduler, controller manager, and API server run within pods (usually \\nstatic Pods ). The etcd component runs in the control plane, and most commonly also as a\\nstatic pod. If your cluster uses kube-proxy, you typically run this as a DaemonSet .\\nLog locations\\nThe way that the kubelet and container runtime write logs depends on the operating system\\nthat the node uses:\\nLinux\\nWindows\\nOn Linux nodes that use systemd, the kubelet and container runtime write to journald by\\ndefault. You use journalctl  to read the systemd journal; for example: journalctl -u kubelet .\\nIf systemd is not present, the kubelet and container runtime write to .log files in the /var/log\\ndirectory. If you want to have logs written elsewhere, you can indirectly run the kubelet via a\\nhelper tool, kube-log-runner , and use that tool to redirect kubelet logs to a directory that you\\nchoose.\\nThe kubelet always directs your container runtime to write logs into directories within /var/log/\\npods .\\nFor more information on kube-log-runner , read System Logs .\\nBy default, the kubelet writes logs to files within the directory C:\\\\var\\\\logs  (notice that this is\\nnot C:\\\\var\\\\log ).\\nAlthough C:\\\\var\\\\log  is the Kubernetes default location for these logs, several cluster\\ndeployment tools set up Windows nodes to log to C:\\\\var\\\\log\\\\kubelet  instead.\\nIf you want to have logs written elsewhere, you can indirectly run the kubelet via a helper tool, \\nkube-log-runner , and use that tool to redirect kubelet logs to a directory that you choose.\\nHowever, the kubelet always directs your container runtime to write logs within the directory \\nC:\\\\var\\\\log\\\\pods .\\nFor more information on kube-log-runner , read System Logs .\\nFor Kubernetes cluster components that run in pods, these write to files inside the /var/log\\ndirectory, bypassing the default logging mechanism (the components do not write to the\\nsystemd journal). You can use Kubernetes' storage mechanisms to map persistent storage into\\nthe container that runs the component.• \\n• \\n• \\n•\", metadata={'source': './PDFS/Concepts.pdf', 'page': 547}),\n",
       " Document(page_content=\"For details about etcd and its logs, view the etcd documentation . Again, you can use\\nKubernetes' storage mechanisms to map persistent storage into the container that runs the\\ncomponent.\\nNote:\\nIf you deploy Kubernetes cluster components (such as the scheduler) to log to a volume shared\\nfrom the parent node, you need to consider and ensure that those logs are rotated. Kubernetes\\ndoes not manage that log rotation .\\nYour operating system may automatically implement some log rotation - for example, if you\\nshare the directory /var/log  into a static Pod for a component, node-level log rotation treats a\\nfile in that directory the same as a file written by any component outside Kubernetes.\\nSome deploy tools account for that log rotation and automate it; others leave this as your\\nresponsibility.\\nCluster-level logging architectures\\nWhile Kubernetes does not provide a native solution for cluster-level logging, there are several\\ncommon approaches you can consider. Here are some options:\\nUse a node-level logging agent that runs on every node.\\nInclude a dedicated sidecar container for logging in an application pod.\\nPush logs directly to a backend from within an application.\\nUsing a node logging agent\\nUsing a node level logging agent\\nYou can implement cluster-level logging by including a node-level logging agent  on each node.\\nThe logging agent is a dedicated tool that exposes logs or pushes logs to a backend. Commonly,\\nthe logging agent is a container that has access to a directory with log files from all of the\\napplication containers on that node.\\nBecause the logging agent must run on every node, it is recommended to run the agent as a \\nDaemonSet .\\nNode-level logging creates only one agent per node and doesn't require any changes to the\\napplications running on the node.\\nContainers write to stdout and stderr, but with no agreed format. A node-level agent collects\\nthese logs and forwards them for aggregation.\\nUsing a sidecar container with the logging agent\\nYou can use a sidecar container in one of the following ways:\\nThe sidecar container streams application logs to its own stdout .\\nThe sidecar container runs a logging agent, which is configured to pick up logs from an\\napplication container.• \\n• \\n• \\n• \\n•\", metadata={'source': './PDFS/Concepts.pdf', 'page': 548}),\n",
       " Document(page_content='Streaming sidecar container\\nSidecar container with a streaming container\\nBy having your sidecar containers write to their own stdout  and stderr  streams, you can take\\nadvantage of the kubelet and the logging agent that already run on each node. The sidecar\\ncontainers read logs from a file, a socket, or journald. Each sidecar container prints a log to its\\nown stdout  or stderr  stream.\\nThis approach allows you to separate several log streams from different parts of your\\napplication, some of which can lack support for writing to stdout  or stderr . The logic behind\\nredirecting logs is minimal, so it\\'s not a significant overhead. Additionally, because stdout  and \\nstderr  are handled by the kubelet, you can use built-in tools like kubectl logs .\\nFor example, a pod runs a single container, and the container writes to two different log files\\nusing two different formats. Here\\'s a manifest for the Pod:\\nadmin/logging/two-files-counter-pod.yaml  \\napiVersion : v1\\nkind: Pod\\nmetadata :\\n  name : counter\\nspec:\\n  containers :\\n  - name : count\\n    image : busybox:1.28\\n    args:\\n    - /bin/sh\\n    - -c\\n    - >\\n      i=0;\\n      while true;\\n      do\\n        echo \"$i: $(date)\" >> /var/log/1.log;\\n        echo \"$(date) INFO $i\" >> /var/log/2.log;\\n        i=$((i+1));\\n        sleep 1;\\n      done       \\n    volumeMounts :\\n    - name : varlog\\n      mountPath : /var/log\\n  volumes :\\n  - name : varlog\\n    emptyDir : {}\\nIt is not recommended to write log entries with different formats to the same log stream, even if\\nyou managed to redirect both components to the stdout  stream of the container. Instead, you\\ncan create two sidecar containers. Each sidecar container could tail a particular log file from a\\nshared volume and then redirect the logs to its own stdout  stream.\\nHere\\'s a manifest for a pod that has two sidecar containers:\\nadmin/logging/two-files-counter-pod-streaming-sidecar.yaml', metadata={'source': './PDFS/Concepts.pdf', 'page': 549}),\n",
       " Document(page_content='apiVersion : v1\\nkind: Pod\\nmetadata :\\n  name : counter\\nspec:\\n  containers :\\n  - name : count\\n    image : busybox:1.28\\n    args:\\n    - /bin/sh\\n    - -c\\n    - >\\n      i=0;\\n      while true;\\n      do\\n        echo \"$i: $(date)\" >> /var/log/1.log;\\n        echo \"$(date) INFO $i\" >> /var/log/2.log;\\n        i=$((i+1));\\n        sleep 1;\\n      done       \\n    volumeMounts :\\n    - name : varlog\\n      mountPath : /var/log\\n  - name : count-log-1\\n    image : busybox:1.28\\n    args: [/bin/sh, -c, \\'tail -n+1 -F /var/log/1.log\\']\\n    volumeMounts :\\n    - name : varlog\\n      mountPath : /var/log\\n  - name : count-log-2\\n    image : busybox:1.28\\n    args: [/bin/sh, -c, \\'tail -n+1 -F /var/log/2.log\\']\\n    volumeMounts :\\n    - name : varlog\\n      mountPath : /var/log\\n  volumes :\\n  - name : varlog\\n    emptyDir : {}\\nNow when you run this pod, you can access each log stream separately by running the\\nfollowing commands:\\nkubectl logs counter count-log-1\\nThe output is similar to:\\n0: Fri Apr  1 11:42:26 UTC 2022\\n1: Fri Apr  1 11:42:27 UTC 2022\\n2: Fri Apr  1 11:42:28 UTC 2022\\n...\\nkubectl logs counter count-log-2\\nThe output is similar to:', metadata={'source': './PDFS/Concepts.pdf', 'page': 550}),\n",
       " Document(page_content=\"Fri Apr  1 11:42:29 UTC 2022 INFO 0\\nFri Apr  1 11:42:30 UTC 2022 INFO 0\\nFri Apr  1 11:42:31 UTC 2022 INFO 0\\n...\\nIf you installed a node-level agent in your cluster, that agent picks up those log streams\\nautomatically without any further configuration. If you like, you can configure the agent to\\nparse log lines depending on the source container.\\nEven for Pods that only have low CPU and memory usage (order of a couple of millicores for\\ncpu and order of several megabytes for memory), writing logs to a file and then streaming them\\nto stdout  can double how much storage you need on the node. If you have an application that\\nwrites to a single file, it's recommended to set /dev/stdout  as the destination rather than\\nimplement the streaming sidecar container approach.\\nSidecar containers can also be used to rotate log files that cannot be rotated by the application\\nitself. An example of this approach is a small container running logrotate  periodically. However,\\nit's more straightforward to use stdout  and stderr  directly, and leave rotation and retention\\npolicies to the kubelet.\\nSidecar container with a logging agent\\nSidecar container with a logging agent\\nIf the node-level logging agent is not flexible enough for your situation, you can create a sidecar\\ncontainer with a separate logging agent that you have configured specifically to run with your\\napplication.\\nNote:  Using a logging agent in a sidecar container can lead to significant resource\\nconsumption. Moreover, you won't be able to access those logs using kubectl logs  because they\\nare not controlled by the kubelet.\\nHere are two example manifests that you can use to implement a sidecar container with a\\nlogging agent. The first manifest contains a ConfigMap  to configure fluentd.\\nadmin/logging/fluentd-sidecar-config.yaml  \\napiVersion : v1\\nkind: ConfigMap\\nmetadata :\\n  name : fluentd-config\\ndata:\\n  fluentd.conf : |\\n    <source>\\n      type tail\\n      format none\\n      path /var/log/1.log\\n      pos_file /var/log/1.log.pos\\n      tag count.format1\\n    </source>\\n    <source>\\n      type tail\", metadata={'source': './PDFS/Concepts.pdf', 'page': 551}),\n",
       " Document(page_content='format none\\n      path /var/log/2.log\\n      pos_file /var/log/2.log.pos\\n      tag count.format2\\n    </source>\\n    <match **>\\n      type google_cloud\\n    </match>     \\nNote:  In the sample configurations, you can replace fluentd with any logging agent, reading\\nfrom any source inside an application container.\\nThe second manifest describes a pod that has a sidecar container running fluentd. The pod\\nmounts a volume where fluentd can pick up its configuration data.\\nadmin/logging/two-files-counter-pod-agent-sidecar.yaml  \\napiVersion : v1\\nkind: Pod\\nmetadata :\\n  name : counter\\nspec:\\n  containers :\\n  - name : count\\n    image : busybox:1.28\\n    args:\\n    - /bin/sh\\n    - -c\\n    - >\\n      i=0;\\n      while true;\\n      do\\n        echo \"$i: $(date)\" >> /var/log/1.log;\\n        echo \"$(date) INFO $i\" >> /var/log/2.log;\\n        i=$((i+1));\\n        sleep 1;\\n      done       \\n    volumeMounts :\\n    - name : varlog\\n      mountPath : /var/log\\n  - name : count-agent\\n    image : registry.k8s.io/fluentd-gcp:1.30\\n    env:\\n    - name : FLUENTD_ARGS\\n      value : -c /etc/fluentd-config/fluentd.conf\\n    volumeMounts :\\n    - name : varlog\\n      mountPath : /var/log\\n    - name : config-volume\\n      mountPath : /etc/fluentd-config\\n  volumes :\\n  - name : varlog', metadata={'source': './PDFS/Concepts.pdf', 'page': 552}),\n",
       " Document(page_content=\"emptyDir : {}\\n  - name : config-volume\\n    configMap :\\n      name : fluentd-config\\nExposing logs directly from the application\\nExposing logs directly from the application\\nCluster-logging that exposes or pushes logs directly from every application is outside the scope\\nof Kubernetes.\\nWhat's next\\nRead about Kubernetes system logs\\nLearn about Traces For Kubernetes System Components\\nLearn how to customise the termination message  that Kubernetes records when a Pod\\nfails\\nMetrics For Kubernetes System\\nComponents\\nSystem component metrics can give a better look into what is happening inside them. Metrics\\nare particularly useful for building dashboards and alerts.\\nKubernetes components emit metrics in Prometheus format . This format is structured plain\\ntext, designed so that people and machines can both read it.\\nMetrics in Kubernetes\\nIn most cases metrics are available on /metrics  endpoint of the HTTP server. For components\\nthat don't expose endpoint by default, it can be enabled using --bind-address  flag.\\nExamples of those components:\\nkube-controller-manager\\nkube-proxy\\nkube-apiserver\\nkube-scheduler\\nkubelet\\nIn a production environment you may want to configure Prometheus Server  or some other\\nmetrics scraper to periodically gather these metrics and make them available in some kind of\\ntime series database.\\nNote that kubelet  also exposes metrics in /metrics/cadvisor , /metrics/resource  and /metrics/\\nprobes  endpoints. Those metrics do not have the same lifecycle.\\nIf your cluster uses RBAC , reading metrics requires authorization via a user, group or\\nServiceAccount with a ClusterRole that allows accessing /metrics . For example:• \\n• \\n• \\n• \\n• \\n• \\n• \\n•\", metadata={'source': './PDFS/Concepts.pdf', 'page': 553}),\n",
       " Document(page_content='apiVersion : rbac.authorization.k8s.io/v1\\nkind: ClusterRole\\nmetadata :\\n  name : prometheus\\nrules :\\n  - nonResourceURLs :\\n      - \"/metrics\"\\n    verbs :\\n      - get\\nMetric lifecycle\\nAlpha metric → Stable metric → Deprecated metric → Hidden metric → Deleted metric\\nAlpha metrics have no stability guarantees. These metrics can be modified or deleted at any\\ntime.\\nStable metrics are guaranteed to not change. This means:\\nA stable metric without a deprecated signature will not be deleted or renamed\\nA stable metric\\'s type will not be modified\\nDeprecated metrics are slated for deletion, but are still available for use. These metrics include\\nan annotation about the version in which they became deprecated.\\nFor example:\\nBefore deprecation\\n# HELP some_counter this counts things\\n# TYPE some_counter counter\\nsome_counter 0\\nAfter deprecation\\n# HELP some_counter (Deprecated since 1.15.0) this counts things\\n# TYPE some_counter counter\\nsome_counter 0\\nHidden metrics are no longer published for scraping, but are still available for use. To use a\\nhidden metric, please refer to the Show hidden metrics  section.\\nDeleted metrics are no longer published and cannot be used.\\nShow hidden metrics\\nAs described above, admins can enable hidden metrics through a command-line flag on a\\nspecific binary. This intends to be used as an escape hatch for admins if they missed the\\nmigration of the metrics deprecated in the last release.\\nThe flag show-hidden-metrics-for-version  takes a version for which you want to show metrics\\ndeprecated in that release. The version is expressed as x.y, where x is the major version, y is the\\nminor version. The patch version is not needed even though a metrics can be deprecated in a• \\n• \\n• \\n•', metadata={'source': './PDFS/Concepts.pdf', 'page': 554}),\n",
       " Document(page_content='patch release, the reason for that is the metrics deprecation policy runs against the minor\\nrelease.\\nThe flag can only take the previous minor version as it\\'s value. All metrics hidden in previous\\nwill be emitted if admins set the previous version to show-hidden-metrics-for-version . The too\\nold version is not allowed because this violates the metrics deprecated policy.\\nTake metric A as an example, here assumed that A is deprecated in 1.n. According to metrics\\ndeprecated policy, we can reach the following conclusion:\\nIn release 1.n, the metric is deprecated, and it can be emitted by default.\\nIn release 1.n+1 , the metric is hidden by default and it can be emitted by command line \\nshow-hidden-metrics-for-version=1.n .\\nIn release 1.n+2 , the metric should be removed from the codebase. No escape hatch\\nanymore.\\nIf you\\'re upgrading from release 1.12 to 1.13, but still depend on a metric A deprecated in 1.12,\\nyou should set hidden metrics via command line: --show-hidden-metrics=1.12  and remember to\\nremove this metric dependency before upgrading to 1.14\\nComponent metrics\\nkube-controller-manager metrics\\nController manager metrics provide important insight into the performance and health of the\\ncontroller manager. These metrics include common Go language runtime metrics such as\\ngo_routine count and controller specific metrics such as etcd request latencies or Cloudprovider\\n(AWS, GCE, OpenStack) API latencies that can be used to gauge the health of a cluster.\\nStarting from Kubernetes 1.7, detailed Cloudprovider metrics are available for storage\\noperations for GCE, AWS, Vsphere and OpenStack. These metrics can be used to monitor health\\nof persistent volume operations.\\nFor example, for GCE these metrics are called:\\ncloudprovider_gce_api_request_duration_seconds { request = \"instance_list\"}\\ncloudprovider_gce_api_request_duration_seconds { request = \"disk_insert\"}\\ncloudprovider_gce_api_request_duration_seconds { request = \"disk_delete\"}\\ncloudprovider_gce_api_request_duration_seconds { request = \"attach_disk\"}\\ncloudprovider_gce_api_request_duration_seconds { request = \"detach_disk\"}\\ncloudprovider_gce_api_request_duration_seconds { request = \"list_disk\"}\\nkube-scheduler metrics\\nFEATURE STATE:  Kubernetes v1.21 [beta]\\nThe scheduler exposes optional metrics that reports the requested resources and the desired\\nlimits of all running pods. These metrics can be used to build capacity planning dashboards,\\nassess current or historical scheduling limits, quickly identify workloads that cannot schedule\\ndue to lack of resources, and compare actual usage to the pod\\'s request.• \\n• \\n•', metadata={'source': './PDFS/Concepts.pdf', 'page': 555}),\n",
       " Document(page_content=\"The kube-scheduler identifies the resource requests and limits  configured for each Pod; when\\neither a request or limit is non-zero, the kube-scheduler reports a metrics timeseries. The time\\nseries is labelled by:\\nnamespace\\npod name\\nthe node where the pod is scheduled or an empty string if not yet scheduled\\npriority\\nthe assigned scheduler for that pod\\nthe name of the resource (for example, cpu)\\nthe unit of the resource if known (for example, cores )\\nOnce a pod reaches completion (has a restartPolicy  of Never  or OnFailure  and is in the \\nSucceeded  or Failed  pod phase, or has been deleted and all containers have a terminated state)\\nthe series is no longer reported since the scheduler is now free to schedule other pods to run.\\nThe two metrics are called kube_pod_resource_request  and kube_pod_resource_limit .\\nThe metrics are exposed at the HTTP endpoint /metrics/resources  and require the same\\nauthorization as the /metrics  endpoint on the scheduler. You must use the --show-hidden-\\nmetrics-for-version=1.20  flag to expose these alpha stability metrics.\\nDisabling metrics\\nYou can explicitly turn off metrics via command line flag --disabled-metrics . This may be\\ndesired if, for example, a metric is causing a performance problem. The input is a list of disabled\\nmetrics (i.e. --disabled-metrics=metric1,metric2 ).\\nMetric cardinality enforcement\\nMetrics with unbounded dimensions could cause memory issues in the components they\\ninstrument. To limit resource use, you can use the --allow-label-value  command line option to\\ndynamically configure an allow-list of label values for a metric.\\nIn alpha stage, the flag can only take in a series of mappings as metric label allow-list. Each\\nmapping is of the format <metric_name>,<label_name>=<allowed_labels>  where \\n<allowed_labels>  is a comma-separated list of acceptable label names.\\nThe overall format looks like:\\n--allow-label-value <metric_name>,<label_name>='<allow_value1>, <allow_value2>...', \\n<metric_name2>,<label_name>='<allow_value1>, <allow_value2>...', ...\\nHere is an example:\\n--allow-label-value number_count_metric,odd_number='1,3,5', \\nnumber_count_metric,even_number='2,4,6', date_gauge_metric,weekend='Saturday,Sunday'\\nWhat's next\\nRead about the Prometheus text format  for metrics\\nSee the list of stable Kubernetes metrics\\nRead about the Kubernetes deprecation policy• \\n• \\n• \\n• \\n• \\n• \\n• \\n• \\n• \\n•\", metadata={'source': './PDFS/Concepts.pdf', 'page': 556}),\n",
       " Document(page_content='System Logs\\nSystem component logs record events happening in cluster, which can be very useful for\\ndebugging. You can configure log verbosity to see more or less detail. Logs can be as coarse-\\ngrained as showing errors within a component, or as fine-grained as showing step-by-step\\ntraces of events (like HTTP access logs, pod state changes, controller actions, or scheduler\\ndecisions).\\nWarning:  In contrast to the command line flags described here, the log output  itself does not\\nfall under the Kubernetes API stability guarantees: individual log entries and their formatting\\nmay change from one release to the next!\\nKlog\\nklog is the Kubernetes logging library. klog generates log messages for the Kubernetes system\\ncomponents.\\nKubernetes is in the process of simplifying logging in its components. The following klog\\ncommand line flags are deprecated  starting with Kubernetes v1.23 and removed in Kubernetes\\nv1.26:\\n--add-dir-header\\n--alsologtostderr\\n--log-backtrace-at\\n--log-dir\\n--log-file\\n--log-file-max-size\\n--logtostderr\\n--one-output\\n--skip-headers\\n--skip-log-headers\\n--stderrthreshold\\nOutput will always be written to stderr, regardless of the output format. Output redirection is\\nexpected to be handled by the component which invokes a Kubernetes component. This can be\\na POSIX shell or a tool like systemd.\\nIn some cases, for example a distroless container or a Windows system service, those options\\nare not available. Then the kube-log-runner  binary can be used as wrapper around a Kubernetes\\ncomponent to redirect output. A prebuilt binary is included in several Kubernetes base images\\nunder its traditional name as /go-runner  and as kube-log-runner  in server and node release\\narchives.\\nThis table shows how kube-log-runner  invocations correspond to shell redirection:\\nUsagePOSIX shell (such\\nas bash)kube-log-runner <options> <cmd>\\nMerge stderr and stdout,\\nwrite to stdout2>&1 kube-log-runner  (default behavior)\\nRedirect both into log file 1>>/tmp/log 2>&1 kube-log-runner -log-file=/tmp/log\\n2>&1 | tee -a /tmp/log• \\n• \\n• \\n• \\n• \\n• \\n• \\n• \\n• \\n• \\n•', metadata={'source': './PDFS/Concepts.pdf', 'page': 557}),\n",
       " Document(page_content='UsagePOSIX shell (such\\nas bash)kube-log-runner <options> <cmd>\\nCopy into log file and to\\nstdoutkube-log-runner -log-file=/tmp/log -also-\\nstdout\\nRedirect only stdout into log\\nfile>/tmp/logkube-log-runner -log-file=/tmp/log -\\nredirect-stderr=false\\nKlog output\\nAn example of the traditional klog native format:\\nI1025 00:15:15.525108       1 httplog.go:79] GET /api/v1/namespaces/kube-system/pods/metrics-\\nserver-v0.3.1-57c75779f-9p8wg: (1.512ms) 200 [pod_nanny/v0.0.0 (linux/amd64) kubernetes/\\n$Format 10.56.1.19:51756]\\nThe message string may contain line breaks:\\nI1025 00:15:15.525108       1 example.go:79] This is a message\\nwhich has a line break.\\nStructured Logging\\nFEATURE STATE:  Kubernetes v1.23 [beta]\\nWarning:\\nMigration to structured log messages is an ongoing process. Not all log messages are structured\\nin this version. When parsing log files, you must also handle unstructured log messages.\\nLog formatting and value serialization are subject to change.\\nStructured logging introduces a uniform structure in log messages allowing for programmatic\\nextraction of information. You can store and process structured logs with less effort and cost.\\nThe code which generates a log message determines whether it uses the traditional\\nunstructured klog output or structured logging.\\nThe default formatting of structured log messages is as text, with a format that is backward\\ncompatible with traditional klog:\\n<klog header> \"<message>\" <key1>=\"<value1>\" <key2>=\"<value2>\" ...\\nExample:\\nI1025 00:15:15.525108       1 controller_utils.go:116] \"Pod status updated\" pod=\"kube-system/\\nkubedns\" status=\"ready\"\\nStrings are quoted. Other values are formatted with %+v, which may cause log messages to\\ncontinue on the next line depending on the data .\\nI1025 00:15:15.525108       1 example.go:116] \"Example\" data=\"This is text with a line break\\\\nand \\n\\\\\"quotation marks\\\\\".\" someInt=1 someFloat=0.1 someStruct={StringField: First line,\\nsecond line.}', metadata={'source': './PDFS/Concepts.pdf', 'page': 558}),\n",
       " Document(page_content='Contextual Logging\\nFEATURE STATE:  Kubernetes v1.24 [alpha]\\nContextual logging builds on top of structured logging. It is primarily about how developers use\\nlogging calls: code based on that concept is more flexible and supports additional use cases as\\ndescribed in the Contextual Logging KEP .\\nIf developers use additional functions like WithValues  or WithName  in their components, then\\nlog entries contain additional information that gets passed into functions by their caller.\\nCurrently this is gated behind the StructuredLogging  feature gate and disabled by default. The\\ninfrastructure for this was added in 1.24 without modifying components. The component-base/\\nlogs/example  command demonstrates how to use the new logging calls and how a component\\nbehaves that supports contextual logging.\\n$ cd $GOPATH /src/k8s.io/kubernetes/staging/src/k8s.io/component-base/logs/example/cmd/\\n$ go run . --help\\n...\\n      --feature-gates mapStringBool  A set of key=value pairs that describe feature gates for \\nalpha/experimental features. Options are:\\n                                     AllAlpha=true|false (ALPHA - default=false)\\n                                     AllBeta=true|false (BETA - default=false)\\n                                     ContextualLogging=true|false (ALPHA - default=false)\\n$ go run . --feature-gates ContextualLogging =true\\n...\\nI0404 18:00:02.916429  451895 logger.go:94] \"example/myname: runtime\" foo=\"bar\" \\nduration=\"1m0s\"\\nI0404 18:00:02.916447  451895 logger.go:95] \"example: another runtime\" foo=\"bar\" \\nduration=\"1m0s\"\\nThe example  prefix and foo=\"bar\"  were added by the caller of the function which logs the \\nruntime  message and duration=\"1m0s\"  value, without having to modify that function.\\nWith contextual logging disable, WithValues  and WithName  do nothing and log calls go\\nthrough the global klog logger. Therefore this additional information is not in the log output\\nanymore:\\n$ go run . --feature-gates ContextualLogging =false\\n...\\nI0404 18:03:31.171945  452150 logger.go:94] \"runtime\" duration=\"1m0s\"\\nI0404 18:03:31.171962  452150 logger.go:95] \"another runtime\" duration=\"1m0s\"\\nJSON log format\\nFEATURE STATE:  Kubernetes v1.19 [alpha]\\nWarning:\\nJSON output does not support many standard klog flags. For list of unsupported klog flags, see\\nthe Command line tool reference .\\nNot all logs are guaranteed to be written in JSON format (for example, during process start). If\\nyou intend to parse logs, make sure you can handle log lines that are not JSON as well.', metadata={'source': './PDFS/Concepts.pdf', 'page': 559}),\n",
       " Document(page_content='Field names and JSON serialization are subject to change.\\nThe --logging-format=json  flag changes the format of logs from klog native format to JSON\\nformat. Example of JSON log format (pretty printed):\\n{\\n   \"ts\": 1580306777.04728 ,\\n   \"v\": 4,\\n   \"msg\" : \"Pod status updated\" ,\\n   \"pod\" :{\\n      \"name\" : \"nginx-1\" ,\\n      \"namespace\" : \"default\"\\n   },\\n   \"status\" : \"ready\"\\n}\\nKeys with special meaning:\\nts - timestamp as Unix time (required, float)\\nv - verbosity (only for info and not for error messages, int)\\nerr - error string (optional, string)\\nmsg - message (required, string)\\nList of components currently supporting JSON format:\\nkube-controller-manager\\nkube-apiserver\\nkube-scheduler\\nkubelet\\nLog verbosity level\\nThe -v flag controls log verbosity. Increasing the value increases the number of logged events.\\nDecreasing the value decreases the number of logged events. Increasing verbosity settings logs\\nincreasingly less severe events. A verbosity setting of 0 logs only critical events.\\nLog location\\nThere are two types of system components: those that run in a container and those that do not\\nrun in a container. For example:\\nThe Kubernetes scheduler and kube-proxy run in a container.\\nThe kubelet and container runtime  do not run in containers.\\nOn machines with systemd, the kubelet and container runtime write to journald. Otherwise,\\nthey write to .log files in the /var/log  directory. System components inside containers always\\nwrite to .log files in the /var/log  directory, bypassing the default logging mechanism. Similar to\\nthe container logs, you should rotate system component logs in the /var/log  directory. In\\nKubernetes clusters created by the kube-up.sh  script, log rotation is configured by the logrotate\\ntool. The logrotate  tool rotates logs daily, or once the log size is greater than 100MB.• \\n• \\n• \\n• \\n• \\n• \\n• \\n• \\n• \\n•', metadata={'source': './PDFS/Concepts.pdf', 'page': 560}),\n",
       " Document(page_content='Log query\\nFEATURE STATE:  Kubernetes v1.27 [alpha]\\nTo help with debugging issues on nodes, Kubernetes v1.27 introduced a feature that allows\\nviewing logs of services running on the node. To use the feature, ensure that the \\nNodeLogQuery  feature gate  is enabled for that node, and that the kubelet configuration options \\nenableSystemLogHandler  and enableSystemLogQuery  are both set to true. On Linux we assume\\nthat service logs are available via journald. On Windows we assume that service logs are\\navailable in the application log provider. On both operating systems, logs are also available by\\nreading files within /var/log/ .\\nProvided you are authorized to interact with node objects, you can try out this alpha feature on\\nall your nodes or just a subset. Here is an example to retrieve the kubelet service logs from a\\nnode:\\n# Fetch kubelet logs from a node named node-1.example\\nkubectl get --raw \"/api/v1/nodes/node-1.example/proxy/logs/?query=kubelet\"\\nYou can also fetch files, provided that the files are in a directory that the kubelet allows for log\\nfetches. For example, you can fetch a log from /var/log  on a Linux node:\\nkubectl get --raw \"/api/v1/nodes/<insert-node-name-here>/proxy/logs/?query=/<insert-log-file-\\nname-here>\"\\nThe kubelet uses heuristics to retrieve logs. This helps if you are not aware whether a given\\nsystem service is writing logs to the operating system\\'s native logger like journald or to a log\\nfile in /var/log/ . The heuristics first checks the native logger and if that is not available attempts\\nto retrieve the first logs from /var/log/<servicename>  or /var/log/<servicename>.log  or /var/\\nlog/<servicename>/<servicename>.log .\\nThe complete list of options that can be used are:\\nOption Description\\nboot boot show messages from a specific system boot\\npattern pattern filters log entries by the provided PERL-compatible regular expression\\nquery query specifies services(s) or files from which to return logs (required)\\nsinceTime an RFC3339  timestamp from which to show logs (inclusive)\\nuntilTime an RFC3339  timestamp until which to show logs (inclusive)\\ntailLinesspecify how many lines from the end of the log to retrieve; the default is to fetch the\\nwhole log\\nExample of a more complex query:\\n# Fetch kubelet logs from a node named node-1.example that have the word \"error\"\\nkubectl get --raw \"/api/v1/nodes/node-1.example/proxy/logs/?query=kubelet&pattern=error\"\\nWhat\\'s next\\nRead about the Kubernetes Logging Architecture\\nRead about Structured Logging\\nRead about Contextual Logging• \\n• \\n•', metadata={'source': './PDFS/Concepts.pdf', 'page': 561}),\n",
       " Document(page_content='Read about deprecation of klog flags\\nRead about the Conventions for logging severity\\nTraces For Kubernetes System\\nComponents\\nFEATURE STATE:  Kubernetes v1.27 [beta]\\nSystem component traces record the latency of and relationships between operations in the\\ncluster.\\nKubernetes components emit traces using the OpenTelemetry Protocol  with the gRPC exporter\\nand can be collected and routed to tracing backends using an OpenTelemetry Collector .\\nTrace Collection\\nFor a complete guide to collecting traces and using the collector, see Getting Started with the\\nOpenTelemetry Collector . However, there are a few things to note that are specific to\\nKubernetes components.\\nBy default, Kubernetes components export traces using the grpc exporter for OTLP on the \\nIANA OpenTelemetry port , 4317. As an example, if the collector is running as a sidecar to a\\nKubernetes component, the following receiver configuration will collect spans and log them to\\nstandard output:\\nreceivers :\\n  otlp:\\n    protocols :\\n      grpc:\\nexporters :\\n  # Replace this exporter with the exporter for your backend\\n  logging :\\n    logLevel : debug\\nservice :\\n  pipelines :\\n    traces :\\n      receivers : [otlp]\\n      exporters : [logging]\\nComponent traces\\nkube-apiserver traces\\nThe kube-apiserver generates spans for incoming HTTP requests, and for outgoing requests to\\nwebhooks, etcd, and re-entrant requests. It propagates the W3C Trace Context  with outgoing\\nrequests but does not make use of the trace context attached to incoming requests, as the kube-\\napiserver is often a public endpoint.• \\n•', metadata={'source': './PDFS/Concepts.pdf', 'page': 562}),\n",
       " Document(page_content='Enabling tracing in the kube-apiserver\\nTo enable tracing, provide the kube-apiserver with a tracing configuration file with --tracing-\\nconfig-file=<path-to-config> . This is an example config that records spans for 1 in 10000\\nrequests, and uses the default OpenTelemetry endpoint:\\napiVersion : apiserver.config.k8s.io/v1beta1\\nkind: TracingConfiguration\\n# default value\\n#endpoint: localhost:4317\\nsamplingRatePerMillion : 100\\nFor more information about the TracingConfiguration  struct, see API server config API\\n(v1beta1) .\\nkubelet traces\\nFEATURE STATE:  Kubernetes v1.27 [beta]\\nThe kubelet CRI interface and authenticated http servers are instrumented to generate trace\\nspans. As with the apiserver, the endpoint and sampling rate are configurable. Trace context\\npropagation is also configured. A parent span\\'s sampling decision is always respected. A\\nprovided tracing configuration sampling rate will apply to spans without a parent. Enabled\\nwithout a configured endpoint, the default OpenTelemetry Collector receiver address of\\n\"localhost:4317\" is set.\\nEnabling tracing in the kubelet\\nTo enable tracing, apply the tracing configuration . This is an example snippet of a kubelet\\nconfig that records spans for 1 in 10000 requests, and uses the default OpenTelemetry endpoint:\\napiVersion : kubelet.config.k8s.io/v1beta1\\nkind: KubeletConfiguration\\nfeatureGates :\\n  KubeletTracing : true\\ntracing :\\n  # default value\\n  #endpoint: localhost:4317\\n  samplingRatePerMillion : 100\\nIf the samplingRatePerMillion  is set to one million ( 1000000 ), then every span will be sent to the\\nexporter.\\nThe kubelet in Kubernetes v1.28 collects spans from the garbage collection, pod\\nsynchronization routine as well as every gRPC method. The kubelet propagates trace context\\nwith gRPC requests so that container runtimes with trace instrumentation, such as CRI-O and\\ncontainerd, can associate their exported spans with the trace context from the kubelet. The\\nresulting traces will have parent-child links between kubelet and container runtime spans,\\nproviding helpful context when debugging node issues.\\nPlease note that exporting spans always comes with a small performance overhead on the\\nnetworking and CPU side, depending on the overall configuration of the system. If there is any\\nissue like that in a cluster which is running with tracing enabled, then mitigate the problem by', metadata={'source': './PDFS/Concepts.pdf', 'page': 563}),\n",
       " Document(page_content=\"either reducing the samplingRatePerMillion  or disabling tracing completely by removing the\\nconfiguration.\\nStability\\nTracing instrumentation is still under active development, and may change in a variety of ways.\\nThis includes span names, attached attributes, instrumented endpoints, etc. Until this feature\\ngraduates to stable, there are no guarantees of backwards compatibility for tracing\\ninstrumentation.\\nWhat's next\\nRead about Getting Started with the OpenTelemetry Collector\\nProxies in Kubernetes\\nThis page explains proxies used with Kubernetes.\\nProxies\\nThere are several different proxies you may encounter when using Kubernetes:\\nThe kubectl proxy :\\nruns on a user's desktop or in a pod\\nproxies from a localhost address to the Kubernetes apiserver\\nclient to proxy uses HTTP\\nproxy to apiserver uses HTTPS\\nlocates apiserver\\nadds authentication headers\\nThe apiserver proxy :\\nis a bastion built into the apiserver\\nconnects a user outside of the cluster to cluster IPs which otherwise might not be\\nreachable\\nruns in the apiserver processes\\nclient to proxy uses HTTPS (or http if apiserver so configured)\\nproxy to target may use HTTP or HTTPS as chosen by proxy using available\\ninformation\\ncan be used to reach a Node, Pod, or Service\\ndoes load balancing when used to reach a Service\\nThe kube proxy :\\nruns on each node\\nproxies UDP, TCP and SCTP\\ndoes not understand HTTP\\nprovides load balancing\\nis only used to reach services• \\n1. \\n◦ \\n◦ \\n◦ \\n◦ \\n◦ \\n◦ \\n2. \\n◦ \\n◦ \\n◦ \\n◦ \\n◦ \\n◦ \\n◦ \\n3. \\n◦ \\n◦ \\n◦ \\n◦ \\n◦\", metadata={'source': './PDFS/Concepts.pdf', 'page': 564}),\n",
       " Document(page_content='A Proxy/Load-balancer in front of apiserver(s):\\nexistence and implementation varies from cluster to cluster (e.g. nginx)\\nsits between all clients and one or more apiservers\\nacts as load balancer if there are several apiservers.\\nCloud Load Balancers on external services:\\nare provided by some cloud providers (e.g. AWS ELB, Google Cloud Load Balancer)\\nare created automatically when the Kubernetes service has type LoadBalancer\\nusually supports UDP/TCP only\\nSCTP support is up to the load balancer implementation of the cloud provider\\nimplementation varies by cloud provider.\\nKubernetes users will typically not need to worry about anything other than the first two types.\\nThe cluster admin will typically ensure that the latter types are set up correctly.\\nRequesting redirects\\nProxies have replaced redirect capabilities. Redirects have been deprecated.\\nAPI Priority and Fairness\\nFEATURE STATE:  Kubernetes v1.20 [beta]\\nControlling the behavior of the Kubernetes API server in an overload situation is a key task for\\ncluster administrators. The kube-apiserver  has some controls available (i.e. the --max-requests-\\ninflight  and --max-mutating-requests-inflight  command-line flags) to limit the amount of\\noutstanding work that will be accepted, preventing a flood of inbound requests from\\noverloading and potentially crashing the API server, but these flags are not enough to ensure\\nthat the most important requests get through in a period of high traffic.\\nThe API Priority and Fairness feature (APF) is an alternative that improves upon\\naforementioned max-inflight limitations. APF classifies and isolates requests in a more fine-\\ngrained way. It also introduces a limited amount of queuing, so that no requests are rejected in\\ncases of very brief bursts. Requests are dispatched from queues using a fair queuing technique\\nso that, for example, a poorly-behaved controller  need not starve others (even at the same\\npriority level).\\nThis feature is designed to work well with standard controllers, which use informers and react\\nto failures of API requests with exponential back-off, and other clients that also work this way.\\nCaution:  Some requests classified as \"long-running\"—such as remote command execution or\\nlog tailing—are not subject to the API Priority and Fairness filter. This is also true for the --max-\\nrequests-inflight  flag without the API Priority and Fairness feature enabled. API Priority and\\nFairness does apply to watch  requests. When API Priority and Fairness is disabled, watch\\nrequests are not subject to the --max-requests-inflight  limit.4. \\n◦ \\n◦ \\n◦ \\n5. \\n◦ \\n◦ \\n◦ \\n◦ \\n◦', metadata={'source': './PDFS/Concepts.pdf', 'page': 565}),\n",
       " Document(page_content='Enabling/Disabling API Priority and Fairness\\nThe API Priority and Fairness feature is controlled by a feature gate and is enabled by default.\\nSee Feature Gates  for a general explanation of feature gates and how to enable and disable\\nthem. The name of the feature gate for APF is \"APIPriorityAndFairness\". This feature also\\ninvolves an API Group  with: (a) a v1alpha1  version and a v1beta1  version, disabled by default,\\nand (b) v1beta2  and v1beta3  versions, enabled by default. You can disable the feature gate and\\nAPI group beta versions by adding the following command-line flags to your kube-apiserver\\ninvocation:\\nkube-apiserver \\\\\\n--feature-gates =APIPriorityAndFairness =false \\\\\\n--runtime-config =flowcontrol.apiserver.k8s.io/v1beta2 =false,flowcontrol.apiserver.k8s.io/\\nv1beta3 =false \\\\\\n # ...and other flags as usual\\nAlternatively, you can enable the v1alpha1 and v1beta1 versions of the API group with --\\nruntime-config=flowcontrol.apiserver.k8s.io/v1alpha1=true,flowcontrol.apiserver.k8s.io/\\nv1beta1=true .\\nThe command-line flag --enable-priority-and-fairness=false  will disable the API Priority and\\nFairness feature, even if other flags have enabled it.\\nConcepts\\nThere are several distinct features involved in the API Priority and Fairness feature. Incoming\\nrequests are classified by attributes of the request using FlowSchemas , and assigned to priority\\nlevels. Priority levels add a degree of isolation by maintaining separate concurrency limits, so\\nthat requests assigned to different priority levels cannot starve each other. Within a priority\\nlevel, a fair-queuing algorithm prevents requests from different flows  from starving each other,\\nand allows for requests to be queued to prevent bursty traffic from causing failed requests when\\nthe average load is acceptably low.\\nPriority Levels\\nWithout APF enabled, overall concurrency in the API server is limited by the kube-apiserver\\nflags --max-requests-inflight  and --max-mutating-requests-inflight . With APF enabled, the\\nconcurrency limits defined by these flags are summed and then the sum is divided up among a\\nconfigurable set of priority levels . Each incoming request is assigned to a single priority level,\\nand each priority level will only dispatch as many concurrent requests as its particular limit\\nallows.\\nThe default configuration, for example, includes separate priority levels for leader-election\\nrequests, requests from built-in controllers, and requests from Pods. This means that an ill-\\nbehaved Pod that floods the API server with requests cannot prevent leader election or actions\\nby the built-in controllers from succeeding.\\nThe concurrency limits of the priority levels are periodically adjusted, allowing under-utilized\\npriority levels to temporarily lend concurrency to heavily-utilized levels. These limits are based\\non nominal limits and bounds on how much concurrency a priority level may lend and how\\nmuch it may borrow, all derived from the configuration objects mentioned below.', metadata={'source': './PDFS/Concepts.pdf', 'page': 566}),\n",
       " Document(page_content='Seats Occupied by a Request\\nThe above description of concurrency management is the baseline story. Requests have different\\ndurations but are counted equally at any given moment when comparing against a priority\\nlevel\\'s concurrency limit. In the baseline story, each request occupies one unit of concurrency.\\nThe word \"seat\" is used to mean one unit of concurrency, inspired by the way each passenger\\non a train or aircraft takes up one of the fixed supply of seats.\\nBut some requests take up more than one seat. Some of these are list requests that the server\\nestimates will return a large number of objects. These have been found to put an exceptionally\\nheavy burden on the server. For this reason, the server estimates the number of objects that will\\nbe returned and considers the request to take a number of seats that is proportional to that\\nestimated number.\\nExecution time tweaks for watch requests\\nAPI Priority and Fairness manages watch  requests, but this involves a couple more excursions\\nfrom the baseline behavior. The first concerns how long a watch  request is considered to\\noccupy its seat. Depending on request parameters, the response to a watch  request may or may\\nnot begin with create  notifications for all the relevant pre-existing objects. API Priority and\\nFairness considers a watch  request to be done with its seat once that initial burst of\\nnotifications, if any, is over.\\nThe normal notifications are sent in a concurrent burst to all relevant watch  response streams\\nwhenever the server is notified of an object create/update/delete. To account for this work, API\\nPriority and Fairness considers every write request to spend some additional time occupying\\nseats after the actual writing is done. The server estimates the number of notifications to be sent\\nand adjusts the write request\\'s number of seats and seat occupancy time to include this extra\\nwork.\\nQueuing\\nEven within a priority level there may be a large number of distinct sources of traffic. In an\\noverload situation, it is valuable to prevent one stream of requests from starving others (in\\nparticular, in the relatively common case of a single buggy client flooding the kube-apiserver\\nwith requests, that buggy client would ideally not have much measurable impact on other\\nclients at all). This is handled by use of a fair-queuing algorithm to process requests that are\\nassigned the same priority level. Each request is assigned to a flow, identified by the name of\\nthe matching FlowSchema plus a flow distinguisher  — which is either the requesting user, the\\ntarget resource\\'s namespace, or nothing — and the system attempts to give approximately equal\\nweight to requests in different flows of the same priority level. To enable distinct handling of\\ndistinct instances, controllers that have many instances should authenticate with distinct\\nusernames\\nAfter classifying a request into a flow, the API Priority and Fairness feature then may assign the\\nrequest to a queue. This assignment uses a technique known as shuffle sharding , which makes\\nrelatively efficient use of queues to insulate low-intensity flows from high-intensity flows.\\nThe details of the queuing algorithm are tunable for each priority level, and allow\\nadministrators to trade off memory use, fairness (the property that independent flows will all\\nmake progress when total traffic exceeds capacity), tolerance for bursty traffic, and the added\\nlatency induced by queuing.', metadata={'source': './PDFS/Concepts.pdf', 'page': 567}),\n",
       " Document(page_content='Exempt requests\\nSome requests are considered sufficiently important that they are not subject to any of the\\nlimitations imposed by this feature. These exemptions prevent an improperly-configured flow\\ncontrol configuration from totally disabling an API server.\\nResources\\nThe flow control API involves two kinds of resources. PriorityLevelConfigurations  define the\\navailable priority levels, the share of the available concurrency budget that each can handle,\\nand allow for fine-tuning queuing behavior. FlowSchemas  are used to classify individual\\ninbound requests, matching each to a single PriorityLevelConfiguration. There is also a \\nv1alpha1  version of the same API group, and it has the same Kinds with the same syntax and\\nsemantics.\\nPriorityLevelConfiguration\\nA PriorityLevelConfiguration represents a single priority level. Each PriorityLevelConfiguration\\nhas an independent limit on the number of outstanding requests, and limitations on the number\\nof queued requests.\\nThe nominal concurrency limit for a PriorityLevelConfiguration is not specified in an absolute\\nnumber of seats, but rather in \"nominal concurrency shares.\" The total concurrency limit for the\\nAPI Server is distributed among the existing PriorityLevelConfigurations in proportion to these\\nshares, to give each level its nominal limit in terms of seats. This allows a cluster administrator\\nto scale up or down the total amount of traffic to a server by restarting kube-apiserver  with a\\ndifferent value for --max-requests-inflight  (or --max-mutating-requests-inflight ), and all\\nPriorityLevelConfigurations will see their maximum allowed concurrency go up (or down) by\\nthe same fraction.\\nCaution:  In the versions before v1beta3  the relevant PriorityLevelConfiguration field is named\\n\"assured concurrency shares\" rather than \"nominal concurrency shares\". Also, in Kubernetes\\nrelease 1.25 and earlier there were no periodic adjustments: the nominal/assured limits were\\nalways applied without adjustment.\\nThe bounds on how much concurrency a priority level may lend and how much it may borrow\\nare expressed in the PriorityLevelConfiguration as percentages of the level\\'s nominal limit.\\nThese are resolved to absolute numbers of seats by multiplying with the nominal limit / 100.0\\nand rounding. The dynamically adjusted concurrency limit of a priority level is constrained to\\nlie between (a) a lower bound of its nominal limit minus its lendable seats and (b) an upper\\nbound of its nominal limit plus the seats it may borrow. At each adjustment the dynamic limits\\nare derived by each priority level reclaiming any lent seats for which demand recently appeared\\nand then jointly fairly responding to the recent seat demand on the priority levels, within the\\nbounds just described.\\nCaution:  With the Priority and Fairness feature enabled, the total concurrency limit for the\\nserver is set to the sum of --max-requests-inflight  and --max-mutating-requests-inflight . There\\nis no longer any distinction made between mutating and non-mutating requests; if you want to\\ntreat them separately for a given resource, make separate FlowSchemas that match the\\nmutating and non-mutating verbs respectively.', metadata={'source': './PDFS/Concepts.pdf', 'page': 568}),\n",
       " Document(page_content='When the volume of inbound requests assigned to a single PriorityLevelConfiguration is more\\nthan its permitted concurrency level, the type field of its specification determines what will\\nhappen to extra requests. A type of Reject  means that excess traffic will immediately be rejected\\nwith an HTTP 429 (Too Many Requests) error. A type of Queue  means that requests above the\\nthreshold will be queued, with the shuffle sharding and fair queuing techniques used to balance\\nprogress between request flows.\\nThe queuing configuration allows tuning the fair queuing algorithm for a priority level. Details\\nof the algorithm can be read in the enhancement proposal , but in short:\\nIncreasing queues  reduces the rate of collisions between different flows, at the cost of\\nincreased memory usage. A value of 1 here effectively disables the fair-queuing logic, but\\nstill allows requests to be queued.\\nIncreasing queueLengthLimit  allows larger bursts of traffic to be sustained without\\ndropping any requests, at the cost of increased latency and memory usage.\\nChanging handSize  allows you to adjust the probability of collisions between different\\nflows and the overall concurrency available to a single flow in an overload situation.\\nNote:  A larger handSize  makes it less likely for two individual flows to collide (and\\ntherefore for one to be able to starve the other), but more likely that a small number of\\nflows can dominate the apiserver. A larger handSize  also potentially increases the amount\\nof latency that a single high-traffic flow can cause. The maximum number of queued\\nrequests possible from a single flow is handSize * queueLengthLimit .\\nFollowing is a table showing an interesting collection of shuffle sharding configurations,\\nshowing for each the probability that a given mouse (low-intensity flow) is squished by the\\nelephants (high-intensity flows) for an illustrative collection of numbers of elephants. See \\nhttps://play.golang.org/p/Gi0PLgVHiUg  , which computes this table.\\nExample Shuffle Sharding Configurations\\nHandSize Queues 1 elephant 4 elephants 16 elephants\\n12 32 4.428838398950118e-09 0.11431348830099144 0.9935089607656024\\n10 32 1.550093439632541e-08 0.0626479840223545 0.9753101519027554\\n10 64 6.601827268370426e-12 0.00045571320990370776 0.49999929150089345\\n9 64 3.6310049976037345e-11 0.00045501212304112273 0.4282314876454858\\n8 64 2.25929199850899e-10 0.0004886697053040446 0.35935114681123076\\n8 128 6.994461389026097e-13 3.4055790161620863e-06 0.02746173137155063\\n7 128 1.0579122850901972e-11 6.960839379258192e-06 0.02406157386340147\\n7 256 7.597695465552631e-14 6.728547142019406e-08 0.0006709661542533682\\n6 256 2.7134626662687968e-12 2.9516464018476436e-07 0.0008895654642000348\\n6 512 4.116062922897309e-14 4.982983350480894e-09 2.26025764343413e-05\\n6 1024 6.337324016514285e-16 8.09060164312957e-11 4.517408062903668e-07\\nFlowSchema\\nA FlowSchema matches some inbound requests and assigns them to a priority level. Every\\ninbound request is tested against FlowSchemas, starting with those with the numerically lowest\\nmatchingPrecedence  and working upward. The first match wins.• \\n• \\n•', metadata={'source': './PDFS/Concepts.pdf', 'page': 569}),\n",
       " Document(page_content=\"Caution:  Only the first matching FlowSchema for a given request matters. If multiple\\nFlowSchemas match a single inbound request, it will be assigned based on the one with the\\nhighest matchingPrecedence . If multiple FlowSchemas with equal matchingPrecedence  match\\nthe same request, the one with lexicographically smaller name  will win, but it's better not to\\nrely on this, and instead to ensure that no two FlowSchemas have the same \\nmatchingPrecedence .\\nA FlowSchema matches a given request if at least one of its rules  matches. A rule matches if at\\nleast one of its subjects  and at least one of its resourceRules  or nonResourceRules  (depending\\non whether the incoming request is for a resource or non-resource URL) match the request.\\nFor the name  field in subjects, and the verbs , apiGroups , resources , namespaces , and \\nnonResourceURLs  fields of resource and non-resource rules, the wildcard * may be specified to\\nmatch all values for the given field, effectively removing it from consideration.\\nA FlowSchema's distinguisherMethod.type  determines how requests matching that schema will\\nbe separated into flows. It may be ByUser , in which one requesting user will not be able to\\nstarve other users of capacity; ByNamespace , in which requests for resources in one namespace\\nwill not be able to starve requests for resources in other namespaces of capacity; or blank (or \\ndistinguisherMethod  may be omitted entirely), in which all requests matched by this\\nFlowSchema will be considered part of a single flow. The correct choice for a given FlowSchema\\ndepends on the resource and your particular environment.\\nDefaults\\nEach kube-apiserver maintains two sorts of APF configuration objects: mandatory and\\nsuggested.\\nMandatory Configuration Objects\\nThe four mandatory configuration objects reflect fixed built-in guardrail behavior. This is\\nbehavior that the servers have before those objects exist, and when those objects exist their\\nspecs reflect this behavior. The four mandatory objects are as follows.\\nThe mandatory exempt  priority level is used for requests that are not subject to flow\\ncontrol at all: they will always be dispatched immediately. The mandatory exempt\\nFlowSchema classifies all requests from the system:masters  group into this priority level.\\nYou may define other FlowSchemas that direct other requests to this priority level, if\\nappropriate.\\nThe mandatory catch-all  priority level is used in combination with the mandatory catch-\\nall FlowSchema to make sure that every request gets some kind of classification.\\nTypically you should not rely on this catch-all configuration, and should create your own\\ncatch-all FlowSchema and PriorityLevelConfiguration (or use the suggested global-default\\npriority level that is installed by default) as appropriate. Because it is not expected to be\\nused normally, the mandatory catch-all  priority level has a very small concurrency share\\nand does not queue requests.\\nSuggested Configuration Objects\\nThe suggested FlowSchemas and PriorityLevelConfigurations constitute a reasonable default\\nconfiguration. You can modify these and/or create additional configuration objects if you want.• \\n•\", metadata={'source': './PDFS/Concepts.pdf', 'page': 570}),\n",
       " Document(page_content=\"If your cluster is likely to experience heavy load then you should consider what configuration\\nwill work best.\\nThe suggested configuration groups requests into six priority levels:\\nThe node-high  priority level is for health updates from nodes.\\nThe system  priority level is for non-health requests from the system:nodes  group, i.e.\\nKubelets, which must be able to contact the API server in order for workloads to be able\\nto schedule on them.\\nThe leader-election  priority level is for leader election requests from built-in controllers\\n(in particular, requests for endpoints , configmaps , or leases  coming from the \\nsystem:kube-controller-manager  or system:kube-scheduler  users and service accounts in\\nthe kube-system  namespace). These are important to isolate from other traffic because\\nfailures in leader election cause their controllers to fail and restart, which in turn causes\\nmore expensive traffic as the new controllers sync their informers.\\nThe workload-high  priority level is for other requests from built-in controllers.\\nThe workload-low  priority level is for requests from any other service account, which\\nwill typically include all requests from controllers running in Pods.\\nThe global-default  priority level handles all other traffic, e.g. interactive kubectl\\ncommands run by nonprivileged users.\\nThe suggested FlowSchemas serve to steer requests into the above priority levels, and are not\\nenumerated here.\\nMaintenance of the Mandatory and Suggested Configuration Objects\\nEach kube-apiserver  independently maintains the mandatory and suggested configuration\\nobjects, using initial and periodic behavior. Thus, in a situation with a mixture of servers of\\ndifferent versions there may be thrashing as long as different servers have different opinions of\\nthe proper content of these objects.\\nEach kube-apiserver  makes an initial maintenance pass over the mandatory and suggested\\nconfiguration objects, and after that does periodic maintenance (once per minute) of those\\nobjects.\\nFor the mandatory configuration objects, maintenance consists of ensuring that the object\\nexists and, if it does, has the proper spec. The server refuses to allow a creation or update with a\\nspec that is inconsistent with the server's guardrail behavior.\\nMaintenance of suggested configuration objects is designed to allow their specs to be\\noverridden. Deletion, on the other hand, is not respected: maintenance will restore the object. If\\nyou do not want a suggested configuration object then you need to keep it around but set its\\nspec to have minimal consequences. Maintenance of suggested objects is also designed to\\nsupport automatic migration when a new version of the kube-apiserver  is rolled out, albeit\\npotentially with thrashing while there is a mixed population of servers.\\nMaintenance of a suggested configuration object consists of creating it --- with the server's\\nsuggested spec --- if the object does not exist. OTOH, if the object already exists, maintenance\\nbehavior depends on whether the kube-apiservers  or the users control the object. In the former• \\n• \\n• \\n• \\n• \\n•\", metadata={'source': './PDFS/Concepts.pdf', 'page': 571}),\n",
       " Document(page_content='case, the server ensures that the object\\'s spec is what the server suggests; in the latter case, the\\nspec is left alone.\\nThe question of who controls the object is answered by first looking for an annotation with key \\napf.kubernetes.io/autoupdate-spec . If there is such an annotation and its value is true then the\\nkube-apiservers control the object. If there is such an annotation and its value is false then the\\nusers control the object. If neither of those conditions holds then the metadata.generation  of the\\nobject is consulted. If that is 1 then the kube-apiservers control the object. Otherwise the users\\ncontrol the object. These rules were introduced in release 1.22 and their consideration of \\nmetadata.generation  is for the sake of migration from the simpler earlier behavior. Users who\\nwish to control a suggested configuration object should set its apf.kubernetes.io/autoupdate-\\nspec annotation to false.\\nMaintenance of a mandatory or suggested configuration object also includes ensuring that it\\nhas an apf.kubernetes.io/autoupdate-spec  annotation that accurately reflects whether the kube-\\napiservers control the object.\\nMaintenance also includes deleting objects that are neither mandatory nor suggested but are\\nannotated apf.kubernetes.io/autoupdate-spec=true .\\nHealth check concurrency exemption\\nThe suggested configuration gives no special treatment to the health check requests on kube-\\napiservers from their local kubelets --- which tend to use the secured port but supply no\\ncredentials. With the suggested config, these requests get assigned to the global-default\\nFlowSchema and the corresponding global-default  priority level, where other traffic can crowd\\nthem out.\\nIf you add the following additional FlowSchema, this exempts those requests from rate limiting.\\nCaution:  Making this change also allows any hostile party to then send health-check requests\\nthat match this FlowSchema, at any volume they like. If you have a web traffic filter or similar\\nexternal security mechanism to protect your cluster\\'s API server from general internet traffic,\\nyou can configure rules to block any health check requests that originate from outside your\\ncluster.\\npriority-and-fairness/health-for-strangers.yaml  \\napiVersion : flowcontrol.apiserver.k8s.io/v1beta3\\nkind: FlowSchema\\nmetadata :\\n  name : health-for-strangers\\nspec:\\n  matchingPrecedence : 1000\\n  priorityLevelConfiguration :\\n    name : exempt\\n  rules :\\n    - nonResourceRules :\\n      - nonResourceURLs :\\n          - \"/healthz\"\\n          - \"/livez\"\\n          - \"/readyz\"\\n        verbs :\\n          - \"*\"', metadata={'source': './PDFS/Concepts.pdf', 'page': 572}),\n",
       " Document(page_content='subjects :\\n        - kind: Group\\n          group :\\n            name : \"system:unauthenticated\"\\nObservability\\nMetrics\\nNote:  In versions of Kubernetes before v1.20, the labels flow_schema  and priority_level  were\\ninconsistently named flowSchema  and priorityLevel , respectively. If you\\'re running Kubernetes\\nversions v1.19 and earlier, you should refer to the documentation for your version.\\nWhen you enable the API Priority and Fairness feature, the kube-apiserver exports additional\\nmetrics. Monitoring these can help you determine whether your configuration is\\ninappropriately throttling important traffic, or find poorly-behaved workloads that may be\\nharming system health.\\nMaturity level BETA\\napiserver_flowcontrol_rejected_requests_total  is a counter vector (cumulative since\\nserver start) of requests that were rejected, broken down by the labels flow_schema\\n(indicating the one that matched the request), priority_level  (indicating the one to which\\nthe request was assigned), and reason . The reason  label will be one of the following\\nvalues:\\nqueue-full , indicating that too many requests were already queued.\\nconcurrency-limit , indicating that the PriorityLevelConfiguration is configured to\\nreject rather than queue excess requests.\\ntime-out , indicating that the request was still in the queue when its queuing time\\nlimit expired.\\ncancelled , indicating that the request is not purge locked and has been ejected from\\nthe queue.\\napiserver_flowcontrol_dispatched_requests_total  is a counter vector (cumulative since\\nserver start) of requests that began executing, broken down by flow_schema  and \\npriority_level .\\napiserver_flowcontrol_current_inqueue_requests  is a gauge vector holding the\\ninstantaneous number of queued (not executing) requests, broken down by priority_level\\nand flow_schema .\\napiserver_flowcontrol_current_executing_requests  is a gauge vector holding the\\ninstantaneous number of executing (not waiting in a queue) requests, broken down by \\npriority_level  and flow_schema .\\napiserver_flowcontrol_current_executing_seats  is a gauge vector holding the\\ninstantaneous number of occupied seats, broken down by priority_level  and \\nflow_schema .• \\n◦ \\n◦ \\n◦ \\n◦ \\n• \\n• \\n• \\n•', metadata={'source': './PDFS/Concepts.pdf', 'page': 573}),\n",
       " Document(page_content=\"apiserver_flowcontrol_request_wait_duration_seconds  is a histogram vector of how long\\nrequests spent queued, broken down by the labels flow_schema , priority_level , and \\nexecute . The execute  label indicates whether the request has started executing.\\nNote:  Since each FlowSchema always assigns requests to a single\\nPriorityLevelConfiguration, you can add the histograms for all the FlowSchemas for one\\npriority level to get the effective histogram for requests assigned to that priority level.\\napiserver_flowcontrol_nominal_limit_seats  is a gauge vector holding each priority level's\\nnominal concurrency limit, computed from the API server's total concurrency limit and\\nthe priority level's configured nominal concurrency shares.\\nMaturity level ALPHA\\napiserver_current_inqueue_requests  is a gauge vector of recent high water marks of the\\nnumber of queued requests, grouped by a label named request_kind  whose value is \\nmutating  or readOnly . These high water marks describe the largest number seen in the\\none second window most recently completed. These complement the older \\napiserver_current_inflight_requests  gauge vector that holds the last window's high water\\nmark of number of requests actively being served.\\napiserver_current_inqueue_seats  is a gauge vector of the sum over queued requests of the\\nlargest number of seats each will occupy, grouped by labels named flow_schema  and \\npriority_level .\\napiserver_flowcontrol_read_vs_write_current_requests  is a histogram vector of\\nobservations, made at the end of every nanosecond, of the number of requests broken\\ndown by the labels phase  (which takes on the values waiting  and executing ) and \\nrequest_kind  (which takes on the values mutating  and readOnly ). Each observed value is\\na ratio, between 0 and 1, of the number of requests divided by the corresponding limit on\\nthe number of requests (queue volume limit for waiting and concurrency limit for\\nexecuting).\\napiserver_flowcontrol_request_concurrency_in_use  is a gauge vector holding the\\ninstantaneous number of occupied seats, broken down by priority_level  and \\nflow_schema .\\napiserver_flowcontrol_priority_level_request_utilization  is a histogram vector of\\nobservations, made at the end of each nanosecond, of the number of requests broken\\ndown by the labels phase  (which takes on the values waiting  and executing ) and \\npriority_level . Each observed value is a ratio, between 0 and 1, of a number of requests\\ndivided by the corresponding limit on the number of requests (queue volume limit for\\nwaiting and concurrency limit for executing).\\napiserver_flowcontrol_priority_level_seat_utilization  is a histogram vector of\\nobservations, made at the end of each nanosecond, of the utilization of a priority level's\\nconcurrency limit, broken down by priority_level . This utilization is the fraction (number\\nof seats occupied) / (concurrency limit). This metric considers all stages of execution\\n(both normal and the extra delay at the end of a write to cover for the corresponding\\nnotification work) of all requests except WATCHes; for those it considers only the initial\\nstage that delivers notifications of pre-existing objects. Each histogram in the vector is\\nalso labeled with phase: executing  (there is no seat limit for the waiting phase).• \\n• \\n• \\n• \\n• \\n• \\n• \\n•\", metadata={'source': './PDFS/Concepts.pdf', 'page': 574}),\n",
       " Document(page_content=\"apiserver_flowcontrol_request_queue_length_after_enqueue  is a histogram vector of\\nqueue lengths for the queues, broken down by priority_level  and flow_schema , as\\nsampled by the enqueued requests. Each request that gets queued contributes one sample\\nto its histogram, reporting the length of the queue immediately after the request was\\nadded. Note that this produces different statistics than an unbiased survey would.\\nNote:  An outlier value in a histogram here means it is likely that a single flow (i.e.,\\nrequests by one user or for one namespace, depending on configuration) is flooding the\\nAPI server, and being throttled. By contrast, if one priority level's histogram shows that\\nall queues for that priority level are longer than those for other priority levels, it may be\\nappropriate to increase that PriorityLevelConfiguration's concurrency shares.\\napiserver_flowcontrol_request_concurrency_limit  is the same as \\napiserver_flowcontrol_nominal_limit_seats . Before the introduction of concurrency\\nborrowing between priority levels, this was always equal to \\napiserver_flowcontrol_current_limit_seats  (which did not exist as a distinct metric).\\napiserver_flowcontrol_lower_limit_seats  is a gauge vector holding the lower bound on\\neach priority level's dynamic concurrency limit.\\napiserver_flowcontrol_upper_limit_seats  is a gauge vector holding the upper bound on\\neach priority level's dynamic concurrency limit.\\napiserver_flowcontrol_demand_seats  is a histogram vector counting observations, at the\\nend of every nanosecond, of each priority level's ratio of (seat demand) / (nominal\\nconcurrency limit). A priority level's seat demand is the sum, over both queued requests\\nand those in the initial phase of execution, of the maximum of the number of seats\\noccupied in the request's initial and final execution phases.\\napiserver_flowcontrol_demand_seats_high_watermark  is a gauge vector holding, for each\\npriority level, the maximum seat demand seen during the last concurrency borrowing\\nadjustment period.\\napiserver_flowcontrol_demand_seats_average  is a gauge vector holding, for each priority\\nlevel, the time-weighted average seat demand seen during the last concurrency\\nborrowing adjustment period.\\napiserver_flowcontrol_demand_seats_stdev  is a gauge vector holding, for each priority\\nlevel, the time-weighted population standard deviation of seat demand seen during the\\nlast concurrency borrowing adjustment period.\\napiserver_flowcontrol_demand_seats_smoothed  is a gauge vector holding, for each\\npriority level, the smoothed enveloped seat demand determined at the last concurrency\\nadjustment.\\napiserver_flowcontrol_target_seats  is a gauge vector holding, for each priority level, the\\nconcurrency target going into the borrowing allocation problem.\\napiserver_flowcontrol_seat_fair_frac  is a gauge holding the fair allocation fraction\\ndetermined in the last borrowing adjustment.\\napiserver_flowcontrol_current_limit_seats  is a gauge vector holding, for each priority\\nlevel, the dynamic concurrency limit derived in the last adjustment.• \\n• \\n• \\n• \\n• \\n• \\n• \\n• \\n• \\n• \\n• \\n•\", metadata={'source': './PDFS/Concepts.pdf', 'page': 575}),\n",
       " Document(page_content=\"apiserver_flowcontrol_request_execution_seconds  is a histogram vector of how long\\nrequests took to actually execute, broken down by flow_schema  and priority_level .\\napiserver_flowcontrol_watch_count_samples  is a histogram vector of the number of\\nactive WATCH requests relevant to a given write, broken down by flow_schema  and \\npriority_level .\\napiserver_flowcontrol_work_estimated_seats  is a histogram vector of the number of\\nestimated seats (maximum of initial and final stage of execution) associated with\\nrequests, broken down by flow_schema  and priority_level .\\napiserver_flowcontrol_request_dispatch_no_accommodation_total  is a counter vector of\\nthe number of events that in principle could have led to a request being dispatched but\\ndid not, due to lack of available concurrency, broken down by flow_schema  and \\npriority_level .\\napiserver_flowcontrol_epoch_advance_total  is a counter vector of the number of\\nattempts to jump a priority level's progress meter backward to avoid numeric overflow,\\ngrouped by priority_level  and success .\\nGood practices for using API Priority and Fairness\\nWhen a given priority level exceeds its permitted concurrency, requests can experience\\nincreased latency or be dropped with an HTTP 429 (Too Many Requests) error. To prevent these\\nside effects of APF, you can modify your workload or tweak your APF settings to ensure there\\nare sufficient seats available to serve your requests.\\nTo detect whether requests are being rejected due to APF, check the following metrics:\\napiserver_flowcontrol_rejected_requests_total: the total number of requests rejected per\\nFlowSchema and PriorityLevelConfiguration.\\napiserver_flowcontrol_current_inqueue_requests: the current number of requests queued\\nper FlowSchema and PriorityLevelConfiguration.\\napiserver_flowcontrol_request_wait_duration_seconds: the latency added to requests\\nwaiting in queues.\\napiserver_flowcontrol_priority_level_seat_utilization: the seat utilization per\\nPriorityLevelConfiguration.\\nWorkload modifications\\nTo prevent requests from queuing and adding latency or being dropped due to APF, you can\\noptimize your requests by:\\nReducing the rate at which requests are executed. A fewer number of requests over a\\nfixed period will result in a fewer number of seats being needed at a given time.\\nAvoid issuing a large number of expensive requests concurrently. Requests can be\\noptimized to use fewer seats or have lower latency so that these requests hold those seats\\nfor a shorter duration. List requests can occupy more than 1 seat depending on the\\nnumber of objects fetched during the request. Restricting the number of objects retrieved\\nin a list request, for example by using pagination, will use less total seats over a shorter\\nperiod. Furthermore, replacing list requests with watch requests will require lower total\\nconcurrency shares as watch requests only occupy 1 seat during its initial burst of\\nnotifications. If using streaming lists in versions 1.27 and later, watch requests will• \\n• \\n• \\n• \\n• \\n• \\n• \\n• \\n• \\n• \\n•\", metadata={'source': './PDFS/Concepts.pdf', 'page': 576}),\n",
       " Document(page_content='occupy the same number of seats as a list request for its initial burst of notifications\\nbecause the entire state of the collection has to be streamed. Note that in both cases, a\\nwatch request will not hold any seats after this initial phase.\\nKeep in mind that queuing or rejected requests from APF could be induced by either an\\nincrease in the number of requests or an increase in latency for existing requests. For example,\\nif requests that normally take 1s to execute start taking 60s, it is possible that APF will start\\nrejecting requests because requests are occupying seats for a longer duration than normal due\\nto this increase in latency. If APF starts rejecting requests across multiple priority levels\\nwithout a significant change in workload, it is possible there is an underlying issue with control\\nplane performance rather than the workload or APF settings.\\nPriority and fairness settings\\nYou can also modify the default FlowSchema and PriorityLevelConfiguration objects or create\\nnew objects of these types to better accommodate your workload.\\nAPF settings can be modified to:\\nGive more seats to high priority requests.\\nIsolate non-essential or expensive requests that would starve a concurrency level if it was\\nshared with other flows.\\nGive more seats to high priority requests\\nIf possible, the number of seats available across all priority levels for a particular kube-\\napiserver  can be increased by increasing the values for the max-requests-inflight  and \\nmax-mutating-requests-inflight  flags. Alternatively, horizontally scaling the number of \\nkube-apiserver  instances will increase the total concurrency per priority level across the\\ncluster assuming there is sufficient load balancing of requests.\\nYou can create a new FlowSchema which references a PriorityLevelConfiguration with a\\nlarger concurrency level. This new PriorityLevelConfiguration could be an existing level\\nor a new level with its own set of nominal concurrency shares. For example, a new\\nFlowSchema could be introduced to change the PriorityLevelConfiguration for your\\nrequests from global-default to workload-low to increase the number of seats available to\\nyour user. Creating a new PriorityLevelConfiguration will reduce the number of seats\\ndesignated for existing levels. Recall that editing a default FlowSchema or\\nPriorityLevelConfiguration will require setting the apf.kubernetes.io/autoupdate-spec\\nannotation to false.\\nYou can also increase the NominalConcurrencyShares for the PriorityLevelConfiguration\\nwhich is serving your high priority requests. Alternatively, for versions 1.26 and later,\\nyou can increase the LendablePercent for competing priority levels so that the given\\npriority level has a higher pool of seats it can borrow.\\nIsolate non-essential requests from starving other flows\\nFor request isolation, you can create a FlowSchema whose subject matches the user making\\nthese requests or create a FlowSchema that matches what the request is (corresponding to the\\nresourceRules). Next, you can map this FlowSchema to a PriorityLevelConfiguration with a low\\nshare of seats.• \\n• \\n1. \\n2. \\n3.', metadata={'source': './PDFS/Concepts.pdf', 'page': 577}),\n",
       " Document(page_content=\"For example, suppose list event requests from Pods running in the default namespace are using\\n10 seats each and execute for 1 minute. To prevent these expensive requests from impacting\\nrequests from other Pods using the existing service-accounts FlowSchema, you can apply the\\nfollowing FlowSchema to isolate these list calls from other requests.\\nExample FlowSchema object to isolate list event requests:\\npriority-and-fairness/list-events-default-service-account.yaml  \\napiVersion : flowcontrol.apiserver.k8s.io/v1beta3\\nkind: FlowSchema\\nmetadata :\\n  name : list-events-default-service-account\\nspec:\\n  distinguisherMethod :\\n    type: ByUser\\n  matchingPrecedence : 8000\\n  priorityLevelConfiguration :\\n    name : catch-all\\n  rules :\\n    - resourceRules :\\n      - apiGroups :\\n          - '*'\\n        namespaces :\\n          - default\\n        resources :\\n          - events\\n        verbs :\\n          - list\\n      subjects :\\n        - kind: ServiceAccount\\n          serviceAccount :\\n            name : default\\n            namespace : default\\nThis FlowSchema captures all list event calls made by the default service account in the\\ndefault namespace. The matching precedence 8000 is lower than the value of 9000 used by\\nthe existing service-accounts FlowSchema so these list event calls will match list-events-\\ndefault-service-account rather than service-accounts.\\nThe catch-all PriorityLevelConfiguration is used to isolate these requests. The catch-all\\npriority level has a very small concurrency share and does not queue requests.\\nWhat's next\\nYou can visit flow control reference doc  to learn more about troubleshooting.\\nFor background information on design details for API priority and fairness, see the \\nenhancement proposal .\\nYou can make suggestions and feature requests via SIG API Machinery  or the feature's \\nslack channel .• \\n• \\n• \\n• \\n•\", metadata={'source': './PDFS/Concepts.pdf', 'page': 578}),\n",
       " Document(page_content=\"Installing Addons\\nNote:  This section links to third party projects that provide functionality required by\\nKubernetes. The Kubernetes project authors aren't responsible for these projects, which are\\nlisted alphabetically. To add a project to this list, read the content guide  before submitting a\\nchange. More information.\\nAdd-ons extend the functionality of Kubernetes.\\nThis page lists some of the available add-ons and links to their respective installation\\ninstructions. The list does not try to be exhaustive.\\nNetworking and Network Policy\\nACI provides integrated container networking and network security with Cisco ACI.\\nAntrea  operates at Layer 3/4 to provide networking and security services for Kubernetes,\\nleveraging Open vSwitch as the networking data plane. Antrea is a CNCF project at the\\nSandbox level .\\nCalico  is a networking and network policy provider. Calico supports a flexible set of\\nnetworking options so you can choose the most efficient option for your situation,\\nincluding non-overlay and overlay networks, with or without BGP. Calico uses the same\\nengine to enforce network policy for hosts, pods, and (if using Istio & Envoy) applications\\nat the service mesh layer.\\nCanal  unites Flannel and Calico, providing networking and network policy.\\nCilium  is a networking, observability, and security solution with an eBPF-based data\\nplane. Cilium provides a simple flat Layer 3 network with the ability to span multiple\\nclusters in either a native routing or overlay/encapsulation mode, and can enforce\\nnetwork policies on L3-L7 using an identity-based security model that is decoupled from\\nnetwork addressing. Cilium can act as a replacement for kube-proxy; it also offers\\nadditional, opt-in observability and security features. Cilium is a CNCF project at the\\nGraduated level .\\nCNI-Genie  enables Kubernetes to seamlessly connect to a choice of CNI plugins, such as\\nCalico, Canal, Flannel, or Weave. CNI-Genie is a CNCF project at the Sandbox level .\\nContiv  provides configurable networking (native L3 using BGP, overlay using vxlan,\\nclassic L2, and Cisco-SDN/ACI) for various use cases and a rich policy framework. Contiv\\nproject is fully open sourced . The installer  provides both kubeadm and non-kubeadm\\nbased installation options.\\nContrail , based on Tungsten Fabric , is an open source, multi-cloud network virtualization\\nand policy management platform. Contrail and Tungsten Fabric are integrated with\\norchestration systems such as Kubernetes, OpenShift, OpenStack and Mesos, and provide\\nisolation modes for virtual machines, containers/pods and bare metal workloads.\\nFlannel  is an overlay network provider that can be used with Kubernetes.\\nGateway API  is an open source project managed by the SIG Network  community and\\nprovides an expressive, extensible, and role-oriented API for modeling service\\nnetworking.\\nKnitter  is a plugin to support multiple network interfaces in a Kubernetes pod.\\nMultus  is a Multi plugin for multiple network support in Kubernetes to support all CNI\\nplugins (e.g. Calico, Cilium, Contiv, Flannel), in addition to SRIOV, DPDK, OVS-DPDK\\nand VPP based workloads in Kubernetes.\\nOVN-Kubernetes  is a networking provider for Kubernetes based on OVN (Open Virtual\\nNetwork) , a virtual networking implementation that came out of the Open vSwitch (OVS)• \\n• \\n• \\n• \\n• \\n• \\n• \\n• \\n• \\n• \\n• \\n• \\n•\", metadata={'source': './PDFS/Concepts.pdf', 'page': 579}),\n",
       " Document(page_content='project. OVN-Kubernetes provides an overlay based networking implementation for\\nKubernetes, including an OVS based implementation of load balancing and network\\npolicy.\\nNodus  is an OVN based CNI controller plugin to provide cloud native based Service\\nfunction chaining(SFC).\\nNSX-T  Container Plug-in (NCP) provides integration between VMware NSX-T and\\ncontainer orchestrators such as Kubernetes, as well as integration between NSX-T and\\ncontainer-based CaaS/PaaS platforms such as Pivotal Container Service (PKS) and\\nOpenShift.\\nNuage  is an SDN platform that provides policy-based networking between Kubernetes\\nPods and non-Kubernetes environments with visibility and security monitoring.\\nRomana  is a Layer 3 networking solution for pod networks that also supports the \\nNetworkPolicy  API.\\nWeave Net  provides networking and network policy, will carry on working on both sides\\nof a network partition, and does not require an external database.\\nService Discovery\\nCoreDNS  is a flexible, extensible DNS server which can be installed  as the in-cluster DNS\\nfor pods.\\nVisualization & Control\\nDashboard  is a dashboard web interface for Kubernetes.\\nWeave Scope  is a tool for visualizing your containers, Pods, Services and more.\\nInfrastructure\\nKubeVirt  is an add-on to run virtual machines on Kubernetes. Usually run on bare-metal\\nclusters.\\nThe node problem detector  runs on Linux nodes and reports system issues as either \\nEvents  or Node conditions .\\nLegacy Add-ons\\nThere are several other add-ons documented in the deprecated cluster/addons  directory.\\nWell-maintained ones should be linked to here. PRs welcome!\\nWindows in Kubernetes\\nKubernetes supports nodes that run Microsoft Windows.\\nKubernetes supports worker nodes  running either Linux or Microsoft Windows.\\n This item links to a third party project or product that is not part of Kubernetes itself. More\\ninformation• \\n• \\n• \\n• \\n• \\n• \\n• \\n• \\n• \\n•', metadata={'source': './PDFS/Concepts.pdf', 'page': 580}),\n",
       " Document(page_content=\"The CNCF and its parent the Linux Foundation take a vendor-neutral approach towards\\ncompatibility. It is possible to join your Windows server  as a worker node to a Kubernetes\\ncluster.\\nYou can install and set up kubectl on Windows  no matter what operating system you use\\nwithin your cluster.\\nIf you are using Windows nodes, you can read:\\nNetworking On Windows\\nWindows Storage In Kubernetes\\nResource Management for Windows Nodes\\nConfigure RunAsUserName for Windows Pods and Containers\\nCreate A Windows HostProcess Pod\\nConfigure Group Managed Service Accounts for Windows Pods and Containers\\nSecurity For Windows Nodes\\nWindows Debugging Tips\\nGuide for Scheduling Windows Containers in Kubernetes\\nor, for an overview, read:\\nWindows containers in Kubernetes\\nGuide for Running Windows Containers in Kubernetes\\nWindows containers in Kubernetes\\nWindows applications constitute a large portion of the services and applications that run in\\nmany organizations. Windows containers  provide a way to encapsulate processes and package\\ndependencies, making it easier to use DevOps practices and follow cloud native patterns for\\nWindows applications.\\nOrganizations with investments in Windows-based applications and Linux-based applications\\ndon't have to look for separate orchestrators to manage their workloads, leading to increased\\noperational efficiencies across their deployments, regardless of operating system.\\nWindows nodes in Kubernetes\\nTo enable the orchestration of Windows containers in Kubernetes, include Windows nodes in\\nyour existing Linux cluster. Scheduling Windows containers in Pods  on Kubernetes is similar to\\nscheduling Linux-based containers.\\nIn order to run Windows containers, your Kubernetes cluster must include multiple operating\\nsystems. While you can only run the control plane  on Linux, you can deploy worker nodes\\nrunning either Windows or Linux.\\nWindows nodes  are supported  provided that the operating system is Windows Server 2019.\\nThis document uses the term Windows containers  to mean Windows containers with process\\nisolation. Kubernetes does not support running Windows containers with Hyper-V isolation .• \\n• \\n• \\n• \\n• \\n• \\n• \\n• \\n• \\n• \\n•\", metadata={'source': './PDFS/Concepts.pdf', 'page': 581}),\n",
       " Document(page_content='Compatibility and limitations\\nSome node features are only available if you use a specific container runtime ; others are not\\navailable on Windows nodes, including:\\nHugePages: not supported for Windows containers\\nPrivileged containers: not supported for Windows containers. HostProcess Containers\\noffer similar functionality.\\nTerminationGracePeriod: requires containerD\\nNot all features of shared namespaces are supported. See API compatibility  for more details.\\nSee Windows OS version compatibility  for details on the Windows versions that Kubernetes is\\ntested against.\\nFrom an API and kubectl perspective, Windows containers behave in much the same way as\\nLinux-based containers. However, there are some notable differences in key functionality which\\nare outlined in this section.\\nComparison with Linux\\nKey Kubernetes elements work the same way in Windows as they do in Linux. This section\\nrefers to several key workload abstractions and how they map to Windows.\\nPods\\nA Pod is the basic building block of Kubernetes–the smallest and simplest unit in the\\nKubernetes object model that you create or deploy. You may not deploy Windows and\\nLinux containers in the same Pod. All containers in a Pod are scheduled onto a single\\nNode where each Node represents a specific platform and architecture. The following Pod\\ncapabilities, properties and events are supported with Windows containers:\\nSingle or multiple containers per Pod with process isolation and volume sharing\\nPod status  fields\\nReadiness, liveness, and startup probes\\npostStart & preStop container lifecycle hooks\\nConfigMap, Secrets: as environment variables or volumes\\nemptyDir  volumes\\nNamed pipe host mounts\\nResource limits\\nOS field:\\nThe .spec.os.name  field should be set to windows  to indicate that the current Pod\\nuses Windows containers.• \\n• \\n• \\n• \\n◦ \\n◦ \\n◦ \\n◦ \\n◦ \\n◦ \\n◦ \\n◦ \\n◦', metadata={'source': './PDFS/Concepts.pdf', 'page': 582}),\n",
       " Document(page_content='If you set the .spec.os.name  field to windows , you must not set the following fields\\nin the .spec  of that Pod:\\nspec.hostPID\\nspec.hostIPC\\nspec.securityContext.seLinuxOptions\\nspec.securityContext.seccompProfile\\nspec.securityContext.fsGroup\\nspec.securityContext.fsGroupChangePolicy\\nspec.securityContext.sysctls\\nspec.shareProcessNamespace\\nspec.securityContext.runAsUser\\nspec.securityContext.runAsGroup\\nspec.securityContext.supplementalGroups\\nspec.containers[*].securityContext.seLinuxOptions\\nspec.containers[*].securityContext.seccompProfile\\nspec.containers[*].securityContext.capabilities\\nspec.containers[*].securityContext.readOnlyRootFilesystem\\nspec.containers[*].securityContext.privileged\\nspec.containers[*].securityContext.allowPrivilegeEscalation\\nspec.containers[*].securityContext.procMount\\nspec.containers[*].securityContext.runAsUser\\nspec.containers[*].securityContext.runAsGroup\\nIn the above list, wildcards ( *) indicate all elements in a list. For example, \\nspec.containers[*].securityContext  refers to the SecurityContext object for all\\ncontainers. If any of these fields is specified, the Pod will not be admitted by the\\nAPI server.\\nWorkload resources  including:\\nReplicaSet\\nDeployment\\nStatefulSet\\nDaemonSet\\nJob\\nCronJob\\nReplicationController\\nServices  See Load balancing and Services  for more details.\\nPods, workload resources, and Services are critical elements to managing Windows workloads\\non Kubernetes. However, on their own they are not enough to enable the proper lifecycle\\nmanagement of Windows workloads in a dynamic cloud native environment.\\nkubectl exec\\nPod and container metrics\\nHorizontal pod autoscaling\\nResource quotas\\nScheduler preemption▪ \\n▪ \\n▪ \\n▪ \\n▪ \\n▪ \\n▪ \\n▪ \\n▪ \\n▪ \\n▪ \\n▪ \\n▪ \\n▪ \\n▪ \\n▪ \\n▪ \\n▪ \\n▪ \\n▪ \\n• \\n◦ \\n◦ \\n◦ \\n◦ \\n◦ \\n◦ \\n◦ \\n• \\n• \\n• \\n• \\n• \\n•', metadata={'source': './PDFS/Concepts.pdf', 'page': 583}),\n",
       " Document(page_content=\"Command line options for the kubelet\\nSome kubelet command line options behave differently on Windows, as described below:\\nThe --windows-priorityclass  lets you set the scheduling priority of the kubelet process\\n(see CPU resource management )\\nThe --kube-reserved , --system-reserved  , and --eviction-hard  flags update \\nNodeAllocatable\\nEviction by using --enforce-node-allocable  is not implemented\\nEviction by using --eviction-hard  and --eviction-soft  are not implemented\\nWhen running on a Windows node the kubelet does not have memory or CPU\\nrestrictions. --kube-reserved  and --system-reserved  only subtract from NodeAllocatable\\nand do not guarantee resource provided for workloads. See Resource Management for\\nWindows nodes  for more information.\\nThe MemoryPressure  Condition is not implemented\\nThe kubelet does not take OOM eviction actions\\nAPI compatibility\\nThere are subtle differences in the way the Kubernetes APIs work for Windows due to the OS\\nand container runtime. Some workload properties were designed for Linux, and fail to run on\\nWindows.\\nAt a high level, these OS concepts are different:\\nIdentity - Linux uses userID (UID) and groupID (GID) which are represented as integer\\ntypes. User and group names are not canonical - they are just an alias in /etc/groups  or /\\netc/passwd  back to UID+GID. Windows uses a larger binary security identifier  (SID)\\nwhich is stored in the Windows Security Access Manager (SAM) database. This database\\nis not shared between the host and containers, or between containers.\\nFile permissions - Windows uses an access control list based on (SIDs), whereas POSIX\\nsystems such as Linux use a bitmask based on object permissions and UID+GID, plus \\noptional  access control lists.\\nFile paths - the convention on Windows is to use \\\\ instead of /. The Go IO libraries\\ntypically accept both and just make it work, but when you're setting a path or command\\nline that's interpreted inside a container, \\\\ may be needed.\\nSignals - Windows interactive apps handle termination differently, and can implement\\none or more of these:\\nA UI thread handles well-defined messages including WM_CLOSE .\\nConsole apps handle Ctrl-C or Ctrl-break using a Control Handler.\\nServices register a Service Control Handler function that can accept \\nSERVICE_CONTROL_STOP  control codes.\\nContainer exit codes follow the same convention where 0 is success, and nonzero is failure. The\\nspecific error codes may differ across Windows and Linux. However, exit codes passed from the\\nKubernetes components (kubelet, kube-proxy) are unchanged.• \\n• \\n• \\n• \\n• \\n• \\n• \\n• \\n• \\n• \\n• \\n◦ \\n◦ \\n◦\", metadata={'source': './PDFS/Concepts.pdf', 'page': 584}),\n",
       " Document(page_content=\"Field compatibility for container specifications\\nThe following list documents differences between how Pod container specifications work\\nbetween Windows and Linux:\\nHuge pages are not implemented in the Windows container runtime, and are not\\navailable. They require asserting a user privilege  that's not configurable for containers.\\nrequests.cpu  and requests.memory  - requests are subtracted from node available\\nresources, so they can be used to avoid overprovisioning a node. However, they cannot be\\nused to guarantee resources in an overprovisioned node. They should be applied to all\\ncontainers as a best practice if the operator wants to avoid overprovisioning entirely.\\nsecurityContext.allowPrivilegeEscalation  - not possible on Windows; none of the\\ncapabilities are hooked up\\nsecurityContext.capabilities  - POSIX capabilities are not implemented on Windows\\nsecurityContext.privileged  - Windows doesn't support privileged containers, use \\nHostProcess Containers  instead\\nsecurityContext.procMount  - Windows doesn't have a /proc  filesystem\\nsecurityContext.readOnlyRootFilesystem  - not possible on Windows; write access is\\nrequired for registry & system processes to run inside the container\\nsecurityContext.runAsGroup  - not possible on Windows as there is no GID support\\nsecurityContext.runAsNonRoot  - this setting will prevent containers from running as \\nContainerAdministrator  which is the closest equivalent to a root user on Windows.\\nsecurityContext.runAsUser  - use runAsUserName  instead\\nsecurityContext.seLinuxOptions  - not possible on Windows as SELinux is Linux-specific\\nterminationMessagePath  - this has some limitations in that Windows doesn't support\\nmapping single files. The default value is /dev/termination-log , which does work because\\nit does not exist on Windows by default.\\nField compatibility for Pod specifications\\nThe following list documents differences between how Pod specifications work between\\nWindows and Linux:\\nhostIPC  and hostpid  - host namespace sharing is not possible on Windows\\nhostNetwork  - see below\\ndnsPolicy  - setting the Pod dnsPolicy  to ClusterFirstWithHostNet  is not supported on\\nWindows because host networking is not provided. Pods always run with a container\\nnetwork.\\npodSecurityContext  see below\\nshareProcessNamespace  - this is a beta feature, and depends on Linux namespaces which\\nare not implemented on Windows. Windows cannot share process namespaces or the\\ncontainer's root filesystem. Only the network can be shared.\\nterminationGracePeriodSeconds  - this is not fully implemented in Docker on Windows,\\nsee the GitHub issue . The behavior today is that the ENTRYPOINT process is sent\\nCTRL_SHUTDOWN_EVENT, then Windows waits 5 seconds by default, and finally shuts\\ndown all processes using the normal Windows shutdown behavior. The 5 second default\\nis actually in the Windows registry inside the container , so it can be overridden when the\\ncontainer is built.\\nvolumeDevices  - this is a beta feature, and is not implemented on Windows. Windows\\ncannot attach raw block devices to pods.\\nvolumes\\nIf you define an emptyDir  volume, you cannot set its volume source to memory .• \\n• \\n• \\n• \\n• \\n• \\n• \\n• \\n• \\n• \\n• \\n• \\n• \\n• \\n• \\n• \\n• \\n• \\n• \\n• \\n◦\", metadata={'source': './PDFS/Concepts.pdf', 'page': 585}),\n",
       " Document(page_content=\"You cannot enable mountPropagation  for volume mounts as this is not supported on\\nWindows.\\nField compatibility for hostNetwork\\nFEATURE STATE:  Kubernetes v1.26 [alpha]\\nThe kubelet can now request that pods running on Windows nodes use the host's network\\nnamespace instead of creating a new pod network namespace. To enable this functionality pass \\n--feature-gates=WindowsHostNetwork=true  to the kubelet.\\nNote:  This functionality requires a container runtime that supports this functionality.\\nField compatibility for Pod security context\\nOnly the securityContext.runAsNonRoot  and securityContext.windowsOptions  from the Pod \\nsecurityContext  fields work on Windows.\\nNode problem detector\\nThe node problem detector (see Monitor Node Health ) has preliminary support for Windows.\\nFor more information, visit the project's GitHub page .\\nPause container\\nIn a Kubernetes Pod, an infrastructure or “pause” container is first created to host the container.\\nIn Linux, the cgroups and namespaces that make up a pod need a process to maintain their\\ncontinued existence; the pause process provides this. Containers that belong to the same pod,\\nincluding infrastructure and worker containers, share a common network endpoint (same IPv4\\nand / or IPv6 address, same network port spaces). Kubernetes uses pause containers to allow for\\nworker containers crashing or restarting without losing any of the networking configuration.\\nKubernetes maintains a multi-architecture image that includes support for Windows. For\\nKubernetes v1.28.4 the recommended pause image is registry.k8s.io/pause:3.6 . The source code\\nis available on GitHub.\\nMicrosoft maintains a different multi-architecture image, with Linux and Windows amd64\\nsupport, that you can find as mcr.microsoft.com/oss/kubernetes/pause:3.6 . This image is built\\nfrom the same source as the Kubernetes maintained image but all of the Windows binaries are \\nauthenticode signed  by Microsoft. The Kubernetes project recommends using the Microsoft\\nmaintained image if you are deploying to a production or production-like environment that\\nrequires signed binaries.\\nContainer runtimes\\nYou need to install a container runtime  into each node in the cluster so that Pods can run there.\\nThe following container runtimes work with Windows:\\nNote:  This section links to third party projects that provide functionality required by\\nKubernetes. The Kubernetes project authors aren't responsible for these projects, which are•\", metadata={'source': './PDFS/Concepts.pdf', 'page': 586}),\n",
       " Document(page_content='listed alphabetically. To add a project to this list, read the content guide  before submitting a\\nchange. More information.\\nContainerD\\nFEATURE STATE:  Kubernetes v1.20 [stable]\\nYou can use ContainerD  1.4.0+ as the container runtime for Kubernetes nodes that run\\nWindows.\\nLearn how to install ContainerD on a Windows node .\\nNote:  There is a known limitation  when using GMSA with containerd to access Windows\\nnetwork shares, which requires a kernel patch.\\nMirantis Container Runtime\\nMirantis Container Runtime  (MCR) is available as a container runtime for all Windows Server\\n2019 and later versions.\\nSee Install MCR on Windows Servers  for more information.\\nWindows OS version compatibility\\nOn Windows nodes, strict compatibility rules apply where the host OS version must match the\\ncontainer base image OS version. Only Windows containers with a container operating system\\nof Windows Server 2019 are fully supported.\\nFor Kubernetes v1.28, operating system compatibility for Windows nodes (and Pods) is as\\nfollows:\\nWindows Server LTSC release\\nWindows Server 2019\\nWindows Server 2022\\nWindows Server SAC release\\nWindows Server version 20H2\\nThe Kubernetes version-skew policy  also applies.\\nGetting help and troubleshooting\\nYour main source of help for troubleshooting your Kubernetes cluster should start with the \\nTroubleshooting  page.\\nSome additional, Windows-specific troubleshooting help is included in this section. Logs are an\\nimportant element of troubleshooting issues in Kubernetes. Make sure to include them any time\\nyou seek troubleshooting assistance from other contributors. Follow the instructions in the SIG\\nWindows contributing guide on gathering logs .', metadata={'source': './PDFS/Concepts.pdf', 'page': 587}),\n",
       " Document(page_content='Reporting issues and feature requests\\nIf you have what looks like a bug, or you would like to make a feature request, please follow the\\nSIG Windows contributing guide  to create a new issue. You should first search the list of issues\\nin case it was reported previously and comment with your experience on the issue and add\\nadditional logs. SIG Windows channel on the Kubernetes Slack is also a great avenue to get\\nsome initial support and troubleshooting ideas prior to creating a ticket.\\nDeployment tools\\nThe kubeadm tool helps you to deploy a Kubernetes cluster, providing the control plane to\\nmanage the cluster it, and nodes to run your workloads.\\nThe Kubernetes cluster API  project also provides means to automate deployment of Windows\\nnodes.\\nWindows distribution channels\\nFor a detailed explanation of Windows distribution channels see the Microsoft documentation .\\nInformation on the different Windows Server servicing channels including their support models\\ncan be found at Windows Server servicing channels .\\nGuide for Running Windows Containers in\\nKubernetes\\nThis page provides a walkthrough for some steps you can follow to run Windows containers\\nusing Kubernetes. The page also highlights some Windows specific functionality within\\nKubernetes.\\nIt is important to note that creating and deploying services and workloads on Kubernetes\\nbehaves in much the same way for Linux and Windows containers. The kubectl commands  to\\ninterface with the cluster are identical. The examples in this page are provided to jumpstart\\nyour experience with Windows containers.\\nObjectives\\nConfigure an example deployment to run Windows containers on a Windows node.\\nBefore you begin\\nYou should already have access to a Kubernetes cluster that includes a worker node running\\nWindows Server.', metadata={'source': './PDFS/Concepts.pdf', 'page': 588}),\n",
       " Document(page_content='Getting Started: Deploying a Windows workload\\nThe example YAML file below deploys a simple webserver application running inside a\\nWindows container.\\nCreate a manifest named win-webserver.yaml  with the contents below:\\n---\\napiVersion : v1\\nkind: Service\\nmetadata :\\n  name : win-webserver\\n  labels :\\n    app: win-webserver\\nspec:\\n  ports :\\n    # the port that this service should serve on\\n    - port: 80\\n      targetPort : 80\\n  selector :\\n    app: win-webserver\\n  type: NodePort\\n---\\napiVersion : apps/v1\\nkind: Deployment\\nmetadata :\\n  labels :\\n    app: win-webserver\\n  name : win-webserver\\nspec:\\n  replicas : 2\\n  selector :\\n    matchLabels :\\n      app: win-webserver\\n  template :\\n    metadata :\\n      labels :\\n        app: win-webserver\\n      name : win-webserver\\n    spec:\\n     containers :\\n      - name : windowswebserver\\n        image : mcr.microsoft.com/windows/servercore:ltsc2019\\n        command :\\n        - powershell.exe\\n        - -command\\n        - \\n\"<#code used from https://gist.github.com/19WAS85/5424431#> ; $$listener = New-Object \\nSystem.Net.HttpListener ; $$listener.Prefixes.Add(\\'http://*:80/\\') ; $$listener.Start() ; $\\n$callerCounts = @{} ; Write-Host(\\'Listening at http://*:80/\\') ; while ($$listener.IsListening) { ;$\\n$context = $$listener.GetContext() ;$$requestUrl = $$context.Request.Url ;$$clientIP = $\\n$context.Request.RemoteEndPoint.Address ;$$response = $$context.Response ;Write-Host', metadata={'source': './PDFS/Concepts.pdf', 'page': 589}),\n",
       " Document(page_content='\\'\\' ;Write-Host(\\'> {0}\\' -f $$requestUrl) ;  ;$$count = 1 ;$$k=$$callerCounts.Get_Item($$clientIP) ;if \\n($$k -ne $$null) { $$count += $$k } ;$$callerCounts.Set_Item($$clientIP, $$count) ;$$ip=(Get-\\nNetAdapter | Get-NetIpAddress); $$header=\\'<html><body><H1>Windows Container Web \\nServer</H1>\\' ;$$callerCountsString=\\'\\' ;$$callerCounts.Keys | % { $$callerCountsString+=\\'<p>IP \\n{0} callerCount {1} \\' -f $$ip[1].IPAddress,$$callerCounts.Item($$_) } ;$$footer=\\'</body></html>\\' ;\\n$$content=\\'{0}{1}{2}\\' -f $$header,$$callerCountsString,$$footer ;Write-Output $$content ;$\\n$buffer = [System.Text.Encoding]::UTF8.GetBytes($$content) ;$$response.ContentLength64 = $\\n$buffer.Length ;$$response.OutputStream.Write($$buffer, 0, $$buffer.Length) ;$\\n$response.Close() ;$$responseStatus = $$response.StatusCode ;Write-Host(\\'< {0}\\' -f $\\n$responseStatus)  } ; \"\\n     nodeSelector :\\n      kubernetes.io/os : windows\\nNote:  Port mapping is also supported, but for simplicity this example exposes port 80 of the\\ncontainer directly to the Service.\\nCheck that all nodes are healthy:\\nkubectl get nodes\\nDeploy the service and watch for pod updates:\\nkubectl apply -f win-webserver.yaml\\nkubectl get pods -o wide -w\\nWhen the service is deployed correctly both Pods are marked as Ready. To exit the watch\\ncommand, press Ctrl+C.\\nCheck that the deployment succeeded. To verify:\\nSeveral pods listed from the Linux control plane node, use kubectl get pods\\nNode-to-pod communication across the network, curl port 80 of your pod IPs from\\nthe Linux control plane node to check for a web server response\\nPod-to-pod communication, ping between pods (and across hosts, if you have more\\nthan one Windows node) using kubectl exec\\nService-to-pod communication, curl the virtual service IP (seen under kubectl get \\nservices ) from the Linux control plane node and from individual pods\\nService discovery, curl the service name with the Kubernetes default DNS suffix\\nInbound connectivity, curl the NodePort from the Linux control plane node or\\nmachines outside of the cluster\\nOutbound connectivity, curl external IPs from inside the pod using kubectl exec\\nNote:  Windows container hosts are not able to access the IP of services scheduled on them due\\nto current platform limitations of the Windows networking stack. Only Windows pods are able\\nto access service IPs.\\nObservability\\nCapturing logs from workloads\\nLogs are an important element of observability; they enable users to gain insights into the\\noperational aspect of workloads and are a key ingredient to troubleshooting issues. Because\\nWindows containers and workloads inside Windows containers behave differently from Linux1. \\n2. \\n3. \\n◦ \\n◦ \\n◦ \\n◦ \\n◦ \\n◦ \\n◦', metadata={'source': './PDFS/Concepts.pdf', 'page': 590}),\n",
       " Document(page_content='containers, users had a hard time collecting logs, limiting operational visibility. Windows\\nworkloads for example are usually configured to log to ETW (Event Tracing for Windows) or\\npush entries to the application event log. LogMonitor , an open source tool by Microsoft, is the\\nrecommended way to monitor configured log sources inside a Windows container. LogMonitor\\nsupports monitoring event logs, ETW providers, and custom application logs, piping them to\\nSTDOUT for consumption by kubectl logs <pod> .\\nFollow the instructions in the LogMonitor GitHub page to copy its binaries and configuration\\nfiles to all your containers and add the necessary entrypoints for LogMonitor to push your logs\\nto STDOUT.\\nConfiguring container user\\nUsing configurable Container usernames\\nWindows containers can be configured to run their entrypoints and processes with different\\nusernames than the image defaults. Learn more about it here.\\nManaging Workload Identity with Group Managed Service Accounts\\nWindows container workloads can be configured to use Group Managed Service Accounts\\n(GMSA). Group Managed Service Accounts are a specific type of Active Directory account that\\nprovide automatic password management, simplified service principal name (SPN)\\nmanagement, and the ability to delegate the management to other administrators across\\nmultiple servers. Containers configured with a GMSA can access external Active Directory\\nDomain resources while carrying the identity configured with the GMSA. Learn more about\\nconfiguring and using GMSA for Windows containers here.\\nTaints and tolerations\\nUsers need to use some combination of taint  and node selectors in order to schedule Linux and\\nWindows workloads to their respective OS-specific nodes. The recommended approach is\\noutlined below, with one of its main goals being that this approach should not break\\ncompatibility for existing Linux workloads.\\nYou can (and should) set .spec.os.name  for each Pod, to indicate the operating system that the\\ncontainers in that Pod are designed for. For Pods that run Linux containers, set .spec.os.name  to \\nlinux . For Pods that run Windows containers, set .spec.os.name  to windows .\\nNote:  If you are running a version of Kubernetes older than 1.24, you may need to enable the \\nIdentifyPodOS  feature gate  to be able to set a value for .spec.pod.os .\\nThe scheduler does not use the value of .spec.os.name  when assigning Pods to nodes. You\\nshould use normal Kubernetes mechanisms for assigning pods to nodes  to ensure that the\\ncontrol plane for your cluster places pods onto nodes that are running the appropriate\\noperating system.\\nThe .spec.os.name  value has no effect on the scheduling of the Windows pods, so taints and\\ntolerations (or node selectors) are still required to ensure that the Windows pods land onto\\nappropriate Windows nodes.', metadata={'source': './PDFS/Concepts.pdf', 'page': 591}),\n",
       " Document(page_content='Ensuring OS-specific workloads land on the appropriate container host\\nUsers can ensure Windows containers can be scheduled on the appropriate host using taints\\nand tolerations. All Kubernetes nodes running Kubernetes 1.28 have the following default\\nlabels:\\nkubernetes.io/os = [windows|linux]\\nkubernetes.io/arch = [amd64|arm64|...]\\nIf a Pod specification does not specify a nodeSelector  such as \"kubernetes.io/os\": windows , it is\\npossible the Pod can be scheduled on any host, Windows or Linux. This can be problematic\\nsince a Windows container can only run on Windows and a Linux container can only run on\\nLinux. The best practice for Kubernetes 1.28 is to use a nodeSelector .\\nHowever, in many cases users have a pre-existing large number of deployments for Linux\\ncontainers, as well as an ecosystem of off-the-shelf configurations, such as community Helm\\ncharts, and programmatic Pod generation cases, such as with operators. In those situations, you\\nmay be hesitant to make the configuration change to add nodeSelector  fields to all Pods and\\nPod templates. The alternative is to use taints. Because the kubelet can set taints during\\nregistration, it could easily be modified to automatically add a taint when running on Windows\\nonly.\\nFor example: --register-with-taints=\\'os=windows:NoSchedule\\'\\nBy adding a taint to all Windows nodes, nothing will be scheduled on them (that includes\\nexisting Linux Pods). In order for a Windows Pod to be scheduled on a Windows node, it would\\nneed both the nodeSelector  and the appropriate matching toleration to choose Windows.\\nnodeSelector :\\n    kubernetes.io/os : windows\\n    node.kubernetes.io/windows-build : \\'10.0.17763\\'\\ntolerations :\\n    - key: \"os\"\\n      operator : \"Equal\"\\n      value : \"windows\"\\n      effect : \"NoSchedule\"\\nHandling multiple Windows versions in the same cluster\\nThe Windows Server version used by each pod must match that of the node. If you want to use\\nmultiple Windows Server versions in the same cluster, then you should set additional node\\nlabels and nodeSelector  fields.\\nKubernetes automatically adds a label, node.kubernetes.io/windows-build  to simplify this.\\nThis label reflects the Windows major, minor, and build number that need to match for\\ncompatibility. Here are values used for each Windows Server version:\\nProduct Name Version\\nWindows Server 2019 10.0.17763\\nWindows Server 2022 10.0.20348• \\n•', metadata={'source': './PDFS/Concepts.pdf', 'page': 592}),\n",
       " Document(page_content='Simplifying with RuntimeClass\\nRuntimeClass  can be used to simplify the process of using taints and tolerations. A cluster\\nadministrator can create a RuntimeClass  object which is used to encapsulate these taints and\\ntolerations.\\nSave this file to runtimeClasses.yml . It includes the appropriate nodeSelector  for the\\nWindows OS, architecture, and version.\\n---\\napiVersion : node.k8s.io/v1\\nkind: RuntimeClass\\nmetadata :\\n  name : windows-2019\\nhandler : example-container-runtime-handler\\nscheduling :\\n  nodeSelector :\\n    kubernetes.io/os : \\'windows\\'\\n    kubernetes.io/arch : \\'amd64\\'\\n    node.kubernetes.io/windows-build : \\'10.0.17763\\'\\n  tolerations :\\n  - effect : NoSchedule\\n    key: os\\n    operator : Equal\\n    value : \"windows\"\\nRun kubectl create -f runtimeClasses.yml  using as a cluster administrator\\nAdd runtimeClassName: windows-2019  as appropriate to Pod specs\\nFor example:\\n---\\napiVersion : apps/v1\\nkind: Deployment\\nmetadata :\\n  name : iis-2019\\n  labels :\\n    app: iis-2019\\nspec:\\n  replicas : 1\\n  template :\\n    metadata :\\n      name : iis-2019\\n      labels :\\n        app: iis-2019\\n    spec:\\n      runtimeClassName : windows-2019\\n      containers :\\n      - name : iis\\n        image : mcr.microsoft.com/windows/servercore/iis:windowsservercore-ltsc2019\\n        resources :\\n          limits :1. \\n2. \\n3.', metadata={'source': './PDFS/Concepts.pdf', 'page': 593}),\n",
       " Document(page_content='cpu: 1\\n            memory : 800Mi\\n          requests :\\n            cpu: .1\\n            memory : 300Mi\\n        ports :\\n          - containerPort : 80\\n selector :\\n    matchLabels :\\n      app: iis-2019\\n---\\napiVersion : v1\\nkind: Service\\nmetadata :\\n  name : iis\\nspec:\\n  type: LoadBalancer\\n  ports :\\n  - protocol : TCP\\n    port: 80\\n  selector :\\n    app: iis-2019\\nExtending Kubernetes\\nDifferent ways to change the behavior of your Kubernetes cluster.\\nKubernetes is highly configurable and extensible. As a result, there is rarely a need to fork or\\nsubmit patches to the Kubernetes project code.\\nThis guide describes the options for customizing a Kubernetes cluster. It is aimed at cluster\\noperators  who want to understand how to adapt their Kubernetes cluster to the needs of their\\nwork environment. Developers who are prospective Platform Developers  or Kubernetes Project \\nContributors  will also find it useful as an introduction to what extension points and patterns\\nexist, and their trade-offs and limitations.\\nCustomization approaches can be broadly divided into configuration , which only involves\\nchanging command line arguments, local configuration files, or API resources; and extensions ,\\nwhich involve running additional programs, additional network services, or both. This\\ndocument is primarily about extensions .\\nConfiguration\\nConfiguration files  and command arguments  are documented in the Reference  section of the\\nonline documentation, with a page for each binary:\\nkube-apiserver\\nkube-controller-manager\\nkube-scheduler\\nkubelet\\nkube-proxy• \\n• \\n• \\n• \\n•', metadata={'source': './PDFS/Concepts.pdf', 'page': 594}),\n",
       " Document(page_content=\"Command arguments and configuration files may not always be changeable in a hosted\\nKubernetes service or a distribution with managed installation. When they are changeable, they\\nare usually only changeable by the cluster operator. Also, they are subject to change in future\\nKubernetes versions, and setting them may require restarting processes. For those reasons, they\\nshould be used only when there are no other options.\\nBuilt-in policy APIs , such as ResourceQuota , NetworkPolicy  and Role-based Access Control\\n(RBAC ), are built-in Kubernetes APIs that provide declaratively configured policy settings. APIs\\nare typically usable even with hosted Kubernetes services and with managed Kubernetes\\ninstallations. The built-in policy APIs follow the same conventions as other Kubernetes\\nresources such as Pods. When you use a policy APIs that is stable , you benefit from a defined\\nsupport policy  like other Kubernetes APIs. For these reasons, policy APIs are recommended\\nover configuration files  and command arguments  where suitable.\\nExtensions\\nExtensions are software components that extend and deeply integrate with Kubernetes. They\\nadapt it to support new types and new kinds of hardware.\\nMany cluster administrators use a hosted or distribution instance of Kubernetes. These clusters\\ncome with extensions pre-installed. As a result, most Kubernetes users will not need to install\\nextensions and even fewer users will need to author new ones.\\nExtension patterns\\nKubernetes is designed to be automated by writing client programs. Any program that reads\\nand/or writes to the Kubernetes API can provide useful automation. Automation  can run on the\\ncluster or off it. By following the guidance in this doc you can write highly available and robust\\nautomation. Automation generally works with any Kubernetes cluster, including hosted\\nclusters and managed installations.\\nThere is a specific pattern for writing client programs that work well with Kubernetes called the\\ncontroller  pattern. Controllers typically read an object's .spec , possibly do things, and then\\nupdate the object's .status .\\nA controller is a client of the Kubernetes API. When Kubernetes is the client and calls out to a\\nremote service, Kubernetes calls this a webhook . The remote service is called a webhook backend .\\nAs with custom controllers, webhooks do add a point of failure.\\nNote:  Outside of Kubernetes, the term “webhook” typically refers to a mechanism for\\nasynchronous notifications, where the webhook call serves as a one-way notification to another\\nsystem or component. In the Kubernetes ecosystem, even synchronous HTTP callouts are often\\ndescribed as “webhooks”.\\nIn the webhook model, Kubernetes makes a network request to a remote service. With the\\nalternative binary Plugin  model, Kubernetes executes a binary (program). Binary plugins are\\nused by the kubelet (for example, CSI storage plugins  and CNI network plugins ), and by kubectl\\n(see Extend kubectl with plugins ).\\nExtension points\\nThis diagram shows the extension points in a Kubernetes cluster and the clients that access it.\", metadata={'source': './PDFS/Concepts.pdf', 'page': 595}),\n",
       " Document(page_content=\"Symbolic representation of seven numbered extension points for Kubernetes\\nKubernetes extension points\\nKey to the figure\\nUsers often interact with the Kubernetes API using kubectl . Plugins  customise the\\nbehaviour of clients. There are generic extensions that can apply to different clients, as\\nwell as specific ways to extend kubectl .\\nThe API server handles all requests. Several types of extension points in the API server\\nallow authenticating requests, or blocking them based on their content, editing content,\\nand handling deletion. These are described in the API Access Extensions  section.\\nThe API server serves various kinds of resources . Built-in resource kinds , such as pods , are\\ndefined by the Kubernetes project and can't be changed. Read API extensions  to learn\\nabout extending the Kubernetes API.\\nThe Kubernetes scheduler decides  which nodes to place pods on. There are several ways\\nto extend scheduling, which are described in the Scheduling extensions  section.\\nMuch of the behavior of Kubernetes is implemented by programs called controllers , that\\nare clients of the API server. Controllers are often used in conjunction with custom\\nresources. Read combining new APIs with automation  and Changing built-in resources  to\\nlearn more.\\nThe kubelet runs on servers (nodes), and helps pods appear like virtual servers with their\\nown IPs on the cluster network. Network Plugins  allow for different implementations of\\npod networking.\\nYou can use Device Plugins  to integrate custom hardware or other special node-local\\nfacilities, and make these available to Pods running in your cluster. The kubelet includes\\nsupport for working with device plugins.\\nThe kubelet also mounts and unmounts volume  for pods and their containers. You can\\nuse Storage Plugins  to add support for new kinds of storage and other volume types.\\nExtension point choice flowchart\\nIf you are unsure where to start, this flowchart can help. Note that some solutions may involve\\nseveral types of extensions.\\nFlowchart with questions about use cases and guidance for implementers. Green\\ncircles indicate yes; red circles indicate no.\\nFlowchart guide to select an extension approach\\nClient extensions\\nPlugins for kubectl are separate binaries that add or replace the behavior of specific\\nsubcommands. The kubectl  tool can also integrate with credential plugins  These extensions\\nonly affect a individual user's local environment, and so cannot enforce site-wide policies.1. \\n2. \\n3. \\n4. \\n5. \\n6. \\n7.\", metadata={'source': './PDFS/Concepts.pdf', 'page': 596}),\n",
       " Document(page_content=\"If you want to extend the kubectl  tool, read Extend kubectl with plugins .\\nAPI extensions\\nCustom resource definitions\\nConsider adding a Custom Resource  to Kubernetes if you want to define new controllers,\\napplication configuration objects or other declarative APIs, and to manage them using\\nKubernetes tools, such as kubectl .\\nFor more about Custom Resources, see the Custom Resources  concept guide.\\nAPI aggregation layer\\nYou can use Kubernetes' API Aggregation Layer  to integrate the Kubernetes API with\\nadditional services such as for metrics .\\nCombining new APIs with automation\\nA combination of a custom resource API and a control loop is called the controllers  pattern. If\\nyour controller takes the place of a human operator deploying infrastructure based on a desired\\nstate, then the controller may also be following the operator pattern . The Operator pattern is\\nused to manage specific applications; usually, these are applications that maintain state and\\nrequire care in how they are managed.\\nYou can also make your own custom APIs and control loops that manage other resources, such\\nas storage, or to define policies (such as an access control restriction).\\nChanging built-in resources\\nWhen you extend the Kubernetes API by adding custom resources, the added resources always\\nfall into a new API Groups. You cannot replace or change existing API groups. Adding an API\\ndoes not directly let you affect the behavior of existing APIs (such as Pods), whereas API Access\\nExtensions  do.\\nAPI access extensions\\nWhen a request reaches the Kubernetes API Server, it is first authenticated , then authorized , and\\nis then subject to various types of admission control  (some requests are in fact not\\nauthenticated, and get special treatment). See Controlling Access to the Kubernetes API  for\\nmore on this flow.\\nEach of the steps in the Kubernetes authentication / authorization flow offers extension points.\\nAuthentication\\nAuthentication  maps headers or certificates in all requests to a username for the client making\\nthe request.\", metadata={'source': './PDFS/Concepts.pdf', 'page': 597}),\n",
       " Document(page_content=\"Kubernetes has several built-in authentication methods that it supports. It can also sit behind\\nan authenticating proxy, and it can send a token from an Authorization:  header to a remote\\nservice for verification (an authentication webhook ) if those don't meet your needs.\\nAuthorization\\nAuthorization  determines whether specific users can read, write, and do other operations on\\nAPI resources. It works at the level of whole resources -- it doesn't discriminate based on\\narbitrary object fields.\\nIf the built-in authorization options don't meet your needs, an authorization webhook  allows\\ncalling out to custom code that makes an authorization decision.\\nDynamic admission control\\nAfter a request is authorized, if it is a write operation, it also goes through Admission Control\\nsteps. In addition to the built-in steps, there are several extensions:\\nThe Image Policy webhook  restricts what images can be run in containers.\\nTo make arbitrary admission control decisions, a general Admission webhook  can be\\nused. Admission webhooks can reject creations or updates. Some admission webhooks\\nmodify the incoming request data before it is handled further by Kubernetes.\\nInfrastructure extensions\\nDevice plugins\\nDevice plugins  allow a node to discover new Node resources (in addition to the builtin ones like\\ncpu and memory) via a Device Plugin .\\nStorage plugins\\nContainer Storage Interface  (CSI) plugins provide a way to extend Kubernetes with supports for\\nnew kinds of volumes. The volumes can be backed by durable external storage, or provide\\nephemeral storage, or they might offer a read-only interface to information using a filesystem\\nparadigm.\\nKubernetes also includes support for FlexVolume  plugins, which are deprecated since\\nKubernetes v1.23 (in favour of CSI).\\nFlexVolume plugins allow users to mount volume types that aren't natively supported by\\nKubernetes. When you run a Pod that relies on FlexVolume storage, the kubelet calls a binary\\nplugin to mount the volume. The archived FlexVolume  design proposal has more detail on this\\napproach.\\nThe Kubernetes Volume Plugin FAQ for Storage Vendors  includes general information on\\nstorage plugins.\\nNetwork plugins\\nYour Kubernetes cluster needs a network plugin  in order to have a working Pod network and to\\nsupport other aspects of the Kubernetes network model.• \\n•\", metadata={'source': './PDFS/Concepts.pdf', 'page': 598}),\n",
       " Document(page_content=\"Network Plugins  allow Kubernetes to work with different networking topologies and\\ntechnologies.\\nKubelet image credential provider plugins\\nFEATURE STATE:  Kubernetes v1.26 [stable]\\nKubelet image credential providers are plugins for the kubelet to dynamically retrieve image\\nregistry credentials. The credentials are then used when pulling images from container image\\nregistries that match the configuration.\\nThe plugins can communicate with external services or use local files to obtain credentials. This\\nway, the kubelet does not need to have static credentials for each registry, and can support\\nvarious authentication methods and protocols.\\nFor plugin configuration details, see Configure a kubelet image credential provider .\\nScheduling extensions\\nThe scheduler is a special type of controller that watches pods, and assigns pods to nodes. The\\ndefault scheduler can be replaced entirely, while continuing to use other Kubernetes\\ncomponents, or multiple schedulers  can run at the same time.\\nThis is a significant undertaking, and almost all Kubernetes users find they do not need to\\nmodify the scheduler.\\nYou can control which scheduling plugins  are active, or associate sets of plugins with different\\nnamed scheduler profiles . You can also write your own plugin that integrates with one or more\\nof the kube-scheduler's extension points .\\nFinally, the built in kube-scheduler  component supports a webhook  that permits a remote\\nHTTP backend (scheduler extension) to filter and / or prioritize the nodes that the kube-\\nscheduler chooses for a pod.\\nNote:  You can only affect node filtering and node prioritization with a scheduler extender\\nwebhook; other extension points are not available through the webhook integration.\\nWhat's next\\nLearn more about infrastructure extensions\\nDevice Plugins\\nNetwork Plugins\\nCSI storage plugins\\nLearn about kubectl plugins\\nLearn more about Custom Resources\\nLearn more about Extension API Servers\\nLearn about Dynamic admission control\\nLearn about the Operator pattern• \\n◦ \\n◦ \\n◦ \\n• \\n• \\n• \\n• \\n•\", metadata={'source': './PDFS/Concepts.pdf', 'page': 599}),\n",
       " Document(page_content=\"Compute, Storage, and Networking\\nExtensions\\nThis section covers extensions to your cluster that do not come as part as Kubernetes itself. You\\ncan use these extensions to enhance the nodes in your cluster, or to provide the network fabric\\nthat links Pods together.\\nCSI and FlexVolume  storage plugins\\nContainer Storage Interface  (CSI) plugins provide a way to extend Kubernetes with\\nsupports for new kinds of volumes. The volumes can be backed by durable external\\nstorage, or provide ephemeral storage, or they might offer a read-only interface to\\ninformation using a filesystem paradigm.\\nKubernetes also includes support for FlexVolume  plugins, which are deprecated since\\nKubernetes v1.23 (in favour of CSI).\\nFlexVolume plugins allow users to mount volume types that aren't natively supported by\\nKubernetes. When you run a Pod that relies on FlexVolume storage, the kubelet calls a\\nbinary plugin to mount the volume. The archived FlexVolume  design proposal has more\\ndetail on this approach.\\nThe Kubernetes Volume Plugin FAQ for Storage Vendors  includes general information on\\nstorage plugins.\\nDevice plugins\\nDevice plugins allow a node to discover new Node facilities (in addition to the built-in\\nnode resources such as cpu and memory ), and provide these custom node-local facilities\\nto Pods that request them.\\nNetwork plugins\\nA network plugin allow Kubernetes to work with different networking topologies and\\ntechnologies. Your Kubernetes cluster needs a network plugin  in order to have a working\\nPod network and to support other aspects of the Kubernetes network model.\\nKubernetes 1.28 is compatible with CNI network plugins.\\nNetwork Plugins\\nKubernetes 1.28 supports Container Network Interface  (CNI) plugins for cluster networking.\\nYou must use a CNI plugin that is compatible with your cluster and that suits your needs.\\nDifferent plugins are available (both open- and closed- source) in the wider Kubernetes\\necosystem.\\nA CNI plugin is required to implement the Kubernetes network model .\\nYou must use a CNI plugin that is compatible with the v0.4.0  or later releases of the CNI\\nspecification. The Kubernetes project recommends using a plugin that is compatible with the \\nv1.0.0  CNI specification (plugins can be compatible with multiple spec versions).• \\n• \\n•\", metadata={'source': './PDFS/Concepts.pdf', 'page': 600}),\n",
       " Document(page_content='Installation\\nA Container Runtime, in the networking context, is a daemon on a node configured to provide\\nCRI Services for kubelet. In particular, the Container Runtime must be configured to load the\\nCNI plugins required to implement the Kubernetes network model.\\nNote:\\nPrior to Kubernetes 1.24, the CNI plugins could also be managed by the kubelet using the cni-\\nbin-dir  and network-plugin  command-line parameters. These command-line parameters were\\nremoved in Kubernetes 1.24, with management of the CNI no longer in scope for kubelet.\\nSee Troubleshooting CNI plugin-related errors  if you are facing issues following the removal of\\ndockershim.\\nFor specific information about how a Container Runtime manages the CNI plugins, see the\\ndocumentation for that Container Runtime, for example:\\ncontainerd\\nCRI-O\\nFor specific information about how to install and manage a CNI plugin, see the documentation\\nfor that plugin or networking provider .\\nNetwork Plugin Requirements\\nFor plugin developers and users who regularly build or deploy Kubernetes, the plugin may also\\nneed specific configuration to support kube-proxy. The iptables proxy depends on iptables, and\\nthe plugin may need to ensure that container traffic is made available to iptables. For example,\\nif the plugin connects containers to a Linux bridge, the plugin must set the net/bridge/bridge-\\nnf-call-iptables  sysctl to 1 to ensure that the iptables proxy functions correctly. If the plugin\\ndoes not use a Linux bridge, but uses something like Open vSwitch or some other mechanism\\ninstead, it should ensure container traffic is appropriately routed for the proxy.\\nBy default, if no kubelet network plugin is specified, the noop  plugin is used, which sets net/\\nbridge/bridge-nf-call-iptables=1  to ensure simple configurations (like Docker with a bridge)\\nwork correctly with the iptables proxy.\\nLoopback CNI\\nIn addition to the CNI plugin installed on the nodes for implementing the Kubernetes network\\nmodel, Kubernetes also requires the container runtimes to provide a loopback interface lo,\\nwhich is used for each sandbox (pod sandboxes, vm sandboxes, ...). Implementing the loopback\\ninterface can be accomplished by re-using the CNI loopback plugin.  or by developing your own\\ncode to achieve this (see this example from CRI-O ).\\nSupport hostPort\\nThe CNI networking plugin supports hostPort . You can use the official portmap  plugin offered\\nby the CNI plugin team or use your own plugin with portMapping functionality.• \\n•', metadata={'source': './PDFS/Concepts.pdf', 'page': 601}),\n",
       " Document(page_content='If you want to enable hostPort  support, you must specify portMappings capability  in your cni-\\nconf-dir . For example:\\n{\\n  \"name\" : \"k8s-pod-network\" ,\\n  \"cniVersion\" : \"0.4.0\" ,\\n  \"plugins\" : [\\n    {\\n      \"type\" : \"calico\" ,\\n      \"log_level\" : \"info\" ,\\n      \"datastore_type\" : \"kubernetes\" ,\\n      \"nodename\" : \"127.0.0.1\" ,\\n      \"ipam\" : {\\n        \"type\" : \"host-local\" ,\\n        \"subnet\" : \"usePodCidr\"\\n      },\\n      \"policy\" : {\\n        \"type\" : \"k8s\"\\n      },\\n      \"kubernetes\" : {\\n        \"kubeconfig\" : \"/etc/cni/net.d/calico-kubeconfig\"\\n      }\\n    },\\n    {\\n      \"type\" : \"portmap\" ,\\n      \"capabilities\" : {\"portMappings\" : true},\\n      \"externalSetMarkChain\" : \"KUBE-MARK-MASQ\"\\n    }\\n  ]\\n}\\nSupport traffic shaping\\nExperimental Feature\\nThe CNI networking plugin also supports pod ingress and egress traffic shaping. You can use\\nthe official bandwidth  plugin offered by the CNI plugin team or use your own plugin with\\nbandwidth control functionality.\\nIf you want to enable traffic shaping support, you must add the bandwidth  plugin to your CNI\\nconfiguration file (default /etc/cni/net.d ) and ensure that the binary is included in your CNI bin\\ndir (default /opt/cni/bin ).\\n{\\n  \"name\" : \"k8s-pod-network\" ,\\n  \"cniVersion\" : \"0.4.0\" ,\\n  \"plugins\" : [\\n    {\\n      \"type\" : \"calico\" ,\\n      \"log_level\" : \"info\" ,\\n      \"datastore_type\" : \"kubernetes\" ,\\n      \"nodename\" : \"127.0.0.1\" ,\\n      \"ipam\" : {', metadata={'source': './PDFS/Concepts.pdf', 'page': 602}),\n",
       " Document(page_content='\"type\" : \"host-local\" ,\\n        \"subnet\" : \"usePodCidr\"\\n      },\\n      \"policy\" : {\\n        \"type\" : \"k8s\"\\n      },\\n      \"kubernetes\" : {\\n        \"kubeconfig\" : \"/etc/cni/net.d/calico-kubeconfig\"\\n      }\\n    },\\n    {\\n      \"type\" : \"bandwidth\" ,\\n      \"capabilities\" : {\"bandwidth\" : true}\\n    }\\n  ]\\n}\\nNow you can add the kubernetes.io/ingress-bandwidth  and kubernetes.io/egress-bandwidth\\nannotations to your Pod. For example:\\napiVersion : v1\\nkind: Pod\\nmetadata :\\n  annotations :\\n    kubernetes.io/ingress-bandwidth : 1M\\n    kubernetes.io/egress-bandwidth : 1M\\n...\\nWhat\\'s next\\nLearn more about Cluster Networking\\nLearn more about Network Policies\\nLearn about the Troubleshooting CNI plugin-related errors\\nDevice Plugins\\nDevice plugins let you configure your cluster with support for devices or resources that require\\nvendor-specific setup, such as GPUs, NICs, FPGAs, or non-volatile main memory.\\nFEATURE STATE:  Kubernetes v1.26 [stable]\\nKubernetes provides a device plugin framework that you can use to advertise system hardware\\nresources to the Kubelet .\\nInstead of customizing the code for Kubernetes itself, vendors can implement a device plugin\\nthat you deploy either manually or as a DaemonSet . The targeted devices include GPUs, high-\\nperformance NICs, FPGAs, InfiniBand adapters, and other similar computing resources that\\nmay require vendor specific initialization and setup.\\nDevice plugin registration\\nThe kubelet exports a Registration  gRPC service:• \\n• \\n•', metadata={'source': './PDFS/Concepts.pdf', 'page': 603}),\n",
       " Document(page_content='service Registration {\\nrpc Register(RegisterRequest) returns (Empty) {}\\n}\\nA device plugin can register itself with the kubelet through this gRPC service. During the\\nregistration, the device plugin needs to send:\\nThe name of its Unix socket.\\nThe Device Plugin API version against which it was built.\\nThe ResourceName  it wants to advertise. Here ResourceName  needs to follow the \\nextended resource naming scheme  as vendor-domain/resourcetype . (For example, an\\nNVIDIA GPU is advertised as nvidia.com/gpu .)\\nFollowing a successful registration, the device plugin sends the kubelet the list of devices it\\nmanages, and the kubelet is then in charge of advertising those resources to the API server as\\npart of the kubelet node status update. For example, after a device plugin registers hardware-\\nvendor.example/foo  with the kubelet and reports two healthy devices on a node, the node status\\nis updated to advertise that the node has 2 \"Foo\" devices installed and available.\\nThen, users can request devices as part of a Pod specification (see container ). Requesting\\nextended resources is similar to how you manage requests and limits for other resources, with\\nthe following differences:\\nExtended resources are only supported as integer resources and cannot be\\novercommitted.\\nDevices cannot be shared between containers.\\nExample\\nSuppose a Kubernetes cluster is running a device plugin that advertises resource hardware-\\nvendor.example/foo  on certain nodes. Here is an example of a pod requesting this resource to\\nrun a demo workload:\\n---\\napiVersion : v1\\nkind: Pod\\nmetadata :\\n  name : demo-pod\\nspec:\\n  containers :\\n    - name : demo-container-1\\n      image : registry.k8s.io/pause:2.0\\n      resources :\\n        limits :\\n          hardware-vendor.example/foo : 2\\n#\\n# This Pod needs 2 of the hardware-vendor.example/foo devices\\n# and can only schedule onto a Node that\\'s able to satisfy\\n# that need.\\n#\\n# If the Node has more than 2 of those devices available, the\\n# remainder would be available for other Pods to use.• \\n• \\n• \\n• \\n•', metadata={'source': './PDFS/Concepts.pdf', 'page': 604}),\n",
       " Document(page_content='Device plugin implementation\\nThe general workflow of a device plugin includes the following steps:\\nInitialization. During this phase, the device plugin performs vendor-specific initialization\\nand setup to make sure the devices are in a ready state.\\nThe plugin starts a gRPC service, with a Unix socket under the host path /var/lib/kubelet/\\ndevice-plugins/ , that implements the following interfaces:\\nservice DevicePlugin {\\n      // GetDevicePluginOptions returns options to be communicated with Device \\nManager.\\n      rpc GetDevicePluginOptions(Empty) returns (DevicePluginOptions) {}\\n      // ListAndWatch returns a stream of List of Devices\\n      // Whenever a Device state change or a Device disappears, ListAndWatch\\n      // returns the new list\\n      rpc ListAndWatch(Empty) returns (stream ListAndWatchResponse) {}\\n      // Allocate is called during container creation so that the Device\\n      // Plugin can run device specific operations and instruct Kubelet\\n      // of the steps to make the Device available in the container\\n      rpc Allocate(AllocateRequest) returns (AllocateResponse) {}\\n      // GetPreferredAllocation returns a preferred set of devices to allocate\\n      // from a list of available ones. The resulting preferred allocation is not\\n      // guaranteed to be the allocation ultimately performed by the\\n      // devicemanager. It is only designed to help the devicemanager make a more\\n      // informed allocation decision when possible.\\n      rpc GetPreferredAllocation(PreferredAllocationRequest) returns \\n(PreferredAllocationResponse) {}\\n      // PreStartContainer is called, if indicated by Device Plugin during registeration \\nphase,\\n      // before each container start. Device plugin can run device specific operations\\n      // such as resetting the device before making devices available to the container.\\n      rpc PreStartContainer(PreStartContainerRequest) returns \\n(PreStartContainerResponse) {}\\n}\\nNote:  Plugins are not required to provide useful implementations for \\nGetPreferredAllocation()  or PreStartContainer() . Flags indicating the availability of these\\ncalls, if any, should be set in the DevicePluginOptions  message sent back by a call to \\nGetDevicePluginOptions() . The kubelet  will always call GetDevicePluginOptions()  to see\\nwhich optional functions are available, before calling any of them directly.\\nThe plugin registers itself with the kubelet through the Unix socket at host path /var/lib/\\nkubelet/device-plugins/kubelet.sock .\\nNote:  The ordering of the workflow is important. A plugin MUST start serving gRPC\\nservice before registering itself with kubelet for successful registration.1. \\n2. \\n3.', metadata={'source': './PDFS/Concepts.pdf', 'page': 605}),\n",
       " Document(page_content=\"After successfully registering itself, the device plugin runs in serving mode, during which\\nit keeps monitoring device health and reports back to the kubelet upon any device state\\nchanges. It is also responsible for serving Allocate  gRPC requests. During Allocate , the\\ndevice plugin may do device-specific preparation; for example, GPU cleanup or QRNG\\ninitialization. If the operations succeed, the device plugin returns an AllocateResponse\\nthat contains container runtime configurations for accessing the allocated devices. The\\nkubelet passes this information to the container runtime.\\nAn AllocateResponse  contains zero or more ContainerAllocateResponse  objects. In these,\\nthe device plugin defines modifications that must be made to a container's definition to\\nprovide access to the device. These modifications include:\\nAnnotations\\ndevice nodes\\nenvironment variables\\nmounts\\nfully-qualified CDI device names\\nNote:  The processing of the fully-qualified CDI device names by the Device Manager\\nrequires that the DevicePluginCDIDevices  feature gate  is enabled for the kubelet and the\\nkube-apiserver. This was added as an alpha feature in Kubernetes v1.28.\\nHandling kubelet restarts\\nA device plugin is expected to detect kubelet restarts and re-register itself with the new kubelet\\ninstance. A new kubelet instance deletes all the existing Unix sockets under /var/lib/kubelet/\\ndevice-plugins  when it starts. A device plugin can monitor the deletion of its Unix socket and\\nre-register itself upon such an event.\\nDevice plugin deployment\\nYou can deploy a device plugin as a DaemonSet, as a package for your node's operating system,\\nor manually.\\nThe canonical directory /var/lib/kubelet/device-plugins  requires privileged access, so a device\\nplugin must run in a privileged security context. If you're deploying a device plugin as a\\nDaemonSet, /var/lib/kubelet/device-plugins  must be mounted as a Volume  in the plugin's \\nPodSpec .\\nIf you choose the DaemonSet approach you can rely on Kubernetes to: place the device plugin's\\nPod onto Nodes, to restart the daemon Pod after failure, and to help automate upgrades.\\nAPI compatibility\\nPreviously, the versioning scheme required the Device Plugin's API version to match exactly\\nthe Kubelet's version. Since the graduation of this feature to Beta in v1.12 this is no longer a\\nhard requirement. The API is versioned and has been stable since Beta graduation of this\\nfeature. Because of this, kubelet upgrades should be seamless but there still may be changes in\\nthe API before stabilization making upgrades not guaranteed to be non-breaking.\\nNote:  Although the Device Manager component of Kubernetes is a generally available feature,\\nthe device plugin API  is not stable. For information on the device plugin API and version\\ncompatibility, read Device Plugin API versions .4. \\n◦ \\n◦ \\n◦ \\n◦ \\n◦\", metadata={'source': './PDFS/Concepts.pdf', 'page': 606}),\n",
       " Document(page_content='As a project, Kubernetes recommends that device plugin developers:\\nWatch for Device Plugin API changes in the future releases.\\nSupport multiple versions of the device plugin API for backward/forward compatibility.\\nTo run device plugins on nodes that need to be upgraded to a Kubernetes release with a newer\\ndevice plugin API version, upgrade your device plugins to support both versions before\\nupgrading these nodes. Taking that approach will ensure the continuous functioning of the\\ndevice allocations during the upgrade.\\nMonitoring device plugin resources\\nFEATURE STATE:  Kubernetes v1.28 [stable]\\nIn order to monitor resources provided by device plugins, monitoring agents need to be able to\\ndiscover the set of devices that are in-use on the node and obtain metadata to describe which\\ncontainer the metric should be associated with. Prometheus  metrics exposed by device\\nmonitoring agents should follow the Kubernetes Instrumentation Guidelines , identifying\\ncontainers using pod, namespace , and container  prometheus labels.\\nThe kubelet provides a gRPC service to enable discovery of in-use devices, and to provide\\nmetadata for these devices:\\n// PodResourcesLister is a service provided by the kubelet that provides information about the\\n// node resources consumed by pods and containers on the node\\nservice PodResourcesLister {\\n    rpc List(ListPodResourcesRequest) returns (ListPodResourcesResponse) {}\\n    rpc GetAllocatableResources(AllocatableResourcesRequest) returns \\n(AllocatableResourcesResponse) {}\\n    rpc Get(GetPodResourcesRequest) returns (GetPodResourcesResponse) {}\\n}\\nList gRPC endpoint\\nThe List endpoint provides information on resources of running pods, with details such as the\\nid of exclusively allocated CPUs, device id as it was reported by device plugins and id of the\\nNUMA node where these devices are allocated. Also, for NUMA-based machines, it contains the\\ninformation about memory and hugepages reserved for a container.\\nStarting from Kubernetes v1.27, the List endpoint can provide information on resources of\\nrunning pods allocated in ResourceClaims  by the DynamicResourceAllocation  API. To enable\\nthis feature kubelet  must be started with the following flags:\\n--feature-\\ngates=DynamicResourceAllocation=true,KubeletPodResourcesDynamicResources=true\\n// ListPodResourcesResponse is the response returned by List function\\nmessage ListPodResourcesResponse {\\n    repeated PodResources pod_resources = 1;\\n}\\n// PodResources contains information about the node resources assigned to a pod\\nmessage PodResources {• \\n•', metadata={'source': './PDFS/Concepts.pdf', 'page': 607}),\n",
       " Document(page_content='string name = 1;\\n    string namespace = 2;\\n    repeated ContainerResources containers = 3;\\n}\\n// ContainerResources contains information about the resources assigned to a container\\nmessage ContainerResources {\\n    string name = 1;\\n    repeated ContainerDevices devices = 2;\\n    repeated int64 cpu_ids = 3;\\n    repeated ContainerMemory memory = 4;\\n    repeated DynamicResource dynamic_resources = 5;\\n}\\n// ContainerMemory contains information about memory and hugepages assigned to a \\ncontainer\\nmessage ContainerMemory {\\n    string memory_type = 1;\\n    uint64 size = 2;\\n    TopologyInfo topology = 3;\\n}\\n// Topology describes hardware topology of the resource\\nmessage TopologyInfo {\\n        repeated NUMANode nodes = 1;\\n}\\n// NUMA representation of NUMA node\\nmessage NUMANode {\\n        int64 ID = 1;\\n}\\n// ContainerDevices contains information about the devices assigned to a container\\nmessage ContainerDevices {\\n    string resource_name = 1;\\n    repeated string device_ids = 2;\\n    TopologyInfo topology = 3;\\n}\\n// DynamicResource contains information about the devices assigned to a container by \\nDynamic Resource Allocation\\nmessage DynamicResource {\\n    string class_name = 1;\\n    string claim_name = 2;\\n    string claim_namespace = 3;\\n    repeated ClaimResource claim_resources = 4;\\n}\\n// ClaimResource contains per-plugin resource information\\nmessage ClaimResource {\\n    repeated CDIDevice cdi_devices = 1 [(gogoproto.customname) = \"CDIDevices\"];\\n}', metadata={'source': './PDFS/Concepts.pdf', 'page': 608}),\n",
       " Document(page_content='// CDIDevice specifies a CDI device information\\nmessage CDIDevice {\\n    // Fully qualified CDI device name\\n    // for example: vendor.com/gpu=gpudevice1\\n    // see more details in the CDI specification:\\n    // https://github.com/container-orchestrated-devices/container-device-interface/blob/main/\\nSPEC.md\\n    string name = 1;\\n}\\nNote:\\ncpu_ids in the ContainerResources  in the List endpoint correspond to exclusive CPUs allocated\\nto a particular container. If the goal is to evaluate CPUs that belong to the shared pool, the List\\nendpoint needs to be used in conjunction with the GetAllocatableResources  endpoint as\\nexplained below:\\nCall GetAllocatableResources  to get a list of all the allocatable CPUs\\nCall GetCpuIds  on all ContainerResources  in the system\\nSubtract out all of the CPUs from the GetCpuIds  calls from the GetAllocatableResources\\ncall\\nGetAllocatableResources  gRPC endpoint\\nFEATURE STATE:  Kubernetes v1.28 [stable]\\nGetAllocatableResources provides information on resources initially available on the worker\\nnode. It provides more information than kubelet exports to APIServer.\\nNote:\\nGetAllocatableResources  should only be used to evaluate allocatable  resources on a node. If the\\ngoal is to evaluate free/unallocated resources it should be used in conjunction with the List()\\nendpoint. The result obtained by GetAllocatableResources  would remain the same unless the\\nunderlying resources exposed to kubelet change. This happens rarely but when it does (for\\nexample: hotplug/hotunplug, device health changes), client is expected to call \\nGetAlloctableResources  endpoint.\\nHowever, calling GetAllocatableResources  endpoint is not sufficient in case of cpu and/or\\nmemory update and Kubelet needs to be restarted to reflect the correct resource capacity and\\nallocatable.\\n// AllocatableResourcesResponses contains informations about all the devices known by the \\nkubelet\\nmessage AllocatableResourcesResponse {\\n    repeated ContainerDevices devices = 1;\\n    repeated int64 cpu_ids = 2;\\n    repeated ContainerMemory memory = 3;\\n}\\nContainerDevices  do expose the topology information declaring to which NUMA cells the\\ndevice is affine. The NUMA cells are identified using a opaque integer ID, which value is\\nconsistent to what device plugins report when they register themselves to the kubelet .1. \\n2. \\n3.', metadata={'source': './PDFS/Concepts.pdf', 'page': 609}),\n",
       " Document(page_content=\"The gRPC service is served over a unix socket at /var/lib/kubelet/pod-resources/kubelet.sock .\\nMonitoring agents for device plugin resources can be deployed as a daemon, or as a\\nDaemonSet. The canonical directory /var/lib/kubelet/pod-resources  requires privileged access,\\nso monitoring agents must run in a privileged security context. If a device monitoring agent is\\nrunning as a DaemonSet, /var/lib/kubelet/pod-resources  must be mounted as a Volume  in the\\ndevice monitoring agent's PodSpec .\\nNote:\\nWhen accessing the /var/lib/kubelet/pod-resources/kubelet.sock  from DaemonSet or any other\\napp deployed as a container on the host, which is mounting socket as a volume, it is a good\\npractice to mount directory /var/lib/kubelet/pod-resources/  instead of the /var/lib/kubelet/pod-\\nresources/kubelet.sock . This will ensure that after kubelet restart, container will be able to re-\\nconnect to this socket.\\nContainer mounts are managed by inode referencing the socket or directory, depending on\\nwhat was mounted. When kubelet restarts, socket is deleted and a new socket is created, while\\ndirectory stays untouched. So the original inode for the socket become unusable. Inode to\\ndirectory will continue working.\\nGet gRPC endpoint\\nFEATURE STATE:  Kubernetes v1.27 [alpha]\\nThe Get endpoint provides information on resources of a running Pod. It exposes information\\nsimilar to those described in the List endpoint. The Get endpoint requires PodName  and \\nPodNamespace  of the running Pod.\\n// GetPodResourcesRequest contains information about the pod\\nmessage GetPodResourcesRequest {\\n    string pod_name = 1;\\n    string pod_namespace = 2;\\n}\\nTo enable this feature, you must start your kubelet services with the following flag:\\n--feature-gates=KubeletPodResourcesGet=true\\nThe Get endpoint can provide Pod information related to dynamic resources allocated by the\\ndynamic resource allocation API. To enable this feature, you must ensure your kubelet services\\nare started with the following flags:\\n--feature-\\ngates=KubeletPodResourcesGet=true,DynamicResourceAllocation=true,KubeletPodResourcesD\\nynamicResources=true\\nDevice plugin integration with the Topology Manager\\nFEATURE STATE:  Kubernetes v1.27 [stable]\\nThe Topology Manager is a Kubelet component that allows resources to be co-ordinated in a\\nTopology aligned manner. In order to do this, the Device Plugin API was extended to include a \\nTopologyInfo  struct.\", metadata={'source': './PDFS/Concepts.pdf', 'page': 610}),\n",
       " Document(page_content='message TopologyInfo {\\n    repeated NUMANode nodes = 1;\\n}\\nmessage NUMANode {\\n    int64 ID = 1;\\n}\\nDevice Plugins that wish to leverage the Topology Manager can send back a populated\\nTopologyInfo struct as part of the device registration, along with the device IDs and the health\\nof the device. The device manager will then use this information to consult with the Topology\\nManager and make resource assignment decisions.\\nTopologyInfo  supports setting a nodes  field to either nil or a list of NUMA nodes. This allows\\nthe Device Plugin to advertise a device that spans multiple NUMA nodes.\\nSetting TopologyInfo  to nil or providing an empty list of NUMA nodes for a given device\\nindicates that the Device Plugin does not have a NUMA affinity preference for that device.\\nAn example TopologyInfo  struct populated for a device by a Device Plugin:\\npluginapi.Device{ID: \"25102017\", Health: pluginapi.Healthy, \\nTopology:&pluginapi.TopologyInfo{Nodes: \\n[]*pluginapi.NUMANode{&pluginapi.NUMANode{ID: 0,},}}}\\nDevice plugin examples\\nNote:  This section links to third party projects that provide functionality required by\\nKubernetes. The Kubernetes project authors aren\\'t responsible for these projects, which are\\nlisted alphabetically. To add a project to this list, read the content guide  before submitting a\\nchange. More information.\\nHere are some examples of device plugin implementations:\\nThe AMD GPU device plugin\\nThe generic device plugin  for generic Linux devices and USB devices\\nThe Intel device plugins  for Intel GPU, FPGA, QAT, VPU, SGX, DSA, DLB and IAA\\ndevices\\nThe KubeVirt device plugins  for hardware-assisted virtualization\\nThe NVIDIA GPU device plugin for Container-Optimized OS\\nThe RDMA device plugin\\nThe SocketCAN device plugin\\nThe Solarflare device plugin\\nThe SR-IOV Network device plugin\\nThe Xilinx FPGA device plugins  for Xilinx FPGA devices\\nWhat\\'s next\\nLearn about scheduling GPU resources  using device plugins\\nLearn about advertising extended resources  on a node\\nLearn about the Topology Manager\\nRead about using hardware acceleration for TLS ingress  with Kubernetes• \\n• \\n• \\n• \\n• \\n• \\n• \\n• \\n• \\n• \\n• \\n• \\n• \\n•', metadata={'source': './PDFS/Concepts.pdf', 'page': 611}),\n",
       " Document(page_content=\"Extending the Kubernetes API\\nCustom Resources\\nKubernetes API Aggregation Layer\\nCustom Resources\\nCustom resources  are extensions of the Kubernetes API. This page discusses when to add a\\ncustom resource to your Kubernetes cluster and when to use a standalone service. It describes\\nthe two methods for adding custom resources and how to choose between them.\\nCustom resources\\nA resource  is an endpoint in the Kubernetes API  that stores a collection of API objects  of a\\ncertain kind; for example, the built-in pods resource contains a collection of Pod objects.\\nA custom resource  is an extension of the Kubernetes API that is not necessarily available in a\\ndefault Kubernetes installation. It represents a customization of a particular Kubernetes\\ninstallation. However, many core Kubernetes functions are now built using custom resources,\\nmaking Kubernetes more modular.\\nCustom resources can appear and disappear in a running cluster through dynamic registration,\\nand cluster admins can update custom resources independently of the cluster itself. Once a\\ncustom resource is installed, users can create and access its objects using kubectl , just as they\\ndo for built-in resources like Pods.\\nCustom controllers\\nOn their own, custom resources let you store and retrieve structured data. When you combine a\\ncustom resource with a custom controller , custom resources provide a true declarative API .\\nThe Kubernetes declarative API  enforces a separation of responsibilities. You declare the desired\\nstate of your resource. The Kubernetes controller keeps the current state of Kubernetes objects\\nin sync with your declared desired state. This is in contrast to an imperative API, where you \\ninstruct  a server what to do.\\nYou can deploy and update a custom controller on a running cluster, independently of the\\ncluster's lifecycle. Custom controllers can work with any kind of resource, but they are\\nespecially effective when combined with custom resources. The Operator pattern  combines\\ncustom resources and custom controllers. You can use custom controllers to encode domain\\nknowledge for specific applications into an extension of the Kubernetes API.\\nShould I add a custom resource to my Kubernetes cluster?\\nWhen creating a new API, consider whether to aggregate your API with the Kubernetes cluster\\nAPIs  or let your API stand alone.\", metadata={'source': './PDFS/Concepts.pdf', 'page': 612}),\n",
       " Document(page_content='Consider API aggregation if: Prefer a stand-alone API if:\\nYour API is Declarative .Your API does not fit the Declarative\\nmodel.\\nYou want your new types to be readable and writable\\nusing kubectl .kubectl  support is not required\\nYou want to view your new types in a Kubernetes UI,\\nsuch as dashboard, alongside built-in types.Kubernetes UI support is not required.\\nYou are developing a new API.You already have a program that serves\\nyour API and works well.\\nYou are willing to accept the format restriction that\\nKubernetes puts on REST resource paths, such as API\\nGroups and Namespaces. (See the API Overview .)You need to have specific REST paths to\\nbe compatible with an already defined\\nREST API.\\nYour resources are naturally scoped to a cluster or\\nnamespaces of a cluster.Cluster or namespace scoped resources\\nare a poor fit; you need control over the\\nspecifics of resource paths.\\nYou want to reuse Kubernetes API support features .You don\\'t need those features.\\nDeclarative APIs\\nIn a Declarative API, typically:\\nYour API consists of a relatively small number of relatively small objects (resources).\\nThe objects define configuration of applications or infrastructure.\\nThe objects are updated relatively infrequently.\\nHumans often need to read and write the objects.\\nThe main operations on the objects are CRUD-y (creating, reading, updating and\\ndeleting).\\nTransactions across objects are not required: the API represents a desired state, not an\\nexact state.\\nImperative APIs are not declarative. Signs that your API might not be declarative include:\\nThe client says \"do this\", and then gets a synchronous response back when it is done.\\nThe client says \"do this\", and then gets an operation ID back, and has to check a separate\\nOperation object to determine completion of the request.\\nYou talk about Remote Procedure Calls (RPCs).\\nDirectly storing large amounts of data; for example, > a few kB per object, or > 1000s of\\nobjects.\\nHigh bandwidth access (10s of requests per second sustained) needed.\\nStore end-user data (such as images, PII, etc.) or other large-scale data processed by\\napplications.\\nThe natural operations on the objects are not CRUD-y.\\nThe API is not easily modeled as objects.\\nYou chose to represent pending operations with an operation ID or an operation object.\\nShould I use a ConfigMap or a custom resource?\\nUse a ConfigMap if any of the following apply:\\nThere is an existing, well-documented configuration file format, such as a mysql.cnf  or \\npom.xml .• \\n• \\n• \\n• \\n• \\n• \\n• \\n• \\n• \\n• \\n• \\n• \\n• \\n• \\n• \\n•', metadata={'source': './PDFS/Concepts.pdf', 'page': 613}),\n",
       " Document(page_content='You want to put the entire configuration into one key of a ConfigMap.\\nThe main use of the configuration file is for a program running in a Pod on your cluster\\nto consume the file to configure itself.\\nConsumers of the file prefer to consume via file in a Pod or environment variable in a\\npod, rather than the Kubernetes API.\\nYou want to perform rolling updates via Deployment, etc., when the file is updated.\\nNote:  Use a Secret  for sensitive data, which is similar to a ConfigMap but more secure.\\nUse a custom resource (CRD or Aggregated API) if most of the following apply:\\nYou want to use Kubernetes client libraries and CLIs to create and update the new\\nresource.\\nYou want top-level support from kubectl ; for example, kubectl get my-object object-name .\\nYou want to build new automation that watches for updates on the new object, and then\\nCRUD other objects, or vice versa.\\nYou want to write automation that handles updates to the object.\\nYou want to use Kubernetes API conventions like .spec , .status , and .metadata .\\nYou want the object to be an abstraction over a collection of controlled resources, or a\\nsummarization of other resources.\\nAdding custom resources\\nKubernetes provides two ways to add custom resources to your cluster:\\nCRDs are simple and can be created without any programming.\\nAPI Aggregation  requires programming, but allows more control over API behaviors like\\nhow data is stored and conversion between API versions.\\nKubernetes provides these two options to meet the needs of different users, so that neither ease\\nof use nor flexibility is compromised.\\nAggregated APIs are subordinate API servers that sit behind the primary API server, which acts\\nas a proxy. This arrangement is called API Aggregation (AA). To users, the Kubernetes API\\nappears extended.\\nCRDs allow users to create new types of resources without adding another API server. You do\\nnot need to understand API Aggregation to use CRDs.\\nRegardless of how they are installed, the new resources are referred to as Custom Resources to\\ndistinguish them from built-in Kubernetes resources (like pods).\\nNote:\\nAvoid using a Custom Resource as data storage for application, end user, or monitoring data:\\narchitecture designs that store application data within the Kubernetes API typically represent a\\ndesign that is too closely coupled.\\nArchitecturally, cloud native  application architectures favor loose coupling between\\ncomponents. If part of your workload requires a backing service for its routine operation, run\\nthat backing service as a component or consume it as an external service. This way, your\\nworkload does not rely on the Kubernetes API for its normal operation.• \\n• \\n• \\n• \\n• \\n• \\n• \\n• \\n• \\n• \\n• \\n•', metadata={'source': './PDFS/Concepts.pdf', 'page': 614}),\n",
       " Document(page_content='CustomResourceDefinitions\\nThe CustomResourceDefinition  API resource allows you to define custom resources. Defining a\\nCRD object creates a new custom resource with a name and schema that you specify. The\\nKubernetes API serves and handles the storage of your custom resource. The name of a CRD\\nobject must be a valid DNS subdomain name .\\nThis frees you from writing your own API server to handle the custom resource, but the generic\\nnature of the implementation means you have less flexibility than with API server aggregation .\\nRefer to the custom controller example  for an example of how to register a new custom\\nresource, work with instances of your new resource type, and use a controller to handle events.\\nAPI server aggregation\\nUsually, each resource in the Kubernetes API requires code that handles REST requests and\\nmanages persistent storage of objects. The main Kubernetes API server handles built-in\\nresources like pods and services , and can also generically handle custom resources through \\nCRDs .\\nThe aggregation layer  allows you to provide specialized implementations for your custom\\nresources by writing and deploying your own API server. The main API server delegates\\nrequests to your API server for the custom resources that you handle, making them available to\\nall of its clients.\\nChoosing a method for adding custom resources\\nCRDs are easier to use. Aggregated APIs are more flexible. Choose the method that best meets\\nyour needs.\\nTypically, CRDs are a good fit if:\\nYou have a handful of fields\\nYou are using the resource within your company, or as part of a small open-source\\nproject (as opposed to a commercial product)\\nComparing ease of use\\nCRDs are easier to create than Aggregated APIs.\\nCRDs Aggregated API\\nDo not require programming. Users can choose any\\nlanguage for a CRD controller.Requires programming and building\\nbinary and image.\\nNo additional service to run; CRDs are handled by\\nAPI server.An additional service to create and that\\ncould fail.\\nNo ongoing support once the CRD is created. Any\\nbug fixes are picked up as part of normal\\nKubernetes Master upgrades.May need to periodically pickup bug fixes\\nfrom upstream and rebuild and update the\\nAggregated API server.• \\n•', metadata={'source': './PDFS/Concepts.pdf', 'page': 615}),\n",
       " Document(page_content=\"CRDs Aggregated API\\nNo need to handle multiple versions of your API;\\nfor example, when you control the client for this\\nresource, you can upgrade it in sync with the API.You need to handle multiple versions of\\nyour API; for example, when developing\\nan extension to share with the world.\\nAdvanced features and flexibility\\nAggregated APIs offer more advanced API features and customization of other features; for\\nexample, the storage layer.\\nFeature Description CRDsAggregated\\nAPI\\nValidationHelp users prevent errors\\nand allow you to evolve\\nyour API independently of\\nyour clients. These features\\nare most useful when there\\nare many clients who can't\\nall update at the same time.Yes. Most validation can be specified in\\nthe CRD using OpenAPI v3.0\\nvalidation . CRDValidationRatcheting\\nfeature gate allows failing validations\\nspecified using OpenAPI also can be\\nignored if the failing part of the\\nresource was unchanged. Any other\\nvalidations supported by addition of a \\nValidating Webhook .Yes,\\narbitrary\\nvalidation\\nchecks\\nDefaulting See aboveYes, either via OpenAPI v3.0 validation  \\ndefault  keyword (GA in 1.17), or via a \\nMutating Webhook  (though this will\\nnot be run when reading from etcd for\\nold objects).Yes\\nMulti-\\nversioningAllows serving the same\\nobject through two API\\nversions. Can help ease API\\nchanges like renaming\\nfields. Less important if you\\ncontrol your client versions.Yes Yes\\nCustom\\nStorageIf you need storage with a\\ndifferent performance mode\\n(for example, a time-series\\ndatabase instead of key-\\nvalue store) or isolation for\\nsecurity (for example,\\nencryption of sensitive\\ninformation, etc.)No Yes\\nCustom\\nBusiness\\nLogicPerform arbitrary checks or\\nactions when creating,\\nreading, updating or\\ndeleting an objectYes, using Webhooks . Yes\\nScale\\nSubresourceAllows systems like\\nHorizontalPodAutoscaler\\nand PodDisruptionBudget\\ninteract with your new\\nresourceYes Yes\\nStatus\\nSubresourceAllows fine-grained access\\ncontrol where user writes Yes Yes\", metadata={'source': './PDFS/Concepts.pdf', 'page': 616}),\n",
       " Document(page_content='Feature Description CRDsAggregated\\nAPI\\nthe spec section and the\\ncontroller writes the status\\nsection. Allows\\nincrementing object\\nGeneration on custom\\nresource data mutation\\n(requires separate spec and\\nstatus sections in the\\nresource)\\nOther\\nSubresourcesAdd operations other than\\nCRUD, such as \"logs\" or\\n\"exec\".No Yes\\nstrategic-\\nmerge-patchThe new endpoints support\\nPATCH with \\nContent-Type: application/\\nstrategic-merge-patch+json .\\nUseful for updating objects\\nthat may be modified both\\nlocally, and by the server.\\nFor more information, see \\n\"Update API Objects in\\nPlace Using kubectl patch\"No Yes\\nProtocol\\nBuffersThe new resource supports\\nclients that want to use\\nProtocol BuffersNo Yes\\nOpenAPI\\nSchemaIs there an OpenAPI\\n(swagger) schema for the\\ntypes that can be\\ndynamically fetched from\\nthe server? Is the user\\nprotected from misspelling\\nfield names by ensuring\\nonly allowed fields are set?\\nAre types enforced (in\\nother words, don\\'t put an \\nint in a string  field?)Yes, based on the OpenAPI v3.0\\nvalidation  schema (GA in 1.16).Yes\\nCommon Features\\nWhen you create a custom resource, either via a CRD or an AA, you get many features for your\\nAPI, compared to implementing it outside the Kubernetes platform:\\nFeature What it does\\nCRUDThe new endpoints support CRUD basic operations via HTTP and \\nkubectl\\nWatch The new endpoints support Kubernetes Watch operations via HTTP\\nDiscoveryClients like kubectl  and dashboard automatically offer list, display, and\\nfield edit operations on your resources\\njson-patch', metadata={'source': './PDFS/Concepts.pdf', 'page': 617}),\n",
       " Document(page_content=\"Feature What it does\\nThe new endpoints support PATCH with Content-Type: application/\\njson-patch+json\\nmerge-patchThe new endpoints support PATCH with Content-Type: application/\\nmerge-patch+json\\nHTTPS The new endpoints uses HTTPS\\nBuilt-in\\nAuthenticationAccess to the extension uses the core API server (aggregation layer) for\\nauthentication\\nBuilt-in AuthorizationAccess to the extension can reuse the authorization used by the core\\nAPI server; for example, RBAC.\\nFinalizers Block deletion of extension resources until external cleanup happens.\\nAdmission WebhooksSet default values and validate extension resources during any create/\\nupdate/delete operation.\\nUI/CLI Display Kubectl, dashboard can display extension resources.\\nUnset versus Empty Clients can distinguish unset fields from zero-valued fields.\\nClient Libraries\\nGenerationKubernetes provides generic client libraries, as well as tools to generate\\ntype-specific client libraries.\\nLabels and\\nannotationsCommon metadata across objects that tools know how to edit for core\\nand custom resources.\\nPreparing to install a custom resource\\nThere are several points to be aware of before adding a custom resource to your cluster.\\nThird party code and new points of failure\\nWhile creating a CRD does not automatically add any new points of failure (for example, by\\ncausing third party code to run on your API server), packages (for example, Charts) or other\\ninstallation bundles often include CRDs as well as a Deployment of third-party code that\\nimplements the business logic for a new custom resource.\\nInstalling an Aggregated API server always involves running a new Deployment.\\nStorage\\nCustom resources consume storage space in the same way that ConfigMaps do. Creating too\\nmany custom resources may overload your API server's storage space.\\nAggregated API servers may use the same storage as the main API server, in which case the\\nsame warning applies.\\nAuthentication, authorization, and auditing\\nCRDs always use the same authentication, authorization, and audit logging as the built-in\\nresources of your API server.\\nIf you use RBAC for authorization, most RBAC roles will not grant access to the new resources\\n(except the cluster-admin role or any role created with wildcard rules). You'll need to explicitly\\ngrant access to the new resources. CRDs and Aggregated APIs often come bundled with new\\nrole definitions for the types they add.\", metadata={'source': './PDFS/Concepts.pdf', 'page': 618}),\n",
       " Document(page_content='Aggregated API servers may or may not use the same authentication, authorization, and\\nauditing as the primary API server.\\nAccessing a custom resource\\nKubernetes client libraries  can be used to access custom resources. Not all client libraries\\nsupport custom resources. The Go and Python  client libraries do.\\nWhen you add a custom resource, you can access it using:\\nkubectl\\nThe Kubernetes dynamic client.\\nA REST client that you write.\\nA client generated using Kubernetes client generation tools  (generating one is an\\nadvanced undertaking, but some projects may provide a client along with the CRD or\\nAA).\\nWhat\\'s next\\nLearn how to Extend the Kubernetes API with the aggregation layer .\\nLearn how to Extend the Kubernetes API with CustomResourceDefinition .\\nKubernetes API Aggregation Layer\\nThe aggregation layer allows Kubernetes to be extended with additional APIs, beyond what is\\noffered by the core Kubernetes APIs. The additional APIs can either be ready-made solutions\\nsuch as a metrics server , or APIs that you develop yourself.\\nThe aggregation layer is different from Custom Resources , which are a way to make the kube-\\napiserver  recognise new kinds of object.\\nAggregation layer\\nThe aggregation layer runs in-process with the kube-apiserver. Until an extension resource is\\nregistered, the aggregation layer will do nothing. To register an API, you add an APIService\\nobject, which \"claims\" the URL path in the Kubernetes API. At that point, the aggregation layer\\nwill proxy anything sent to that API path (e.g. /apis/myextension.mycompany.io/v1/... ) to the\\nregistered APIService.\\nThe most common way to implement the APIService is to run an extension API server  in Pod(s)\\nthat run in your cluster. If you\\'re using the extension API server to manage resources in your\\ncluster, the extension API server (also written as \"extension-apiserver\") is typically paired with\\none or more controllers . The apiserver-builder library provides a skeleton for both extension\\nAPI servers and the associated controller(s).\\nResponse latency\\nExtension API servers should have low latency networking to and from the kube-apiserver.\\nDiscovery requests are required to round-trip from the kube-apiserver in five seconds or less.• \\n• \\n• \\n• \\n• \\n•', metadata={'source': './PDFS/Concepts.pdf', 'page': 619}),\n",
       " Document(page_content=\"If your extension API server cannot achieve that latency requirement, consider making changes\\nthat let you meet it.\\nWhat's next\\nTo get the aggregator working in your environment, configure the aggregation layer .\\nThen, setup an extension api-server  to work with the aggregation layer.\\nRead about APIService  in the API reference\\nAlternatively: learn how to extend the Kubernetes API using Custom Resource Definitions .\\nOperator pattern\\nOperators are software extensions to Kubernetes that make use of custom resources  to manage\\napplications and their components. Operators follow Kubernetes principles, notably the control\\nloop.\\nMotivation\\nThe operator pattern  aims to capture the key aim of a human operator who is managing a\\nservice or set of services. Human operators who look after specific applications and services\\nhave deep knowledge of how the system ought to behave, how to deploy it, and how to react if\\nthere are problems.\\nPeople who run workloads on Kubernetes often like to use automation to take care of\\nrepeatable tasks. The operator pattern captures how you can write code to automate a task\\nbeyond what Kubernetes itself provides.\\nOperators in Kubernetes\\nKubernetes is designed for automation. Out of the box, you get lots of built-in automation from\\nthe core of Kubernetes. You can use Kubernetes to automate deploying and running workloads, \\nand you can automate how Kubernetes does that.\\nKubernetes' operator pattern  concept lets you extend the cluster's behaviour without modifying\\nthe code of Kubernetes itself by linking controllers  to one or more custom resources. Operators\\nare clients of the Kubernetes API that act as controllers for a Custom Resource .\\nAn example operator\\nSome of the things that you can use an operator to automate include:\\ndeploying an application on demand\\ntaking and restoring backups of that application's state\\nhandling upgrades of the application code alongside related changes such as database\\nschemas or extra configuration settings\\npublishing a Service to applications that don't support Kubernetes APIs to discover them\\nsimulating failure in all or part of your cluster to test its resilience• \\n• \\n• \\n• \\n• \\n• \\n• \\n•\", metadata={'source': './PDFS/Concepts.pdf', 'page': 620}),\n",
       " Document(page_content=\"choosing a leader for a distributed application without an internal member election\\nprocess\\nWhat might an operator look like in more detail? Here's an example:\\nA custom resource named SampleDB, that you can configure into the cluster.\\nA Deployment that makes sure a Pod is running that contains the controller part of the\\noperator.\\nA container image of the operator code.\\nController code that queries the control plane to find out what SampleDB resources are\\nconfigured.\\nThe core of the operator is code to tell the API server how to make reality match the\\nconfigured resources.\\nIf you add a new SampleDB, the operator sets up PersistentVolumeClaims to\\nprovide durable database storage, a StatefulSet to run SampleDB and a Job to\\nhandle initial configuration.\\nIf you delete it, the operator takes a snapshot, then makes sure that the StatefulSet\\nand Volumes are also removed.\\nThe operator also manages regular database backups. For each SampleDB resource, the\\noperator determines when to create a Pod that can connect to the database and take\\nbackups. These Pods would rely on a ConfigMap and / or a Secret that has database\\nconnection details and credentials.\\nBecause the operator aims to provide robust automation for the resource it manages,\\nthere would be additional supporting code. For this example, code checks to see if the\\ndatabase is running an old version and, if so, creates Job objects that upgrade it for you.\\nDeploying operators\\nThe most common way to deploy an operator is to add the Custom Resource Definition and its\\nassociated Controller to your cluster. The Controller will normally run outside of the control\\nplane , much as you would run any containerized application. For example, you can run the\\ncontroller in your cluster as a Deployment.\\nUsing an operator\\nOnce you have an operator deployed, you'd use it by adding, modifying or deleting the kind of\\nresource that the operator uses. Following the above example, you would set up a Deployment\\nfor the operator itself, and then:\\nkubectl get SampleDB                   # find configured databases\\nkubectl edit SampleDB/example-database # manually change some settings\\n...and that's it! The operator will take care of applying the changes as well as keeping the\\nexisting service in good shape.\\nWriting your own operator\\nIf there isn't an operator in the ecosystem that implements the behavior you want, you can\\ncode your own.• \\n1. \\n2. \\n3. \\n4. \\n5. \\n◦ \\n◦ \\n6. \\n7.\", metadata={'source': './PDFS/Concepts.pdf', 'page': 621}),\n",
       " Document(page_content=\"You also implement an operator (that is, a Controller) using any language / runtime that can act\\nas a client for the Kubernetes API .\\nFollowing are a few libraries and tools you can use to write your own cloud native operator.\\nNote:  This section links to third party projects that provide functionality required by\\nKubernetes. The Kubernetes project authors aren't responsible for these projects, which are\\nlisted alphabetically. To add a project to this list, read the content guide  before submitting a\\nchange. More information.\\nCharmed Operator Framework\\nJava Operator SDK\\nKopf  (Kubernetes Operator Pythonic Framework)\\nkube-rs  (Rust)\\nkubebuilder\\nKubeOps  (.NET operator SDK)\\nKUDO  (Kubernetes Universal Declarative Operator)\\nMast\\nMetacontroller  along with WebHooks that you implement yourself\\nOperator Framework\\nshell-operator\\nWhat's next\\nRead the CNCF  Operator White Paper .\\nLearn more about Custom Resources\\nFind ready-made operators on OperatorHub.io  to suit your use case\\nPublish  your operator for other people to use\\nRead CoreOS' original article  that introduced the operator pattern (this is an archived\\nversion of the original article).\\nRead an article  from Google Cloud about best practices for building operators• \\n• \\n• \\n• \\n• \\n• \\n• \\n• \\n• \\n• \\n• \\n• \\n• \\n• \\n• \\n• \\n•\", metadata={'source': './PDFS/Concepts.pdf', 'page': 622})]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pages"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PDF내용 분리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "text_spliter = CharacterTextSplitter(\n",
    "  separator=\"\\n\",\n",
    "  chunk_size=1000,\n",
    "  chunk_overlap=100,\n",
    "  length_function=len,\n",
    ")\n",
    "\n",
    "texts = text_spliter.split_documents(pages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(page_content='The Concepts section helps you learn about the parts of the Kubernetes system and the\\nabstractions Kubernetes uses to represent your cluster , and helps you obtain a deeper\\nunderstanding of how Kubernetes works.\\nOverview\\nKubernetes is a portable, extensible, open source platform for managing containerized\\nworkloads and services, that facilitates both declarative configuration and automation. It has a\\nlarge, rapidly growing ecosystem. Kubernetes services, support, and tools are widely available.\\nCluster Architecture\\nThe architectural concepts behind Kubernetes.\\nContainers\\nTechnology for packaging an application along with its runtime dependencies.\\nWorkloads\\nUnderstand Pods, the smallest deployable compute object in Kubernetes, and the higher-level\\nabstractions that help you to run them.\\nServices, Load Balancing, and Networking\\nConcepts and resources behind networking in Kubernetes.\\nStorage\\nWays to provide both long-term and temporary storage to Pods in your cluster.\\nConfiguration', metadata={'source': './PDFS/Concepts.pdf', 'page': 0}),\n",
       " Document(page_content='Storage\\nWays to provide both long-term and temporary storage to Pods in your cluster.\\nConfiguration\\nResources that Kubernetes provides for configuring Pods.\\nSecurity\\nConcepts for keeping your cloud-native workload secure.\\nPolicies\\nManage security and best-practices with policies.\\nScheduling, Preemption and Eviction\\nIn Kubernetes, scheduling refers to making sure that Pods are matched to Nodes so that the\\nkubelet can run them. Preemption is the process of terminating Pods with lower Priority so that', metadata={'source': './PDFS/Concepts.pdf', 'page': 0}),\n",
       " Document(page_content='Pods with higher Priority can schedule on Nodes. Eviction is the process of proactively\\nterminating one or more Pods on resource-starved Nodes.\\nCluster Administration\\nLower-level detail relevant to creating or administering a Kubernetes cluster.\\nWindows in Kubernetes\\nKubernetes supports nodes that run Microsoft Windows.\\nExtending Kubernetes\\nDifferent ways to change the behavior of your Kubernetes cluster.\\nOverview\\nKubernetes is a portable, extensible, open source platform for managing containerized\\nworkloads and services, that facilitates both declarative configuration and automation. It has a\\nlarge, rapidly growing ecosystem. Kubernetes services, support, and tools are widely available.\\nThis page is an overview of Kubernetes.\\nKubernetes is a portable, extensible, open source platform for managing containerized\\nworkloads and services, that facilitates both declarative configuration and automation. It has a', metadata={'source': './PDFS/Concepts.pdf', 'page': 1}),\n",
       " Document(page_content='workloads and services, that facilitates both declarative configuration and automation. It has a\\nlarge, rapidly growing ecosystem. Kubernetes services, support, and tools are widely available.\\nThe name Kubernetes originates from Greek, meaning helmsman or pilot. K8s as an\\nabbreviation results from counting the eight letters between the \"K\" and the \"s\". Google open-\\nsourced the Kubernetes project in 2014. Kubernetes combines over 15 years of Google\\'s\\nexperience  running production workloads at scale with best-of-breed ideas and practices from\\nthe community.\\nGoing back in time\\nLet\\'s take a look at why Kubernetes is so useful by going back in time.\\nDeployment evolution\\nTraditional deployment era:  Early on, organizations ran applications on physical servers.\\nThere was no way to define resource boundaries for applications in a physical server, and this\\ncaused resource allocation issues. For example, if multiple applications run on a physical server,', metadata={'source': './PDFS/Concepts.pdf', 'page': 1}),\n",
       " Document(page_content=\"caused resource allocation issues. For example, if multiple applications run on a physical server,\\nthere can be instances where one application would take up most of the resources, and as a\\nresult, the other applications would underperform. A solution for this would be to run each\\napplication on a different physical server. But this did not scale as resources were underutilized,\\nand it was expensive for organizations to maintain many physical servers.\\nVirtualized deployment era:  As a solution, virtualization was introduced. It allows you to\\nrun multiple Virtual Machines (VMs) on a single physical server's CPU. Virtualization allows\\napplications to be isolated between VMs and provides a level of security as the information of\\none application cannot be freely accessed by another application.\", metadata={'source': './PDFS/Concepts.pdf', 'page': 1}),\n",
       " Document(page_content='Virtualization allows better utilization of resources in a physical server and allows better\\nscalability because an application can be added or updated easily, reduces hardware costs, and\\nmuch more. With virtualization you can present a set of physical resources as a cluster of\\ndisposable virtual machines.\\nEach VM is a full machine running all the components, including its own operating system, on\\ntop of the virtualized hardware.\\nContainer deployment era:  Containers are similar to VMs, but they have relaxed isolation\\nproperties to share the Operating System (OS) among the applications. Therefore, containers\\nare considered lightweight. Similar to a VM, a container has its own filesystem, share of CPU,\\nmemory, process space, and more. As they are decoupled from the underlying infrastructure,\\nthey are portable across clouds and OS distributions.\\nContainers have become popular because they provide extra benefits, such as:', metadata={'source': './PDFS/Concepts.pdf', 'page': 2}),\n",
       " Document(page_content='Containers have become popular because they provide extra benefits, such as:\\nAgile application creation and deployment: increased ease and efficiency of container\\nimage creation compared to VM image use.\\nContinuous development, integration, and deployment: provides for reliable and frequent\\ncontainer image build and deployment with quick and efficient rollbacks (due to image\\nimmutability).\\nDev and Ops separation of concerns: create application container images at build/release\\ntime rather than deployment time, thereby decoupling applications from infrastructure.\\nObservability: not only surfaces OS-level information and metrics, but also application\\nhealth and other signals.\\nEnvironmental consistency across development, testing, and production: runs the same\\non a laptop as it does in the cloud.\\nCloud and OS distribution portability: runs on Ubuntu, RHEL, CoreOS, on-premises, on\\nmajor public clouds, and anywhere else.', metadata={'source': './PDFS/Concepts.pdf', 'page': 2}),\n",
       " Document(page_content=\"major public clouds, and anywhere else.\\nApplication-centric management: raises the level of abstraction from running an OS on\\nvirtual hardware to running an application on an OS using logical resources.\\nLoosely coupled, distributed, elastic, liberated micro-services: applications are broken\\ninto smaller, independent pieces and can be deployed and managed dynamically – not a\\nmonolithic stack running on one big single-purpose machine.\\nResource isolation: predictable application performance.\\nResource utilization: high efficiency and density.\\nWhy you need Kubernetes and what it can do\\nContainers are a good way to bundle and run your applications. In a production environment,\\nyou need to manage the containers that run the applications and ensure that there is no\\ndowntime. For example, if a container goes down, another container needs to start. Wouldn't it\\nbe easier if this behavior was handled by a system?\", metadata={'source': './PDFS/Concepts.pdf', 'page': 2}),\n",
       " Document(page_content=\"be easier if this behavior was handled by a system?\\nThat's how Kubernetes comes to the rescue! Kubernetes provides you with a framework to run\\ndistributed systems resiliently. It takes care of scaling and failover for your application, provides\\ndeployment patterns, and more. For example: Kubernetes can easily manage a canary\\ndeployment for your system.• \\n• \\n• \\n• \\n• \\n• \\n• \\n• \\n• \\n•\", metadata={'source': './PDFS/Concepts.pdf', 'page': 2}),\n",
       " Document(page_content='Kubernetes provides you with:\\nService discovery and load balancing  Kubernetes can expose a container using the\\nDNS name or using their own IP address. If traffic to a container is high, Kubernetes is\\nable to load balance and distribute the network traffic so that the deployment is stable.\\nStorage orchestration  Kubernetes allows you to automatically mount a storage system\\nof your choice, such as local storages, public cloud providers, and more.\\nAutomated rollouts and rollbacks  You can describe the desired state for your deployed\\ncontainers using Kubernetes, and it can change the actual state to the desired state at a\\ncontrolled rate. For example, you can automate Kubernetes to create new containers for\\nyour deployment, remove existing containers and adopt all their resources to the new\\ncontainer.\\nAutomatic bin packing  You provide Kubernetes with a cluster of nodes that it can use\\nto run containerized tasks. You tell Kubernetes how much CPU and memory (RAM) each', metadata={'source': './PDFS/Concepts.pdf', 'page': 3}),\n",
       " Document(page_content=\"to run containerized tasks. You tell Kubernetes how much CPU and memory (RAM) each\\ncontainer needs. Kubernetes can fit containers onto your nodes to make the best use of\\nyour resources.\\nSelf-healing  Kubernetes restarts containers that fail, replaces containers, kills containers\\nthat don't respond to your user-defined health check, and doesn't advertise them to\\nclients until they are ready to serve.\\nSecret and configuration management  Kubernetes lets you store and manage\\nsensitive information, such as passwords, OAuth tokens, and SSH keys. You can deploy\\nand update secrets and application configuration without rebuilding your container\\nimages, and without exposing secrets in your stack configuration.\\nBatch execution  In addition to services, Kubernetes can manage your batch and CI\\nworkloads, replacing containers that fail, if desired.\\nHorizontal scaling  Scale your application up and down with a simple command, with a\\nUI, or automatically based on CPU usage.\", metadata={'source': './PDFS/Concepts.pdf', 'page': 3}),\n",
       " Document(page_content='UI, or automatically based on CPU usage.\\nIPv4/IPv6 dual-stack  Allocation of IPv4 and IPv6 addresses to Pods and Services\\nDesigned for extensibility  Add features to your Kubernetes cluster without changing\\nupstream source code.\\nWhat Kubernetes is not\\nKubernetes is not a traditional, all-inclusive PaaS (Platform as a Service) system. Since\\nKubernetes operates at the container level rather than at the hardware level, it provides some\\ngenerally applicable features common to PaaS offerings, such as deployment, scaling, load\\nbalancing, and lets users integrate their logging, monitoring, and alerting solutions. However,\\nKubernetes is not monolithic, and these default solutions are optional and pluggable.\\nKubernetes provides the building blocks for building developer platforms, but preserves user\\nchoice and flexibility where it is important.\\nKubernetes:\\nDoes not limit the types of applications supported. Kubernetes aims to support an', metadata={'source': './PDFS/Concepts.pdf', 'page': 3}),\n",
       " Document(page_content='Kubernetes:\\nDoes not limit the types of applications supported. Kubernetes aims to support an\\nextremely diverse variety of workloads, including stateless, stateful, and data-processing\\nworkloads. If an application can run in a container, it should run great on Kubernetes.\\nDoes not deploy source code and does not build your application. Continuous Integration,\\nDelivery, and Deployment (CI/CD) workflows are determined by organization cultures\\nand preferences as well as technical requirements.\\nDoes not provide application-level services, such as middleware (for example, message\\nbuses), data-processing frameworks (for example, Spark), databases (for example,\\nMySQL), caches, nor cluster storage systems (for example, Ceph) as built-in services.• \\n• \\n• \\n• \\n• \\n• \\n• \\n• \\n• \\n• \\n• \\n• \\n•', metadata={'source': './PDFS/Concepts.pdf', 'page': 3}),\n",
       " Document(page_content='Such components can run on Kubernetes, and/or can be accessed by applications running\\non Kubernetes through portable mechanisms, such as the Open Service Broker .\\nDoes not dictate logging, monitoring, or alerting solutions. It provides some integrations\\nas proof of concept, and mechanisms to collect and export metrics.\\nDoes not provide nor mandate a configuration language/system (for example, Jsonnet). It\\nprovides a declarative API that may be targeted by arbitrary forms of declarative\\nspecifications.\\nDoes not provide nor adopt any comprehensive machine configuration, maintenance,\\nmanagement, or self-healing systems.\\nAdditionally, Kubernetes is not a mere orchestration system. In fact, it eliminates the\\nneed for orchestration. The technical definition of orchestration is execution of a defined\\nworkflow: first do A, then B, then C. In contrast, Kubernetes comprises a set of\\nindependent, composable control processes that continuously drive the current state', metadata={'source': './PDFS/Concepts.pdf', 'page': 4}),\n",
       " Document(page_content=\"independent, composable control processes that continuously drive the current state\\ntowards the provided desired state. It shouldn't matter how you get from A to C.\\nCentralized control is also not required. This results in a system that is easier to use and\\nmore powerful, robust, resilient, and extensible.\\nWhat's next\\nTake a look at the Kubernetes Components\\nTake a look at the The Kubernetes API\\nTake a look at the Cluster Architecture\\nReady to Get Started ?\\nObjects In Kubernetes\\nKubernetes objects are persistent entities in the Kubernetes system. Kubernetes uses these\\nentities to represent the state of your cluster. Learn about the Kubernetes object model and how\\nto work with these objects.\\nThis page explains how Kubernetes objects are represented in the Kubernetes API, and how you\\ncan express them in .yaml  format.\\nUnderstanding Kubernetes objects\\nKubernetes objects  are persistent entities in the Kubernetes system. Kubernetes uses these\", metadata={'source': './PDFS/Concepts.pdf', 'page': 4}),\n",
       " Document(page_content='Kubernetes objects  are persistent entities in the Kubernetes system. Kubernetes uses these\\nentities to represent the state of your cluster. Specifically, they can describe:\\nWhat containerized applications are running (and on which nodes)\\nThe resources available to those applications\\nThe policies around how those applications behave, such as restart policies, upgrades, and\\nfault-tolerance\\nA Kubernetes object is a \"record of intent\"--once you create the object, the Kubernetes system\\nwill constantly work to ensure that object exists. By creating an object, you\\'re effectively telling\\nthe Kubernetes system what you want your cluster\\'s workload to look like; this is your cluster\\'s\\ndesired state .\\nTo work with Kubernetes objects—whether to create, modify, or delete them—you\\'ll need to use\\nthe Kubernetes API . When you use the kubectl  command-line interface, for example, the CLI• \\n• \\n• \\n• \\n• \\n• \\n• \\n• \\n• \\n• \\n•', metadata={'source': './PDFS/Concepts.pdf', 'page': 4}),\n",
       " Document(page_content=\"makes the necessary Kubernetes API calls for you. You can also use the Kubernetes API directly\\nin your own programs using one of the Client Libraries .\\nObject spec and status\\nAlmost every Kubernetes object includes two nested object fields that govern the object's\\nconfiguration: the object spec and the object status . For objects that have a spec, you have to set\\nthis when you create the object, providing a description of the characteristics you want the\\nresource to have: its desired state .\\nThe status  describes the current state  of the object, supplied and updated by the Kubernetes\\nsystem and its components. The Kubernetes control plane  continually and actively manages\\nevery object's actual state to match the desired state you supplied.\\nFor example: in Kubernetes, a Deployment is an object that can represent an application\\nrunning on your cluster. When you create the Deployment, you might set the Deployment spec\", metadata={'source': './PDFS/Concepts.pdf', 'page': 5}),\n",
       " Document(page_content='running on your cluster. When you create the Deployment, you might set the Deployment spec\\nto specify that you want three replicas of the application to be running. The Kubernetes system\\nreads the Deployment spec and starts three instances of your desired application--updating the\\nstatus to match your spec. If any of those instances should fail (a status change), the Kubernetes\\nsystem responds to the difference between spec and status by making a correction--in this case,\\nstarting a replacement instance.\\nFor more information on the object spec, status, and metadata, see the Kubernetes API\\nConventions .\\nDescribing a Kubernetes object\\nWhen you create an object in Kubernetes, you must provide the object spec that describes its\\ndesired state, as well as some basic information about the object (such as a name). When you\\nuse the Kubernetes API to create the object (either directly or via kubectl ), that API request', metadata={'source': './PDFS/Concepts.pdf', 'page': 5}),\n",
       " Document(page_content=\"use the Kubernetes API to create the object (either directly or via kubectl ), that API request\\nmust include that information as JSON in the request body. Most often, you provide the\\ninformation to kubectl  in file known as a manifest . By convention, manifests are YAML (you\\ncould also use JSON format). Tools such as kubectl  convert the information from a manifest into\\nJSON or another supported serialization format when making the API request over HTTP.\\nHere's an example manifest that shows the required fields and object spec for a Kubernetes\\nDeployment:\\napplication/deployment.yaml  \\napiVersion : apps/v1\\nkind: Deployment\\nmetadata :\\n  name : nginx-deployment\\nspec:\\n  selector :\\n    matchLabels :\\n      app: nginx\\n  replicas : 2 # tells deployment to run 2 pods matching the template\\n  template :\\n    metadata :\\n      labels :\\n        app: nginx\", metadata={'source': './PDFS/Concepts.pdf', 'page': 5}),\n",
       " Document(page_content=\"spec:\\n      containers :\\n      - name : nginx\\n        image : nginx:1.14.2\\n        ports :\\n        - containerPort : 80\\nOne way to create a Deployment using a manifest file like the one above is to use the kubectl \\napply  command in the kubectl  command-line interface, passing the .yaml  file as an argument.\\nHere's an example:\\nkubectl apply -f https://k8s.io/examples/application/deployment.yaml\\nThe output is similar to this:\\ndeployment.apps/nginx-deployment created\\nRequired fields\\nIn the manifest (YAML or JSON file) for the Kubernetes object you want to create, you'll need to\\nset values for the following fields:\\napiVersion  - Which version of the Kubernetes API you're using to create this object\\nkind - What kind of object you want to create\\nmetadata  - Data that helps uniquely identify the object, including a name  string, UID, and\\noptional namespace\\nspec - What state you desire for the object\\nThe precise format of the object spec is different for every Kubernetes object, and contains\", metadata={'source': './PDFS/Concepts.pdf', 'page': 6}),\n",
       " Document(page_content='The precise format of the object spec is different for every Kubernetes object, and contains\\nnested fields specific to that object. The Kubernetes API Reference  can help you find the spec\\nformat for all of the objects you can create using Kubernetes.\\nFor example, see the spec field  for the Pod API reference. For each Pod, the .spec  field specifies\\nthe pod and its desired state (such as the container image name for each container within that\\npod). Another example of an object specification is the spec field  for the StatefulSet API. For\\nStatefulSet, the .spec  field specifies the StatefulSet and its desired state. Within the .spec  of a\\nStatefulSet is a template  for Pod objects. That template describes Pods that the StatefulSet\\ncontroller will create in order to satisfy the StatefulSet specification. Different kinds of object\\ncan also have different .status ; again, the API reference pages detail the structure of that .status\\nfield, and its content for each different type of object.', metadata={'source': './PDFS/Concepts.pdf', 'page': 6}),\n",
       " Document(page_content='field, and its content for each different type of object.\\nNote:  See Configuration Best Practices  for additional information on writing YAML\\nconfiguration files.\\nServer side field validation\\nStarting with Kubernetes v1.25, the API server offers server side field validation  that detects\\nunrecognized or duplicate fields in an object. It provides all the functionality of kubectl --\\nvalidate  on the server side.\\nThe kubectl  tool uses the --validate  flag to set the level of field validation. It accepts the values \\nignore , warn , and strict  while also accepting the values true (equivalent to strict ) and false\\n(equivalent to ignore ). The default validation setting for kubectl  is --validate=true .• \\n• \\n• \\n•', metadata={'source': './PDFS/Concepts.pdf', 'page': 6}),\n",
       " Document(page_content=\"Strict\\nStrict field validation, errors on validation failure\\nWarn\\nField validation is performed, but errors are exposed as warnings rather than failing the\\nrequest\\nIgnore\\nNo server side field validation is performed\\nWhen kubectl  cannot connect to an API server that supports field validation it will fall back to\\nusing client-side validation. Kubernetes 1.27 and later versions always offer field validation;\\nolder Kubernetes releases might not. If your cluster is older than v1.27, check the\\ndocumentation for your version of Kubernetes.\\nWhat's next\\nIf you're new to Kubernetes, read more about the following:\\nPods  which are the most important basic Kubernetes objects.\\nDeployment  objects.\\nControllers  in Kubernetes.\\nkubectl  and kubectl commands .\\nKubernetes Object Management  explains how to use kubectl  to manage objects. You might need\\nto install kubectl  if you don't already have it available.\\nTo learn about the Kubernetes API in general, visit:\\nKubernetes API overview\", metadata={'source': './PDFS/Concepts.pdf', 'page': 7}),\n",
       " Document(page_content='To learn about the Kubernetes API in general, visit:\\nKubernetes API overview\\nTo learn about objects in Kubernetes in more depth, read other pages in this section:\\nKubernetes Object Management\\nObject Names and IDs\\nLabels and Selectors\\nNamespaces\\nAnnotations\\nField Selectors\\nFinalizers\\nOwners and Dependents\\nRecommended Labels\\nKubernetes Object Management\\nThe kubectl  command-line tool supports several different ways to create and manage\\nKubernetes objects . This document provides an overview of the different approaches. Read the \\nKubectl book  for details of managing objects by Kubectl.\\nManagement techniques\\nWarning:  A Kubernetes object should be managed using only one technique. Mixing and\\nmatching techniques for the same object results in undefined behavior.• \\n• \\n• \\n• \\n• \\n• \\n• \\n• \\n• \\n• \\n• \\n• \\n• \\n•', metadata={'source': './PDFS/Concepts.pdf', 'page': 7}),\n",
       " Document(page_content='Management\\ntechniqueOperates onRecommended\\nenvironmentSupported\\nwritersLearning\\ncurve\\nImperative commands Live objects Development projects 1+ Lowest\\nImperative object\\nconfigurationIndividual files Production projects 1 Moderate\\nDeclarative object\\nconfigurationDirectories of\\nfilesProduction projects 1+ Highest\\nImperative commands\\nWhen using imperative commands, a user operates directly on live objects in a cluster. The user\\nprovides operations to the kubectl  command as arguments or flags.\\nThis is the recommended way to get started or to run a one-off task in a cluster. Because this\\ntechnique operates directly on live objects, it provides no history of previous configurations.\\nExamples\\nRun an instance of the nginx container by creating a Deployment object:\\nkubectl create deployment nginx --image nginx\\nTrade-offs\\nAdvantages compared to object configuration:\\nCommands are expressed as a single action word.\\nCommands require only a single step to make changes to the cluster.', metadata={'source': './PDFS/Concepts.pdf', 'page': 8}),\n",
       " Document(page_content='Commands require only a single step to make changes to the cluster.\\nDisadvantages compared to object configuration:\\nCommands do not integrate with change review processes.\\nCommands do not provide an audit trail associated with changes.\\nCommands do not provide a source of records except for what is live.\\nCommands do not provide a template for creating new objects.\\nImperative object configuration\\nIn imperative object configuration, the kubectl command specifies the operation (create,\\nreplace, etc.), optional flags and at least one file name. The file specified must contain a full\\ndefinition of the object in YAML or JSON format.\\nSee the API reference  for more details on object definitions.\\nWarning:  The imperative replace  command replaces the existing spec with the newly provided\\none, dropping all changes to the object missing from the configuration file. This approach\\nshould not be used with resource types whose specs are updated independently of the', metadata={'source': './PDFS/Concepts.pdf', 'page': 8}),\n",
       " Document(page_content='should not be used with resource types whose specs are updated independently of the\\nconfiguration file. Services of type LoadBalancer , for example, have their externalIPs  field\\nupdated independently from the configuration by the cluster.• \\n• \\n• \\n• \\n• \\n•', metadata={'source': './PDFS/Concepts.pdf', 'page': 8}),\n",
       " Document(page_content='Examples\\nCreate the objects defined in a configuration file:\\nkubectl create -f nginx.yaml\\nDelete the objects defined in two configuration files:\\nkubectl delete -f nginx.yaml -f redis.yaml\\nUpdate the objects defined in a configuration file by overwriting the live configuration:\\nkubectl replace -f nginx.yaml\\nTrade-offs\\nAdvantages compared to imperative commands:\\nObject configuration can be stored in a source control system such as Git.\\nObject configuration can integrate with processes such as reviewing changes before push\\nand audit trails.\\nObject configuration provides a template for creating new objects.\\nDisadvantages compared to imperative commands:\\nObject configuration requires basic understanding of the object schema.\\nObject configuration requires the additional step of writing a YAML file.\\nAdvantages compared to declarative object configuration:\\nImperative object configuration behavior is simpler and easier to understand.', metadata={'source': './PDFS/Concepts.pdf', 'page': 9}),\n",
       " Document(page_content='Imperative object configuration behavior is simpler and easier to understand.\\nAs of Kubernetes version 1.5, imperative object configuration is more mature.\\nDisadvantages compared to declarative object configuration:\\nImperative object configuration works best on files, not directories.\\nUpdates to live objects must be reflected in configuration files, or they will be lost during\\nthe next replacement.\\nDeclarative object configuration\\nWhen using declarative object configuration, a user operates on object configuration files\\nstored locally, however the user does not define the operations to be taken on the files. Create,\\nupdate, and delete operations are automatically detected per-object by kubectl . This enables\\nworking on directories, where different operations might be needed for different objects.\\nNote:  Declarative object configuration retains changes made by other writers, even if the\\nchanges are not merged back to the object configuration file. This is possible by using the patch', metadata={'source': './PDFS/Concepts.pdf', 'page': 9}),\n",
       " Document(page_content='changes are not merged back to the object configuration file. This is possible by using the patch\\nAPI operation to write only observed differences, instead of using the replace  API operation to\\nreplace the entire object configuration.• \\n• \\n• \\n• \\n• \\n• \\n• \\n• \\n•', metadata={'source': './PDFS/Concepts.pdf', 'page': 9}),\n",
       " Document(page_content=\"Examples\\nProcess all object configuration files in the configs  directory, and create or patch the live\\nobjects. You can first diff to see what changes are going to be made, and then apply:\\nkubectl diff -f configs/\\nkubectl apply -f configs/\\nRecursively process directories:\\nkubectl diff -R -f configs/\\nkubectl apply -R -f configs/\\nTrade-offs\\nAdvantages compared to imperative object configuration:\\nChanges made directly to live objects are retained, even if they are not merged back into\\nthe configuration files.\\nDeclarative object configuration has better support for operating on directories and\\nautomatically detecting operation types (create, patch, delete) per-object.\\nDisadvantages compared to imperative object configuration:\\nDeclarative object configuration is harder to debug and understand results when they are\\nunexpected.\\nPartial updates using diffs create complex merge and patch operations.\\nWhat's next\\nManaging Kubernetes Objects Using Imperative Commands\", metadata={'source': './PDFS/Concepts.pdf', 'page': 10}),\n",
       " Document(page_content=\"What's next\\nManaging Kubernetes Objects Using Imperative Commands\\nImperative Management of Kubernetes Objects Using Configuration Files\\nDeclarative Management of Kubernetes Objects Using Configuration Files\\nDeclarative Management of Kubernetes Objects Using Kustomize\\nKubectl Command Reference\\nKubectl Book\\nKubernetes API Reference\\nObject Names and IDs\\nEach object  in your cluster has a Name  that is unique for that type of resource. Every\\nKubernetes object also has a UID that is unique across your whole cluster.\\nFor example, you can only have one Pod named myapp-1234  within the same namespace , but\\nyou can have one Pod and one Deployment that are each named myapp-1234 .\\nFor non-unique user-provided attributes, Kubernetes provides labels  and annotations .• \\n• \\n• \\n• \\n• \\n• \\n• \\n• \\n• \\n• \\n•\", metadata={'source': './PDFS/Concepts.pdf', 'page': 10}),\n",
       " Document(page_content='Names\\nA client-provided string that refers to an object in a resource URL, such as /api/v1/pods/some-\\nname .\\nOnly one object of a given kind can have a given name at a time. However, if you delete the\\nobject, you can make a new object with the same name.\\nNames must be unique across all API versions  of the same resource. API resources are\\ndistinguished by their API group, resource type, namespace (for namespaced\\nresources), and name. In other words, API version is irrelevant in this context.\\nNote:  In cases when objects represent a physical entity, like a Node representing a physical\\nhost, when the host is re-created under the same name without deleting and re-creating the\\nNode, Kubernetes treats the new host as the old one, which may lead to inconsistencies.\\nBelow are four types of commonly used name constraints for resources.\\nDNS Subdomain Names\\nMost resource types require a name that can be used as a DNS subdomain name as defined in \\nRFC 1123 . This means the name must:', metadata={'source': './PDFS/Concepts.pdf', 'page': 11}),\n",
       " Document(page_content=\"RFC 1123 . This means the name must:\\ncontain no more than 253 characters\\ncontain only lowercase alphanumeric characters, '-' or '.'\\nstart with an alphanumeric character\\nend with an alphanumeric character\\nRFC 1123 Label Names\\nSome resource types require their names to follow the DNS label standard as defined in RFC\\n1123. This means the name must:\\ncontain at most 63 characters\\ncontain only lowercase alphanumeric characters or '-'\\nstart with an alphanumeric character\\nend with an alphanumeric character\\nRFC 1035 Label Names\\nSome resource types require their names to follow the DNS label standard as defined in RFC\\n1035. This means the name must:\\ncontain at most 63 characters\\ncontain only lowercase alphanumeric characters or '-'\\nstart with an alphabetic character\\nend with an alphanumeric character\\nPath Segment Names\\nSome resource types require their names to be able to be safely encoded as a path segment. In\", metadata={'source': './PDFS/Concepts.pdf', 'page': 11}),\n",
       " Document(page_content='Some resource types require their names to be able to be safely encoded as a path segment. In\\nother words, the name may not be \".\" or \"..\" and the name may not contain \"/\" or \"%\".• \\n• \\n• \\n• \\n• \\n• \\n• \\n• \\n• \\n• \\n• \\n•', metadata={'source': './PDFS/Concepts.pdf', 'page': 11}),\n",
       " Document(page_content=\"Here's an example manifest for a Pod named nginx-demo .\\napiVersion : v1\\nkind: Pod\\nmetadata :\\n  name : nginx-demo\\nspec:\\n  containers :\\n  - name : nginx\\n    image : nginx:1.14.2\\n    ports :\\n    - containerPort : 80\\nNote:  Some resource types have additional restrictions on their names.\\nUIDs\\nA Kubernetes systems-generated string to uniquely identify objects.\\nEvery object created over the whole lifetime of a Kubernetes cluster has a distinct UID. It is\\nintended to distinguish between historical occurrences of similar entities.\\nKubernetes UIDs are universally unique identifiers (also known as UUIDs). UUIDs are\\nstandardized as ISO/IEC 9834-8 and as ITU-T X.667.\\nWhat's next\\nRead about labels  and annotations  in Kubernetes.\\nSee the Identifiers and Names in Kubernetes  design document.\\nLabels and Selectors\\nLabels  are key/value pairs that are attached to objects  such as Pods. Labels are intended to be\", metadata={'source': './PDFS/Concepts.pdf', 'page': 12}),\n",
       " Document(page_content='Labels  are key/value pairs that are attached to objects  such as Pods. Labels are intended to be\\nused to specify identifying attributes of objects that are meaningful and relevant to users, but\\ndo not directly imply semantics to the core system. Labels can be used to organize and to select\\nsubsets of objects. Labels can be attached to objects at creation time and subsequently added\\nand modified at any time. Each object can have a set of key/value labels defined. Each Key must\\nbe unique for a given object.\\n\"metadata\" : {\\n  \"labels\" : {\\n    \"key1\"  : \"value1\" ,\\n    \"key2\"  : \"value2\"\\n  }\\n}\\nLabels allow for efficient queries and watches and are ideal for use in UIs and CLIs. Non-\\nidentifying information should be recorded using annotations .• \\n•', metadata={'source': './PDFS/Concepts.pdf', 'page': 12}),\n",
       " Document(page_content='Motivation\\nLabels enable users to map their own organizational structures onto system objects in a loosely\\ncoupled fashion, without requiring clients to store these mappings.\\nService deployments and batch processing pipelines are often multi-dimensional entities (e.g.,\\nmultiple partitions or deployments, multiple release tracks, multiple tiers, multiple micro-\\nservices per tier). Management often requires cross-cutting operations, which breaks\\nencapsulation of strictly hierarchical representations, especially rigid hierarchies determined by\\nthe infrastructure rather than by users.\\nExample labels:\\n\"release\" : \"stable\" , \"release\" : \"canary\"\\n\"environment\" : \"dev\" , \"environment\" : \"qa\" , \"environment\" : \"production\"\\n\"tier\" : \"frontend\" , \"tier\" : \"backend\" , \"tier\" : \"cache\"\\n\"partition\" : \"customerA\" , \"partition\" : \"customerB\"\\n\"track\" : \"daily\" , \"track\" : \"weekly\"\\nThese are examples of commonly used labels ; you are free to develop your own conventions.', metadata={'source': './PDFS/Concepts.pdf', 'page': 13}),\n",
       " Document(page_content='These are examples of commonly used labels ; you are free to develop your own conventions.\\nKeep in mind that label Key must be unique for a given object.\\nSyntax and character set\\nLabels  are key/value pairs. Valid label keys have two segments: an optional prefix and name,\\nseparated by a slash ( /). The name segment is required and must be 63 characters or less,\\nbeginning and ending with an alphanumeric character ( [a-z0-9A-Z] ) with dashes ( -),\\nunderscores ( _), dots ( .), and alphanumerics between. The prefix is optional. If specified, the\\nprefix must be a DNS subdomain: a series of DNS labels separated by dots ( .), not longer than\\n253 characters in total, followed by a slash ( /).\\nIf the prefix is omitted, the label Key is presumed to be private to the user. Automated system\\ncomponents (e.g. kube-scheduler , kube-controller-manager , kube-apiserver , kubectl , or other\\nthird-party automation) which add labels to end-user objects must specify a prefix.', metadata={'source': './PDFS/Concepts.pdf', 'page': 13}),\n",
       " Document(page_content=\"third-party automation) which add labels to end-user objects must specify a prefix.\\nThe kubernetes.io/  and k8s.io/  prefixes are reserved  for Kubernetes core components.\\nValid label value:\\nmust be 63 characters or less (can be empty),\\nunless empty, must begin and end with an alphanumeric character ( [a-z0-9A-Z] ),\\ncould contain dashes ( -), underscores ( _), dots ( .), and alphanumerics between.\\nFor example, here's a manifest for a Pod that has two labels environment: production  and app: \\nnginx :\\napiVersion : v1\\nkind: Pod\\nmetadata :\\n  name : label-demo\\n  labels :\\n    environment : production\\n    app: nginx• \\n• \\n• \\n• \\n• \\n• \\n• \\n•\", metadata={'source': './PDFS/Concepts.pdf', 'page': 13}),\n",
       " Document(page_content='spec:\\n  containers :\\n  - name : nginx\\n    image : nginx:1.14.2\\n    ports :\\n    - containerPort : 80\\nLabel selectors\\nUnlike names and UIDs , labels do not provide uniqueness. In general, we expect many objects\\nto carry the same label(s).\\nVia a label selector , the client/user can identify a set of objects. The label selector is the core\\ngrouping primitive in Kubernetes.\\nThe API currently supports two types of selectors: equality-based  and set-based . A label selector\\ncan be made of multiple requirements  which are comma-separated. In the case of multiple\\nrequirements, all must be satisfied so the comma separator acts as a logical AND  (&&) operator.\\nThe semantics of empty or non-specified selectors are dependent on the context, and API types\\nthat use selectors should document the validity and meaning of them.\\nNote:  For some API types, such as ReplicaSets, the label selectors of two instances must not', metadata={'source': './PDFS/Concepts.pdf', 'page': 14}),\n",
       " Document(page_content='Note:  For some API types, such as ReplicaSets, the label selectors of two instances must not\\noverlap within a namespace, or the controller can see that as conflicting instructions and fail to\\ndetermine how many replicas should be present.\\nCaution:  For both equality-based and set-based conditions there is no logical OR (||) operator.\\nEnsure your filter statements are structured accordingly.\\nEquality-based  requirement\\nEquality-  or inequality-based  requirements allow filtering by label keys and values. Matching\\nobjects must satisfy all of the specified label constraints, though they may have additional labels\\nas well. Three kinds of operators are admitted =,==,!=. The first two represent equality  (and are\\nsynonyms), while the latter represents inequality . For example:\\nenvironment = production\\ntier != frontend\\nThe former selects all resources with key equal to environment  and value equal to production .', metadata={'source': './PDFS/Concepts.pdf', 'page': 14}),\n",
       " Document(page_content='The former selects all resources with key equal to environment  and value equal to production .\\nThe latter selects all resources with key equal to tier and value distinct from frontend , and all\\nresources with no labels with the tier key. One could filter for resources in production\\nexcluding frontend  using the comma operator: environment=production,tier!=frontend\\nOne usage scenario for equality-based label requirement is for Pods to specify node selection\\ncriteria. For example, the sample Pod below selects nodes with the label \" accelerator=nvidia-\\ntesla-p100 \".\\napiVersion : v1\\nkind: Pod\\nmetadata :\\n  name : cuda-test\\nspec:\\n  containers :', metadata={'source': './PDFS/Concepts.pdf', 'page': 14}),\n",
       " Document(page_content='- name : cuda-test\\n      image : \"registry.k8s.io/cuda-vector-add:v0.1\"\\n      resources :\\n        limits :\\n          nvidia.com/gpu : 1\\n  nodeSelector :\\n    accelerator : nvidia-tesla-p100\\nSet-based  requirement\\nSet-based  label requirements allow filtering keys according to a set of values. Three kinds of\\noperators are supported: in,notin  and exists  (only the key identifier). For example:\\nenvironment in (production, qa)\\ntier notin (frontend, backend)\\npartition\\n!partition\\nThe first example selects all resources with key equal to environment  and value equal to \\nproduction  or qa.\\nThe second example selects all resources with key equal to tier and values other than \\nfrontend  and backend , and all resources with no labels with the tier key.\\nThe third example selects all resources including a label with key partition ; no values are\\nchecked.\\nThe fourth example selects all resources without a label with key partition ; no values are\\nchecked.', metadata={'source': './PDFS/Concepts.pdf', 'page': 15}),\n",
       " Document(page_content='The fourth example selects all resources without a label with key partition ; no values are\\nchecked.\\nSimilarly the comma separator acts as an AND  operator. So filtering resources with a partition\\nkey (no matter the value) and with environment  different than qa can be achieved using \\npartition,environment notin (qa) . The set-based  label selector is a general form of equality since \\nenvironment=production  is equivalent to environment in (production) ; similarly for != and \\nnotin .\\nSet-based  requirements can be mixed with equality-based  requirements. For example: partition \\nin (customerA, customerB),environment!=qa .\\nAPI\\nLIST and WATCH filtering\\nLIST and WATCH operations may specify label selectors to filter the sets of objects returned\\nusing a query parameter. Both requirements are permitted (presented here as they would\\nappear in a URL query string):\\nequality-based  requirements: ?labelSelector=environment%3Dproduction,tier%3Dfrontend', metadata={'source': './PDFS/Concepts.pdf', 'page': 15}),\n",
       " Document(page_content='equality-based  requirements: ?labelSelector=environment%3Dproduction,tier%3Dfrontend\\nset-based  requirements: ?labelSelector=environment+in+\\n%28production%2Cqa%29%2Ctier+in+%28frontend%29\\nBoth label selector styles can be used to list or watch resources via a REST client. For example,\\ntargeting apiserver  with kubectl  and using equality-based  one may write:\\nkubectl get pods -l environment =production,tier =frontend• \\n• \\n• \\n• \\n• \\n•', metadata={'source': './PDFS/Concepts.pdf', 'page': 15}),\n",
       " Document(page_content='or using set-based  requirements:\\nkubectl get pods -l \\'environment in (production),tier in (frontend)\\'\\nAs already mentioned set-based  requirements are more expressive. For instance, they can\\nimplement the OR operator on values:\\nkubectl get pods -l \\'environment in (production, qa)\\'\\nor restricting negative matching via notin  operator:\\nkubectl get pods -l \\'environment,environment notin (frontend)\\'\\nSet references in API objects\\nSome Kubernetes objects, such as services  and replicationcontrollers , also use label selectors to\\nspecify sets of other resources, such as pods .\\nService and ReplicationController\\nThe set of pods that a service  targets is defined with a label selector. Similarly, the population of\\npods that a replicationcontroller  should manage is also defined with a label selector.\\nLabel selectors for both objects are defined in json or yaml  files using maps, and only equality-\\nbased  requirement selectors are supported:\\n\"selector\" : {\\n    \"component\"  : \"redis\" ,\\n}\\nor', metadata={'source': './PDFS/Concepts.pdf', 'page': 16}),\n",
       " Document(page_content='based  requirement selectors are supported:\\n\"selector\" : {\\n    \"component\"  : \"redis\" ,\\n}\\nor\\nselector :\\n  component : redis\\nThis selector (respectively in json or yaml  format) is equivalent to component=redis  or \\ncomponent in (redis) .\\nResources that support set-based requirements\\nNewer resources, such as Job, Deployment , ReplicaSet , and DaemonSet , support set-based\\nrequirements as well.\\nselector :\\n  matchLabels :\\n    component : redis\\n  matchExpressions :\\n    - { key: tier, operator: In, values : [cache] }\\n    - { key: environment, operator: NotIn, values : [dev] }\\nmatchLabels  is a map of {key,value}  pairs. A single {key,value}  in the matchLabels  map is\\nequivalent to an element of matchExpressions , whose key field is \"key\", the operator  is \"In\", and\\nthe values  array contains only \"value\". matchExpressions  is a list of pod selector requirements.', metadata={'source': './PDFS/Concepts.pdf', 'page': 16}),\n",
       " Document(page_content='Valid operators include In, NotIn, Exists, and DoesNotExist. The values set must be non-empty\\nin the case of In and NotIn. All of the requirements, from both matchLabels  and \\nmatchExpressions  are ANDed together -- they must all be satisfied in order to match.\\nSelecting sets of nodes\\nOne use case for selecting over labels is to constrain the set of nodes onto which a pod can\\nschedule. See the documentation on node selection  for more information.\\nUsing labels effectively\\nYou can apply a single label to any resources, but this is not always the best practice. There are\\nmany scenarios where multiple labels should be used to distinguish resource sets from one\\nanother.\\nFor instance, different applications would use different values for the app label, but a multi-tier\\napplication, such as the guestbook example , would additionally need to distinguish each tier.\\nThe frontend could carry the following labels:\\nlabels :\\n  app: guestbook\\n  tier: frontend', metadata={'source': './PDFS/Concepts.pdf', 'page': 17}),\n",
       " Document(page_content='The frontend could carry the following labels:\\nlabels :\\n  app: guestbook\\n  tier: frontend\\nwhile the Redis master and replica would have different tier labels, and perhaps even an\\nadditional role label:\\nlabels :\\n  app: guestbook\\n  tier: backend\\n  role: master\\nand\\nlabels :\\n  app: guestbook\\n  tier: backend\\n  role: replica\\nThe labels allow for slicing and dicing the resources along any dimension specified by a label:\\nkubectl apply -f examples/guestbook/all-in-one/guestbook-all-in-one.yaml\\nkubectl get pods -Lapp -Ltier -Lrole\\nNAME                           READY  STATUS    RESTARTS   AGE   APP         TIER       ROLE\\nguestbook-fe-4nlpb             1/1    Running   0          1m    guestbook   frontend   <none>\\nguestbook-fe-ght6d             1/1    Running   0          1m    guestbook   frontend   <none>\\nguestbook-fe-jpy62             1/1    Running   0          1m    guestbook   frontend   <none>\\nguestbook-redis-master-5pg3b   1/1    Running   0          1m    guestbook   backend    master', metadata={'source': './PDFS/Concepts.pdf', 'page': 17}),\n",
       " Document(page_content='guestbook-redis-master-5pg3b   1/1    Running   0          1m    guestbook   backend    master\\nguestbook-redis-replica-2q2yf  1/1    Running   0          1m    guestbook   backend    replica\\nguestbook-redis-replica-qgazl  1/1    Running   0          1m    guestbook   backend    replica\\nmy-nginx-divi2                 1/1    Running   0          29m   nginx       <none>     <none>\\nmy-nginx-o0ef1                 1/1    Running   0          29m   nginx       <none>     <none>', metadata={'source': './PDFS/Concepts.pdf', 'page': 17}),\n",
       " Document(page_content='kubectl get pods -lapp =guestbook,role =replica\\nNAME                           READY  STATUS   RESTARTS  AGE\\nguestbook-redis-replica-2q2yf  1/1    Running  0         3m\\nguestbook-redis-replica-qgazl  1/1    Running  0         3m\\nUpdating labels\\nSometimes you may want to relabel existing pods and other resources before creating new\\nresources. This can be done with kubectl label . For example, if you want to label all your\\nNGINX Pods as frontend tier, run:\\nkubectl label pods -l app=nginx tier=fe\\npod/my-nginx-2035384211-j5fhi labeled\\npod/my-nginx-2035384211-u2c7e labeled\\npod/my-nginx-2035384211-u3t6x labeled\\nThis first filters all pods with the label \"app=nginx\", and then labels them with the \"tier=fe\". To\\nsee the pods you labeled, run:\\nkubectl get pods -l app=nginx -L tier\\nNAME                        READY     STATUS    RESTARTS   AGE       TIER\\nmy-nginx-2035384211-j5fhi   1/1       Running   0          23m       fe\\nmy-nginx-2035384211-u2c7e   1/1       Running   0          23m       fe', metadata={'source': './PDFS/Concepts.pdf', 'page': 18}),\n",
       " Document(page_content='my-nginx-2035384211-u2c7e   1/1       Running   0          23m       fe\\nmy-nginx-2035384211-u3t6x   1/1       Running   0          23m       fe\\nThis outputs all \"app=nginx\" pods, with an additional label column of pods\\' tier (specified with -\\nL or --label-columns ).\\nFor more information, please see kubectl label .\\nWhat\\'s next\\nLearn how to add a label to a node\\nFind Well-known labels, Annotations and Taints\\nSee Recommended labels\\nEnforce Pod Security Standards with Namespace Labels\\nRead a blog on Writing a Controller for Pod Labels\\nNamespaces\\nIn Kubernetes, namespaces  provides a mechanism for isolating groups of resources within a\\nsingle cluster. Names of resources need to be unique within a namespace, but not across\\nnamespaces. Namespace-based scoping is applicable only for namespaced objects  (e.g.\\nDeployments, Services, etc)  and not for cluster-wide objects (e.g. StorageClass, Nodes,\\nPersistentVolumes, etc) .• \\n• \\n• \\n• \\n•', metadata={'source': './PDFS/Concepts.pdf', 'page': 18}),\n",
       " Document(page_content='When to Use Multiple Namespaces\\nNamespaces are intended for use in environments with many users spread across multiple\\nteams, or projects. For clusters with a few to tens of users, you should not need to create or\\nthink about namespaces at all. Start using namespaces when you need the features they\\nprovide.\\nNamespaces provide a scope for names. Names of resources need to be unique within a\\nnamespace, but not across namespaces. Namespaces cannot be nested inside one another and\\neach Kubernetes resource can only be in one namespace.\\nNamespaces are a way to divide cluster resources between multiple users (via resource quota ).\\nIt is not necessary to use multiple namespaces to separate slightly different resources, such as\\ndifferent versions of the same software: use labels  to distinguish resources within the same\\nnamespace.\\nNote:  For a production cluster, consider not using the default  namespace. Instead, make other\\nnamespaces and use those.\\nInitial namespaces', metadata={'source': './PDFS/Concepts.pdf', 'page': 19}),\n",
       " Document(page_content='namespaces and use those.\\nInitial namespaces\\nKubernetes starts with four initial namespaces:\\ndefault\\nKubernetes includes this namespace so that you can start using your new cluster without\\nfirst creating a namespace.\\nkube-node-lease\\nThis namespace holds Lease  objects associated with each node. Node leases allow the\\nkubelet to send heartbeats  so that the control plane can detect node failure.\\nkube-public\\nThis namespace is readable by all clients (including those not authenticated). This\\nnamespace is mostly reserved for cluster usage, in case that some resources should be\\nvisible and readable publicly throughout the whole cluster. The public aspect of this\\nnamespace is only a convention, not a requirement.\\nkube-system\\nThe namespace for objects created by the Kubernetes system.\\nWorking with Namespaces\\nCreation and deletion of namespaces are described in the Admin Guide documentation for\\nnamespaces .', metadata={'source': './PDFS/Concepts.pdf', 'page': 19}),\n",
       " Document(page_content='Creation and deletion of namespaces are described in the Admin Guide documentation for\\nnamespaces .\\nNote:  Avoid creating namespaces with the prefix kube- , since it is reserved for Kubernetes\\nsystem namespaces.\\nViewing namespaces\\nYou can list the current namespaces in a cluster using:\\nkubectl get namespace', metadata={'source': './PDFS/Concepts.pdf', 'page': 19}),\n",
       " Document(page_content='NAME              STATUS   AGE\\ndefault           Active   1d\\nkube-node-lease   Active   1d\\nkube-public       Active   1d\\nkube-system       Active   1d\\nSetting the namespace for a request\\nTo set the namespace for a current request, use the --namespace  flag.\\nFor example:\\nkubectl run nginx --image =nginx --namespace =<insert-namespace-name-here>\\nkubectl get pods --namespace =<insert-namespace-name-here>\\nSetting the namespace preference\\nYou can permanently save the namespace for all subsequent kubectl commands in that context.\\nkubectl config set-context --current --namespace =<insert-namespace-name-here>\\n# Validate it\\nkubectl config view --minify | grep namespace:\\nNamespaces and DNS\\nWhen you create a Service , it creates a corresponding DNS entry . This entry is of the form \\n<service-name>.<namespace-name>.svc.cluster.local , which means that if a container only uses \\n<service-name> , it will resolve to the service which is local to a namespace. This is useful for', metadata={'source': './PDFS/Concepts.pdf', 'page': 20}),\n",
       " Document(page_content='<service-name> , it will resolve to the service which is local to a namespace. This is useful for\\nusing the same configuration across multiple namespaces such as Development, Staging and\\nProduction. If you want to reach across namespaces, you need to use the fully qualified domain\\nname (FQDN).\\nAs a result, all namespace names must be valid RFC 1123 DNS labels .\\nWarning:\\nBy creating namespaces with the same name as public top-level domains , Services in these\\nnamespaces can have short DNS names that overlap with public DNS records. Workloads from\\nany namespace performing a DNS lookup without a trailing dot  will be redirected to those\\nservices, taking precedence over public DNS.\\nTo mitigate this, limit privileges for creating namespaces to trusted users. If required, you could\\nadditionally configure third-party security controls, such as admission webhooks , to block\\ncreating any namespace with the name of public TLDs .\\nNot all objects are in a namespace', metadata={'source': './PDFS/Concepts.pdf', 'page': 20}),\n",
       " Document(page_content=\"creating any namespace with the name of public TLDs .\\nNot all objects are in a namespace\\nMost Kubernetes resources (e.g. pods, services, replication controllers, and others) are in some\\nnamespaces. However namespace resources are not themselves in a namespace. And low-level\\nresources, such as nodes  and persistentVolumes , are not in any namespace.\\nTo see which Kubernetes resources are and aren't in a namespace:\", metadata={'source': './PDFS/Concepts.pdf', 'page': 20}),\n",
       " Document(page_content=\"# In a namespace\\nkubectl api-resources --namespaced =true\\n# Not in a namespace\\nkubectl api-resources --namespaced =false\\nAutomatic labelling\\nFEATURE STATE:  Kubernetes 1.22 [stable]\\nThe Kubernetes control plane sets an immutable label  kubernetes.io/metadata.name  on all\\nnamespaces. The value of the label is the namespace name.\\nWhat's next\\nLearn more about creating a new namespace .\\nLearn more about deleting a namespace .\\nAnnotations\\nYou can use Kubernetes annotations to attach arbitrary non-identifying metadata to objects .\\nClients such as tools and libraries can retrieve this metadata.\\nAttaching metadata to objects\\nYou can use either labels or annotations to attach metadata to Kubernetes objects. Labels can be\\nused to select objects and to find collections of objects that satisfy certain conditions. In\\ncontrast, annotations are not used to identify and select objects. The metadata in an annotation\", metadata={'source': './PDFS/Concepts.pdf', 'page': 21}),\n",
       " Document(page_content='contrast, annotations are not used to identify and select objects. The metadata in an annotation\\ncan be small or large, structured or unstructured, and can include characters not permitted by\\nlabels. It is possible to use labels as well as annotations in the metadata of the same object.\\nAnnotations, like labels, are key/value maps:\\n\"metadata\" : {\\n  \"annotations\" : {\\n    \"key1\"  : \"value1\" ,\\n    \"key2\"  : \"value2\"\\n  }\\n}\\nNote:  The keys and the values in the map must be strings. In other words, you cannot use\\nnumeric, boolean, list or other types for either the keys or the values.\\nHere are some examples of information that could be recorded in annotations:\\nFields managed by a declarative configuration layer. Attaching these fields as annotations\\ndistinguishes them from default values set by clients or servers, and from auto-generated\\nfields and fields set by auto-sizing or auto-scaling systems.• \\n• \\n•', metadata={'source': './PDFS/Concepts.pdf', 'page': 21}),\n",
       " Document(page_content='Build, release, or image information like timestamps, release IDs, git branch, PR numbers,\\nimage hashes, and registry address.\\nPointers to logging, monitoring, analytics, or audit repositories.\\nClient library or tool information that can be used for debugging purposes: for example,\\nname, version, and build information.\\nUser or tool/system provenance information, such as URLs of related objects from other\\necosystem components.\\nLightweight rollout tool metadata: for example, config or checkpoints.\\nPhone or pager numbers of persons responsible, or directory entries that specify where\\nthat information can be found, such as a team web site.\\nDirectives from the end-user to the implementations to modify behavior or engage non-\\nstandard features.\\nInstead of using annotations, you could store this type of information in an external database or\\ndirectory, but that would make it much harder to produce shared client libraries and tools for\\ndeployment, management, introspection, and the like.', metadata={'source': './PDFS/Concepts.pdf', 'page': 22}),\n",
       " Document(page_content='deployment, management, introspection, and the like.\\nSyntax and character set\\nAnnotations  are key/value pairs. Valid annotation keys have two segments: an optional prefix\\nand name, separated by a slash ( /). The name segment is required and must be 63 characters or\\nless, beginning and ending with an alphanumeric character ( [a-z0-9A-Z] ) with dashes ( -),\\nunderscores ( _), dots ( .), and alphanumerics between. The prefix is optional. If specified, the\\nprefix must be a DNS subdomain: a series of DNS labels separated by dots ( .), not longer than\\n253 characters in total, followed by a slash ( /).\\nIf the prefix is omitted, the annotation Key is presumed to be private to the user. Automated\\nsystem components (e.g. kube-scheduler , kube-controller-manager , kube-apiserver , kubectl , or\\nother third-party automation) which add annotations to end-user objects must specify a prefix.\\nThe kubernetes.io/  and k8s.io/  prefixes are reserved for Kubernetes core components.', metadata={'source': './PDFS/Concepts.pdf', 'page': 22}),\n",
       " Document(page_content='The kubernetes.io/  and k8s.io/  prefixes are reserved for Kubernetes core components.\\nFor example, here\\'s a manifest for a Pod that has the annotation imageregistry: https://\\nhub.docker.com/  :\\napiVersion : v1\\nkind: Pod\\nmetadata :\\n  name : annotations-demo\\n  annotations :\\n    imageregistry : \"https://hub.docker.com/\"\\nspec:\\n  containers :\\n  - name : nginx\\n    image : nginx:1.14.2\\n    ports :\\n    - containerPort : 80• \\n• \\n• \\n• \\n• \\n• \\n•', metadata={'source': './PDFS/Concepts.pdf', 'page': 22}),\n",
       " Document(page_content='What\\'s next\\nLearn more about Labels and Selectors .\\nFind Well-known labels, Annotations and Taints\\nField Selectors\\nField selectors  let you select Kubernetes objects  based on the value of one or more resource\\nfields. Here are some examples of field selector queries:\\nmetadata.name=my-service\\nmetadata.namespace!=default\\nstatus.phase=Pending\\nThis kubectl  command selects all Pods for which the value of the status.phase  field is Running :\\nkubectl get pods --field-selector status.phase =Running\\nNote:  Field selectors are essentially resource filters . By default, no selectors/filters are applied,\\nmeaning that all resources of the specified type are selected. This makes the kubectl  queries \\nkubectl get pods  and kubectl get pods --field-selector \"\"  equivalent.\\nSupported fields\\nSupported field selectors vary by Kubernetes resource type. All resource types support the \\nmetadata.name  and metadata.namespace  fields. Using unsupported field selectors produces an\\nerror. For example:', metadata={'source': './PDFS/Concepts.pdf', 'page': 23}),\n",
       " Document(page_content='error. For example:\\nkubectl get ingress --field-selector foo.bar =baz\\nError from server (BadRequest): Unable to find \"ingresses\" that match label selector \"\", field \\nselector \"foo.bar=baz\": \"foo.bar\" is not a known field selector: only \"metadata.name\", \\n\"metadata.namespace\"\\nSupported operators\\nYou can use the =, ==, and != operators with field selectors ( = and == mean the same thing).\\nThis kubectl  command, for example, selects all Kubernetes Services that aren\\'t in the default\\nnamespace:\\nkubectl get services  --all-namespaces --field-selector metadata.namespace! =default\\nNote:  Set-based operators  (in, notin , exists ) are not supported for field selectors.\\nChained selectors\\nAs with label  and other selectors, field selectors can be chained together as a comma-separated\\nlist. This kubectl  command selects all Pods for which the status.phase  does not equal Running\\nand the spec.restartPolicy  field equals Always :• \\n• \\n• \\n• \\n•', metadata={'source': './PDFS/Concepts.pdf', 'page': 23}),\n",
       " Document(page_content='kubectl get pods --field-selector =status.phase! =Running,spec.restartPolicy =Always\\nMultiple resource types\\nYou can use field selectors across multiple resource types. This kubectl  command selects all\\nStatefulsets and Services that are not in the default  namespace:\\nkubectl get statefulsets,services --all-namespaces --field-selector metadata.namespace! =default\\nFinalizers\\nFinalizers are namespaced keys that tell Kubernetes to wait until specific conditions are met\\nbefore it fully deletes resources marked for deletion. Finalizers alert controllers  to clean up\\nresources the deleted object owned.\\nWhen you tell Kubernetes to delete an object that has finalizers specified for it, the Kubernetes\\nAPI marks the object for deletion by populating .metadata.deletionTimestamp , and returns a \\n202 status code (HTTP \"Accepted\"). The target object remains in a terminating state while the\\ncontrol plane, or other components, take the actions defined by the finalizers. After these', metadata={'source': './PDFS/Concepts.pdf', 'page': 24}),\n",
       " Document(page_content=\"control plane, or other components, take the actions defined by the finalizers. After these\\nactions are complete, the controller removes the relevant finalizers from the target object.\\nWhen the metadata.finalizers  field is empty, Kubernetes considers the deletion complete and\\ndeletes the object.\\nYou can use finalizers to control garbage collection  of resources. For example, you can define a\\nfinalizer to clean up related resources or infrastructure before the controller deletes the target\\nresource.\\nYou can use finalizers to control garbage collection  of objects  by alerting controllers  to perform\\nspecific cleanup tasks before deleting the target resource.\\nFinalizers don't usually specify the code to execute. Instead, they are typically lists of keys on a\\nspecific resource similar to annotations. Kubernetes specifies some finalizers automatically, but\\nyou can also specify your own.\\nHow finalizers work\", metadata={'source': './PDFS/Concepts.pdf', 'page': 24}),\n",
       " Document(page_content='you can also specify your own.\\nHow finalizers work\\nWhen you create a resource using a manifest file, you can specify finalizers in the \\nmetadata.finalizers  field. When you attempt to delete the resource, the API server handling the\\ndelete request notices the values in the finalizers  field and does the following:\\nModifies the object to add a metadata.deletionTimestamp  field with the time you started\\nthe deletion.\\nPrevents the object from being removed until all items are removed from its \\nmetadata.finalizers  field\\nReturns a 202 status code (HTTP \"Accepted\")\\nThe controller managing that finalizer notices the update to the object setting the \\nmetadata.deletionTimestamp , indicating deletion of the object has been requested. The\\ncontroller then attempts to satisfy the requirements of the finalizers specified for that resource.\\nEach time a finalizer condition is satisfied, the controller removes that key from the resource\\'s', metadata={'source': './PDFS/Concepts.pdf', 'page': 24}),\n",
       " Document(page_content=\"Each time a finalizer condition is satisfied, the controller removes that key from the resource's \\nfinalizers  field. When the finalizers  field is emptied, an object with a deletionTimestamp  field• \\n• \\n•\", metadata={'source': './PDFS/Concepts.pdf', 'page': 24}),\n",
       " Document(page_content=\"set is automatically deleted. You can also use finalizers to prevent deletion of unmanaged\\nresources.\\nA common example of a finalizer is kubernetes.io/pv-protection , which prevents accidental\\ndeletion of PersistentVolume  objects. When a PersistentVolume  object is in use by a Pod,\\nKubernetes adds the pv-protection  finalizer. If you try to delete the PersistentVolume , it enters\\na Terminating  status, but the controller can't delete it because the finalizer exists. When the Pod\\nstops using the PersistentVolume , Kubernetes clears the pv-protection  finalizer, and the\\ncontroller deletes the volume.\\nNote:\\nWhen you DELETE  an object, Kubernetes adds the deletion timestamp for that object and\\nthen immediately starts to restrict changes to the .metadata.finalizers  field for the object\\nthat is now pending deletion. You can remove existing finalizers (deleting an entry from\\nthe finalizers  list) but you cannot add a new finalizer. You also cannot modify the\", metadata={'source': './PDFS/Concepts.pdf', 'page': 25}),\n",
       " Document(page_content='the finalizers  list) but you cannot add a new finalizer. You also cannot modify the \\ndeletionTimestamp  for an object once it is set.\\nAfter the deletion is requested, you can not resurrect this object. The only way is to delete\\nit and make a new similar object.\\nOwner references, labels, and finalizers\\nLike labels , owner references  describe the relationships between objects in Kubernetes, but are\\nused for a different purpose. When a controller  manages objects like Pods, it uses labels to track\\nchanges to groups of related objects. For example, when a Job creates one or more Pods, the Job\\ncontroller applies labels to those pods and tracks changes to any Pods in the cluster with the\\nsame label.\\nThe Job controller also adds owner references  to those Pods, pointing at the Job that created the\\nPods. If you delete the Job while these Pods are running, Kubernetes uses the owner references\\n(not labels) to determine which Pods in the cluster need cleanup.', metadata={'source': './PDFS/Concepts.pdf', 'page': 25}),\n",
       " Document(page_content=\"(not labels) to determine which Pods in the cluster need cleanup.\\nKubernetes also processes finalizers when it identifies owner references on a resource targeted\\nfor deletion.\\nIn some situations, finalizers can block the deletion of dependent objects, which can cause the\\ntargeted owner object to remain for longer than expected without being fully deleted. In these\\nsituations, you should check finalizers and owner references on the target owner and\\ndependent objects to troubleshoot the cause.\\nNote:  In cases where objects are stuck in a deleting state, avoid manually removing finalizers to\\nallow deletion to continue. Finalizers are usually added to resources for a reason, so forcefully\\nremoving them can lead to issues in your cluster. This should only be done when the purpose of\\nthe finalizer is understood and is accomplished in another way (for example, manually cleaning\\nup some dependent object).\\nWhat's next\\nRead Using Finalizers to Control Deletion  on the Kubernetes blog.• \\n• \\n•\", metadata={'source': './PDFS/Concepts.pdf', 'page': 25}),\n",
       " Document(page_content='Owners and Dependents\\nIn Kubernetes, some objects  are owners  of other objects. For example, a ReplicaSet  is the owner\\nof a set of Pods. These owned objects are dependents  of their owner.\\nOwnership is different from the labels and selectors  mechanism that some resources also use.\\nFor example, consider a Service that creates EndpointSlice  objects. The Service uses labels  to\\nallow the control plane to determine which EndpointSlice  objects are used for that Service. In\\naddition to the labels, each EndpointSlice  that is managed on behalf of a Service has an owner\\nreference. Owner references help different parts of Kubernetes avoid interfering with objects\\nthey don’t control.\\nOwner references in object specifications\\nDependent objects have a metadata.ownerReferences  field that references their owner object. A\\nvalid owner reference consists of the object name and a UID within the same namespace  as the', metadata={'source': './PDFS/Concepts.pdf', 'page': 26}),\n",
       " Document(page_content=\"valid owner reference consists of the object name and a UID within the same namespace  as the\\ndependent object. Kubernetes sets the value of this field automatically for objects that are\\ndependents of other objects like ReplicaSets, DaemonSets, Deployments, Jobs and CronJobs, and\\nReplicationControllers. You can also configure these relationships manually by changing the\\nvalue of this field. However, you usually don't need to and can allow Kubernetes to\\nautomatically manage the relationships.\\nDependent objects also have an ownerReferences.blockOwnerDeletion  field that takes a\\nboolean value and controls whether specific dependents can block garbage collection from\\ndeleting their owner object. Kubernetes automatically sets this field to true if a controller  (for\\nexample, the Deployment controller) sets the value of the metadata.ownerReferences  field. You\\ncan also set the value of the blockOwnerDeletion  field manually to control which dependents\\nblock garbage collection.\", metadata={'source': './PDFS/Concepts.pdf', 'page': 26}),\n",
       " Document(page_content='block garbage collection.\\nA Kubernetes admission controller controls user access to change this field for dependent\\nresources, based on the delete permissions of the owner. This control prevents unauthorized\\nusers from delaying owner object deletion.\\nNote:\\nCross-namespace owner references are disallowed by design. Namespaced dependents can\\nspecify cluster-scoped or namespaced owners. A namespaced owner must  exist in the same\\nnamespace as the dependent. If it does not, the owner reference is treated as absent, and the\\ndependent is subject to deletion once all owners are verified absent.\\nCluster-scoped dependents can only specify cluster-scoped owners. In v1.20+, if a cluster-\\nscoped dependent specifies a namespaced kind as an owner, it is treated as having an\\nunresolvable owner reference, and is not able to be garbage collected.\\nIn v1.20+, if the garbage collector detects an invalid cross-namespace ownerReference , or a', metadata={'source': './PDFS/Concepts.pdf', 'page': 26}),\n",
       " Document(page_content='In v1.20+, if the garbage collector detects an invalid cross-namespace ownerReference , or a\\ncluster-scoped dependent with an ownerReference  referencing a namespaced kind, a warning\\nEvent with a reason of OwnerRefInvalidNamespace  and an involvedObject  of the invalid\\ndependent is reported. You can check for that kind of Event by running kubectl get events -A --\\nfield-selector=reason=OwnerRefInvalidNamespace .', metadata={'source': './PDFS/Concepts.pdf', 'page': 26}),\n",
       " Document(page_content='Ownership and finalizers\\nWhen you tell Kubernetes to delete a resource, the API server allows the managing controller\\nto process any finalizer rules  for the resource. Finalizers  prevent accidental deletion of\\nresources your cluster may still need to function correctly. For example, if you try to delete a \\nPersistentVolume  that is still in use by a Pod, the deletion does not happen immediately because\\nthe PersistentVolume  has the kubernetes.io/pv-protection  finalizer on it. Instead, the volume\\nremains in the Terminating  status until Kubernetes clears the finalizer, which only happens\\nafter the PersistentVolume  is no longer bound to a Pod.\\nKubernetes also adds finalizers to an owner resource when you use either foreground or orphan\\ncascading deletion . In foreground deletion, it adds the foreground  finalizer so that the controller\\nmust delete dependent resources that also have ownerReferences.blockOwnerDeletion=true', metadata={'source': './PDFS/Concepts.pdf', 'page': 27}),\n",
       " Document(page_content=\"must delete dependent resources that also have ownerReferences.blockOwnerDeletion=true\\nbefore it deletes the owner. If you specify an orphan deletion policy, Kubernetes adds the \\norphan  finalizer so that the controller ignores dependent resources after it deletes the owner\\nobject.\\nWhat's next\\nLearn more about Kubernetes finalizers .\\nLearn about garbage collection .\\nRead the API reference for object metadata .\\nRecommended Labels\\nYou can visualize and manage Kubernetes objects with more tools than kubectl and the\\ndashboard. A common set of labels allows tools to work interoperably, describing objects in a\\ncommon manner that all tools can understand.\\nIn addition to supporting tooling, the recommended labels describe applications in a way that\\ncan be queried.\\nThe metadata is organized around the concept of an application . Kubernetes is not a platform as\\na service (PaaS) and doesn't have or enforce a formal notion of an application. Instead,\", metadata={'source': './PDFS/Concepts.pdf', 'page': 27}),\n",
       " Document(page_content=\"a service (PaaS) and doesn't have or enforce a formal notion of an application. Instead,\\napplications are informal and described with metadata. The definition of what an application\\ncontains is loose.\\nNote:  These are recommended labels. They make it easier to manage applications but aren't\\nrequired for any core tooling.\\nShared labels and annotations share a common prefix: app.kubernetes.io . Labels without a\\nprefix are private to users. The shared prefix ensures that shared labels do not interfere with\\ncustom user labels.\\nLabels\\nIn order to take full advantage of using these labels, they should be applied on every resource\\nobject.• \\n• \\n•\", metadata={'source': './PDFS/Concepts.pdf', 'page': 27}),\n",
       " Document(page_content='Key Description Example Type\\napp.kubernetes.io/name The name of the application mysql string\\napp.kubernetes.io/\\ninstanceA unique name identifying the instance of an\\napplicationmysql-\\nabcxzystring\\napp.kubernetes.io/\\nversionThe current version of the application (e.g., a \\nSemVer 1.0 , revision hash, etc.)5.7.21 string\\napp.kubernetes.io/\\ncomponentThe component within the architecture database string\\napp.kubernetes.io/part-ofThe name of a higher level application this one is\\npart ofwordpress string\\napp.kubernetes.io/\\nmanaged-byThe tool being used to manage the operation of\\nan applicationhelm string\\nTo illustrate these labels in action, consider the following StatefulSet  object:\\n# This is an excerpt\\napiVersion : apps/v1\\nkind: StatefulSet\\nmetadata :\\n  labels :\\n    app.kubernetes.io/name : mysql\\n    app.kubernetes.io/instance : mysql-abcxzy\\n    app.kubernetes.io/version : \"5.7.21\"\\n    app.kubernetes.io/component : database\\n    app.kubernetes.io/part-of : wordpress', metadata={'source': './PDFS/Concepts.pdf', 'page': 28}),\n",
       " Document(page_content='app.kubernetes.io/component : database\\n    app.kubernetes.io/part-of : wordpress\\n    app.kubernetes.io/managed-by : helm\\nApplications And Instances Of Applications\\nAn application can be installed one or more times into a Kubernetes cluster and, in some cases,\\nthe same namespace. For example, WordPress can be installed more than once where different\\nwebsites are different installations of WordPress.\\nThe name of an application and the instance name are recorded separately. For example,\\nWordPress has a app.kubernetes.io/name  of wordpress  while it has an instance name,\\nrepresented as app.kubernetes.io/instance  with a value of wordpress-abcxzy . This enables the\\napplication and instance of the application to be identifiable. Every instance of an application\\nmust have a unique name.\\nExamples\\nTo illustrate different ways to use these labels the following examples have varying complexity.\\nA Simple Stateless Service', metadata={'source': './PDFS/Concepts.pdf', 'page': 28}),\n",
       " Document(page_content='A Simple Stateless Service\\nConsider the case for a simple stateless service deployed using Deployment  and Service  objects.\\nThe following two snippets represent how the labels could be used in their simplest form.\\nThe Deployment  is used to oversee the pods running the application itself.', metadata={'source': './PDFS/Concepts.pdf', 'page': 28}),\n",
       " Document(page_content='apiVersion : apps/v1\\nkind: Deployment\\nmetadata :\\n  labels :\\n    app.kubernetes.io/name : myservice\\n    app.kubernetes.io/instance : myservice-abcxzy\\n...\\nThe Service  is used to expose the application.\\napiVersion : v1\\nkind: Service\\nmetadata :\\n  labels :\\n    app.kubernetes.io/name : myservice\\n    app.kubernetes.io/instance : myservice-abcxzy\\n...\\nWeb Application With A Database\\nConsider a slightly more complicated application: a web application (WordPress) using a\\ndatabase (MySQL), installed using Helm. The following snippets illustrate the start of objects\\nused to deploy this application.\\nThe start to the following Deployment  is used for WordPress:\\napiVersion : apps/v1\\nkind: Deployment\\nmetadata :\\n  labels :\\n    app.kubernetes.io/name : wordpress\\n    app.kubernetes.io/instance : wordpress-abcxzy\\n    app.kubernetes.io/version : \"4.9.4\"\\n    app.kubernetes.io/managed-by : helm\\n    app.kubernetes.io/component : server\\n    app.kubernetes.io/part-of : wordpress\\n...', metadata={'source': './PDFS/Concepts.pdf', 'page': 29}),\n",
       " Document(page_content='app.kubernetes.io/component : server\\n    app.kubernetes.io/part-of : wordpress\\n...\\nThe Service  is used to expose WordPress:\\napiVersion : v1\\nkind: Service\\nmetadata :\\n  labels :\\n    app.kubernetes.io/name : wordpress\\n    app.kubernetes.io/instance : wordpress-abcxzy\\n    app.kubernetes.io/version : \"4.9.4\"\\n    app.kubernetes.io/managed-by : helm\\n    app.kubernetes.io/component : server\\n    app.kubernetes.io/part-of : wordpress\\n...', metadata={'source': './PDFS/Concepts.pdf', 'page': 29}),\n",
       " Document(page_content='MySQL is exposed as a StatefulSet  with metadata for both it and the larger application it\\nbelongs to:\\napiVersion : apps/v1\\nkind: StatefulSet\\nmetadata :\\n  labels :\\n    app.kubernetes.io/name : mysql\\n    app.kubernetes.io/instance : mysql-abcxzy\\n    app.kubernetes.io/version : \"5.7.21\"\\n    app.kubernetes.io/managed-by : helm\\n    app.kubernetes.io/component : database\\n    app.kubernetes.io/part-of : wordpress\\n...\\nThe Service  is used to expose MySQL as part of WordPress:\\napiVersion : v1\\nkind: Service\\nmetadata :\\n  labels :\\n    app.kubernetes.io/name : mysql\\n    app.kubernetes.io/instance : mysql-abcxzy\\n    app.kubernetes.io/version : \"5.7.21\"\\n    app.kubernetes.io/managed-by : helm\\n    app.kubernetes.io/component : database\\n    app.kubernetes.io/part-of : wordpress\\n...\\nWith the MySQL StatefulSet  and Service  you\\'ll notice information about both MySQL and\\nWordPress, the broader application, are included.\\nKubernetes Components', metadata={'source': './PDFS/Concepts.pdf', 'page': 30}),\n",
       " Document(page_content='WordPress, the broader application, are included.\\nKubernetes Components\\nA Kubernetes cluster consists of the components that are a part of the control plane and a set of\\nmachines called nodes.\\nWhen you deploy Kubernetes, you get a cluster.\\nA Kubernetes cluster consists of a set of worker machines, called nodes , that run containerized\\napplications. Every cluster has at least one worker node.\\nThe worker node(s) host the Pods  that are the components of the application workload. The \\ncontrol plane  manages the worker nodes and the Pods in the cluster. In production\\nenvironments, the control plane usually runs across multiple computers and a cluster usually\\nruns multiple nodes, providing fault-tolerance and high availability.\\nThis document outlines the various components you need to have for a complete and working\\nKubernetes cluster.\\nComponents of Kubernetes\\nThe components of a Kubernetes cluster', metadata={'source': './PDFS/Concepts.pdf', 'page': 30}),\n",
       " Document(page_content=\"Control Plane Components\\nThe control plane's components make global decisions about the cluster (for example,\\nscheduling), as well as detecting and responding to cluster events (for example, starting up a\\nnew pod when a deployment's replicas  field is unsatisfied).\\nControl plane components can be run on any machine in the cluster. However, for simplicity,\\nset up scripts typically start all control plane components on the same machine, and do not run\\nuser containers on this machine. See Creating Highly Available clusters with kubeadm  for an\\nexample control plane setup that runs across multiple machines.\\nkube-apiserver\\nThe API server is a component of the Kubernetes control plane  that exposes the Kubernetes\\nAPI. The API server is the front end for the Kubernetes control plane.\\nThe main implementation of a Kubernetes API server is kube-apiserver . kube-apiserver is\\ndesigned to scale horizontally—that is, it scales by deploying more instances. You can run\", metadata={'source': './PDFS/Concepts.pdf', 'page': 31}),\n",
       " Document(page_content=\"designed to scale horizontally—that is, it scales by deploying more instances. You can run\\nseveral instances of kube-apiserver and balance traffic between those instances.\\netcd\\nConsistent and highly-available key value store used as Kubernetes' backing store for all cluster\\ndata.\\nIf your Kubernetes cluster uses etcd as its backing store, make sure you have a back up  plan for\\nthe data.\\nYou can find in-depth information about etcd in the official documentation .\\nkube-scheduler\\nControl plane component that watches for newly created Pods  with no assigned node , and\\nselects a node for them to run on.\\nFactors taken into account for scheduling decisions include: individual and collective resource\\nrequirements, hardware/software/policy constraints, affinity and anti-affinity specifications,\\ndata locality, inter-workload interference, and deadlines.\\nkube-controller-manager\\nControl plane component that runs controller  processes.\", metadata={'source': './PDFS/Concepts.pdf', 'page': 31}),\n",
       " Document(page_content='kube-controller-manager\\nControl plane component that runs controller  processes.\\nLogically, each controller  is a separate process, but to reduce complexity, they are all compiled\\ninto a single binary and run in a single process.\\nThere are many different types of controllers. Some examples of them are:\\nNode controller: Responsible for noticing and responding when nodes go down.\\nJob controller: Watches for Job objects that represent one-off tasks, then creates Pods to\\nrun those tasks to completion.• \\n•', metadata={'source': './PDFS/Concepts.pdf', 'page': 31}),\n",
       " Document(page_content=\"EndpointSlice controller: Populates EndpointSlice objects (to provide a link between\\nServices and Pods).\\nServiceAccount controller: Create default ServiceAccounts for new namespaces.\\nThe above is not an exhaustive list.\\ncloud-controller-manager\\nA Kubernetes control plane  component that embeds cloud-specific control logic. The cloud\\ncontroller manager  lets you link your cluster into your cloud provider's API, and separates out\\nthe components that interact with that cloud platform from components that only interact with\\nyour cluster.\\nThe cloud-controller-manager only runs controllers that are specific to your cloud provider. If\\nyou are running Kubernetes on your own premises, or in a learning environment inside your\\nown PC, the cluster does not have a cloud controller manager.\\nAs with the kube-controller-manager, the cloud-controller-manager combines several logically\\nindependent control loops into a single binary that you run as a single process. You can scale\", metadata={'source': './PDFS/Concepts.pdf', 'page': 32}),\n",
       " Document(page_content=\"independent control loops into a single binary that you run as a single process. You can scale\\nhorizontally (run more than one copy) to improve performance or to help tolerate failures.\\nThe following controllers can have cloud provider dependencies:\\nNode controller: For checking the cloud provider to determine if a node has been deleted\\nin the cloud after it stops responding\\nRoute controller: For setting up routes in the underlying cloud infrastructure\\nService controller: For creating, updating and deleting cloud provider load balancers\\nNode Components\\nNode components run on every node, maintaining running pods and providing the Kubernetes\\nruntime environment.\\nkubelet\\nAn agent that runs on each node  in the cluster. It makes sure that containers  are running in a \\nPod.\\nThe kubelet  takes a set of PodSpecs that are provided through various mechanisms and ensures\\nthat the containers described in those PodSpecs are running and healthy. The kubelet doesn't\", metadata={'source': './PDFS/Concepts.pdf', 'page': 32}),\n",
       " Document(page_content=\"that the containers described in those PodSpecs are running and healthy. The kubelet doesn't\\nmanage containers which were not created by Kubernetes.\\nkube-proxy\\nkube-proxy is a network proxy that runs on each node  in your cluster, implementing part of the\\nKubernetes Service  concept.\\nkube-proxy  maintains network rules on nodes. These network rules allow network\\ncommunication to your Pods from network sessions inside or outside of your cluster.\\nkube-proxy uses the operating system packet filtering layer if there is one and it's available.\\nOtherwise, kube-proxy forwards the traffic itself.• \\n• \\n• \\n• \\n•\", metadata={'source': './PDFS/Concepts.pdf', 'page': 32}),\n",
       " Document(page_content='Container runtime\\nA fundamental component that empowers Kubernetes to run containers effectively. It is\\nresponsible for managing the execution and lifecycle of containers within the Kubernetes\\nenvironment.\\nKubernetes supports container runtimes such as containerd , CRI-O , and any other\\nimplementation of the Kubernetes CRI (Container Runtime Interface) .\\nAddons\\nAddons use Kubernetes resources ( DaemonSet , Deployment , etc) to implement cluster features.\\nBecause these are providing cluster-level features, namespaced resources for addons belong\\nwithin the kube-system  namespace.\\nSelected addons are described below; for an extended list of available addons, please see \\nAddons .\\nDNS\\nWhile the other addons are not strictly required, all Kubernetes clusters should have cluster\\nDNS , as many examples rely on it.\\nCluster DNS is a DNS server, in addition to the other DNS server(s) in your environment, which\\nserves DNS records for Kubernetes services.', metadata={'source': './PDFS/Concepts.pdf', 'page': 33}),\n",
       " Document(page_content='serves DNS records for Kubernetes services.\\nContainers started by Kubernetes automatically include this DNS server in their DNS searches.\\nWeb UI (Dashboard)\\nDashboard  is a general purpose, web-based UI for Kubernetes clusters. It allows users to\\nmanage and troubleshoot applications running in the cluster, as well as the cluster itself.\\nContainer Resource Monitoring\\nContainer Resource Monitoring  records generic time-series metrics about containers in a\\ncentral database, and provides a UI for browsing that data.\\nCluster-level Logging\\nA cluster-level logging  mechanism is responsible for saving container logs to a central log store\\nwith search/browsing interface.\\nNetwork Plugins\\nNetwork plugins  are software components that implement the container network interface\\n(CNI) specification. They are responsible for allocating IP addresses to pods and enabling them\\nto communicate with each other within the cluster.', metadata={'source': './PDFS/Concepts.pdf', 'page': 33}),\n",
       " Document(page_content=\"What's next\\nLearn more about the following:\\nNodes  and their communication  with the control plane.\\nKubernetes controllers .\\nkube-scheduler  which is the default scheduler for Kubernetes.\\nEtcd's official documentation .\\nSeveral container runtimes  in Kubernetes.\\nIntegrating with cloud providers using cloud-controller-manager .\\nkubectl  commands.\\nThe Kubernetes API\\nThe Kubernetes API lets you query and manipulate the state of objects in Kubernetes. The core\\nof Kubernetes' control plane is the API server and the HTTP API that it exposes. Users, the\\ndifferent parts of your cluster, and external components all communicate with one another\\nthrough the API server.\\nThe core of Kubernetes' control plane  is the API server . The API server exposes an HTTP API\\nthat lets end users, different parts of your cluster, and external components communicate with\\none another.\\nThe Kubernetes API lets you query and manipulate the state of API objects in Kubernetes (for\", metadata={'source': './PDFS/Concepts.pdf', 'page': 34}),\n",
       " Document(page_content='The Kubernetes API lets you query and manipulate the state of API objects in Kubernetes (for\\nexample: Pods, Namespaces, ConfigMaps, and Events).\\nMost operations can be performed through the kubectl  command-line interface or other\\ncommand-line tools, such as kubeadm , which in turn use the API. However, you can also access\\nthe API directly using REST calls.\\nConsider using one of the client libraries  if you are writing an application using the Kubernetes\\nAPI.\\nOpenAPI specification\\nComplete API details are documented using OpenAPI .\\nOpenAPI V2\\nThe Kubernetes API server serves an aggregated OpenAPI v2 spec via the /openapi/v2\\nendpoint. You can request the response format using request headers as follows:\\nValid request header values for OpenAPI v2 queries\\nHeader Possible values Notes\\nAccept-\\nEncodinggzipnot supplying this header is also\\nacceptable\\nAcceptapplication/com.github.proto-\\nopenapi.spec.v2@v1.0+protobufmainly for intra-cluster use\\napplication/json default', metadata={'source': './PDFS/Concepts.pdf', 'page': 34}),\n",
       " Document(page_content='openapi.spec.v2@v1.0+protobufmainly for intra-cluster use\\napplication/json default\\n* serves application/json• \\n• \\n• \\n• \\n• \\n• \\n•', metadata={'source': './PDFS/Concepts.pdf', 'page': 34}),\n",
       " Document(page_content='Kubernetes implements an alternative Protobuf based serialization format that is primarily\\nintended for intra-cluster communication. For more information about this format, see the \\nKubernetes Protobuf serialization  design proposal and the Interface Definition Language (IDL)\\nfiles for each schema located in the Go packages that define the API objects.\\nOpenAPI V3\\nFEATURE STATE:  Kubernetes v1.27 [stable]\\nKubernetes supports publishing a description of its APIs as OpenAPI v3.\\nA discovery endpoint /openapi/v3  is provided to see a list of all group/versions available. This\\nendpoint only returns JSON. These group/versions are provided in the following format:\\n{\\n    \"paths\": {\\n        ...,\\n        \"api/v1\": {\\n            \"serverRelativeURL\": \"/openapi/v3/api/v1?\\nhash=CC0E9BFD992D8C59AEC98A1E2336F899E8318D3CF4C68944C3DEC640AF5AB52D864A\\nC50DAA8D145B3494F75FA3CFF939FCBDDA431DAD3CA79738B297795818CF\"\\n        },\\n        \"apis/admissionregistration.k8s.io/v1\": {', metadata={'source': './PDFS/Concepts.pdf', 'page': 35}),\n",
       " Document(page_content='},\\n        \"apis/admissionregistration.k8s.io/v1\": {\\n            \"serverRelativeURL\": \"/openapi/v3/apis/admissionregistration.k8s.io/v1?\\nhash=E19CC93A116982CE5422FC42B590A8AFAD92CDE9AE4D59B5CAAD568F083AD07946E6\\nCB5817531680BCE6E215C16973CD39003B0425F3477CFD854E89A9DB6597\"\\n        },\\n        ....\\n    }\\n}\\nThe relative URLs are pointing to immutable OpenAPI descriptions, in order to improve client-\\nside caching. The proper HTTP caching headers are also set by the API server for that purpose\\n(Expires  to 1 year in the future, and Cache-Control  to immutable ). When an obsolete URL is\\nused, the API server returns a redirect to the newest URL.\\nThe Kubernetes API server publishes an OpenAPI v3 spec per Kubernetes group version at the /\\nopenapi/v3/apis/<group>/<version>?hash=<hash>  endpoint.\\nRefer to the table below for accepted request headers.\\nValid request header values for OpenAPI v3 queries\\nHeader Possible values Notes\\nAccept-\\nEncodinggzipnot supplying this header is also', metadata={'source': './PDFS/Concepts.pdf', 'page': 35}),\n",
       " Document(page_content='Header Possible values Notes\\nAccept-\\nEncodinggzipnot supplying this header is also\\nacceptable\\nAcceptapplication/com.github.proto-\\nopenapi.spec.v3@v1.0+protobufmainly for intra-cluster use\\napplication/json default\\n* serves application/json\\nA Golang implementation to fetch the OpenAPI V3 is provided in the package k8s.io/client-go/\\nopenapi3 .', metadata={'source': './PDFS/Concepts.pdf', 'page': 35}),\n",
       " Document(page_content='Persistence\\nKubernetes stores the serialized state of objects by writing them into etcd.\\nAPI Discovery\\nA list of all group versions supported by a cluster is published at the /api and /apis  endpoints.\\nEach group version also advertises the list of resources supported via /apis/<group>/<version>\\n(for example: /apis/rbac.authorization.k8s.io/v1alpha1 ). These endpoints are used by kubectl to\\nfetch the list of resources supported by a cluster.\\nAggregated Discovery\\nFEATURE STATE:  Kubernetes v1.27 [beta]\\nKubernetes offers beta support for aggregated discovery, publishing all resources supported by\\na cluster through two endpoints ( /api and /apis ) compared to one for every group version.\\nRequesting this endpoint drastically reduces the number of requests sent to fetch the discovery\\nfor the average Kubernetes cluster. This may be accessed by requesting the respective endpoints\\nwith an Accept header indicating the aggregated discovery resource: Accept: application/', metadata={'source': './PDFS/Concepts.pdf', 'page': 36}),\n",
       " Document(page_content='with an Accept header indicating the aggregated discovery resource: Accept: application/\\njson;v=v2beta1;g=apidiscovery.k8s.io;as=APIGroupDiscoveryList .\\nThe endpoint also supports ETag and protobuf encoding.\\nAPI groups and versioning\\nTo make it easier to eliminate fields or restructure resource representations, Kubernetes\\nsupports multiple API versions, each at a different API path, such as /api/v1  or /apis/\\nrbac.authorization.k8s.io/v1alpha1 .\\nVersioning is done at the API level rather than at the resource or field level to ensure that the\\nAPI presents a clear, consistent view of system resources and behavior, and to enable\\ncontrolling access to end-of-life and/or experimental APIs.\\nTo make it easier to evolve and to extend its API, Kubernetes implements API groups  that can\\nbe enabled or disabled .\\nAPI resources are distinguished by their API group, resource type, namespace (for namespaced\\nresources), and name. The API server handles the conversion between API versions', metadata={'source': './PDFS/Concepts.pdf', 'page': 36}),\n",
       " Document(page_content='resources), and name. The API server handles the conversion between API versions\\ntransparently: all the different versions are actually representations of the same persisted data.\\nThe API server may serve the same underlying data through multiple API versions.\\nFor example, suppose there are two API versions, v1 and v1beta1 , for the same resource. If you\\noriginally created an object using the v1beta1  version of its API, you can later read, update, or\\ndelete that object using either the v1beta1  or the v1 API version, until the v1beta1  version is\\ndeprecated and removed. At that point you can continue accessing and modifying the object\\nusing the v1 API.', metadata={'source': './PDFS/Concepts.pdf', 'page': 36}),\n",
       " Document(page_content='API changes\\nAny system that is successful needs to grow and change as new use cases emerge or existing\\nones change. Therefore, Kubernetes has designed the Kubernetes API to continuously change\\nand grow. The Kubernetes project aims to not break compatibility with existing clients, and to\\nmaintain that compatibility for a length of time so that other projects have an opportunity to\\nadapt.\\nIn general, new API resources and new resource fields can be added often and frequently.\\nElimination of resources or fields requires following the API deprecation policy .\\nKubernetes makes a strong commitment to maintain compatibility for official Kubernetes APIs\\nonce they reach general availability (GA), typically at API version v1. Additionally, Kubernetes\\nmaintains compatibility with data persisted via beta API versions of official Kubernetes APIs,\\nand ensures that data can be converted and accessed via GA API versions when the feature\\ngoes stable.', metadata={'source': './PDFS/Concepts.pdf', 'page': 37}),\n",
       " Document(page_content='goes stable.\\nIf you adopt a beta API version, you will need to transition to a subsequent beta or stable API\\nversion once the API graduates. The best time to do this is while the beta API is in its\\ndeprecation period, since objects are simultaneously accessible via both API versions. Once the\\nbeta API completes its deprecation period and is no longer served, the replacement API version\\nmust be used.\\nNote:  Although Kubernetes also aims to maintain compatibility for alpha  APIs versions, in\\nsome circumstances this is not possible. If you use any alpha API versions, check the release\\nnotes for Kubernetes when upgrading your cluster, in case the API did change in incompatible\\nways that require deleting all existing alpha objects prior to upgrade.\\nRefer to API versions reference  for more details on the API version level definitions.\\nAPI Extension\\nThe Kubernetes API can be extended in one of two ways:\\nCustom resources  let you declaratively define how the API server should provide your', metadata={'source': './PDFS/Concepts.pdf', 'page': 37}),\n",
       " Document(page_content=\"Custom resources  let you declaratively define how the API server should provide your\\nchosen resource API.\\nYou can also extend the Kubernetes API by implementing an aggregation layer .\\nWhat's next\\nLearn how to extend the Kubernetes API by adding your own CustomResourceDefinition .\\nControlling Access To The Kubernetes API  describes how the cluster manages\\nauthentication and authorization for API access.\\nLearn about API endpoints, resource types and samples by reading API Reference .\\nLearn about what constitutes a compatible change, and how to change the API, from API\\nchanges .\\nCluster Architecture\\nThe architectural concepts behind Kubernetes.1. \\n2. \\n• \\n• \\n• \\n•\", metadata={'source': './PDFS/Concepts.pdf', 'page': 37}),\n",
       " Document(page_content='Components of Kubernetes\\nKubernetes cluster architecture\\nNodes\\nCommunication between Nodes and the Control Plane\\nControllers\\nLeases\\nCloud Controller Manager\\nAbout cgroup v2\\nContainer Runtime Interface (CRI)\\nGarbage Collection\\nMixed Version Proxy\\nNodes\\nKubernetes runs your workload  by placing containers into Pods to run on Nodes . A node may\\nbe a virtual or physical machine, depending on the cluster. Each node is managed by the control\\nplane  and contains the services necessary to run Pods .\\nTypically you have several nodes in a cluster; in a learning or resource-limited environment,\\nyou might have only one node.\\nThe components  on a node include the kubelet , a container runtime , and the kube-proxy .\\nManagement\\nThere are two main ways to have Nodes added to the API server :\\nThe kubelet on a node self-registers to the control plane\\nYou (or another human user) manually add a Node object\\nAfter you create a Node object , or the kubelet on a node self-registers, the control plane checks', metadata={'source': './PDFS/Concepts.pdf', 'page': 38}),\n",
       " Document(page_content='After you create a Node object , or the kubelet on a node self-registers, the control plane checks\\nwhether the new Node object is valid. For example, if you try to create a Node from the\\nfollowing JSON manifest:\\n{\\n  \"kind\" : \"Node\" ,\\n  \"apiVersion\" : \"v1\",\\n  \"metadata\" : {\\n    \"name\" : \"10.240.79.157\" ,\\n    \"labels\" : {1. \\n2.', metadata={'source': './PDFS/Concepts.pdf', 'page': 38}),\n",
       " Document(page_content='\"name\" : \"my-first-k8s-node\"\\n    }\\n  }\\n}\\nKubernetes creates a Node object internally (the representation). Kubernetes checks that a\\nkubelet has registered to the API server that matches the metadata.name  field of the Node. If\\nthe node is healthy (i.e. all necessary services are running), then it is eligible to run a Pod.\\nOtherwise, that node is ignored for any cluster activity until it becomes healthy.\\nNote:\\nKubernetes keeps the object for the invalid Node and continues checking to see whether it\\nbecomes healthy.\\nYou, or a controller , must explicitly delete the Node object to stop that health checking.\\nThe name of a Node object must be a valid DNS subdomain name .\\nNode name uniqueness\\nThe name  identifies a Node. Two Nodes cannot have the same name at the same time.\\nKubernetes also assumes that a resource with the same name is the same object. In case of a\\nNode, it is implicitly assumed that an instance using the same name will have the same state', metadata={'source': './PDFS/Concepts.pdf', 'page': 39}),\n",
       " Document(page_content='Node, it is implicitly assumed that an instance using the same name will have the same state\\n(e.g. network settings, root disk contents) and attributes like node labels. This may lead to\\ninconsistencies if an instance was modified without changing its name. If the Node needs to be\\nreplaced or updated significantly, the existing Node object needs to be removed from API server\\nfirst and re-added after the update.\\nSelf-registration of Nodes\\nWhen the kubelet flag --register-node  is true (the default), the kubelet will attempt to register\\nitself with the API server. This is the preferred pattern, used by most distros.\\nFor self-registration, the kubelet is started with the following options:\\n--kubeconfig  - Path to credentials to authenticate itself to the API server.\\n--cloud-provider  - How to talk to a cloud provider  to read metadata about itself.\\n--register-node  - Automatically register with the API server.', metadata={'source': './PDFS/Concepts.pdf', 'page': 39}),\n",
       " Document(page_content='--register-node  - Automatically register with the API server.\\n--register-with-taints  - Register the node with the given list of taints  (comma separated \\n<key>=<value>:<effect> ).\\nNo-op if register-node  is false.\\n--node-ip  - Optional comma-separated list of the IP addresses for the node. You can only\\nspecify a single address for each address family. For example, in a single-stack IPv4\\ncluster, you set this value to be the IPv4 address that the kubelet should use for the node.\\nSee configure IPv4/IPv6 dual stack  for details of running a dual-stack cluster.• \\n• \\n• \\n• \\n•', metadata={'source': './PDFS/Concepts.pdf', 'page': 39}),\n",
       " Document(page_content=\"If you don't provide this argument, the kubelet uses the node's default IPv4 address, if\\nany; if the node has no IPv4 addresses then the kubelet uses the node's default IPv6\\naddress.\\n--node-labels  - Labels  to add when registering the node in the cluster (see label\\nrestrictions enforced by the NodeRestriction admission plugin ).\\n--node-status-update-frequency  - Specifies how often kubelet posts its node status to the\\nAPI server.\\nWhen the Node authorization mode  and NodeRestriction admission plugin  are enabled,\\nkubelets are only authorized to create/modify their own Node resource.\\nNote:\\nAs mentioned in the Node name uniqueness  section, when Node configuration needs to be\\nupdated, it is a good practice to re-register the node with the API server. For example, if the\\nkubelet being restarted with the new set of --node-labels , but the same Node name is used, the\\nchange will not take an effect, as labels are being set on the Node registration.\", metadata={'source': './PDFS/Concepts.pdf', 'page': 40}),\n",
       " Document(page_content='change will not take an effect, as labels are being set on the Node registration.\\nPods already scheduled on the Node may misbehave or cause issues if the Node configuration\\nwill be changed on kubelet restart. For example, already running Pod may be tainted against\\nthe new labels assigned to the Node, while other Pods, that are incompatible with that Pod will\\nbe scheduled based on this new label. Node re-registration ensures all Pods will be drained and\\nproperly re-scheduled.\\nManual Node administration\\nYou can create and modify Node objects using kubectl .\\nWhen you want to create Node objects manually, set the kubelet flag --register-node=false .\\nYou can modify Node objects regardless of the setting of --register-node . For example, you can\\nset labels on an existing Node or mark it unschedulable.\\nYou can use labels on Nodes in conjunction with node selectors on Pods to control scheduling.\\nFor example, you can constrain a Pod to only be eligible to run on a subset of the available\\nnodes.', metadata={'source': './PDFS/Concepts.pdf', 'page': 40}),\n",
       " Document(page_content='For example, you can constrain a Pod to only be eligible to run on a subset of the available\\nnodes.\\nMarking a node as unschedulable prevents the scheduler from placing new pods onto that Node\\nbut does not affect existing Pods on the Node. This is useful as a preparatory step before a node\\nreboot or other maintenance.\\nTo mark a Node unschedulable, run:\\nkubectl cordon $NODENAME\\nSee Safely Drain a Node  for more details.\\nNote:  Pods that are part of a DaemonSet  tolerate being run on an unschedulable Node.\\nDaemonSets typically provide node-local services that should run on the Node even if it is\\nbeing drained of workload applications.• \\n•', metadata={'source': './PDFS/Concepts.pdf', 'page': 40}),\n",
       " Document(page_content=\"Node status\\nA Node's status contains the following information:\\nAddresses\\nConditions\\nCapacity and Allocatable\\nInfo\\nYou can use kubectl  to view a Node's status and other details:\\nkubectl describe node <insert-node-name-here>\\nSee Node Status  for more details.\\nNode heartbeats\\nHeartbeats, sent by Kubernetes nodes, help your cluster determine the availability of each node,\\nand to take action when failures are detected.\\nFor nodes there are two forms of heartbeats:\\nUpdates to the .status  of a Node.\\nLease  objects within the kube-node-lease  namespace . Each Node has an associated Lease\\nobject.\\nNode controller\\nThe node controller  is a Kubernetes control plane component that manages various aspects of\\nnodes.\\nThe node controller has multiple roles in a node's life. The first is assigning a CIDR block to the\\nnode when it is registered (if CIDR assignment is turned on).\\nThe second is keeping the node controller's internal list of nodes up to date with the cloud\", metadata={'source': './PDFS/Concepts.pdf', 'page': 41}),\n",
       " Document(page_content=\"The second is keeping the node controller's internal list of nodes up to date with the cloud\\nprovider's list of available machines. When running in a cloud environment and whenever a\\nnode is unhealthy, the node controller asks the cloud provider if the VM for that node is still\\navailable. If not, the node controller deletes the node from its list of nodes.\\nThe third is monitoring the nodes' health. The node controller is responsible for:\\nIn the case that a node becomes unreachable, updating the Ready  condition in the\\nNode's .status  field. In this case the node controller sets the Ready  condition to Unknown .\\nIf a node remains unreachable: triggering API-initiated eviction  for all of the Pods on the\\nunreachable node. By default, the node controller waits 5 minutes between marking the\\nnode as Unknown  and submitting the first eviction request.\\nBy default, the node controller checks the state of each node every 5 seconds. This period can\", metadata={'source': './PDFS/Concepts.pdf', 'page': 41}),\n",
       " Document(page_content='By default, the node controller checks the state of each node every 5 seconds. This period can\\nbe configured using the --node-monitor-period  flag on the kube-controller-manager\\ncomponent.• \\n• \\n• \\n• \\n• \\n• \\n• \\n•', metadata={'source': './PDFS/Concepts.pdf', 'page': 41}),\n",
       " Document(page_content=\"Rate limits on eviction\\nIn most cases, the node controller limits the eviction rate to --node-eviction-rate  (default 0.1)\\nper second, meaning it won't evict pods from more than 1 node per 10 seconds.\\nThe node eviction behavior changes when a node in a given availability zone becomes\\nunhealthy. The node controller checks what percentage of nodes in the zone are unhealthy (the \\nReady  condition is Unknown  or False ) at the same time:\\nIf the fraction of unhealthy nodes is at least --unhealthy-zone-threshold  (default 0.55),\\nthen the eviction rate is reduced.\\nIf the cluster is small (i.e. has less than or equal to --large-cluster-size-threshold  nodes -\\ndefault 50), then evictions are stopped.\\nOtherwise, the eviction rate is reduced to --secondary-node-eviction-rate  (default 0.01)\\nper second.\\nThe reason these policies are implemented per availability zone is because one availability zone\\nmight become partitioned from the control plane while the others remain connected. If your\", metadata={'source': './PDFS/Concepts.pdf', 'page': 42}),\n",
       " Document(page_content=\"might become partitioned from the control plane while the others remain connected. If your\\ncluster does not span multiple cloud provider availability zones, then the eviction mechanism\\ndoes not take per-zone unavailability into account.\\nA key reason for spreading your nodes across availability zones is so that the workload can be\\nshifted to healthy zones when one entire zone goes down. Therefore, if all nodes in a zone are\\nunhealthy, then the node controller evicts at the normal rate of --node-eviction-rate . The corner\\ncase is when all zones are completely unhealthy (none of the nodes in the cluster are healthy).\\nIn such a case, the node controller assumes that there is some problem with connectivity\\nbetween the control plane and the nodes, and doesn't perform any evictions. (If there has been\\nan outage and some nodes reappear, the node controller does evict pods from the remaining\\nnodes that are unhealthy or unreachable).\", metadata={'source': './PDFS/Concepts.pdf', 'page': 42}),\n",
       " Document(page_content=\"nodes that are unhealthy or unreachable).\\nThe node controller is also responsible for evicting pods running on nodes with NoExecute\\ntaints, unless those pods tolerate that taint. The node controller also adds taints  corresponding\\nto node problems like node unreachable or not ready. This means that the scheduler won't place\\nPods onto unhealthy nodes.\\nResource capacity tracking\\nNode objects track information about the Node's resource capacity: for example, the amount of\\nmemory available and the number of CPUs. Nodes that self register  report their capacity during\\nregistration. If you manually  add a Node, then you need to set the node's capacity information\\nwhen you add it.\\nThe Kubernetes scheduler  ensures that there are enough resources for all the Pods on a Node.\\nThe scheduler checks that the sum of the requests of containers on the node is no greater than\\nthe node's capacity. That sum of requests includes all containers managed by the kubelet, but\", metadata={'source': './PDFS/Concepts.pdf', 'page': 42}),\n",
       " Document(page_content=\"the node's capacity. That sum of requests includes all containers managed by the kubelet, but\\nexcludes any containers started directly by the container runtime, and also excludes any\\nprocesses running outside of the kubelet's control.\\nNote:  If you want to explicitly reserve resources for non-Pod processes, see reserve resources\\nfor system daemons .• \\n• \\n•\", metadata={'source': './PDFS/Concepts.pdf', 'page': 42}),\n",
       " Document(page_content='Node topology\\nFEATURE STATE:  Kubernetes v1.18 [beta]\\nIf you have enabled the TopologyManager  feature gate , then the kubelet can use topology hints\\nwhen making resource assignment decisions. See Control Topology Management Policies on a\\nNode  for more information.\\nGraceful node shutdown\\nFEATURE STATE:  Kubernetes v1.21 [beta]\\nThe kubelet attempts to detect node system shutdown and terminates pods running on the\\nnode.\\nKubelet ensures that pods follow the normal pod termination process  during the node\\nshutdown. During node shutdown, the kubelet does not accept new Pods (even if those Pods are\\nalready bound to the node).\\nThe Graceful node shutdown feature depends on systemd since it takes advantage of systemd\\ninhibitor locks  to delay the node shutdown with a given duration.\\nGraceful node shutdown is controlled with the GracefulNodeShutdown  feature gate  which is\\nenabled by default in 1.21.\\nNote that by default, both configuration options described below, shutdownGracePeriod  and', metadata={'source': './PDFS/Concepts.pdf', 'page': 43}),\n",
       " Document(page_content='Note that by default, both configuration options described below, shutdownGracePeriod  and \\nshutdownGracePeriodCriticalPods  are set to zero, thus not activating the graceful node\\nshutdown functionality. To activate the feature, the two kubelet config settings should be\\nconfigured appropriately and set to non-zero values.\\nOnce systemd detects or notifies node shutdown, the kubelet sets a NotReady  condition on the\\nNode, with the reason  set to \"node is shutting down\" . The kube-scheduler honors this condition\\nand does not schedule any Pods onto the affected node; other third-party schedulers are\\nexpected to follow the same logic. This means that new Pods won\\'t be scheduled onto that node\\nand therefore none will start.\\nThe kubelet also rejects Pods during the PodAdmission  phase if an ongoing node shutdown has\\nbeen detected, so that even Pods with a toleration  for node.kubernetes.io/not-ready:NoSchedule\\ndo not start there.', metadata={'source': './PDFS/Concepts.pdf', 'page': 43}),\n",
       " Document(page_content='do not start there.\\nAt the same time when kubelet is setting that condition on its Node via the API, the kubelet\\nalso begins terminating any Pods that are running locally.\\nDuring a graceful shutdown, kubelet terminates pods in two phases:\\nTerminate regular pods running on the node.\\nTerminate critical pods  running on the node.\\nGraceful node shutdown feature is configured with two KubeletConfiguration  options:\\nshutdownGracePeriod :\\nSpecifies the total duration that the node should delay the shutdown by. This is the\\ntotal grace period for pod termination for both regular and critical pods .1. \\n2. \\n• \\n◦', metadata={'source': './PDFS/Concepts.pdf', 'page': 43}),\n",
       " Document(page_content='shutdownGracePeriodCriticalPods :\\nSpecifies the duration used to terminate critical pods  during a node shutdown. This\\nvalue should be less than shutdownGracePeriod .\\nNote:  There are cases when Node termination was cancelled by the system (or perhaps\\nmanually by an administrator). In either of those situations the Node will return to the Ready\\nstate. However, Pods which already started the process of termination will not be restored by\\nkubelet and will need to be re-scheduled.\\nFor example, if shutdownGracePeriod=30s , and shutdownGracePeriodCriticalPods=10s , kubelet\\nwill delay the node shutdown by 30 seconds. During the shutdown, the first 20 (30-10) seconds\\nwould be reserved for gracefully terminating normal pods, and the last 10 seconds would be\\nreserved for terminating critical pods .\\nNote:\\nWhen pods were evicted during the graceful node shutdown, they are marked as shutdown.\\nRunning kubectl get pods  shows the status of the evicted pods as Terminated . And kubectl', metadata={'source': './PDFS/Concepts.pdf', 'page': 44}),\n",
       " Document(page_content='Running kubectl get pods  shows the status of the evicted pods as Terminated . And kubectl \\ndescribe pod  indicates that the pod was evicted because of node shutdown:\\nReason:         Terminated\\nMessage:        Pod was terminated in response to imminent node shutdown.\\nPod Priority based graceful node shutdown\\nFEATURE STATE:  Kubernetes v1.24 [beta]\\nTo provide more flexibility during graceful node shutdown around the ordering of pods during\\nshutdown, graceful node shutdown honors the PriorityClass for Pods, provided that you\\nenabled this feature in your cluster. The feature allows cluster administers to explicitly define\\nthe ordering of pods during graceful node shutdown based on priority classes .\\nThe Graceful Node Shutdown  feature, as described above, shuts down pods in two phases, non-\\ncritical pods, followed by critical pods. If additional flexibility is needed to explicitly define the\\nordering of pods during shutdown in a more granular way, pod priority based graceful', metadata={'source': './PDFS/Concepts.pdf', 'page': 44}),\n",
       " Document(page_content='ordering of pods during shutdown in a more granular way, pod priority based graceful\\nshutdown can be used.\\nWhen graceful node shutdown honors pod priorities, this makes it possible to do graceful node\\nshutdown in multiple phases, each phase shutting down a particular priority class of pods. The\\nkubelet can be configured with the exact phases and shutdown time per phase.\\nAssuming the following custom pod priority classes  in a cluster,\\nPod priority class name Pod priority class value\\ncustom-class-a 100000\\ncustom-class-b 10000\\ncustom-class-c 1000\\nregular/unset 0\\nWithin the kubelet configuration  the settings for shutdownGracePeriodByPodPriority  could\\nlook like:• \\n◦', metadata={'source': './PDFS/Concepts.pdf', 'page': 44}),\n",
       " Document(page_content=\"Pod priority class value Shutdown period\\n100000 10 seconds\\n10000 180 seconds\\n1000 120 seconds\\n0 60 seconds\\nThe corresponding kubelet config YAML configuration would be:\\nshutdownGracePeriodByPodPriority :\\n  - priority : 100000\\n    shutdownGracePeriodSeconds : 10\\n  - priority : 10000\\n    shutdownGracePeriodSeconds : 180\\n  - priority : 1000\\n    shutdownGracePeriodSeconds : 120\\n  - priority : 0\\n    shutdownGracePeriodSeconds : 60\\nThe above table implies that any pod with priority  value >= 100000 will get just 10 seconds to\\nstop, any pod with value >= 10000 and < 100000 will get 180 seconds to stop, any pod with\\nvalue >= 1000 and < 10000 will get 120 seconds to stop. Finally, all other pods will get 60\\nseconds to stop.\\nOne doesn't have to specify values corresponding to all of the classes. For example, you could\\ninstead use these settings:\\nPod priority class value Shutdown period\\n100000 300 seconds\\n1000 120 seconds\\n0 60 seconds\", metadata={'source': './PDFS/Concepts.pdf', 'page': 45}),\n",
       " Document(page_content='Pod priority class value Shutdown period\\n100000 300 seconds\\n1000 120 seconds\\n0 60 seconds\\nIn the above case, the pods with custom-class-b  will go into the same bucket as custom-class-c\\nfor shutdown.\\nIf there are no pods in a particular range, then the kubelet does not wait for pods in that\\npriority range. Instead, the kubelet immediately skips to the next priority class value range.\\nIf this feature is enabled and no configuration is provided, then no ordering action will be\\ntaken.\\nUsing this feature requires enabling the GracefulNodeShutdownBasedOnPodPriority  feature\\ngate, and setting ShutdownGracePeriodByPodPriority  in the kubelet config  to the desired\\nconfiguration containing the pod priority class values and their respective shutdown periods.\\nNote:  The ability to take Pod priority into account during graceful node shutdown was\\nintroduced as an Alpha feature in Kubernetes v1.23. In Kubernetes 1.28 the feature is Beta and\\nis enabled by default.', metadata={'source': './PDFS/Concepts.pdf', 'page': 45}),\n",
       " Document(page_content='is enabled by default.\\nMetrics graceful_shutdown_start_time_seconds  and graceful_shutdown_end_time_seconds  are\\nemitted under the kubelet subsystem to monitor node shutdowns.', metadata={'source': './PDFS/Concepts.pdf', 'page': 45}),\n",
       " Document(page_content=\"Non-graceful node shutdown handling\\nFEATURE STATE:  Kubernetes v1.28 [stable]\\nA node shutdown action may not be detected by kubelet's Node Shutdown Manager, either\\nbecause the command does not trigger the inhibitor locks mechanism used by kubelet or\\nbecause of a user error, i.e., the ShutdownGracePeriod and ShutdownGracePeriodCriticalPods\\nare not configured properly. Please refer to above section Graceful Node Shutdown  for more\\ndetails.\\nWhen a node is shutdown but not detected by kubelet's Node Shutdown Manager, the pods that\\nare part of a StatefulSet  will be stuck in terminating status on the shutdown node and cannot\\nmove to a new running node. This is because kubelet on the shutdown node is not available to\\ndelete the pods so the StatefulSet cannot create a new pod with the same name. If there are\\nvolumes used by the pods, the VolumeAttachments will not be deleted from the original\\nshutdown node so the volumes used by these pods cannot be attached to a new running node.\", metadata={'source': './PDFS/Concepts.pdf', 'page': 46}),\n",
       " Document(page_content='shutdown node so the volumes used by these pods cannot be attached to a new running node.\\nAs a result, the application running on the StatefulSet cannot function properly. If the original\\nshutdown node comes up, the pods will be deleted by kubelet and new pods will be created on a\\ndifferent running node. If the original shutdown node does not come up, these pods will be\\nstuck in terminating status on the shutdown node forever.\\nTo mitigate the above situation, a user can manually add the taint node.kubernetes.io/out-of-\\nservice  with either NoExecute  or NoSchedule  effect to a Node marking it out-of-service. If the \\nNodeOutOfServiceVolumeDetach feature gate  is enabled on kube-controller-manager , and a\\nNode is marked out-of-service with this taint, the pods on the node will be forcefully deleted if\\nthere are no matching tolerations on it and volume detach operations for the pods terminating\\non the node will happen immediately. This allows the Pods on the out-of-service node to', metadata={'source': './PDFS/Concepts.pdf', 'page': 46}),\n",
       " Document(page_content='on the node will happen immediately. This allows the Pods on the out-of-service node to\\nrecover quickly on a different node.\\nDuring a non-graceful shutdown, Pods are terminated in the two phases:\\nForce delete the Pods that do not have matching out-of-service  tolerations.\\nImmediately perform detach volume operation for such pods.\\nNote:\\nBefore adding the taint node.kubernetes.io/out-of-service , it should be verified that the\\nnode is already in shutdown or power off state (not in the middle of restarting).\\nThe user is required to manually remove the out-of-service taint after the pods are moved\\nto a new node and the user has checked that the shutdown node has been recovered since\\nthe user was the one who originally added the taint.\\nSwap memory management\\nFEATURE STATE:  Kubernetes v1.28 [beta]\\nTo enable swap on a node, the NodeSwap  feature gate must be enabled on the kubelet, and the \\n--fail-swap-on  command line flag or failSwapOn  configuration setting  must be set to false.', metadata={'source': './PDFS/Concepts.pdf', 'page': 46}),\n",
       " Document(page_content='--fail-swap-on  command line flag or failSwapOn  configuration setting  must be set to false.\\nWarning:  When the memory swap feature is turned on, Kubernetes data such as the content of\\nSecret objects that were written to tmpfs now could be swapped to disk.1. \\n2. \\n• \\n•', metadata={'source': './PDFS/Concepts.pdf', 'page': 46}),\n",
       " Document(page_content='A user can also optionally configure memorySwap.swapBehavior  in order to specify how a\\nnode will use swap memory. For example,\\nmemorySwap :\\n  swapBehavior : UnlimitedSwap\\nUnlimitedSwap  (default): Kubernetes workloads can use as much swap memory as they\\nrequest, up to the system limit.\\nLimitedSwap : The utilization of swap memory by Kubernetes workloads is subject to\\nlimitations. Only Pods of Burstable QoS are permitted to employ swap.\\nIf configuration for memorySwap  is not specified and the feature gate is enabled, by default the\\nkubelet will apply the same behaviour as the UnlimitedSwap  setting.\\nWith LimitedSwap , Pods that do not fall under the Burstable QoS classification (i.e. BestEffort /\\nGuaranteed  Qos Pods) are prohibited from utilizing swap memory. To maintain the\\naforementioned security and node health guarantees, these Pods are not permitted to use swap\\nmemory when LimitedSwap  is in effect.', metadata={'source': './PDFS/Concepts.pdf', 'page': 47}),\n",
       " Document(page_content=\"memory when LimitedSwap  is in effect.\\nPrior to detailing the calculation of the swap limit, it is necessary to define the following terms:\\nnodeTotalMemory : The total amount of physical memory available on the node.\\ntotalPodsSwapAvailable : The total amount of swap memory on the node that is available\\nfor use by Pods (some swap memory may be reserved for system use).\\ncontainerMemoryRequest : The container's memory request.\\nSwap limitation is configured as: (containerMemoryRequest / nodeTotalMemory) * \\ntotalPodsSwapAvailable .\\nIt is important to note that, for containers within Burstable QoS Pods, it is possible to opt-out of\\nswap usage by specifying memory requests that are equal to memory limits. Containers\\nconfigured in this manner will not have access to swap memory.\\nSwap is supported only with cgroup v2 , cgroup v1 is not supported.\\nFor more information, and to assist with testing and provide feedback, please see the blog-post\", metadata={'source': './PDFS/Concepts.pdf', 'page': 47}),\n",
       " Document(page_content=\"For more information, and to assist with testing and provide feedback, please see the blog-post\\nabout Kubernetes 1.28: NodeSwap graduates to Beta1 , KEP-2400  and its design proposal .\\nWhat's next\\nLearn more about the following:\\nComponents  that make up a node.\\nAPI definition for Node .\\nNode  section of the architecture design document.\\nTaints and Tolerations .\\nNode Resource Managers .\\nResource Management for Windows nodes .• \\n• \\n• \\n• \\n• \\n• \\n• \\n• \\n• \\n• \\n•\", metadata={'source': './PDFS/Concepts.pdf', 'page': 47}),\n",
       " Document(page_content='Communication between Nodes and the\\nControl Plane\\nThis document catalogs the communication paths between the API server  and the Kubernetes \\ncluster . The intent is to allow users to customize their installation to harden the network\\nconfiguration such that the cluster can be run on an untrusted network (or on fully public IPs\\non a cloud provider).\\nNode to Control Plane\\nKubernetes has a \"hub-and-spoke\" API pattern. All API usage from nodes (or the pods they run)\\nterminates at the API server. None of the other control plane components are designed to\\nexpose remote services. The API server is configured to listen for remote connections on a\\nsecure HTTPS port (typically 443) with one or more forms of client authentication  enabled. One\\nor more forms of authorization  should be enabled, especially if anonymous requests  or service\\naccount tokens  are allowed.\\nNodes should be provisioned with the public root certificate  for the cluster such that they can', metadata={'source': './PDFS/Concepts.pdf', 'page': 48}),\n",
       " Document(page_content='Nodes should be provisioned with the public root certificate  for the cluster such that they can\\nconnect securely to the API server along with valid client credentials. A good approach is that\\nthe client credentials provided to the kubelet are in the form of a client certificate. See kubelet\\nTLS bootstrapping  for automated provisioning of kubelet client certificates.\\nPods  that wish to connect to the API server can do so securely by leveraging a service account\\nso that Kubernetes will automatically inject the public root certificate and a valid bearer token\\ninto the pod when it is instantiated. The kubernetes  service (in default  namespace) is configured\\nwith a virtual IP address that is redirected (via kube-proxy ) to the HTTPS endpoint on the API\\nserver.\\nThe control plane components also communicate with the API server over the secure port.\\nAs a result, the default operating mode for connections from the nodes and pod running on the', metadata={'source': './PDFS/Concepts.pdf', 'page': 48}),\n",
       " Document(page_content=\"As a result, the default operating mode for connections from the nodes and pod running on the\\nnodes to the control plane is secured by default and can run over untrusted and/or public\\nnetworks.\\nControl plane to node\\nThere are two primary communication paths from the control plane (the API server) to the\\nnodes. The first is from the API server to the kubelet  process which runs on each node in the\\ncluster. The second is from the API server to any node, pod, or service through the API server's \\nproxy  functionality.\\nAPI server to kubelet\\nThe connections from the API server to the kubelet are used for:\\nFetching logs for pods.\\nAttaching (usually through kubectl ) to running pods.\\nProviding the kubelet's port-forwarding functionality.• \\n• \\n•\", metadata={'source': './PDFS/Concepts.pdf', 'page': 48}),\n",
       " Document(page_content=\"These connections terminate at the kubelet's HTTPS endpoint. By default, the API server does\\nnot verify the kubelet's serving certificate, which makes the connection subject to man-in-the-\\nmiddle attacks and unsafe  to run over untrusted and/or public networks.\\nTo verify this connection, use the --kubelet-certificate-authority  flag to provide the API server\\nwith a root certificate bundle to use to verify the kubelet's serving certificate.\\nIf that is not possible, use SSH tunneling  between the API server and kubelet if required to\\navoid connecting over an untrusted or public network.\\nFinally, Kubelet authentication and/or authorization  should be enabled to secure the kubelet\\nAPI.\\nAPI server to nodes, pods, and services\\nThe connections from the API server to a node, pod, or service default to plain HTTP\\nconnections and are therefore neither authenticated nor encrypted. They can be run over a\\nsecure HTTPS connection by prefixing https:  to the node, pod, or service name in the API URL,\", metadata={'source': './PDFS/Concepts.pdf', 'page': 49}),\n",
       " Document(page_content=\"secure HTTPS connection by prefixing https:  to the node, pod, or service name in the API URL,\\nbut they will not validate the certificate provided by the HTTPS endpoint nor provide client\\ncredentials. So while the connection will be encrypted, it will not provide any guarantees of\\nintegrity. These connections are not currently safe  to run over untrusted or public networks.\\nSSH tunnels\\nKubernetes supports SSH tunnels  to protect the control plane to nodes communication paths. In\\nthis configuration, the API server initiates an SSH tunnel to each node in the cluster\\n(connecting to the SSH server listening on port 22) and passes all traffic destined for a kubelet,\\nnode, pod, or service through the tunnel. This tunnel ensures that the traffic is not exposed\\noutside of the network in which the nodes are running.\\nNote:  SSH tunnels are currently deprecated, so you shouldn't opt to use them unless you know\", metadata={'source': './PDFS/Concepts.pdf', 'page': 49}),\n",
       " Document(page_content=\"Note:  SSH tunnels are currently deprecated, so you shouldn't opt to use them unless you know\\nwhat you are doing. The Konnectivity service  is a replacement for this communication channel.\\nKonnectivity service\\nFEATURE STATE:  Kubernetes v1.18 [beta]\\nAs a replacement to the SSH tunnels, the Konnectivity service provides TCP level proxy for the\\ncontrol plane to cluster communication. The Konnectivity service consists of two parts: the\\nKonnectivity server in the control plane network and the Konnectivity agents in the nodes\\nnetwork. The Konnectivity agents initiate connections to the Konnectivity server and maintain\\nthe network connections. After enabling the Konnectivity service, all control plane to nodes\\ntraffic goes through these connections.\\nFollow the Konnectivity service task  to set up the Konnectivity service in your cluster.\\nWhat's next\\nRead about the Kubernetes control plane components\\nLearn more about Hubs and Spoke model\\nLearn how to Secure a Cluster\", metadata={'source': './PDFS/Concepts.pdf', 'page': 49}),\n",
       " Document(page_content='Learn more about Hubs and Spoke model\\nLearn how to Secure a Cluster\\nLearn more about the Kubernetes API• \\n• \\n• \\n•', metadata={'source': './PDFS/Concepts.pdf', 'page': 49}),\n",
       " Document(page_content=\"Set up Konnectivity service\\nUse Port Forwarding to Access Applications in a Cluster\\nLearn how to Fetch logs for Pods , use kubectl port-forward\\nControllers\\nIn robotics and automation, a control loop  is a non-terminating loop that regulates the state of a\\nsystem.\\nHere is one example of a control loop: a thermostat in a room.\\nWhen you set the temperature, that's telling the thermostat about your desired state . The actual\\nroom temperature is the current state . The thermostat acts to bring the current state closer to\\nthe desired state, by turning equipment on or off.\\nIn Kubernetes, controllers are control loops that watch the state of your cluster , then make or\\nrequest changes where needed. Each controller tries to move the current cluster state closer to\\nthe desired state.\\nController pattern\\nA controller tracks at least one Kubernetes resource type. These objects  have a spec field that\\nrepresents the desired state. The controller(s) for that resource are responsible for making the\", metadata={'source': './PDFS/Concepts.pdf', 'page': 50}),\n",
       " Document(page_content=\"represents the desired state. The controller(s) for that resource are responsible for making the\\ncurrent state come closer to that desired state.\\nThe controller might carry the action out itself; more commonly, in Kubernetes, a controller\\nwill send messages to the API server  that have useful side effects. You'll see examples of this\\nbelow.\\nControl via API server\\nThe Job controller is an example of a Kubernetes built-in controller. Built-in controllers manage\\nstate by interacting with the cluster API server.\\nJob is a Kubernetes resource that runs a Pod, or perhaps several Pods, to carry out a task and\\nthen stop.\\n(Once scheduled , Pod objects become part of the desired state for a kubelet).\\nWhen the Job controller sees a new task it makes sure that, somewhere in your cluster, the\\nkubelets on a set of Nodes are running the right number of Pods to get the work done. The Job\\ncontroller does not run any Pods or containers itself. Instead, the Job controller tells the API\", metadata={'source': './PDFS/Concepts.pdf', 'page': 50}),\n",
       " Document(page_content='controller does not run any Pods or containers itself. Instead, the Job controller tells the API\\nserver to create or remove Pods. Other components in the control plane  act on the new\\ninformation (there are new Pods to schedule and run), and eventually the work is done.\\nAfter you create a new Job, the desired state is for that Job to be completed. The Job controller\\nmakes the current state for that Job be nearer to your desired state: creating Pods that do the\\nwork you wanted for that Job, so that the Job is closer to completion.\\nControllers also update the objects that configure them. For example: once the work is done for\\na Job, the Job controller updates that Job object to mark it Finished .• \\n• \\n•', metadata={'source': './PDFS/Concepts.pdf', 'page': 50}),\n",
       " Document(page_content=\"(This is a bit like how some thermostats turn a light off to indicate that your room is now at the\\ntemperature you set).\\nDirect control\\nIn contrast with Job, some controllers need to make changes to things outside of your cluster.\\nFor example, if you use a control loop to make sure there are enough Nodes  in your cluster,\\nthen that controller needs something outside the current cluster to set up new Nodes when\\nneeded.\\nControllers that interact with external state find their desired state from the API server, then\\ncommunicate directly with an external system to bring the current state closer in line.\\n(There actually is a controller  that horizontally scales the nodes in your cluster.)\\nThe important point here is that the controller makes some changes to bring about your desired\\nstate, and then reports the current state back to your cluster's API server. Other control loops\\ncan observe that reported data and take their own actions.\", metadata={'source': './PDFS/Concepts.pdf', 'page': 51}),\n",
       " Document(page_content=\"can observe that reported data and take their own actions.\\nIn the thermostat example, if the room is very cold then a different controller might also turn\\non a frost protection heater. With Kubernetes clusters, the control plane indirectly works with\\nIP address management tools, storage services, cloud provider APIs, and other services by \\nextending Kubernetes  to implement that.\\nDesired versus current state\\nKubernetes takes a cloud-native view of systems, and is able to handle constant change.\\nYour cluster could be changing at any point as work happens and control loops automatically\\nfix failures. This means that, potentially, your cluster never reaches a stable state.\\nAs long as the controllers for your cluster are running and able to make useful changes, it\\ndoesn't matter if the overall state is stable or not.\\nDesign\\nAs a tenet of its design, Kubernetes uses lots of controllers that each manage a particular aspect\", metadata={'source': './PDFS/Concepts.pdf', 'page': 51}),\n",
       " Document(page_content=\"As a tenet of its design, Kubernetes uses lots of controllers that each manage a particular aspect\\nof cluster state. Most commonly, a particular control loop (controller) uses one kind of resource\\nas its desired state, and has a different kind of resource that it manages to make that desired\\nstate happen. For example, a controller for Jobs tracks Job objects (to discover new work) and\\nPod objects (to run the Jobs, and then to see when the work is finished). In this case something\\nelse creates the Jobs, whereas the Job controller creates Pods.\\nIt's useful to have simple controllers rather than one, monolithic set of control loops that are\\ninterlinked. Controllers can fail, so Kubernetes is designed to allow for that.\\nNote:\\nThere can be several controllers that create or update the same kind of object. Behind the\\nscenes, Kubernetes controllers make sure that they only pay attention to the resources linked to\\ntheir controlling resource.\", metadata={'source': './PDFS/Concepts.pdf', 'page': 51}),\n",
       " Document(page_content='For example, you can have Deployments and Jobs; these both create Pods. The Job controller\\ndoes not delete the Pods that your Deployment created, because there is information ( labels ) the\\ncontrollers can use to tell those Pods apart.\\nWays of running controllers\\nKubernetes comes with a set of built-in controllers that run inside the kube-controller-manager .\\nThese built-in controllers provide important core behaviors.\\nThe Deployment controller and Job controller are examples of controllers that come as part of\\nKubernetes itself (\"built-in\" controllers). Kubernetes lets you run a resilient control plane, so\\nthat if any of the built-in controllers were to fail, another part of the control plane will take\\nover the work.\\nYou can find controllers that run outside the control plane, to extend Kubernetes. Or, if you\\nwant, you can write a new controller yourself. You can run your own controller as a set of Pods,', metadata={'source': './PDFS/Concepts.pdf', 'page': 52}),\n",
       " Document(page_content=\"want, you can write a new controller yourself. You can run your own controller as a set of Pods,\\nor externally to Kubernetes. What fits best will depend on what that particular controller does.\\nWhat's next\\nRead about the Kubernetes control plane\\nDiscover some of the basic Kubernetes objects\\nLearn more about the Kubernetes API\\nIf you want to write your own controller, see Extension Patterns  in Extending\\nKubernetes.\\nLeases\\nDistributed systems often have a need for leases , which provide a mechanism to lock shared\\nresources and coordinate activity between members of a set. In Kubernetes, the lease concept is\\nrepresented by Lease  objects in the coordination.k8s.io  API Group , which are used for system-\\ncritical capabilities such as node heartbeats and component-level leader election.\\nNode heartbeats\\nKubernetes uses the Lease API to communicate kubelet node heartbeats to the Kubernetes API\\nserver. For every Node  , there is a Lease  object with a matching name in the kube-node-lease\", metadata={'source': './PDFS/Concepts.pdf', 'page': 52}),\n",
       " Document(page_content='server. For every Node  , there is a Lease  object with a matching name in the kube-node-lease\\nnamespace. Under the hood, every kubelet heartbeat is an update  request to this Lease  object,\\nupdating the spec.renewTime  field for the Lease. The Kubernetes control plane uses the time\\nstamp of this field to determine the availability of this Node .\\nSee Node Lease objects  for more details.\\nLeader election\\nKubernetes also uses Leases to ensure only one instance of a component is running at any\\ngiven time. This is used by control plane components like kube-controller-manager  and kube-\\nscheduler  in HA configurations, where only one instance of the component should be actively\\nrunning while the other instances are on stand-by.• \\n• \\n• \\n•', metadata={'source': './PDFS/Concepts.pdf', 'page': 52}),\n",
       " Document(page_content='API server identity\\nFEATURE STATE:  Kubernetes v1.26 [beta]\\nStarting in Kubernetes v1.26, each kube-apiserver  uses the Lease API to publish its identity to\\nthe rest of the system. While not particularly useful on its own, this provides a mechanism for\\nclients to discover how many instances of kube-apiserver  are operating the Kubernetes control\\nplane. Existence of kube-apiserver leases enables future capabilities that may require\\ncoordination between each kube-apiserver.\\nYou can inspect Leases owned by each kube-apiserver by checking for lease objects in the kube-\\nsystem  namespace with the name kube-apiserver-<sha256-hash> . Alternatively you can use the\\nlabel selector apiserver.kubernetes.io/identity=kube-apiserver :\\nkubectl -n kube-system get lease -l apiserver.kubernetes.io/identity =kube-apiserver\\nNAME                                        HOLDER                                                                           AGE\\napiserver-07a5ea9b9b072c4a5f3d1c3702', metadata={'source': './PDFS/Concepts.pdf', 'page': 53}),\n",
       " Document(page_content='apiserver-07a5ea9b9b072c4a5f3d1c3702        \\napiserver-07a5ea9b9b072c4a5f3d1c3702_0c8914f7-0f35-440e-8676-7844977d3a05        5m33s\\napiserver-7be9e061c59d368b3ddaf1376e        \\napiserver-7be9e061c59d368b3ddaf1376e_84f2a85d-37c1-4b14-b6b9-603e62e4896f        4m23s\\napiserver-1dfef752bcb36637d2763d1868        \\napiserver-1dfef752bcb36637d2763d1868_c5ffa286-8a9a-45d4-91e7-61118ed58d2e        4m43s\\nThe SHA256 hash used in the lease name is based on the OS hostname as seen by that API\\nserver. Each kube-apiserver should be configured to use a hostname that is unique within the\\ncluster. New instances of kube-apiserver that use the same hostname will take over existing\\nLeases using a new holder identity, as opposed to instantiating new Lease objects. You can\\ncheck the hostname used by kube-apisever by checking the value of the kubernetes.io/\\nhostname  label:\\nkubectl -n kube-system get lease apiserver-07a5ea9b9b072c4a5f3d1c3702 -o yaml\\napiVersion : coordination.k8s.io/v1\\nkind: Lease\\nmetadata :', metadata={'source': './PDFS/Concepts.pdf', 'page': 53}),\n",
       " Document(page_content='apiVersion : coordination.k8s.io/v1\\nkind: Lease\\nmetadata :\\n  creationTimestamp : \"2023-07-02T13:16:48Z\"\\n  labels :\\n    apiserver.kubernetes.io/identity : kube-apiserver\\n    kubernetes.io/hostname : master-1\\n  name : apiserver-07a5ea9b9b072c4a5f3d1c3702\\n  namespace : kube-system\\n  resourceVersion : \"334899\"\\n  uid: 90870ab5-1ba9-4523-b215-e4d4e662acb1\\nspec:\\n  holderIdentity : apiserver-07a5ea9b9b072c4a5f3d1c3702_0c8914f7-0f35-440e-8676-7844977d3a05\\n  leaseDurationSeconds : 3600\\n  renewTime : \"2023-07-04T21:58:48.065888Z\"\\nExpired leases from kube-apiservers that no longer exist are garbage collected by new kube-\\napiservers after 1 hour.\\nYou can disable API server identity leases by disabling the APIServerIdentity  feature gate .', metadata={'source': './PDFS/Concepts.pdf', 'page': 53}),\n",
       " Document(page_content=\"Workloads\\nYour own workload can define its own use of Leases. For example, you might run a custom \\ncontroller  where a primary or leader member performs operations that its peers do not. You\\ndefine a Lease so that the controller replicas can select or elect a leader, using the Kubernetes\\nAPI for coordination. If you do use a Lease, it's a good practice to define a name for the Lease\\nthat is obviously linked to the product or component. For example, if you have a component\\nnamed Example Foo, use a Lease named example-foo .\\nIf a cluster operator or another end user could deploy multiple instances of a component, select\\na name prefix and pick a mechanism (such as hash of the name of the Deployment) to avoid\\nname collisions for the Leases.\\nYou can use another approach so long as it achieves the same outcome: different software\\nproducts do not conflict with one another.\\nCloud Controller Manager\\nFEATURE STATE:  Kubernetes v1.11 [beta]\", metadata={'source': './PDFS/Concepts.pdf', 'page': 54}),\n",
       " Document(page_content=\"Cloud Controller Manager\\nFEATURE STATE:  Kubernetes v1.11 [beta]\\nCloud infrastructure technologies let you run Kubernetes on public, private, and hybrid clouds.\\nKubernetes believes in automated, API-driven infrastructure without tight coupling between\\ncomponents.\\nThe cloud-controller-manager is a Kubernetes control plane  component that embeds cloud-\\nspecific control logic. The cloud controller manager  lets you link your cluster into your cloud\\nprovider's API, and separates out the components that interact with that cloud platform from\\ncomponents that only interact with your cluster.\\nBy decoupling the interoperability logic between Kubernetes and the underlying cloud\\ninfrastructure, the cloud-controller-manager component enables cloud providers to release\\nfeatures at a different pace compared to the main Kubernetes project.\\nThe cloud-controller-manager is structured using a plugin mechanism that allows different\\ncloud providers to integrate their platforms with Kubernetes.\\nDesign\", metadata={'source': './PDFS/Concepts.pdf', 'page': 54}),\n",
       " Document(page_content='cloud providers to integrate their platforms with Kubernetes.\\nDesign\\nKubernetes components\\nThe cloud controller manager runs in the control plane as a replicated set of processes (usually,\\nthese are containers in Pods). Each cloud-controller-manager implements multiple controllers\\nin a single process.\\nNote:  You can also run the cloud controller manager as a Kubernetes addon  rather than as part\\nof the control plane.\\nCloud controller manager functions\\nThe controllers inside the cloud controller manager include:', metadata={'source': './PDFS/Concepts.pdf', 'page': 54}),\n",
       " Document(page_content=\"Node controller\\nThe node controller is responsible for updating Node  objects when new servers are created in\\nyour cloud infrastructure. The node controller obtains information about the hosts running\\ninside your tenancy with the cloud provider. The node controller performs the following\\nfunctions:\\nUpdate a Node object with the corresponding server's unique identifier obtained from the\\ncloud provider API.\\nAnnotating and labelling the Node object with cloud-specific information, such as the\\nregion the node is deployed into and the resources (CPU, memory, etc) that it has\\navailable.\\nObtain the node's hostname and network addresses.\\nVerifying the node's health. In case a node becomes unresponsive, this controller checks\\nwith your cloud provider's API to see if the server has been deactivated / deleted /\\nterminated. If the node has been deleted from the cloud, the controller deletes the Node\\nobject from your Kubernetes cluster.\", metadata={'source': './PDFS/Concepts.pdf', 'page': 55}),\n",
       " Document(page_content=\"object from your Kubernetes cluster.\\nSome cloud provider implementations split this into a node controller and a separate node\\nlifecycle controller.\\nRoute controller\\nThe route controller is responsible for configuring routes in the cloud appropriately so that\\ncontainers on different nodes in your Kubernetes cluster can communicate with each other.\\nDepending on the cloud provider, the route controller might also allocate blocks of IP addresses\\nfor the Pod network.\\nService controller\\nServices  integrate with cloud infrastructure components such as managed load balancers, IP\\naddresses, network packet filtering, and target health checking. The service controller interacts\\nwith your cloud provider's APIs to set up load balancers and other infrastructure components\\nwhen you declare a Service resource that requires them.\\nAuthorization\\nThis section breaks down the access that the cloud controller manager requires on various API\\nobjects, in order to perform its operations.\\nNode controller\", metadata={'source': './PDFS/Concepts.pdf', 'page': 55}),\n",
       " Document(page_content='objects, in order to perform its operations.\\nNode controller\\nThe Node controller only works with Node objects. It requires full access to read and modify\\nNode objects.\\nv1/Node :\\nget\\nlist\\ncreate\\nupdate1. \\n2. \\n3. \\n4. \\n• \\n• \\n• \\n•', metadata={'source': './PDFS/Concepts.pdf', 'page': 55}),\n",
       " Document(page_content='patch\\nwatch\\ndelete\\nRoute controller\\nThe route controller listens to Node object creation and configures routes appropriately. It\\nrequires Get access to Node objects.\\nv1/Node :\\nget\\nService controller\\nThe service controller watches for Service object create , update  and delete  events and then\\nconfigures Endpoints for those Services appropriately (for EndpointSlices, the kube-controller-\\nmanager manages these on demand).\\nTo access Services, it requires list, and watch  access. To update Services, it requires patch  and \\nupdate  access.\\nTo set up Endpoints resources for the Services, it requires access to create , list, get, watch ,\\nand update .\\nv1/Service :\\nlist\\nget\\nwatch\\npatch\\nupdate\\nOthers\\nThe implementation of the core of the cloud controller manager requires access to create Event\\nobjects, and to ensure secure operation, it requires access to create ServiceAccounts.\\nv1/Event :\\ncreate\\npatch\\nupdate\\nv1/ServiceAccount :\\ncreate', metadata={'source': './PDFS/Concepts.pdf', 'page': 56}),\n",
       " Document(page_content='v1/Event :\\ncreate\\npatch\\nupdate\\nv1/ServiceAccount :\\ncreate\\nThe RBAC  ClusterRole for the cloud controller manager looks like:\\napiVersion : rbac.authorization.k8s.io/v1\\nkind: ClusterRole\\nmetadata :\\n  name : cloud-controller-manager• \\n• \\n• \\n• \\n• \\n• \\n• \\n• \\n• \\n• \\n• \\n• \\n•', metadata={'source': './PDFS/Concepts.pdf', 'page': 56}),\n",
       " Document(page_content='rules :\\n- apiGroups :\\n  - \"\"\\n  resources :\\n  - events\\n  verbs :\\n  - create\\n  - patch\\n  - update\\n- apiGroups :\\n  - \"\"\\n  resources :\\n  - nodes\\n  verbs :\\n  - \\'*\\'\\n- apiGroups :\\n  - \"\"\\n  resources :\\n  - nodes/status\\n  verbs :\\n  - patch\\n- apiGroups :\\n  - \"\"\\n  resources :\\n  - services\\n  verbs :\\n  - list\\n  - patch\\n  - update\\n  - watch\\n- apiGroups :\\n  - \"\"\\n  resources :\\n  - serviceaccounts\\n  verbs :\\n  - create\\n- apiGroups :\\n  - \"\"\\n  resources :\\n  - persistentvolumes\\n  verbs :\\n  - get\\n  - list\\n  - update\\n  - watch\\n- apiGroups :\\n  - \"\"\\n  resources :\\n  - endpoints\\n  verbs :\\n  - create\\n  - get\\n  - list', metadata={'source': './PDFS/Concepts.pdf', 'page': 57}),\n",
       " Document(page_content=\"- watch\\n  - update\\nWhat's next\\nCloud Controller Manager Administration  has instructions on running and managing the\\ncloud controller manager.\\nTo upgrade a HA control plane to use the cloud controller manager, see Migrate\\nReplicated Control Plane To Use Cloud Controller Manager .\\nWant to know how to implement your own cloud controller manager, or extend an\\nexisting project?\\nThe cloud controller manager uses Go interfaces, specifically, CloudProvider\\ninterface defined in cloud.go  from kubernetes/cloud-provider  to allow\\nimplementations from any cloud to be plugged in.\\nThe implementation of the shared controllers highlighted in this document (Node,\\nRoute, and Service), and some scaffolding along with the shared cloudprovider\\ninterface, is part of the Kubernetes core. Implementations specific to cloud\\nproviders are outside the core of Kubernetes and implement the CloudProvider\\ninterface.\\nFor more information about developing plugins, see Developing Cloud Controller\\nManager .\", metadata={'source': './PDFS/Concepts.pdf', 'page': 58}),\n",
       " Document(page_content='interface.\\nFor more information about developing plugins, see Developing Cloud Controller\\nManager .\\nAbout cgroup v2\\nOn Linux, control groups  constrain resources that are allocated to processes.\\nThe kubelet  and the underlying container runtime need to interface with cgroups to enforce \\nresource management for pods and containers  which includes cpu/memory requests and limits\\nfor containerized workloads.\\nThere are two versions of cgroups in Linux: cgroup v1 and cgroup v2. cgroup v2 is the new\\ngeneration of the cgroup  API.\\nWhat is cgroup v2?\\nFEATURE STATE:  Kubernetes v1.25 [stable]\\ncgroup v2 is the next version of the Linux cgroup  API. cgroup v2 provides a unified control\\nsystem with enhanced resource management capabilities.\\ncgroup v2 offers several improvements over cgroup v1, such as the following:\\nSingle unified hierarchy design in API\\nSafer sub-tree delegation to containers\\nNewer features like Pressure Stall Information', metadata={'source': './PDFS/Concepts.pdf', 'page': 58}),\n",
       " Document(page_content='Safer sub-tree delegation to containers\\nNewer features like Pressure Stall Information\\nEnhanced resource allocation management and isolation across multiple resources\\nUnified accounting for different types of memory allocations (network memory,\\nkernel memory, etc)• \\n• \\n• \\n◦ \\n◦ \\n◦ \\n• \\n• \\n• \\n• \\n◦', metadata={'source': './PDFS/Concepts.pdf', 'page': 58}),\n",
       " Document(page_content='Accounting for non-immediate resource changes such as page cache write backs\\nSome Kubernetes features exclusively use cgroup v2 for enhanced resource management and\\nisolation. For example, the MemoryQoS  feature improves memory QoS and relies on cgroup v2\\nprimitives.\\nUsing cgroup v2\\nThe recommended way to use cgroup v2 is to use a Linux distribution that enables and uses\\ncgroup v2 by default.\\nTo check if your distribution uses cgroup v2, refer to Identify cgroup version on Linux nodes .\\nRequirements\\ncgroup v2 has the following requirements:\\nOS distribution enables cgroup v2\\nLinux Kernel version is 5.8 or later\\nContainer runtime supports cgroup v2. For example:\\ncontainerd  v1.4 and later\\ncri-o  v1.20 and later\\nThe kubelet and the container runtime are configured to use the systemd cgroup driver\\nLinux Distribution cgroup v2 support\\nFor a list of Linux distributions that use cgroup v2, refer to the cgroup v2 documentation\\nContainer Optimized OS (since M97)', metadata={'source': './PDFS/Concepts.pdf', 'page': 59}),\n",
       " Document(page_content=\"Container Optimized OS (since M97)\\nUbuntu (since 21.10, 22.04+ recommended)\\nDebian GNU/Linux (since Debian 11 bullseye)\\nFedora (since 31)\\nArch Linux (since April 2021)\\nRHEL and RHEL-like distributions (since 9)\\nTo check if your distribution is using cgroup v2, refer to your distribution's documentation or\\nfollow the instructions in Identify the cgroup version on Linux nodes .\\nYou can also enable cgroup v2 manually on your Linux distribution by modifying the kernel\\ncmdline boot arguments. If your distribution uses GRUB, systemd.unified_cgroup_hierarchy=1\\nshould be added in GRUB_CMDLINE_LINUX  under /etc/default/grub , followed by sudo update-\\ngrub . However, the recommended approach is to use a distribution that already enables cgroup\\nv2 by default.\\nMigrating to cgroup v2\\nTo migrate to cgroup v2, ensure that you meet the requirements , then upgrade to a kernel\\nversion that enables cgroup v2 by default.\\nThe kubelet automatically detects that the OS is running on cgroup v2 and performs\", metadata={'source': './PDFS/Concepts.pdf', 'page': 59}),\n",
       " Document(page_content='The kubelet automatically detects that the OS is running on cgroup v2 and performs\\naccordingly with no additional configuration required.◦ \\n• \\n• \\n• \\n◦ \\n◦ \\n• \\n• \\n• \\n• \\n• \\n• \\n•', metadata={'source': './PDFS/Concepts.pdf', 'page': 59}),\n",
       " Document(page_content='There should not be any noticeable difference in the user experience when switching to cgroup\\nv2, unless users are accessing the cgroup file system directly, either on the node or from within\\nthe containers.\\ncgroup v2 uses a different API than cgroup v1, so if there are any applications that directly\\naccess the cgroup file system, they need to be updated to newer versions that support cgroup\\nv2. For example:\\nSome third-party monitoring and security agents may depend on the cgroup filesystem.\\nUpdate these agents to versions that support cgroup v2.\\nIf you run cAdvisor  as a stand-alone DaemonSet for monitoring pods and containers,\\nupdate it to v0.43.0 or later.\\nIf you deploy Java applications, prefer to use versions which fully support cgroup v2:\\nOpenJDK / HotSpot : jdk8u372, 11.0.16, 15 and later\\nIBM Semeru Runtimes : 8.0.382.0, 11.0.20.0, 17.0.8.0, and later\\nIBM Java : 8.0.8.6 and later\\nIf you are using the uber-go/automaxprocs  package, make sure the version you use is', metadata={'source': './PDFS/Concepts.pdf', 'page': 60}),\n",
       " Document(page_content=\"If you are using the uber-go/automaxprocs  package, make sure the version you use is\\nv1.5.1 or higher.\\nIdentify the cgroup version on Linux Nodes\\nThe cgroup version depends on the Linux distribution being used and the default cgroup\\nversion configured on the OS. To check which cgroup version your distribution uses, run the \\nstat -fc %T /sys/fs/cgroup/  command on the node:\\nstat -fc %T /sys/fs/cgroup/\\nFor cgroup v2, the output is cgroup2fs .\\nFor cgroup v1, the output is tmpfs.\\nWhat's next\\nLearn more about cgroups\\nLearn more about container runtime\\nLearn more about cgroup drivers\\nContainer Runtime Interface (CRI)\\nThe CRI is a plugin interface which enables the kubelet to use a wide variety of container\\nruntimes, without having a need to recompile the cluster components.\\nYou need a working container runtime  on each Node in your cluster, so that the kubelet  can\\nlaunch Pods  and their containers.\\nThe Container Runtime Interface (CRI) is the main protocol for the communication between the\", metadata={'source': './PDFS/Concepts.pdf', 'page': 60}),\n",
       " Document(page_content='The Container Runtime Interface (CRI) is the main protocol for the communication between the\\nkubelet  and Container Runtime.\\nThe Kubernetes Container Runtime Interface (CRI) defines the main gRPC  protocol for the\\ncommunication between the node components  kubelet  and container runtime .• \\n• \\n• \\n◦ \\n◦ \\n◦ \\n• \\n• \\n• \\n•', metadata={'source': './PDFS/Concepts.pdf', 'page': 60}),\n",
       " Document(page_content=\"The API\\nFEATURE STATE:  Kubernetes v1.23 [stable]\\nThe kubelet acts as a client when connecting to the container runtime via gRPC. The runtime\\nand image service endpoints have to be available in the container runtime, which can be\\nconfigured separately within the kubelet by using the --image-service-endpoint  command line\\nflags .\\nFor Kubernetes v1.28, the kubelet prefers to use CRI v1. If a container runtime does not support \\nv1 of the CRI, then the kubelet tries to negotiate any older supported version. The v1.28 kubelet\\ncan also negotiate CRI v1alpha2 , but this version is considered as deprecated. If the kubelet\\ncannot negotiate a supported CRI version, the kubelet gives up and doesn't register as a node.\\nUpgrading\\nWhen upgrading Kubernetes, the kubelet tries to automatically select the latest CRI version on\\nrestart of the component. If that fails, then the fallback will take place as mentioned above. If a\", metadata={'source': './PDFS/Concepts.pdf', 'page': 61}),\n",
       " Document(page_content=\"restart of the component. If that fails, then the fallback will take place as mentioned above. If a\\ngRPC re-dial was required because the container runtime has been upgraded, then the\\ncontainer runtime must also support the initially selected version or the redial is expected to\\nfail. This requires a restart of the kubelet.\\nWhat's next\\nLearn more about the CRI protocol definition\\nGarbage Collection\\nGarbage collection is a collective term for the various mechanisms Kubernetes uses to clean up\\ncluster resources. This allows the clean up of resources like the following:\\nTerminated pods\\nCompleted Jobs\\nObjects without owner references\\nUnused containers and container images\\nDynamically provisioned PersistentVolumes with a StorageClass reclaim policy of Delete\\nStale or expired CertificateSigningRequests (CSRs)\\nNodes  deleted in the following scenarios:\\nOn a cloud when the cluster uses a cloud controller manager\\nOn-premises when the cluster uses an addon similar to a cloud controller manager\", metadata={'source': './PDFS/Concepts.pdf', 'page': 61}),\n",
       " Document(page_content='On-premises when the cluster uses an addon similar to a cloud controller manager\\nNode Lease objects\\nOwners and dependents\\nMany objects in Kubernetes link to each other through owner references . Owner references tell\\nthe control plane which objects are dependent on others. Kubernetes uses owner references to\\ngive the control plane, and other API clients, the opportunity to clean up related resources\\nbefore deleting an object. In most cases, Kubernetes manages owner references automatically.• \\n• \\n• \\n• \\n• \\n• \\n• \\n• \\n◦ \\n◦ \\n•', metadata={'source': './PDFS/Concepts.pdf', 'page': 61}),\n",
       " Document(page_content='Ownership is different from the labels and selectors  mechanism that some resources also use.\\nFor example, consider a Service  that creates EndpointSlice  objects. The Service uses labels  to\\nallow the control plane to determine which EndpointSlice  objects are used for that Service. In\\naddition to the labels, each EndpointSlice  that is managed on behalf of a Service has an owner\\nreference. Owner references help different parts of Kubernetes avoid interfering with objects\\nthey don’t control.\\nNote:\\nCross-namespace owner references are disallowed by design. Namespaced dependents can\\nspecify cluster-scoped or namespaced owners. A namespaced owner must  exist in the same\\nnamespace as the dependent. If it does not, the owner reference is treated as absent, and the\\ndependent is subject to deletion once all owners are verified absent.\\nCluster-scoped dependents can only specify cluster-scoped owners. In v1.20+, if a cluster-', metadata={'source': './PDFS/Concepts.pdf', 'page': 62}),\n",
       " Document(page_content=\"Cluster-scoped dependents can only specify cluster-scoped owners. In v1.20+, if a cluster-\\nscoped dependent specifies a namespaced kind as an owner, it is treated as having an\\nunresolvable owner reference, and is not able to be garbage collected.\\nIn v1.20+, if the garbage collector detects an invalid cross-namespace ownerReference , or a\\ncluster-scoped dependent with an ownerReference  referencing a namespaced kind, a warning\\nEvent with a reason of OwnerRefInvalidNamespace  and an involvedObject  of the invalid\\ndependent is reported. You can check for that kind of Event by running kubectl get events -A --\\nfield-selector=reason=OwnerRefInvalidNamespace .\\nCascading deletion\\nKubernetes checks for and deletes objects that no longer have owner references, like the pods\\nleft behind when you delete a ReplicaSet. When you delete an object, you can control whether\\nKubernetes deletes the object's dependents automatically, in a process called cascading deletion .\", metadata={'source': './PDFS/Concepts.pdf', 'page': 62}),\n",
       " Document(page_content=\"Kubernetes deletes the object's dependents automatically, in a process called cascading deletion .\\nThere are two types of cascading deletion, as follows:\\nForeground cascading deletion\\nBackground cascading deletion\\nYou can also control how and when garbage collection deletes resources that have owner\\nreferences using Kubernetes finalizers .\\nForeground cascading deletion\\nIn foreground cascading deletion, the owner object you're deleting first enters a deletion in\\nprogress  state. In this state, the following happens to the owner object:\\nThe Kubernetes API server sets the object's metadata.deletionTimestamp  field to the time\\nthe object was marked for deletion.\\nThe Kubernetes API server also sets the metadata.finalizers  field to foregroundDeletion .\\nThe object remains visible through the Kubernetes API until the deletion process is\\ncomplete.\\nAfter the owner object enters the deletion in progress state, the controller deletes the\", metadata={'source': './PDFS/Concepts.pdf', 'page': 62}),\n",
       " Document(page_content='complete.\\nAfter the owner object enters the deletion in progress state, the controller deletes the\\ndependents. After deleting all the dependent objects, the controller deletes the owner object. At\\nthis point, the object is no longer visible in the Kubernetes API.• \\n• \\n• \\n• \\n•', metadata={'source': './PDFS/Concepts.pdf', 'page': 62}),\n",
       " Document(page_content='During foreground cascading deletion, the only dependents that block owner deletion are those\\nthat have the ownerReference.blockOwnerDeletion=true  field. See Use foreground cascading\\ndeletion  to learn more.\\nBackground cascading deletion\\nIn background cascading deletion, the Kubernetes API server deletes the owner object\\nimmediately and the controller cleans up the dependent objects in the background. By default,\\nKubernetes uses background cascading deletion unless you manually use foreground deletion\\nor choose to orphan the dependent objects.\\nSee Use background cascading deletion  to learn more.\\nOrphaned dependents\\nWhen Kubernetes deletes an owner object, the dependents left behind are called orphan  objects.\\nBy default, Kubernetes deletes dependent objects. To learn how to override this behaviour, see \\nDelete owner objects and orphan dependents .\\nGarbage collection of unused containers and images', metadata={'source': './PDFS/Concepts.pdf', 'page': 63}),\n",
       " Document(page_content='Delete owner objects and orphan dependents .\\nGarbage collection of unused containers and images\\nThe kubelet  performs garbage collection on unused images every five minutes and on unused\\ncontainers every minute. You should avoid using external garbage collection tools, as these can\\nbreak the kubelet behavior and remove containers that should exist.\\nTo configure options for unused container and image garbage collection, tune the kubelet using\\na configuration file  and change the parameters related to garbage collection using the \\nKubeletConfiguration  resource type.\\nContainer image lifecycle\\nKubernetes manages the lifecycle of all images through its image manager , which is part of the\\nkubelet, with the cooperation of cadvisor . The kubelet considers the following disk usage limits\\nwhen making garbage collection decisions:\\nHighThresholdPercent\\nLowThresholdPercent\\nDisk usage above the configured HighThresholdPercent  value triggers garbage collection,', metadata={'source': './PDFS/Concepts.pdf', 'page': 63}),\n",
       " Document(page_content='Disk usage above the configured HighThresholdPercent  value triggers garbage collection,\\nwhich deletes images in order based on the last time they were used, starting with the oldest\\nfirst. The kubelet deletes images until disk usage reaches the LowThresholdPercent  value.\\nContainer garbage collection\\nThe kubelet garbage collects unused containers based on the following variables, which you can\\ndefine:\\nMinAge : the minimum age at which the kubelet can garbage collect a container. Disable\\nby setting to 0.\\nMaxPerPodContainer : the maximum number of dead containers each Pod can have.\\nDisable by setting to less than 0.• \\n• \\n• \\n•', metadata={'source': './PDFS/Concepts.pdf', 'page': 63}),\n",
       " Document(page_content='MaxContainers : the maximum number of dead containers the cluster can have. Disable\\nby setting to less than 0.\\nIn addition to these variables, the kubelet garbage collects unidentified and deleted containers,\\ntypically starting with the oldest first.\\nMaxPerPodContainer  and MaxContainers  may potentially conflict with each other in situations\\nwhere retaining the maximum number of containers per Pod ( MaxPerPodContainer ) would go\\noutside the allowable total of global dead containers ( MaxContainers ). In this situation, the\\nkubelet adjusts MaxPerPodContainer  to address the conflict. A worst-case scenario would be to\\ndowngrade MaxPerPodContainer  to 1 and evict the oldest containers. Additionally, containers\\nowned by pods that have been deleted are removed once they are older than MinAge .\\nNote:  The kubelet only garbage collects the containers it manages.\\nConfiguring garbage collection\\nYou can tune garbage collection of resources by configuring options specific to the controllers', metadata={'source': './PDFS/Concepts.pdf', 'page': 64}),\n",
       " Document(page_content=\"You can tune garbage collection of resources by configuring options specific to the controllers\\nmanaging those resources. The following pages show you how to configure garbage collection:\\nConfiguring cascading deletion of Kubernetes objects\\nConfiguring cleanup of finished Jobs\\nWhat's next\\nLearn more about ownership of Kubernetes objects .\\nLearn more about Kubernetes finalizers .\\nLearn about the TTL controller  that cleans up finished Jobs.\\nMixed Version Proxy\\nFEATURE STATE:  Kubernetes v1.28 [alpha]\\nKubernetes 1.28 includes an alpha feature that lets an API Server  proxy a resource requests to\\nother peer API servers. This is useful when there are multiple API servers running different\\nversions of Kubernetes in one cluster (for example, during a long-lived rollout to a new release\\nof Kubernetes).\\nThis enables cluster administrators to configure highly available clusters that can be upgraded\\nmore safely, by directing resource requests (made during the upgrade) to the correct kube-\", metadata={'source': './PDFS/Concepts.pdf', 'page': 64}),\n",
       " Document(page_content='more safely, by directing resource requests (made during the upgrade) to the correct kube-\\napiserver. That proxying prevents users from seeing unexpected 404 Not Found errors that stem\\nfrom the upgrade process.\\nThis mechanism is called the Mixed Version Proxy .\\nEnabling the Mixed Version Proxy\\nEnsure that UnknownVersionInteroperabilityProxy  feature gate  is enabled when you start the \\nAPI Server :• \\n• \\n• \\n• \\n• \\n•', metadata={'source': './PDFS/Concepts.pdf', 'page': 64}),\n",
       " Document(page_content='kube-apiserver \\\\\\n--feature-gates =UnknownVersionInteroperabilityProxy =true \\\\\\n# required command line arguments for this feature\\n--peer-ca-file =<path to kube-apiserver CA cert>\\n--proxy-client-cert-file =<path to aggregator proxy cert>,\\n--proxy-client-key-file =<path to aggregator proxy key>,\\n--requestheader-client-ca-file =<path to aggregator CA cert>,\\n# requestheader-allowed-names can be set to blank to allow any Common Name\\n--requestheader-allowed-names =<valid Common Names to verify proxy client cert against>,\\n# optional flags for this feature\\n--peer-advertise-ip =`IP of this kube-apiserver that should be used by peers to proxy requests `\\n--peer-advertise-port =`port of this kube-apiserver that should be used by peers to proxy \\nrequests `\\n# ...and other flags as usual\\nProxy transport and authentication between API servers\\nThe source kube-apiserver reuses the existing APIserver client authentication flags  --', metadata={'source': './PDFS/Concepts.pdf', 'page': 65}),\n",
       " Document(page_content=\"The source kube-apiserver reuses the existing APIserver client authentication flags  --\\nproxy-client-cert-file  and --proxy-client-key-file  to present its identity that will be\\nverified by its peer (the destination kube-apiserver). The destination API server verifies\\nthat peer connection based on the configuration you specify using the --requestheader-\\nclient-ca-file  command line argument.\\nTo authenticate the destination server's serving certs, you must configure a certificate\\nauthority bundle by specifying the --peer-ca-file  command line argument to the source\\nAPI server.\\nConfiguration for peer API server connectivity\\nTo set the network location of a kube-apiserver that peers will use to proxy requests, use the --\\npeer-advertise-ip  and --peer-advertise-port  command line arguments to kube-apiserver or\\nspecify these fields in the API server configuration file. If these flags are unspecified, peers will\", metadata={'source': './PDFS/Concepts.pdf', 'page': 65}),\n",
       " Document(page_content='use the value from either --advertise-address  or --bind-address  command line argument to the\\nkube-apiserver. If those too, are unset, the host\\'s default interface is used.\\nMixed version proxying\\nWhen you enable mixed version proxying, the aggregation layer  loads a special filter that does\\nthe following:\\nWhen a resource request reaches an API server that cannot serve that API (either because\\nit is at a version pre-dating the introduction of the API or the API is turned off on the API\\nserver) the API server attempts to send the request to a peer API server that can serve the\\nrequested API. It does so by identifying API groups / versions / resources that the local\\nserver doesn\\'t recognise, and tries to proxy those requests to a peer API server that is\\ncapable of handling the request.\\nIf the peer API server fails to respond, the source  API server responds with 503 (\"Service\\nUnavailable\") error.• \\n• \\n• \\n•', metadata={'source': './PDFS/Concepts.pdf', 'page': 65}),\n",
       " Document(page_content='How it works under the hood\\nWhen an API Server receives a resource request, it first checks which API servers can serve the\\nrequested resource. This check happens using the internal StorageVersion  API.\\nIf the resource is known to the API server that received the request (for example, GET /\\napi/v1/pods/some-pod ), the request is handled locally.\\nIf there is no internal StorageVersion  object found for the requested resource (for\\nexample, GET /my-api/v1/my-resource ) and the configured APIService specifies proxying\\nto an extension API server, that proxying happens following the usual flow for extension\\nAPIs.\\nIf a valid internal StorageVersion  object is found for the requested resource (for example, \\nGET /batch/v1/jobs ) and the API server trying to handle the request (the handling API\\nserver ) has the batch  API disabled, then the handling API server  fetches the peer API\\nservers that do serve the relevant API group / version / resource ( api/v1/batch  in this', metadata={'source': './PDFS/Concepts.pdf', 'page': 66}),\n",
       " Document(page_content='servers that do serve the relevant API group / version / resource ( api/v1/batch  in this\\ncase) using the information in the fetched StorageVersion  object. The handling API server\\nthen proxies the request to one of the matching peer kube-apiservers that are aware of\\nthe requested resource.\\nIf there is no peer known for that API group / version / resource, the handling API\\nserver passes the request to its own handler chain which should eventually return a\\n404 (\"Not Found\") response.\\nIf the handling API server has identified and selected a peer API server, but that\\npeer fails to respond (for reasons such as network connectivity issues, or a data\\nrace between the request being received and a controller registering the peer\\'s info\\ninto the control plane), then the handling API server responds with a 503 (\"Service\\nUnavailable\") error.\\nContainers\\nTechnology for packaging an application along with its runtime dependencies.', metadata={'source': './PDFS/Concepts.pdf', 'page': 66}),\n",
       " Document(page_content='Containers\\nTechnology for packaging an application along with its runtime dependencies.\\nEach container that you run is repeatable; the standardization from having dependencies\\nincluded means that you get the same behavior wherever you run it.\\nContainers decouple applications from the underlying host infrastructure. This makes\\ndeployment easier in different cloud or OS environments.\\nEach node  in a Kubernetes cluster runs the containers that form the Pods  assigned to that node.\\nContainers in a Pod are co-located and co-scheduled to run on the same node.\\nContainer images\\nA container image  is a ready-to-run software package containing everything needed to run an\\napplication: the code and any runtime it requires, application and system libraries, and default\\nvalues for any essential settings.\\nContainers are intended to be stateless and immutable : you should not change the code of a\\ncontainer that is already running. If you have a containerized application and want to make• \\n• \\n• \\n◦ \\n◦', metadata={'source': './PDFS/Concepts.pdf', 'page': 66}),\n",
       " Document(page_content='changes, the correct process is to build a new image that includes the change, then recreate the\\ncontainer to start from the updated image.\\nContainer runtimes\\nA fundamental component that empowers Kubernetes to run containers effectively. It is\\nresponsible for managing the execution and lifecycle of containers within the Kubernetes\\nenvironment.\\nKubernetes supports container runtimes such as containerd , CRI-O , and any other\\nimplementation of the Kubernetes CRI (Container Runtime Interface) .\\nUsually, you can allow your cluster to pick the default container runtime for a Pod. If you need\\nto use more than one container runtime in your cluster, you can specify the RuntimeClass  for a\\nPod to make sure that Kubernetes runs those containers using a particular container runtime.\\nYou can also use RuntimeClass to run different Pods with the same container runtime but with\\ndifferent settings.\\nContainer Environment\\nContainer Lifecycle Hooks\\nImages', metadata={'source': './PDFS/Concepts.pdf', 'page': 67}),\n",
       " Document(page_content='different settings.\\nContainer Environment\\nContainer Lifecycle Hooks\\nImages\\nA container image represents binary data that encapsulates an application and all its software\\ndependencies. Container images are executable software bundles that can run standalone and\\nthat make very well defined assumptions about their runtime environment.\\nYou typically create a container image of your application and push it to a registry before\\nreferring to it in a Pod.\\nThis page provides an outline of the container image concept.\\nNote:  If you are looking for the container images for a Kubernetes release (such as v1.28, the\\nlatest minor release), visit Download Kubernetes .\\nImage names\\nContainer images are usually given a name such as pause , example/mycontainer , or kube-\\napiserver . Images can also include a registry hostname; for example: fictional.registry.example/\\nimagename , and possibly a port number as well; for example: fictional.registry.example:10443/\\nimagename .', metadata={'source': './PDFS/Concepts.pdf', 'page': 67}),\n",
       " Document(page_content=\"imagename .\\nIf you don't specify a registry hostname, Kubernetes assumes that you mean the Docker public\\nregistry.\\nAfter the image name part you can add a tag (in the same way you would when using with\\ncommands like docker  or podman ). Tags let you identify different versions of the same series of\\nimages.\", metadata={'source': './PDFS/Concepts.pdf', 'page': 67}),\n",
       " Document(page_content=\"Image tags consist of lowercase and uppercase letters, digits, underscores ( _), periods ( .), and\\ndashes ( -).\\nThere are additional rules about where you can place the separator characters ( _, -, and .) inside\\nan image tag.\\nIf you don't specify a tag, Kubernetes assumes you mean the tag latest .\\nUpdating images\\nWhen you first create a Deployment , StatefulSet , Pod, or other object that includes a Pod\\ntemplate, then by default the pull policy of all containers in that pod will be set to IfNotPresent\\nif it is not explicitly specified. This policy causes the kubelet  to skip pulling an image if it\\nalready exists.\\nImage pull policy\\nThe imagePullPolicy  for a container and the tag of the image affect when the kubelet  attempts\\nto pull (download) the specified image.\\nHere's a list of the values you can set for imagePullPolicy  and the effects these values have:\\nIfNotPresent\\nthe image is pulled only if it is not already present locally.\\nAlways\", metadata={'source': './PDFS/Concepts.pdf', 'page': 68}),\n",
       " Document(page_content=\"IfNotPresent\\nthe image is pulled only if it is not already present locally.\\nAlways\\nevery time the kubelet launches a container, the kubelet queries the container image\\nregistry to resolve the name to an image digest . If the kubelet has a container image with\\nthat exact digest cached locally, the kubelet uses its cached image; otherwise, the kubelet\\npulls the image with the resolved digest, and uses that image to launch the container.\\nNever\\nthe kubelet does not try fetching the image. If the image is somehow already present\\nlocally, the kubelet attempts to start the container; otherwise, startup fails. See pre-pulled\\nimages  for more details.\\nThe caching semantics of the underlying image provider make even imagePullPolicy: Always\\nefficient, as long as the registry is reliably accessible. Your container runtime can notice that the\\nimage layers already exist on the node so that they don't need to be downloaded again.\\nNote:\", metadata={'source': './PDFS/Concepts.pdf', 'page': 68}),\n",
       " Document(page_content=\"image layers already exist on the node so that they don't need to be downloaded again.\\nNote:\\nYou should avoid using the :latest  tag when deploying containers in production as it is harder to\\ntrack which version of the image is running and more difficult to roll back properly.\\nInstead, specify a meaningful tag such as v1.42.0  and/or a digest.\\nTo make sure the Pod always uses the same version of a container image, you can specify the\\nimage's digest; replace <image-name>:<tag>  with <image-name>@<digest>  (for example, \\nimage@sha256:45b23dee08af5e43a7fea6c4cf9c25ccf269ee113168c19722f87876677c5cb2 ).\\nWhen using image tags, if the image registry were to change the code that the tag on that\\nimage represents, you might end up with a mix of Pods running the old and new code. An\\nimage digest uniquely identifies a specific version of the image, so Kubernetes runs the same\\ncode every time it starts a container with that image name and digest specified. Specifying an\", metadata={'source': './PDFS/Concepts.pdf', 'page': 68}),\n",
       " Document(page_content='code every time it starts a container with that image name and digest specified. Specifying an\\nimage by digest fixes the code that you run so that a change at the registry cannot lead to that\\nmix of versions.', metadata={'source': './PDFS/Concepts.pdf', 'page': 68}),\n",
       " Document(page_content=\"There are third-party admission controllers  that mutate Pods (and pod templates) when they are\\ncreated, so that the running workload is defined based on an image digest rather than a tag.\\nThat might be useful if you want to make sure that all your workload is running the same code\\nno matter what tag changes happen at the registry.\\nDefault image pull policy\\nWhen you (or a controller) submit a new Pod to the API server, your cluster sets the \\nimagePullPolicy  field when specific conditions are met:\\nif you omit the imagePullPolicy  field, and you specify the digest for the container image,\\nthe imagePullPolicy  is automatically set to IfNotPresent .\\nif you omit the imagePullPolicy  field, and the tag for the container image is :latest , \\nimagePullPolicy  is automatically set to Always ;\\nif you omit the imagePullPolicy  field, and you don't specify the tag for the container\\nimage, imagePullPolicy  is automatically set to Always ;\", metadata={'source': './PDFS/Concepts.pdf', 'page': 69}),\n",
       " Document(page_content=\"image, imagePullPolicy  is automatically set to Always ;\\nif you omit the imagePullPolicy  field, and you specify the tag for the container image that\\nisn't :latest , the imagePullPolicy  is automatically set to IfNotPresent .\\nNote:\\nThe value of imagePullPolicy  of the container is always set when the object is first created , and\\nis not updated if the image's tag or digest later changes.\\nFor example, if you create a Deployment with an image whose tag is not :latest , and later\\nupdate that Deployment's image to a :latest  tag, the imagePullPolicy  field will not change to \\nAlways . You must manually change the pull policy of any object after its initial creation.\\nRequired image pull\\nIf you would like to always force a pull, you can do one of the following:\\nSet the imagePullPolicy  of the container to Always .\\nOmit the imagePullPolicy  and use :latest  as the tag for the image to use; Kubernetes will\\nset the policy to Always  when you submit the Pod.\", metadata={'source': './PDFS/Concepts.pdf', 'page': 69}),\n",
       " Document(page_content='set the policy to Always  when you submit the Pod.\\nOmit the imagePullPolicy  and the tag for the image to use; Kubernetes will set the policy\\nto Always  when you submit the Pod.\\nEnable the AlwaysPullImages  admission controller.\\nImagePullBackOff\\nWhen a kubelet starts creating containers for a Pod using a container runtime, it might be\\npossible the container is in Waiting  state because of ImagePullBackOff .\\nThe status ImagePullBackOff  means that a container could not start because Kubernetes could\\nnot pull a container image (for reasons such as invalid image name, or pulling from a private\\nregistry without imagePullSecret ). The BackOff  part indicates that Kubernetes will keep trying\\nto pull the image, with an increasing back-off delay.\\nKubernetes raises the delay between each attempt until it reaches a compiled-in limit, which is\\n300 seconds (5 minutes).• \\n• \\n• \\n• \\n• \\n• \\n• \\n•', metadata={'source': './PDFS/Concepts.pdf', 'page': 69}),\n",
       " Document(page_content='Serial and parallel image pulls\\nBy default, kubelet pulls images serially. In other words, kubelet sends only one image pull\\nrequest to the image service at a time. Other image pull requests have to wait until the one\\nbeing processed is complete.\\nNodes make image pull decisions in isolation. Even when you use serialized image pulls, two\\ndifferent nodes can pull the same image in parallel.\\nIf you would like to enable parallel image pulls, you can set the field serializeImagePulls  to false\\nin the kubelet configuration . With serializeImagePulls  set to false, image pull requests will be\\nsent to the image service immediately, and multiple images will be pulled at the same time.\\nWhen enabling parallel image pulls, please make sure the image service of your container\\nruntime can handle parallel image pulls.\\nThe kubelet never pulls multiple images in parallel on behalf of one Pod. For example, if you', metadata={'source': './PDFS/Concepts.pdf', 'page': 70}),\n",
       " Document(page_content='The kubelet never pulls multiple images in parallel on behalf of one Pod. For example, if you\\nhave a Pod that has an init container and an application container, the image pulls for the two\\ncontainers will not be parallelized. However, if you have two Pods that use different images, the\\nkubelet pulls the images in parallel on behalf of the two different Pods, when parallel image\\npulls is enabled.\\nMaximum parallel image pulls\\nFEATURE STATE:  Kubernetes v1.27 [alpha]\\nWhen serializeImagePulls  is set to false, the kubelet defaults to no limit on the maximum\\nnumber of images being pulled at the same time. If you would like to limit the number of\\nparallel image pulls, you can set the field maxParallelImagePulls  in kubelet configuration. With \\nmaxParallelImagePulls  set to n, only n images can be pulled at the same time, and any image\\npull beyond n will have to wait until at least one ongoing image pull is complete.', metadata={'source': './PDFS/Concepts.pdf', 'page': 70}),\n",
       " Document(page_content='pull beyond n will have to wait until at least one ongoing image pull is complete.\\nLimiting the number parallel image pulls would prevent image pulling from consuming too\\nmuch network bandwidth or disk I/O, when parallel image pulling is enabled.\\nYou can set maxParallelImagePulls  to a positive number that is greater than or equal to 1. If you\\nset maxParallelImagePulls  to be greater than or equal to 2, you must set the serializeImagePulls\\nto false. The kubelet will fail to start with invalid maxParallelImagePulls  settings.\\nMulti-architecture images with image indexes\\nAs well as providing binary images, a container registry can also serve a container image index .\\nAn image index can point to multiple image manifests  for architecture-specific versions of a\\ncontainer. The idea is that you can have a name for an image (for example: pause , example/\\nmycontainer , kube-apiserver ) and allow different systems to fetch the right binary image for\\nthe machine architecture they are using.', metadata={'source': './PDFS/Concepts.pdf', 'page': 70}),\n",
       " Document(page_content='the machine architecture they are using.\\nKubernetes itself typically names container images with a suffix -$(ARCH) . For backward\\ncompatibility, please generate the older images with suffixes. The idea is to generate say pause\\nimage which has the manifest for all the arch(es) and say pause-amd64  which is backwards\\ncompatible for older configurations or YAML files which may have hard coded the images with\\nsuffixes.', metadata={'source': './PDFS/Concepts.pdf', 'page': 70}),\n",
       " Document(page_content=\"Using a private registry\\nPrivate registries may require keys to read images from them.\\nCredentials can be provided in several ways:\\nConfiguring Nodes to Authenticate to a Private Registry\\nall pods can read any configured private registries\\nrequires node configuration by cluster administrator\\nKubelet Credential Provider to dynamically fetch credentials for private registries\\nkubelet can be configured to use credential provider exec plugin for the respective\\nprivate registry.\\nPre-pulled Images\\nall pods can use any images cached on a node\\nrequires root access to all nodes to set up\\nSpecifying ImagePullSecrets on a Pod\\nonly pods which provide own keys can access the private registry\\nVendor-specific or local extensions\\nif you're using a custom node configuration, you (or your cloud provider) can\\nimplement your mechanism for authenticating the node to the container registry.\\nThese options are explained in more detail below.\\nConfiguring nodes to authenticate to a private registry\", metadata={'source': './PDFS/Concepts.pdf', 'page': 71}),\n",
       " Document(page_content=\"Configuring nodes to authenticate to a private registry\\nSpecific instructions for setting credentials depends on the container runtime and registry you\\nchose to use. You should refer to your solution's documentation for the most accurate\\ninformation.\\nFor an example of configuring a private container image registry, see the Pull an Image from a\\nPrivate Registry  task. That example uses a private registry in Docker Hub.\\nKubelet credential provider for authenticated image pulls\\nNote:  This approach is especially suitable when kubelet needs to fetch registry credentials\\ndynamically. Most commonly used for registries provided by cloud providers where auth tokens\\nare short-lived.\\nYou can configure the kubelet to invoke a plugin binary to dynamically fetch registry\\ncredentials for a container image. This is the most robust and versatile way to fetch credentials\\nfor private registries, but also requires kubelet-level configuration to enable.\", metadata={'source': './PDFS/Concepts.pdf', 'page': 71}),\n",
       " Document(page_content='for private registries, but also requires kubelet-level configuration to enable.\\nSee Configure a kubelet image credential provider  for more details.\\nInterpretation of config.json\\nThe interpretation of config.json  varies between the original Docker implementation and the\\nKubernetes interpretation. In Docker, the auths  keys can only specify root URLs, whereas\\nKubernetes allows glob URLs as well as prefix-matched paths. The only limitation is that glob\\npatterns ( *) have to include the dot ( .) for each subdomain. The amount of matched subdomains\\nhas to be equal to the amount of glob patterns ( *.), for example:\\n*.kubernetes.io  will not match kubernetes.io , but abc.kubernetes.io• \\n◦ \\n◦ \\n• \\n◦ \\n• \\n◦ \\n◦ \\n• \\n◦ \\n• \\n◦ \\n•', metadata={'source': './PDFS/Concepts.pdf', 'page': 71}),\n",
       " Document(page_content='*.*.kubernetes.io  will not match abc.kubernetes.io , but abc.def.kubernetes.io\\nprefix.*.io  will match prefix.kubernetes.io\\n*-good.kubernetes.io  will match prefix-good.kubernetes.io\\nThis means that a config.json  like this is valid:\\n{\\n    \"auths\" : {\\n        \"my-registry.io/images\" : { \"auth\" : \"...\" },\\n        \"*.my-registry.io/images\" : { \"auth\" : \"...\" }\\n    }\\n}\\nImage pull operations would now pass the credentials to the CRI container runtime for every\\nvalid pattern. For example the following container image names would match successfully:\\nmy-registry.io/images\\nmy-registry.io/images/my-image\\nmy-registry.io/images/another-image\\nsub.my-registry.io/images/my-image\\nBut not:\\na.sub.my-registry.io/images/my-image\\na.b.sub.my-registry.io/images/my-image\\nThe kubelet performs image pulls sequentially for every found credential. This means, that\\nmultiple entries in config.json  for different paths are possible, too:\\n{\\n    \"auths\" : {\\n        \"my-registry.io/images\" : {', metadata={'source': './PDFS/Concepts.pdf', 'page': 72}),\n",
       " Document(page_content='{\\n    \"auths\" : {\\n        \"my-registry.io/images\" : {\\n            \"auth\" : \"...\"\\n        },\\n        \"my-registry.io/images/subpath\" : {\\n            \"auth\" : \"...\"\\n        }\\n    }\\n}\\nIf now a container specifies an image my-registry.io/images/subpath/my-image  to be pulled,\\nthen the kubelet will try to download them from both authentication sources if one of them\\nfails.\\nPre-pulled images\\nNote:  This approach is suitable if you can control node configuration. It will not work reliably if\\nyour cloud provider manages nodes and replaces them automatically.\\nBy default, the kubelet tries to pull each image from the specified registry. However, if the \\nimagePullPolicy  property of the container is set to IfNotPresent  or Never , then a local image is\\nused (preferentially or exclusively, respectively).• \\n• \\n• \\n• \\n• \\n• \\n• \\n• \\n•', metadata={'source': './PDFS/Concepts.pdf', 'page': 72}),\n",
       " Document(page_content='If you want to rely on pre-pulled images as a substitute for registry authentication, you must\\nensure all nodes in the cluster have the same pre-pulled images.\\nThis can be used to preload certain images for speed or as an alternative to authenticating to a\\nprivate registry.\\nAll pods will have read access to any pre-pulled images.\\nSpecifying imagePullSecrets on a Pod\\nNote:  This is the recommended approach to run containers based on images in private\\nregistries.\\nKubernetes supports specifying container image registry keys on a Pod. imagePullSecrets  must\\nall be in the same namespace as the Pod. The referenced Secrets must be of type kubernetes.io/\\ndockercfg  or kubernetes.io/dockerconfigjson .\\nCreating a Secret with a Docker config\\nYou need to know the username, registry password and client email address for authenticating\\nto the registry, as well as its hostname. Run the following command, substituting the\\nappropriate uppercase values:\\nkubectl create secret docker-registry <name> \\\\', metadata={'source': './PDFS/Concepts.pdf', 'page': 73}),\n",
       " Document(page_content='appropriate uppercase values:\\nkubectl create secret docker-registry <name> \\\\\\n  --docker-server =DOCKER_REGISTRY_SERVER \\\\\\n  --docker-username =DOCKER_USER \\\\\\n  --docker-password =DOCKER_PASSWORD \\\\\\n  --docker-email =DOCKER_EMAIL\\nIf you already have a Docker credentials file then, rather than using the above command, you\\ncan import the credentials file as a Kubernetes Secrets .\\nCreate a Secret based on existing Docker credentials  explains how to set this up.\\nThis is particularly useful if you are using multiple private container registries, as \\nkubectl create secret docker-registry  creates a Secret that only works with a single private\\nregistry.\\nNote:  Pods can only reference image pull secrets in their own namespace, so this process needs\\nto be done one time per namespace.\\nReferring to an imagePullSecrets on a Pod\\nNow, you can create pods which reference that secret by adding an imagePullSecrets  section to', metadata={'source': './PDFS/Concepts.pdf', 'page': 73}),\n",
       " Document(page_content='Now, you can create pods which reference that secret by adding an imagePullSecrets  section to\\na Pod definition. Each item in the imagePullSecrets  array can only reference a Secret in the\\nsame namespace.\\nFor example:\\ncat <<EOF > pod.yaml\\napiVersion: v1\\nkind: Pod\\nmetadata:\\n  name: foo', metadata={'source': './PDFS/Concepts.pdf', 'page': 73}),\n",
       " Document(page_content='namespace: awesomeapps\\nspec:\\n  containers:\\n    - name: foo\\n      image: janedoe/awesomeapp:v1\\n  imagePullSecrets:\\n    - name: myregistrykey\\nEOF\\ncat <<EOF >> ./kustomization.yaml\\nresources:\\n- pod.yaml\\nEOF\\nThis needs to be done for each pod that is using a private registry.\\nHowever, setting of this field can be automated by setting the imagePullSecrets in a \\nServiceAccount  resource.\\nCheck Add ImagePullSecrets to a Service Account  for detailed instructions.\\nYou can use this in conjunction with a per-node .docker/config.json . The credentials will be\\nmerged.\\nUse cases\\nThere are a number of solutions for configuring private registries. Here are some common use\\ncases and suggested solutions.\\nCluster running only non-proprietary (e.g. open-source) images. No need to hide images.\\nUse public images from a public registry\\nNo configuration required.\\nSome cloud providers automatically cache or mirror public images, which\\nimproves availability and reduces the time to pull images.', metadata={'source': './PDFS/Concepts.pdf', 'page': 74}),\n",
       " Document(page_content='improves availability and reduces the time to pull images.\\nCluster running some proprietary images which should be hidden to those outside the\\ncompany, but visible to all cluster users.\\nUse a hosted private registry\\nManual configuration may be required on the nodes that need to access to\\nprivate registry\\nOr, run an internal private registry behind your firewall with open read access.\\nNo Kubernetes configuration is required.\\nUse a hosted container image registry service that controls image access\\nIt will work better with cluster autoscaling than manual node configuration.\\nOr, on a cluster where changing the node configuration is inconvenient, use \\nimagePullSecrets .\\nCluster with proprietary images, a few of which require stricter access control.\\nEnsure AlwaysPullImages admission controller  is active. Otherwise, all Pods\\npotentially have access to all images.\\nMove sensitive data into a \"Secret\" resource, instead of packaging it in an image.', metadata={'source': './PDFS/Concepts.pdf', 'page': 74}),\n",
       " Document(page_content='Move sensitive data into a \"Secret\" resource, instead of packaging it in an image.\\nA multi-tenant cluster where each tenant needs own private registry.\\nEnsure AlwaysPullImages admission controller  is active. Otherwise, all Pods of all\\ntenants potentially have access to all images.\\nRun a private registry with authorization required.1. \\n◦ \\n▪ \\n▪ \\n2. \\n◦ \\n▪ \\n◦ \\n▪ \\n◦ \\n▪ \\n◦ \\n3. \\n◦ \\n◦ \\n4. \\n◦ \\n◦', metadata={'source': './PDFS/Concepts.pdf', 'page': 74}),\n",
       " Document(page_content='Generate registry credential for each tenant, put into secret, and populate secret to\\neach tenant namespace.\\nThe tenant adds that secret to imagePullSecrets of each namespace.\\nIf you need access to multiple registries, you can create one secret for each registry.\\nLegacy built-in kubelet credential provider\\nIn older versions of Kubernetes, the kubelet had a direct integration with cloud provider\\ncredentials. This gave it the ability to dynamically fetch credentials for image registries.\\nThere were three built-in implementations of the kubelet credential provider integration: ACR\\n(Azure Container Registry), ECR (Elastic Container Registry), and GCR (Google Container\\nRegistry).\\nFor more information on the legacy mechanism, read the documentation for the version of\\nKubernetes that you are using. Kubernetes v1.26 through to v1.28 do not include the legacy\\nmechanism, so you would need to either:\\nconfigure a kubelet image credential provider on each node', metadata={'source': './PDFS/Concepts.pdf', 'page': 75}),\n",
       " Document(page_content=\"mechanism, so you would need to either:\\nconfigure a kubelet image credential provider on each node\\nspecify image pull credentials using imagePullSecrets  and at least one Secret\\nWhat's next\\nRead the OCI Image Manifest Specification .\\nLearn about container image garbage collection .\\nLearn more about pulling an Image from a Private Registry .\\nContainer Environment\\nThis page describes the resources available to Containers in the Container environment.\\nContainer environment\\nThe Kubernetes Container environment provides several important resources to Containers:\\nA filesystem, which is a combination of an image  and one or more volumes .\\nInformation about the Container itself.\\nInformation about other objects in the cluster.\\nContainer information\\nThe hostname  of a Container is the name of the Pod in which the Container is running. It is\\navailable through the hostname  command or the gethostname  function call in libc.\", metadata={'source': './PDFS/Concepts.pdf', 'page': 75}),\n",
       " Document(page_content='available through the hostname  command or the gethostname  function call in libc.\\nThe Pod name and namespace are available as environment variables through the downward\\nAPI.\\nUser defined environment variables from the Pod definition are also available to the Container,\\nas are any environment variables specified statically in the container image.◦ \\n◦ \\n• \\n• \\n• \\n• \\n• \\n• \\n• \\n•', metadata={'source': './PDFS/Concepts.pdf', 'page': 75}),\n",
       " Document(page_content=\"Cluster information\\nA list of all services that were running when a Container was created is available to that\\nContainer as environment variables. This list is limited to services within the same namespace\\nas the new Container's Pod and Kubernetes control plane services.\\nFor a service named foo that maps to a Container named bar, the following variables are\\ndefined:\\nFOO_SERVICE_HOST =<the host the service is running on>\\nFOO_SERVICE_PORT =<the port the service is running on>\\nServices have dedicated IP addresses and are available to the Container via DNS, if DNS addon\\nis enabled.\\xa0\\nWhat's next\\nLearn more about Container lifecycle hooks .\\nGet hands-on experience attaching handlers to Container lifecycle events .\\nRuntime Class\\nFEATURE STATE:  Kubernetes v1.20 [stable]\\nThis page describes the RuntimeClass resource and runtime selection mechanism.\\nRuntimeClass is a feature for selecting the container runtime configuration. The container\\nruntime configuration is used to run a Pod's containers.\", metadata={'source': './PDFS/Concepts.pdf', 'page': 76}),\n",
       " Document(page_content=\"runtime configuration is used to run a Pod's containers.\\nMotivation\\nYou can set a different RuntimeClass between different Pods to provide a balance of\\nperformance versus security. For example, if part of your workload deserves a high level of\\ninformation security assurance, you might choose to schedule those Pods so that they run in a\\ncontainer runtime that uses hardware virtualization. You'd then benefit from the extra isolation\\nof the alternative runtime, at the expense of some additional overhead.\\nYou can also use RuntimeClass to run different Pods with the same container runtime but with\\ndifferent settings.\\nSetup\\nConfigure the CRI implementation on nodes (runtime dependent)\\nCreate the corresponding RuntimeClass resources\\n1. Configure the CRI implementation on nodes\\nThe configurations available through RuntimeClass are Container Runtime Interface (CRI)\\nimplementation dependent. See the corresponding documentation ( below ) for your CRI\\nimplementation for how to configure.• \\n• \\n1.\", metadata={'source': './PDFS/Concepts.pdf', 'page': 76}),\n",
       " Document(page_content='implementation for how to configure.• \\n• \\n1. \\n2.', metadata={'source': './PDFS/Concepts.pdf', 'page': 76}),\n",
       " Document(page_content='Note:  RuntimeClass assumes a homogeneous node configuration across the cluster by default\\n(which means that all nodes are configured the same way with respect to container runtimes).\\nTo support heterogeneous node configurations, see Scheduling  below.\\nThe configurations have a corresponding handler  name, referenced by the RuntimeClass. The\\nhandler must be a valid DNS label name .\\n2. Create the corresponding RuntimeClass resources\\nThe configurations setup in step 1 should each have an associated handler  name, which\\nidentifies the configuration. For each handler, create a corresponding RuntimeClass object.\\nThe RuntimeClass resource currently only has 2 significant fields: the RuntimeClass name\\n(metadata.name ) and the handler ( handler ). The object definition looks like this:\\n# RuntimeClass is defined in the node.k8s.io API group\\napiVersion : node.k8s.io/v1\\nkind: RuntimeClass\\nmetadata :\\n  # The name the RuntimeClass will be referenced by.', metadata={'source': './PDFS/Concepts.pdf', 'page': 77}),\n",
       " Document(page_content='kind: RuntimeClass\\nmetadata :\\n  # The name the RuntimeClass will be referenced by.\\n  # RuntimeClass is a non-namespaced resource.\\n  name : myclass \\n# The name of the corresponding CRI configuration\\nhandler : myconfiguration \\nThe name of a RuntimeClass object must be a valid DNS subdomain name .\\nNote:  It is recommended that RuntimeClass write operations (create/update/patch/delete) be\\nrestricted to the cluster administrator. This is typically the default. See Authorization Overview\\nfor more details.\\nUsage\\nOnce RuntimeClasses are configured for the cluster, you can specify a runtimeClassName  in the\\nPod spec to use it. For example:\\napiVersion : v1\\nkind: Pod\\nmetadata :\\n  name : mypod\\nspec:\\n  runtimeClassName : myclass\\n  # ...\\nThis will instruct the kubelet to use the named RuntimeClass to run this pod. If the named\\nRuntimeClass does not exist, or the CRI cannot run the corresponding handler, the pod will', metadata={'source': './PDFS/Concepts.pdf', 'page': 77}),\n",
       " Document(page_content='RuntimeClass does not exist, or the CRI cannot run the corresponding handler, the pod will\\nenter the Failed  terminal phase . Look for a corresponding event  for an error message.\\nIf no runtimeClassName  is specified, the default RuntimeHandler will be used, which is\\nequivalent to the behavior when the RuntimeClass feature is disabled.', metadata={'source': './PDFS/Concepts.pdf', 'page': 77}),\n",
       " Document(page_content='CRI Configuration\\nFor more details on setting up CRI runtimes, see CRI installation .\\ncontainerd\\nRuntime handlers are configured through containerd\\'s configuration at /etc/containerd/\\nconfig.toml . Valid handlers are configured under the runtimes section:\\n[plugins.\"io.containerd.grpc.v1.cri\".containerd.runtimes.${HANDLER_NAME}]\\nSee containerd\\'s config documentation  for more details:\\nCRI-O\\nRuntime handlers are configured through CRI-O\\'s configuration at /etc/crio/crio.conf . Valid\\nhandlers are configured under the crio.runtime table :\\n[crio.runtime.runtimes.${HANDLER_NAME}]\\n  runtime_path = \"${PATH_TO_BINARY}\"\\nSee CRI-O\\'s config documentation  for more details.\\nScheduling\\nFEATURE STATE:  Kubernetes v1.16 [beta]\\nBy specifying the scheduling  field for a RuntimeClass, you can set constraints to ensure that\\nPods running with this RuntimeClass are scheduled to nodes that support it. If scheduling  is not\\nset, this RuntimeClass is assumed to be supported by all nodes.', metadata={'source': './PDFS/Concepts.pdf', 'page': 78}),\n",
       " Document(page_content=\"set, this RuntimeClass is assumed to be supported by all nodes.\\nTo ensure pods land on nodes supporting a specific RuntimeClass, that set of nodes should have\\na common label which is then selected by the runtimeclass.scheduling.nodeSelector  field. The\\nRuntimeClass's nodeSelector is merged with the pod's nodeSelector in admission, effectively\\ntaking the intersection of the set of nodes selected by each. If there is a conflict, the pod will be\\nrejected.\\nIf the supported nodes are tainted to prevent other RuntimeClass pods from running on the\\nnode, you can add tolerations  to the RuntimeClass. As with the nodeSelector , the tolerations are\\nmerged with the pod's tolerations in admission, effectively taking the union of the set of nodes\\ntolerated by each.\\nTo learn more about configuring the node selector and tolerations, see Assigning Pods to\\nNodes .\\nPod Overhead\\nFEATURE STATE:  Kubernetes v1.24 [stable]\", metadata={'source': './PDFS/Concepts.pdf', 'page': 78}),\n",
       " Document(page_content='Nodes .\\nPod Overhead\\nFEATURE STATE:  Kubernetes v1.24 [stable]\\nYou can specify overhead  resources that are associated with running a Pod. Declaring overhead\\nallows the cluster (including the scheduler) to account for it when making decisions about Pods\\nand resources.', metadata={'source': './PDFS/Concepts.pdf', 'page': 78}),\n",
       " Document(page_content=\"Pod overhead is defined in RuntimeClass through the overhead  field. Through the use of this\\nfield, you can specify the overhead of running pods utilizing this RuntimeClass and ensure\\nthese overheads are accounted for in Kubernetes.\\nWhat's next\\nRuntimeClass Design\\nRuntimeClass Scheduling Design\\nRead about the Pod Overhead  concept\\nPodOverhead Feature Design\\nContainer Lifecycle Hooks\\nThis page describes how kubelet managed Containers can use the Container lifecycle hook\\nframework to run code triggered by events during their management lifecycle.\\nOverview\\nAnalogous to many programming language frameworks that have component lifecycle hooks,\\nsuch as Angular, Kubernetes provides Containers with lifecycle hooks. The hooks enable\\nContainers to be aware of events in their management lifecycle and run code implemented in a\\nhandler when the corresponding lifecycle hook is executed.\\nContainer hooks\\nThere are two hooks that are exposed to Containers:\\nPostStart\", metadata={'source': './PDFS/Concepts.pdf', 'page': 79}),\n",
       " Document(page_content=\"Container hooks\\nThere are two hooks that are exposed to Containers:\\nPostStart\\nThis hook is executed immediately after a container is created. However, there is no guarantee\\nthat the hook will execute before the container ENTRYPOINT. No parameters are passed to the\\nhandler.\\nPreStop\\nThis hook is called immediately before a container is terminated due to an API request or\\nmanagement event such as a liveness/startup probe failure, preemption, resource contention\\nand others. A call to the PreStop  hook fails if the container is already in a terminated or\\ncompleted state and the hook must complete before the TERM signal to stop the container can\\nbe sent. The Pod's termination grace period countdown begins before the PreStop  hook is\\nexecuted, so regardless of the outcome of the handler, the container will eventually terminate\\nwithin the Pod's termination grace period. No parameters are passed to the handler.\", metadata={'source': './PDFS/Concepts.pdf', 'page': 79}),\n",
       " Document(page_content=\"within the Pod's termination grace period. No parameters are passed to the handler.\\nA more detailed description of the termination behavior can be found in Termination of Pods .• \\n• \\n• \\n•\", metadata={'source': './PDFS/Concepts.pdf', 'page': 79}),\n",
       " Document(page_content='Hook handler implementations\\nContainers can access a hook by implementing and registering a handler for that hook. There\\nare two types of hook handlers that can be implemented for Containers:\\nExec - Executes a specific command, such as pre-stop.sh , inside the cgroups and\\nnamespaces of the Container. Resources consumed by the command are counted against\\nthe Container.\\nHTTP - Executes an HTTP request against a specific endpoint on the Container.\\nHook handler execution\\nWhen a Container lifecycle management hook is called, the Kubernetes management system\\nexecutes the handler according to the hook action, httpGet  and tcpSocket  are executed by the\\nkubelet process, and exec is executed in the container.\\nHook handler calls are synchronous within the context of the Pod containing the Container.\\nThis means that for a PostStart  hook, the Container ENTRYPOINT and hook fire\\nasynchronously. However, if the hook takes too long to run or hangs, the Container cannot\\nreach a running  state.', metadata={'source': './PDFS/Concepts.pdf', 'page': 80}),\n",
       " Document(page_content=\"reach a running  state.\\nPreStop  hooks are not executed asynchronously from the signal to stop the Container; the hook\\nmust complete its execution before the TERM signal can be sent. If a PreStop  hook hangs\\nduring execution, the Pod's phase will be Terminating  and remain there until the Pod is killed\\nafter its terminationGracePeriodSeconds  expires. This grace period applies to the total time it\\ntakes for both the PreStop  hook to execute and for the Container to stop normally. If, for\\nexample, terminationGracePeriodSeconds  is 60, and the hook takes 55 seconds to complete, and\\nthe Container takes 10 seconds to stop normally after receiving the signal, then the Container\\nwill be killed before it can stop normally, since terminationGracePeriodSeconds  is less than the\\ntotal time (55+10) it takes for these two things to happen.\\nIf either a PostStart  or PreStop  hook fails, it kills the Container.\", metadata={'source': './PDFS/Concepts.pdf', 'page': 80}),\n",
       " Document(page_content='If either a PostStart  or PreStop  hook fails, it kills the Container.\\nUsers should make their hook handlers as lightweight as possible. There are cases, however,\\nwhen long running commands make sense, such as when saving state prior to stopping a\\nContainer.\\nHook delivery guarantees\\nHook delivery is intended to be at least once , which means that a hook may be called multiple\\ntimes for any given event, such as for PostStart  or PreStop . It is up to the hook implementation\\nto handle this correctly.\\nGenerally, only single deliveries are made. If, for example, an HTTP hook receiver is down and\\nis unable to take traffic, there is no attempt to resend. In some rare cases, however, double\\ndelivery may occur. For instance, if a kubelet restarts in the middle of sending a hook, the hook\\nmight be resent after the kubelet comes back up.\\nDebugging Hook handlers\\nThe logs for a Hook handler are not exposed in Pod events. If a handler fails for some reason, it', metadata={'source': './PDFS/Concepts.pdf', 'page': 80}),\n",
       " Document(page_content='The logs for a Hook handler are not exposed in Pod events. If a handler fails for some reason, it\\nbroadcasts an event. For PostStart , this is the FailedPostStartHook  event, and for PreStop , this is\\nthe FailedPreStopHook  event. To generate a failed FailedPostStartHook  event yourself, modify• \\n•', metadata={'source': './PDFS/Concepts.pdf', 'page': 80}),\n",
       " Document(page_content='the lifecycle-events.yaml  file to change the postStart command to \"badcommand\" and apply it.\\nHere is some example output of the resulting events you see from running \\nkubectl describe pod lifecycle-demo :\\nEvents:\\n  Type     Reason               Age              From               Message\\n  ----     ------               ----             ----               -------\\n  Normal   Scheduled            7s               default-scheduler  Successfully assigned default/\\nlifecycle-demo to ip-XXX-XXX-XX-XX.us-east-2...\\n  Normal   Pulled               6s               kubelet            Successfully pulled image \"nginx\" in \\n229.604315ms\\n  Normal   Pulling              4s (x2 over 6s)  kubelet            Pulling image \"nginx\"\\n  Normal   Created              4s (x2 over 5s)  kubelet            Created container lifecycle-demo-\\ncontainer\\n  Normal   Started              4s (x2 over 5s)  kubelet            Started container lifecycle-demo-\\ncontainer', metadata={'source': './PDFS/Concepts.pdf', 'page': 81}),\n",
       " Document(page_content='container\\n  Warning  FailedPostStartHook  4s (x2 over 5s)  kubelet            Exec lifecycle hook \\n([badcommand]) for Container \"lifecycle-demo-container\" in Pod \"lifecycle-\\ndemo_default(30229739-9651-4e5a-9a32-a8f1688862db)\" failed - error: command \\'badcommand\\' \\nexited with 126: , message: \"OCI runtime exec failed: exec failed: container_linux.go:380: \\nstarting container process caused: exec: \\\\\"badcommand\\\\\": executable file not found in $PATH: \\nunknown\\\\r\\\\n\"\\n  Normal   Killing              4s (x2 over 5s)  kubelet            FailedPostStartHook\\n  Normal   Pulled               4s               kubelet            Successfully pulled image \"nginx\" in \\n215.66395ms\\n  Warning  BackOff              2s (x2 over 3s)  kubelet            Back-off restarting failed container\\nWhat\\'s next\\nLearn more about the Container environment .\\nGet hands-on experience attaching handlers to Container lifecycle events .\\nWorkloads', metadata={'source': './PDFS/Concepts.pdf', 'page': 81}),\n",
       " Document(page_content=\"Get hands-on experience attaching handlers to Container lifecycle events .\\nWorkloads\\nUnderstand Pods, the smallest deployable compute object in Kubernetes, and the higher-level\\nabstractions that help you to run them.\\nA workload is an application running on Kubernetes. Whether your workload is a single\\ncomponent or several that work together, on Kubernetes you run it inside a set of pods. In\\nKubernetes, a Pod represents a set of running containers  on your cluster.\\nKubernetes pods have a defined lifecycle . For example, once a pod is running in your cluster\\nthen a critical fault on the node  where that pod is running means that all the pods on that node\\nfail. Kubernetes treats that level of failure as final: you would need to create a new Pod to\\nrecover, even if the node later becomes healthy.\\nHowever, to make life considerably easier, you don't need to manage each Pod directly. Instead,\\nyou can use workload resources  that manage a set of pods on your behalf. These resources\", metadata={'source': './PDFS/Concepts.pdf', 'page': 81}),\n",
       " Document(page_content='you can use workload resources  that manage a set of pods on your behalf. These resources\\nconfigure controllers  that make sure the right number of the right kind of pod are running, to\\nmatch the state you specified.• \\n•', metadata={'source': './PDFS/Concepts.pdf', 'page': 81}),\n",
       " Document(page_content='Kubernetes provides several built-in workload resources:\\nDeployment  and ReplicaSet  (replacing the legacy resource ReplicationController ).\\nDeployment is a good fit for managing a stateless application workload on your cluster,\\nwhere any Pod in the Deployment is interchangeable and can be replaced if needed.\\nStatefulSet  lets you run one or more related Pods that do track state somehow. For\\nexample, if your workload records data persistently, you can run a StatefulSet that\\nmatches each Pod with a PersistentVolume . Your code, running in the Pods for that\\nStatefulSet, can replicate data to other Pods in the same StatefulSet to improve overall\\nresilience.\\nDaemonSet  defines Pods that provide facilities that are local to nodes. Every time you add\\na node to your cluster that matches the specification in a DaemonSet, the control plane\\nschedules a Pod for that DaemonSet onto the new node. Each pod in a DaemonSet\\nperforms a job similar to a system daemon on a classic Unix / POSIX server. A', metadata={'source': './PDFS/Concepts.pdf', 'page': 82}),\n",
       " Document(page_content=\"performs a job similar to a system daemon on a classic Unix / POSIX server. A\\nDaemonSet might be fundamental to the operation of your cluster, such as a plugin to run\\ncluster networking , it might help you to manage the node, or it could provide optional\\nbehavior that enhances the container platform you are running.\\nJob and CronJob  provide different ways to define tasks that run to completion and then\\nstop. You can use a Job to define a task that runs to completion, just once. You can use a \\nCronJob  to run the same Job multiple times according a schedule.\\nIn the wider Kubernetes ecosystem, you can find third-party workload resources that provide\\nadditional behaviors. Using a custom resource definition , you can add in a third-party workload\\nresource if you want a specific behavior that's not part of Kubernetes' core. For example, if you\\nwanted to run a group of Pods for your application but stop work unless all the Pods are\", metadata={'source': './PDFS/Concepts.pdf', 'page': 82}),\n",
       " Document(page_content=\"wanted to run a group of Pods for your application but stop work unless all the Pods are\\navailable (perhaps for some high-throughput distributed task), then you can implement or\\ninstall an extension that does provide that feature.\\nWhat's next\\nAs well as reading about each API kind for workload management, you can read how to do\\nspecific tasks:\\nRun a stateless application using a Deployment\\nRun a stateful application either as a single instance  or as a replicated set\\nRun automated tasks with a CronJob\\nTo learn about Kubernetes' mechanisms for separating code from configuration, visit \\nConfiguration .\\nThere are two supporting concepts that provide backgrounds about how Kubernetes manages\\npods for applications:\\nGarbage collection  tidies up objects from your cluster after their owning resource  has been\\nremoved.\\nThe time-to-live after finished  controller  removes Jobs once a defined time has passed\\nsince they completed.\", metadata={'source': './PDFS/Concepts.pdf', 'page': 82}),\n",
       " Document(page_content='since they completed.\\nOnce your application is running, you might want to make it available on the internet as a \\nService  or, for web application only, using an Ingress .• \\n• \\n• \\n• \\n• \\n• \\n• \\n• \\n•', metadata={'source': './PDFS/Concepts.pdf', 'page': 82}),\n",
       " Document(page_content='Pods\\nPods are the smallest deployable units of computing that you can create and manage in\\nKubernetes.\\nA Pod (as in a pod of whales or pea pod) is a group of one or more containers , with shared\\nstorage and network resources, and a specification for how to run the containers. A Pod\\'s\\ncontents are always co-located and co-scheduled, and run in a shared context. A Pod models an\\napplication-specific \"logical host\": it contains one or more application containers which are\\nrelatively tightly coupled. In non-cloud contexts, applications executed on the same physical or\\nvirtual machine are analogous to cloud applications executed on the same logical host.\\nAs well as application containers, a Pod can contain init containers  that run during Pod startup.\\nYou can also inject ephemeral containers  for debugging if your cluster offers this.\\nWhat is a Pod?\\nNote:  You need to install a container runtime  into each node in the cluster so that Pods can run\\nthere.', metadata={'source': './PDFS/Concepts.pdf', 'page': 83}),\n",
       " Document(page_content=\"there.\\nThe shared context of a Pod is a set of Linux namespaces, cgroups, and potentially other facets\\nof isolation - the same things that isolate a container . Within a Pod's context, the individual\\napplications may have further sub-isolations applied.\\nA Pod is similar to a set of containers with shared namespaces and shared filesystem volumes.\\nUsing Pods\\nThe following is an example of a Pod which consists of a container running the image nginx:\\n1.14.2 .\\npods/simple-pod.yaml  \\napiVersion : v1\\nkind: Pod\\nmetadata :\\n  name : nginx\\nspec:\\n  containers :\\n  - name : nginx\\n    image : nginx:1.14.2\\n    ports :\\n    - containerPort : 80\\nTo create the Pod shown above, run the following command:\\nkubectl apply -f https://k8s.io/examples/pods/simple-pod.yaml\\nPods are generally not created directly and are created using workload resources. See Working\\nwith Pods  for more information on how Pods are used with workload resources.\", metadata={'source': './PDFS/Concepts.pdf', 'page': 83}),\n",
       " Document(page_content='Workload resources for managing pods\\nUsually you don\\'t need to create Pods directly, even singleton Pods. Instead, create them using\\nworkload resources such as Deployment  or Job. If your Pods need to track state, consider the \\nStatefulSet  resource.\\nPods in a Kubernetes cluster are used in two main ways:\\nPods that run a single container . The \"one-container-per-Pod\" model is the most\\ncommon Kubernetes use case; in this case, you can think of a Pod as a wrapper around a\\nsingle container; Kubernetes manages Pods rather than managing the containers directly.\\nPods that run multiple containers that need to work together . A Pod can\\nencapsulate an application composed of multiple co-located containers that are tightly\\ncoupled and need to share resources. These co-located containers form a single cohesive\\nunit of service—for example, one container serving data stored in a shared volume to the\\npublic, while a separate sidecar  container refreshes or updates those files. The Pod wraps', metadata={'source': './PDFS/Concepts.pdf', 'page': 84}),\n",
       " Document(page_content='public, while a separate sidecar  container refreshes or updates those files. The Pod wraps\\nthese containers, storage resources, and an ephemeral network identity together as a\\nsingle unit.\\nNote:  Grouping multiple co-located and co-managed containers in a single Pod is a\\nrelatively advanced use case. You should use this pattern only in specific instances in\\nwhich your containers are tightly coupled.\\nEach Pod is meant to run a single instance of a given application. If you want to scale your\\napplication horizontally (to provide more overall resources by running more instances), you\\nshould use multiple Pods, one for each instance. In Kubernetes, this is typically referred to as \\nreplication . Replicated Pods are usually created and managed as a group by a workload resource\\nand its controller .\\nSee Pods and controllers  for more information on how Kubernetes uses workload resources,\\nand their controllers, to implement application scaling and auto-healing.', metadata={'source': './PDFS/Concepts.pdf', 'page': 84}),\n",
       " Document(page_content='and their controllers, to implement application scaling and auto-healing.\\nHow Pods manage multiple containers\\nPods are designed to support multiple cooperating processes (as containers) that form a\\ncohesive unit of service. The containers in a Pod are automatically co-located and co-scheduled\\non the same physical or virtual machine in the cluster. The containers can share resources and\\ndependencies, communicate with one another, and coordinate when and how they are\\nterminated.\\nFor example, you might have a container that acts as a web server for files in a shared volume,\\nand a separate \"sidecar\" container that updates those files from a remote source, as in the\\nfollowing diagram:\\nPod creation diagram\\nSome Pods have init containers  as well as app containers . By default, init containers run and\\ncomplete before the app containers are started.\\nFEATURE STATE:  Kubernetes v1.28 [alpha]\\nEnabling the SidecarContainers  feature gate  allows you to specify restartPolicy: Always  for init', metadata={'source': './PDFS/Concepts.pdf', 'page': 84}),\n",
       " Document(page_content='Enabling the SidecarContainers  feature gate  allows you to specify restartPolicy: Always  for init\\ncontainers. Setting the Always  restart policy ensures that the init containers where you set it• \\n•', metadata={'source': './PDFS/Concepts.pdf', 'page': 84}),\n",
       " Document(page_content=\"are kept running during the entire lifetime of the Pod. See Sidecar containers and restartPolicy\\nfor more details.\\nPods natively provide two kinds of shared resources for their constituent containers: \\nnetworking  and storage .\\nWorking with Pods\\nYou'll rarely create individual Pods directly in Kubernetes—even singleton Pods. This is because\\nPods are designed as relatively ephemeral, disposable entities. When a Pod gets created (directly\\nby you, or indirectly by a controller ), the new Pod is scheduled to run on a Node  in your cluster.\\nThe Pod remains on that node until the Pod finishes execution, the Pod object is deleted, the\\nPod is evicted  for lack of resources, or the node fails.\\nNote:  Restarting a container in a Pod should not be confused with restarting a Pod. A Pod is\\nnot a process, but an environment for running container(s). A Pod persists until it is deleted.\\nThe name of a Pod must be a valid DNS subdomain  value, but this can produce unexpected\", metadata={'source': './PDFS/Concepts.pdf', 'page': 85}),\n",
       " Document(page_content=\"The name of a Pod must be a valid DNS subdomain  value, but this can produce unexpected\\nresults for the Pod hostname. For best compatibility, the name should follow the more\\nrestrictive rules for a DNS label .\\nPod OS\\nFEATURE STATE:  Kubernetes v1.25 [stable]\\nYou should set the .spec.os.name  field to either windows  or linux  to indicate the OS on which\\nyou want the pod to run. These two are the only operating systems supported for now by\\nKubernetes. In future, this list may be expanded.\\nIn Kubernetes v1.28, the value you set for this field has no effect on scheduling  of the pods.\\nSetting the .spec.os.name  helps to identify the pod OS authoritatively and is used for validation.\\nThe kubelet refuses to run a Pod where you have specified a Pod OS, if this isn't the same as the\\noperating system for the node where that kubelet is running. The Pod security standards  also\\nuse this field to avoid enforcing policies that aren't relevant to that operating system.\\nPods and controllers\", metadata={'source': './PDFS/Concepts.pdf', 'page': 85}),\n",
       " Document(page_content='Pods and controllers\\nYou can use workload resources to create and manage multiple Pods for you. A controller for\\nthe resource handles replication and rollout and automatic healing in case of Pod failure. For\\nexample, if a Node fails, a controller notices that Pods on that Node have stopped working and\\ncreates a replacement Pod. The scheduler places the replacement Pod onto a healthy Node.\\nHere are some examples of workload resources that manage one or more Pods:\\nDeployment\\nStatefulSet\\nDaemonSet\\nPod templates\\nControllers for workload  resources create Pods from a pod template  and manage those Pods on\\nyour behalf.• \\n• \\n•', metadata={'source': './PDFS/Concepts.pdf', 'page': 85}),\n",
       " Document(page_content='PodTemplates are specifications for creating Pods, and are included in workload resources such\\nas Deployments , Jobs, and DaemonSets .\\nEach controller for a workload resource uses the PodTemplate  inside the workload object to\\nmake actual Pods. The PodTemplate  is part of the desired state of whatever workload resource\\nyou used to run your app.\\nThe sample below is a manifest for a simple Job with a template  that starts one container. The\\ncontainer in that Pod prints a message then pauses.\\napiVersion : batch/v1\\nkind: Job\\nmetadata :\\n  name : hello\\nspec:\\n  template :\\n    # This is the pod template\\n    spec:\\n      containers :\\n      - name : hello\\n        image : busybox:1.28\\n        command : [\\'sh\\', \\'-c\\', \\'echo \"Hello, Kubernetes!\" && sleep 3600\\' ]\\n      restartPolicy : OnFailure\\n    # The pod template ends here\\nModifying the pod template or switching to a new pod template has no direct effect on the Pods', metadata={'source': './PDFS/Concepts.pdf', 'page': 86}),\n",
       " Document(page_content='Modifying the pod template or switching to a new pod template has no direct effect on the Pods\\nthat already exist. If you change the pod template for a workload resource, that resource needs\\nto create replacement Pods that use the updated template.\\nFor example, the StatefulSet controller ensures that the running Pods match the current pod\\ntemplate for each StatefulSet object. If you edit the StatefulSet to change its pod template, the\\nStatefulSet starts to create new Pods based on the updated template. Eventually, all of the old\\nPods are replaced with new Pods, and the update is complete.\\nEach workload resource implements its own rules for handling changes to the Pod template. If\\nyou want to read more about StatefulSet specifically, read Update strategy  in the StatefulSet\\nBasics tutorial.\\nOn Nodes, the kubelet  does not directly observe or manage any of the details around pod\\ntemplates and updates; those details are abstracted away. That abstraction and separation of', metadata={'source': './PDFS/Concepts.pdf', 'page': 86}),\n",
       " Document(page_content=\"templates and updates; those details are abstracted away. That abstraction and separation of\\nconcerns simplifies system semantics, and makes it feasible to extend the cluster's behavior\\nwithout changing existing code.\\nPod update and replacement\\nAs mentioned in the previous section, when the Pod template for a workload resource is\\nchanged, the controller creates new Pods based on the updated template instead of updating or\\npatching the existing Pods.\", metadata={'source': './PDFS/Concepts.pdf', 'page': 86}),\n",
       " Document(page_content=\"Kubernetes doesn't prevent you from managing Pods directly. It is possible to update some\\nfields of a running Pod, in place. However, Pod update operations like patch , and replace  have\\nsome limitations:\\nMost of the metadata about a Pod is immutable. For example, you cannot change the \\nnamespace , name , uid, or creationTimestamp  fields; the generation  field is unique. It only\\naccepts updates that increment the field's current value.\\nIf the metadata.deletionTimestamp  is set, no new entry can be added to the \\nmetadata.finalizers  list.\\nPod updates may not change fields other than spec.containers[*].image , \\nspec.initContainers[*].image , spec.activeDeadlineSeconds  or spec.tolerations . For \\nspec.tolerations , you can only add new entries.\\nWhen updating the spec.activeDeadlineSeconds  field, two types of updates are allowed:\\nsetting the unassigned field to a positive number;\\nupdating the field from a positive number to a smaller, non-negative number.\", metadata={'source': './PDFS/Concepts.pdf', 'page': 87}),\n",
       " Document(page_content='updating the field from a positive number to a smaller, non-negative number.\\nResource sharing and communication\\nPods enable data sharing and communication among their constituent containers.\\nStorage in Pods\\nA Pod can specify a set of shared storage volumes . All containers in the Pod can access the\\nshared volumes, allowing those containers to share data. Volumes also allow persistent data in a\\nPod to survive in case one of the containers within needs to be restarted. See Storage  for more\\ninformation on how Kubernetes implements shared storage and makes it available to Pods.\\nPod networking\\nEach Pod is assigned a unique IP address for each address family. Every container in a Pod\\nshares the network namespace, including the IP address and network ports. Inside a Pod (and \\nonly  then), the containers that belong to the Pod can communicate with one another using \\nlocalhost . When containers in a Pod communicate with entities outside the Pod , they must', metadata={'source': './PDFS/Concepts.pdf', 'page': 87}),\n",
       " Document(page_content=\"localhost . When containers in a Pod communicate with entities outside the Pod , they must\\ncoordinate how they use the shared network resources (such as ports). Within a Pod, containers\\nshare an IP address and port space, and can find each other via localhost . The containers in a\\nPod can also communicate with each other using standard inter-process communications like\\nSystemV semaphores or POSIX shared memory. Containers in different Pods have distinct IP\\naddresses and can not communicate by OS-level IPC without special configuration. Containers\\nthat want to interact with a container running in a different Pod can use IP networking to\\ncommunicate.\\nContainers within the Pod see the system hostname as being the same as the configured name\\nfor the Pod. There's more about this in the networking  section.\\nPrivileged mode for containers\\nNote:  Your container runtime  must support the concept of a privileged container for this\\nsetting to be relevant.• \\n• \\n• \\n• \\n1. \\n2.\", metadata={'source': './PDFS/Concepts.pdf', 'page': 87}),\n",
       " Document(page_content='Any container in a pod can run in privileged mode to use operating system administrative\\ncapabilities that would otherwise be inaccessible. This is available for both Windows and Linux.\\nLinux privileged containers\\nIn Linux, any container in a Pod can enable privileged mode using the privileged  (Linux) flag on\\nthe security context  of the container spec. This is useful for containers that want to use\\noperating system administrative capabilities such as manipulating the network stack or\\naccessing hardware devices.\\nWindows privileged containers\\nFEATURE STATE:  Kubernetes v1.26 [stable]\\nIn Windows, you can create a Windows HostProcess pod  by setting the \\nwindowsOptions.hostProcess  flag on the security context of the pod spec. All containers in\\nthese pods must run as Windows HostProcess containers. HostProcess pods run directly on the\\nhost and can also be used to perform administrative tasks as is done with Linux privileged\\ncontainers.\\nStatic Pods', metadata={'source': './PDFS/Concepts.pdf', 'page': 88}),\n",
       " Document(page_content='containers.\\nStatic Pods\\nStatic Pods  are managed directly by the kubelet daemon on a specific node, without the API\\nserver  observing them. Whereas most Pods are managed by the control plane (for example, a \\nDeployment ), for static Pods, the kubelet directly supervises each static Pod (and restarts it if it\\nfails).\\nStatic Pods are always bound to one Kubelet  on a specific node. The main use for static Pods is\\nto run a self-hosted control plane: in other words, using the kubelet to supervise the individual \\ncontrol plane components .\\nThe kubelet automatically tries to create a mirror Pod  on the Kubernetes API server for each\\nstatic Pod. This means that the Pods running on a node are visible on the API server, but cannot\\nbe controlled from there. See the guide Create static Pods  for more information.\\nNote:  The spec of a static Pod cannot refer to other API objects (e.g., ServiceAccount , \\nConfigMap , Secret , etc).\\nContainer probes', metadata={'source': './PDFS/Concepts.pdf', 'page': 88}),\n",
       " Document(page_content='ConfigMap , Secret , etc).\\nContainer probes\\nA probe  is a diagnostic performed periodically by the kubelet on a container. To perform a\\ndiagnostic, the kubelet can invoke different actions:\\nExecAction  (performed with the help of the container runtime)\\nTCPSocketAction  (checked directly by the kubelet)\\nHTTPGetAction  (checked directly by the kubelet)\\nYou can read more about probes  in the Pod Lifecycle documentation.• \\n• \\n•', metadata={'source': './PDFS/Concepts.pdf', 'page': 88}),\n",
       " Document(page_content=\"What's next\\nLearn about the lifecycle of a Pod .\\nLearn about RuntimeClass  and how you can use it to configure different Pods with\\ndifferent container runtime configurations.\\nRead about PodDisruptionBudget  and how you can use it to manage application\\navailability during disruptions.\\nPod is a top-level resource in the Kubernetes REST API. The Pod object definition\\ndescribes the object in detail.\\nThe Distributed System Toolkit: Patterns for Composite Containers  explains common\\nlayouts for Pods with more than one container.\\nRead about Pod topology spread constraints\\nTo understand the context for why Kubernetes wraps a common Pod API in other resources\\n(such as StatefulSets  or Deployments ), you can read about the prior art, including:\\nAurora\\nBorg\\nMarathon\\nOmega\\nTupperware .\\nPod Lifecycle\\nThis page describes the lifecycle of a Pod. Pods follow a defined lifecycle, starting in the \\nPending  phase , moving through Running  if at least one of its primary containers starts OK, and\", metadata={'source': './PDFS/Concepts.pdf', 'page': 89}),\n",
       " Document(page_content='Pending  phase , moving through Running  if at least one of its primary containers starts OK, and\\nthen through either the Succeeded  or Failed  phases depending on whether any container in the\\nPod terminated in failure.\\nWhilst a Pod is running, the kubelet is able to restart containers to handle some kind of faults.\\nWithin a Pod, Kubernetes tracks different container states  and determines what action to take\\nto make the Pod healthy again.\\nIn the Kubernetes API, Pods have both a specification and an actual status. The status for a Pod\\nobject consists of a set of Pod conditions . You can also inject custom readiness information  into\\nthe condition data for a Pod, if that is useful to your application.\\nPods are only scheduled  once in their lifetime. Once a Pod is scheduled (assigned) to a Node,\\nthe Pod runs on that Node until it stops or is terminated .\\nPod lifetime\\nLike individual application containers, Pods are considered to be relatively ephemeral (rather', metadata={'source': './PDFS/Concepts.pdf', 'page': 89}),\n",
       " Document(page_content=\"Like individual application containers, Pods are considered to be relatively ephemeral (rather\\nthan durable) entities. Pods are created, assigned a unique ID ( UID), and scheduled to nodes\\nwhere they remain until termination (according to restart policy) or deletion. If a Node  dies, the\\nPods scheduled to that node are scheduled for deletion  after a timeout period.\\nPods do not, by themselves, self-heal. If a Pod is scheduled to a node  that then fails, the Pod is\\ndeleted; likewise, a Pod won't survive an eviction due to a lack of resources or Node\\nmaintenance. Kubernetes uses a higher-level abstraction, called a controller , that handles the\\nwork of managing the relatively disposable Pod instances.• \\n• \\n• \\n• \\n• \\n• \\n• \\n• \\n• \\n• \\n•\", metadata={'source': './PDFS/Concepts.pdf', 'page': 89}),\n",
       " Document(page_content='A given Pod (as defined by a UID) is never \"rescheduled\" to a different node; instead, that Pod\\ncan be replaced by a new, near-identical Pod, with even the same name if desired, but with a\\ndifferent UID.\\nWhen something is said to have the same lifetime as a Pod, such as a volume , that means that\\nthe thing exists as long as that specific Pod (with that exact UID) exists. If that Pod is deleted for\\nany reason, and even if an identical replacement is created, the related thing (a volume, in this\\nexample) is also destroyed and created anew.\\nPod diagram\\nA multi-container Pod that contains a file puller and a web server that uses a persistent volume\\nfor shared storage between the containers.\\nPod phase\\nA Pod\\'s status  field is a PodStatus  object, which has a phase  field.\\nThe phase of a Pod is a simple, high-level summary of where the Pod is in its lifecycle. The\\nphase is not intended to be a comprehensive rollup of observations of container or Pod state,', metadata={'source': './PDFS/Concepts.pdf', 'page': 90}),\n",
       " Document(page_content='phase is not intended to be a comprehensive rollup of observations of container or Pod state,\\nnor is it intended to be a comprehensive state machine.\\nThe number and meanings of Pod phase values are tightly guarded. Other than what is\\ndocumented here, nothing should be assumed about Pods that have a given phase  value.\\nHere are the possible values for phase :\\nValue Description\\nPendingThe Pod has been accepted by the Kubernetes cluster, but one or more of the\\ncontainers has not been set up and made ready to run. This includes time a Pod\\nspends waiting to be scheduled as well as the time spent downloading container\\nimages over the network.\\nRunningThe Pod has been bound to a node, and all of the containers have been created. At\\nleast one container is still running, or is in the process of starting or restarting.\\nSucceeded All containers in the Pod have terminated in success, and will not be restarted.', metadata={'source': './PDFS/Concepts.pdf', 'page': 90}),\n",
       " Document(page_content='Succeeded All containers in the Pod have terminated in success, and will not be restarted.\\nFailedAll containers in the Pod have terminated, and at least one container has terminated\\nin failure. That is, the container either exited with non-zero status or was terminated\\nby the system.\\nUnknownFor some reason the state of the Pod could not be obtained. This phase typically\\noccurs due to an error in communicating with the node where the Pod should be\\nrunning.\\nNote:  When a Pod is being deleted, it is shown as Terminating  by some kubectl commands.\\nThis Terminating  status is not one of the Pod phases. A Pod is granted a term to terminate\\ngracefully, which defaults to 30 seconds. You can use the flag --force  to terminate a Pod by\\nforce .\\nSince Kubernetes 1.27, the kubelet transitions deleted Pods, except for static Pods  and force-\\ndeleted Pods  without a finalizer, to a terminal phase ( Failed  or Succeeded  depending on the exit', metadata={'source': './PDFS/Concepts.pdf', 'page': 90}),\n",
       " Document(page_content='deleted Pods  without a finalizer, to a terminal phase ( Failed  or Succeeded  depending on the exit\\nstatuses of the pod containers) before their deletion from the API server.\\nIf a node dies or is disconnected from the rest of the cluster, Kubernetes applies a policy for\\nsetting the phase  of all Pods on the lost node to Failed.', metadata={'source': './PDFS/Concepts.pdf', 'page': 90}),\n",
       " Document(page_content=\"Container states\\nAs well as the phase  of the Pod overall, Kubernetes tracks the state of each container inside a\\nPod. You can use container lifecycle hooks  to trigger events to run at certain points in a\\ncontainer's lifecycle.\\nOnce the scheduler  assigns a Pod to a Node, the kubelet starts creating containers for that Pod\\nusing a container runtime . There are three possible container states: Waiting , Running , and \\nTerminated .\\nTo check the state of a Pod's containers, you can use kubectl describe pod <name-of-pod> . The\\noutput shows the state for each container within that Pod.\\nEach state has a specific meaning:\\nWaiting\\nIf a container is not in either the Running  or Terminated  state, it is Waiting . A container in the \\nWaiting  state is still running the operations it requires in order to complete start up: for\\nexample, pulling the container image from a container image registry, or applying Secret  data.\", metadata={'source': './PDFS/Concepts.pdf', 'page': 91}),\n",
       " Document(page_content=\"example, pulling the container image from a container image registry, or applying Secret  data.\\nWhen you use kubectl  to query a Pod with a container that is Waiting , you also see a Reason\\nfield to summarize why the container is in that state.\\nRunning\\nThe Running  status indicates that a container is executing without issues. If there was a \\npostStart  hook configured, it has already executed and finished. When you use kubectl  to query\\na Pod with a container that is Running , you also see information about when the container\\nentered the Running  state.\\nTerminated\\nA container in the Terminated  state began execution and then either ran to completion or failed\\nfor some reason. When you use kubectl  to query a Pod with a container that is Terminated , you\\nsee a reason, an exit code, and the start and finish time for that container's period of execution.\\nIf a container has a preStop  hook configured, this hook runs before the container enters the \\nTerminated  state.\", metadata={'source': './PDFS/Concepts.pdf', 'page': 91}),\n",
       " Document(page_content='Terminated  state.\\nContainer restart policy\\nThe spec of a Pod has a restartPolicy  field with possible values Always, OnFailure, and Never.\\nThe default value is Always.\\nThe restartPolicy  applies to all containers in the Pod. restartPolicy  only refers to restarts of the\\ncontainers by the kubelet on the same node. After containers in a Pod exit, the kubelet restarts\\nthem with an exponential back-off delay (10s, 20s, 40s, ...), that is capped at five minutes. Once a\\ncontainer has executed for 10 minutes without any problems, the kubelet resets the restart\\nbackoff timer for that container.', metadata={'source': './PDFS/Concepts.pdf', 'page': 91}),\n",
       " Document(page_content='Pod conditions\\nA Pod has a PodStatus, which has an array of PodConditions  through which the Pod has or has\\nnot passed. Kubelet manages the following PodConditions:\\nPodScheduled : the Pod has been scheduled to a node.\\nPodReadyToStartContainers : (alpha feature; must be enabled explicitly ) the Pod sandbox\\nhas been successfully created and networking configured.\\nContainersReady : all containers in the Pod are ready.\\nInitialized : all init containers  have completed successfully.\\nReady : the Pod is able to serve requests and should be added to the load balancing pools\\nof all matching Services.\\nField name Description\\ntype Name of this Pod condition.\\nstatusIndicates whether that condition is applicable, with possible values \" True \",\\n\"False \", or \" Unknown \".\\nlastProbeTime Timestamp of when the Pod condition was last probed.\\nlastTransitionTime Timestamp for when the Pod last transitioned from one status to another.\\nreasonMachine-readable, UpperCamelCase text indicating the reason for the', metadata={'source': './PDFS/Concepts.pdf', 'page': 92}),\n",
       " Document(page_content='reasonMachine-readable, UpperCamelCase text indicating the reason for the\\ncondition\\'s last transition.\\nmessage Human-readable message indicating details about the last status transition.\\nPod readiness\\nFEATURE STATE:  Kubernetes v1.14 [stable]\\nYour application can inject extra feedback or signals into PodStatus: Pod readiness . To use this,\\nset readinessGates  in the Pod\\'s spec to specify a list of additional conditions that the kubelet\\nevaluates for Pod readiness.\\nReadiness gates are determined by the current state of status.condition  fields for the Pod. If\\nKubernetes cannot find such a condition in the status.conditions  field of a Pod, the status of the\\ncondition is defaulted to \" False \".\\nHere is an example:\\nkind: Pod\\n...\\nspec:\\n  readinessGates :\\n    - conditionType : \"www.example.com/feature-1\"\\nstatus :\\n  conditions :\\n    - type: Ready                              # a built in PodCondition\\n      status : \"False\"\\n      lastProbeTime : null', metadata={'source': './PDFS/Concepts.pdf', 'page': 92}),\n",
       " Document(page_content='status : \"False\"\\n      lastProbeTime : null\\n      lastTransitionTime : 2018-01-01T00:00:00Z\\n    - type: \"www.example.com/feature-1\"         # an extra PodCondition\\n      status : \"False\"\\n      lastProbeTime : null\\n      lastTransitionTime : 2018-01-01T00:00:00Z• \\n• \\n• \\n• \\n•', metadata={'source': './PDFS/Concepts.pdf', 'page': 92}),\n",
       " Document(page_content=\"containerStatuses :\\n    - containerID : docker://abcd...\\n      ready : true\\n...\\nThe Pod conditions you add must have names that meet the Kubernetes label key format .\\nStatus for Pod readiness\\nThe kubectl patch  command does not support patching object status. To set these \\nstatus.conditions  for the Pod, applications and operators  should use the PATCH  action. You can\\nuse a Kubernetes client library  to write code that sets custom Pod conditions for Pod readiness.\\nFor a Pod that uses custom conditions, that Pod is evaluated to be ready only  when both the\\nfollowing statements apply:\\nAll containers in the Pod are ready.\\nAll conditions specified in readinessGates  are True .\\nWhen a Pod's containers are Ready but at least one custom condition is missing or False , the\\nkubelet sets the Pod's condition  to ContainersReady .\\nPod network readiness\\nFEATURE STATE:  Kubernetes v1.25 [alpha]\\nNote:  This condition was renamed from PodHasNetwork to PodReadyToStartContainers.\", metadata={'source': './PDFS/Concepts.pdf', 'page': 93}),\n",
       " Document(page_content='Note:  This condition was renamed from PodHasNetwork to PodReadyToStartContainers.\\nAfter a Pod gets scheduled on a node, it needs to be admitted by the Kubelet and have any\\nvolumes mounted. Once these phases are complete, the Kubelet works with a container runtime\\n(using Container runtime interface (CRI) ) to set up a runtime sandbox and configure\\nnetworking for the Pod. If the PodReadyToStartContainersCondition  feature gate  is enabled,\\nKubelet reports whether a pod has reached this initialization milestone through the \\nPodReadyToStartContainers  condition in the status.conditions  field of a Pod.\\nThe PodReadyToStartContainers  condition is set to False  by the Kubelet when it detects a Pod\\ndoes not have a runtime sandbox with networking configured. This occurs in the following\\nscenarios:\\nEarly in the lifecycle of the Pod, when the kubelet has not yet begun to set up a sandbox\\nfor the Pod using the container runtime.', metadata={'source': './PDFS/Concepts.pdf', 'page': 93}),\n",
       " Document(page_content='for the Pod using the container runtime.\\nLater in the lifecycle of the Pod, when the Pod sandbox has been destroyed due to either:\\nthe node rebooting, without the Pod getting evicted\\nfor container runtimes that use virtual machines for isolation, the Pod sandbox\\nvirtual machine rebooting, which then requires creating a new sandbox and fresh\\ncontainer network configuration.\\nThe PodReadyToStartContainers  condition is set to True  by the kubelet after the successful\\ncompletion of sandbox creation and network configuration for the Pod by the runtime plugin.\\nThe kubelet can start pulling container images and create containers after \\nPodReadyToStartContainers  condition has been set to True .\\nFor a Pod with init containers, the kubelet sets the Initialized  condition to True  after the init\\ncontainers have successfully completed (which happens after successful sandbox creation and• \\n• \\n• \\n• \\n◦ \\n◦', metadata={'source': './PDFS/Concepts.pdf', 'page': 93}),\n",
       " Document(page_content='network configuration by the runtime plugin). For a Pod without init containers, the kubelet\\nsets the Initialized  condition to True  before sandbox creation and network configuration starts.\\nPod scheduling readiness\\nFEATURE STATE:  Kubernetes v1.26 [alpha]\\nSee Pod Scheduling Readiness  for more information.\\nContainer probes\\nA probe  is a diagnostic performed periodically by the kubelet  on a container. To perform a\\ndiagnostic, the kubelet either executes code within the container, or makes a network request.\\nCheck mechanisms\\nThere are four different ways to check a container using a probe. Each probe must define\\nexactly one of these four mechanisms:\\nexec\\nExecutes a specified command inside the container. The diagnostic is considered\\nsuccessful if the command exits with a status code of 0.\\ngrpc\\nPerforms a remote procedure call using gRPC . The target should implement gRPC health\\nchecks . The diagnostic is considered successful if the status  of the response is SERVING .\\nhttpGet', metadata={'source': './PDFS/Concepts.pdf', 'page': 94}),\n",
       " Document(page_content=\"checks . The diagnostic is considered successful if the status  of the response is SERVING .\\nhttpGet\\nPerforms an HTTP GET request against the Pod's IP address on a specified port and path.\\nThe diagnostic is considered successful if the response has a status code greater than or\\nequal to 200 and less than 400.\\ntcpSocket\\nPerforms a TCP check against the Pod's IP address on a specified port. The diagnostic is\\nconsidered successful if the port is open. If the remote system (the container) closes the\\nconnection immediately after it opens, this counts as healthy.\\nCaution:  Unlike the other mechanisms, exec probe's implementation involves the creation/\\nforking of multiple processes each time when executed. As a result, in case of the clusters\\nhaving higher pod densities, lower intervals of initialDelaySeconds , periodSeconds , configuring\\nany probe with exec mechanism might introduce an overhead on the cpu usage of the node. In\", metadata={'source': './PDFS/Concepts.pdf', 'page': 94}),\n",
       " Document(page_content='any probe with exec mechanism might introduce an overhead on the cpu usage of the node. In\\nsuch scenarios, consider using the alternative probe mechanisms to avoid the overhead.\\nProbe outcome\\nEach probe has one of three results:\\nSuccess\\nThe container passed the diagnostic.\\nFailure\\nThe container failed the diagnostic.\\nUnknown\\nThe diagnostic failed (no action should be taken, and the kubelet will make further\\nchecks).', metadata={'source': './PDFS/Concepts.pdf', 'page': 94}),\n",
       " Document(page_content=\"Types of probe\\nThe kubelet can optionally perform and react to three kinds of probes on running containers:\\nlivenessProbe\\nIndicates whether the container is running. If the liveness probe fails, the kubelet kills the\\ncontainer, and the container is subjected to its restart policy . If a container does not\\nprovide a liveness probe, the default state is Success .\\nreadinessProbe\\nIndicates whether the container is ready to respond to requests. If the readiness probe\\nfails, the endpoints controller removes the Pod's IP address from the endpoints of all\\nServices that match the Pod. The default state of readiness before the initial delay is \\nFailure . If a container does not provide a readiness probe, the default state is Success .\\nstartupProbe\\nIndicates whether the application within the container is started. All other probes are\\ndisabled if a startup probe is provided, until it succeeds. If the startup probe fails, the\", metadata={'source': './PDFS/Concepts.pdf', 'page': 95}),\n",
       " Document(page_content=\"disabled if a startup probe is provided, until it succeeds. If the startup probe fails, the\\nkubelet kills the container, and the container is subjected to its restart policy . If a\\ncontainer does not provide a startup probe, the default state is Success .\\nFor more information about how to set up a liveness, readiness, or startup probe, see Configure\\nLiveness, Readiness and Startup Probes .\\nWhen should you use a liveness probe?\\nIf the process in your container is able to crash on its own whenever it encounters an issue or\\nbecomes unhealthy, you do not necessarily need a liveness probe; the kubelet will automatically\\nperform the correct action in accordance with the Pod's restartPolicy .\\nIf you'd like your container to be killed and restarted if a probe fails, then specify a liveness\\nprobe, and specify a restartPolicy  of Always or OnFailure.\\nWhen should you use a readiness probe?\\nIf you'd like to start sending traffic to a Pod only when a probe succeeds, specify a readiness\", metadata={'source': './PDFS/Concepts.pdf', 'page': 95}),\n",
       " Document(page_content=\"If you'd like to start sending traffic to a Pod only when a probe succeeds, specify a readiness\\nprobe. In this case, the readiness probe might be the same as the liveness probe, but the\\nexistence of the readiness probe in the spec means that the Pod will start without receiving any\\ntraffic and only start receiving traffic after the probe starts succeeding.\\nIf you want your container to be able to take itself down for maintenance, you can specify a\\nreadiness probe that checks an endpoint specific to readiness that is different from the liveness\\nprobe.\\nIf your app has a strict dependency on back-end services, you can implement both a liveness\\nand a readiness probe. The liveness probe passes when the app itself is healthy, but the\\nreadiness probe additionally checks that each required back-end service is available. This helps\\nyou avoid directing traffic to Pods that can only respond with error messages.\", metadata={'source': './PDFS/Concepts.pdf', 'page': 95}),\n",
       " Document(page_content='you avoid directing traffic to Pods that can only respond with error messages.\\nIf your container needs to work on loading large data, configuration files, or migrations during\\nstartup, you can use a startup probe . However, if you want to detect the difference between an\\napp that has failed and an app that is still processing its startup data, you might prefer a\\nreadiness probe.', metadata={'source': './PDFS/Concepts.pdf', 'page': 95}),\n",
       " Document(page_content='Note:  If you want to be able to drain requests when the Pod is deleted, you do not necessarily\\nneed a readiness probe; on deletion, the Pod automatically puts itself into an unready state\\nregardless of whether the readiness probe exists. The Pod remains in the unready state while it\\nwaits for the containers in the Pod to stop.\\nWhen should you use a startup probe?\\nStartup probes are useful for Pods that have containers that take a long time to come into\\nservice. Rather than set a long liveness interval, you can configure a separate configuration for\\nprobing the container as it starts up, allowing a time longer than the liveness interval would\\nallow.\\nIf your container usually starts in more than initialDelaySeconds + failureThreshold × \\nperiodSeconds , you should specify a startup probe that checks the same endpoint as the\\nliveness probe. The default for periodSeconds  is 10s. You should then set its failureThreshold', metadata={'source': './PDFS/Concepts.pdf', 'page': 96}),\n",
       " Document(page_content='liveness probe. The default for periodSeconds  is 10s. You should then set its failureThreshold\\nhigh enough to allow the container to start, without changing the default values of the liveness\\nprobe. This helps to protect against deadlocks.\\nTermination of Pods\\nBecause Pods represent processes running on nodes in the cluster, it is important to allow those\\nprocesses to gracefully terminate when they are no longer needed (rather than being abruptly\\nstopped with a KILL  signal and having no chance to clean up).\\nThe design aim is for you to be able to request deletion and know when processes terminate,\\nbut also be able to ensure that deletes eventually complete. When you request deletion of a Pod,\\nthe cluster records and tracks the intended grace period before the Pod is allowed to be\\nforcefully killed. With that forceful shutdown tracking in place, the kubelet  attempts graceful\\nshutdown.\\nTypically, with this graceful termination of the pod, kubelet makes requests to the container', metadata={'source': './PDFS/Concepts.pdf', 'page': 96}),\n",
       " Document(page_content=\"Typically, with this graceful termination of the pod, kubelet makes requests to the container\\nruntime to attempt to stop the containers in the pod by first sending a TERM (aka. SIGTERM)\\nsignal, with a grace period timeout, to the main process in each container. The requests to stop\\nthe containers are processed by the container runtime asynchronously. There is no guarantee to\\nthe order of processing for these requests. Many container runtimes respect the STOPSIGNAL\\nvalue defined in the container image and, if different, send the container image configured\\nSTOPSIGNAL instead of TERM. Once the grace period has expired, the KILL signal is sent to\\nany remaining processes, and the Pod is then deleted from the API Server . If the kubelet or the\\ncontainer runtime's management service is restarted while waiting for processes to terminate,\\nthe cluster retries from the start including the full original grace period.\\nAn example flow:\", metadata={'source': './PDFS/Concepts.pdf', 'page': 96}),\n",
       " Document(page_content='the cluster retries from the start including the full original grace period.\\nAn example flow:\\nYou use the kubectl  tool to manually delete a specific Pod, with the default grace period\\n(30 seconds).\\nThe Pod in the API server is updated with the time beyond which the Pod is considered\\n\"dead\" along with the grace period. If you use kubectl describe  to check the Pod you\\'re\\ndeleting, that Pod shows up as \"Terminating\". On the node where the Pod is running: as1. \\n2.', metadata={'source': './PDFS/Concepts.pdf', 'page': 96}),\n",
       " Document(page_content=\"soon as the kubelet sees that a Pod has been marked as terminating (a graceful shutdown\\nduration has been set), the kubelet begins the local Pod shutdown process.\\nIf one of the Pod's containers has defined a preStop  hook  and the \\nterminationGracePeriodSeconds  in the Pod spec is not set to 0, the kubelet runs\\nthat hook inside of the container. The default terminationGracePeriodSeconds\\nsetting is 30 seconds.\\nIf the preStop  hook is still running after the grace period expires, the kubelet\\nrequests a small, one-off grace period extension of 2 seconds.\\nNote:  If the preStop  hook needs longer to complete than the default grace period\\nallows, you must modify terminationGracePeriodSeconds  to suit this.\\nThe kubelet triggers the container runtime to send a TERM signal to process 1\\ninside each container.\\nNote:  The containers in the Pod receive the TERM signal at different times and in\\nan arbitrary order. If the order of shutdowns matters, consider using a preStop\\nhook to synchronize.\", metadata={'source': './PDFS/Concepts.pdf', 'page': 97}),\n",
       " Document(page_content='an arbitrary order. If the order of shutdowns matters, consider using a preStop\\nhook to synchronize.\\nAt the same time as the kubelet is starting graceful shutdown of the Pod, the control\\nplane evaluates whether to remove that shutting-down Pod from EndpointSlice (and\\nEndpoints) objects, where those objects represent a Service  with a configured selector . \\nReplicaSets  and other workload resources no longer treat the shutting-down Pod as a\\nvalid, in-service replica.\\nPods that shut down slowly should not continue to serve regular traffic and should start\\nterminating and finish processing open connections. Some applications need to go\\nbeyond finishing open connections and need more graceful termination, for example,\\nsession draining and completion.\\nAny endpoints that represent the terminating Pods are not immediately removed from\\nEndpointSlices, and a status indicating terminating state  is exposed from the', metadata={'source': './PDFS/Concepts.pdf', 'page': 97}),\n",
       " Document(page_content=\"EndpointSlices, and a status indicating terminating state  is exposed from the\\nEndpointSlice API (and the legacy Endpoints API). Terminating endpoints always have\\ntheir ready  status as false (for backward compatibility with versions before 1.26), so load\\nbalancers will not use it for regular traffic.\\nIf traffic draining on terminating Pod is needed, the actual readiness can be checked as a\\ncondition serving . You can find more details on how to implement connections draining\\nin the tutorial Pods And Endpoints Termination Flow\\nNote:  If you don't have the EndpointSliceTerminatingCondition  feature gate enabled in your\\ncluster (the gate is on by default from Kubernetes 1.22, and locked to default in 1.26), then the\\nKubernetes control plane removes a Pod from any relevant EndpointSlices as soon as the Pod's\\ntermination grace period begins . The behavior above is described when the feature gate \\nEndpointSliceTerminatingCondition  is enabled.\", metadata={'source': './PDFS/Concepts.pdf', 'page': 97}),\n",
       " Document(page_content='EndpointSliceTerminatingCondition  is enabled.\\nWhen the grace period expires, the kubelet triggers forcible shutdown. The container\\nruntime sends SIGKILL  to any processes still running in any container in the Pod. The\\nkubelet also cleans up a hidden pause  container if that container runtime uses one.\\nThe kubelet transitions the Pod into a terminal phase ( Failed  or Succeeded  depending on\\nthe end state of its containers). This step is guaranteed since version 1.27.1. \\n2. \\n3. \\n1. \\n2.', metadata={'source': './PDFS/Concepts.pdf', 'page': 97}),\n",
       " Document(page_content=\"The kubelet triggers forcible removal of Pod object from the API server, by setting grace\\nperiod to 0 (immediate deletion).\\nThe API server deletes the Pod's API object, which is then no longer visible from any\\nclient.\\nForced Pod termination\\nCaution:  Forced deletions can be potentially disruptive for some workloads and their Pods.\\nBy default, all deletes are graceful within 30 seconds. The kubectl delete  command supports the \\n--grace-period=<seconds>  option which allows you to override the default and specify your\\nown value.\\nSetting the grace period to 0 forcibly and immediately deletes the Pod from the API server. If\\nthe Pod was still running on a node, that forcible deletion triggers the kubelet to begin\\nimmediate cleanup.\\nNote:  You must specify an additional flag --force  along with --grace-period=0  in order to\\nperform force deletions.\\nWhen a force deletion is performed, the API server does not wait for confirmation from the\", metadata={'source': './PDFS/Concepts.pdf', 'page': 98}),\n",
       " Document(page_content=\"When a force deletion is performed, the API server does not wait for confirmation from the\\nkubelet that the Pod has been terminated on the node it was running on. It removes the Pod in\\nthe API immediately so a new Pod can be created with the same name. On the node, Pods that\\nare set to terminate immediately will still be given a small grace period before being force\\nkilled.\\nCaution:  Immediate deletion does not wait for confirmation that the running resource has\\nbeen terminated. The resource may continue to run on the cluster indefinitely.\\nIf you need to force-delete Pods that are part of a StatefulSet, refer to the task documentation\\nfor deleting Pods from a StatefulSet .\\nGarbage collection of Pods\\nFor failed Pods, the API objects remain in the cluster's API until a human or controller  process\\nexplicitly removes them.\\nThe Pod garbage collector (PodGC), which is a controller in the control plane, cleans up\", metadata={'source': './PDFS/Concepts.pdf', 'page': 98}),\n",
       " Document(page_content='The Pod garbage collector (PodGC), which is a controller in the control plane, cleans up\\nterminated Pods (with a phase of Succeeded  or Failed ), when the number of Pods exceeds the\\nconfigured threshold (determined by terminated-pod-gc-threshold  in the kube-controller-\\nmanager). This avoids a resource leak as Pods are created and terminated over time.\\nAdditionally, PodGC cleans up any Pods which satisfy any of the following conditions:\\nare orphan Pods - bound to a node which no longer exists,\\nare unscheduled terminating Pods,\\nare terminating Pods, bound to a non-ready node tainted with node.kubernetes.io/out-of-\\nservice , when the NodeOutOfServiceVolumeDetach  feature gate is enabled.\\nWhen the PodDisruptionConditions  feature gate is enabled, along with cleaning up the Pods,\\nPodGC will also mark them as failed if they are in a non-terminal phase. Also, PodGC adds a\\nPod disruption condition when cleaning up an orphan Pod. See Pod disruption conditions  for\\nmore details.3. \\n4. \\n1. \\n2.', metadata={'source': './PDFS/Concepts.pdf', 'page': 98}),\n",
       " Document(page_content='more details.3. \\n4. \\n1. \\n2. \\n3.', metadata={'source': './PDFS/Concepts.pdf', 'page': 98}),\n",
       " Document(page_content=\"What's next\\nGet hands-on experience attaching handlers to container lifecycle events .\\nGet hands-on experience configuring Liveness, Readiness and Startup Probes .\\nLearn more about container lifecycle hooks .\\nFor detailed information about Pod and container status in the API, see the API reference\\ndocumentation covering status  for Pod.\\nInit Containers\\nThis page provides an overview of init containers: specialized containers that run before app\\ncontainers in a Pod. Init containers can contain utilities or setup scripts not present in an app\\nimage.\\nYou can specify init containers in the Pod specification alongside the containers  array (which\\ndescribes app containers).\\nUnderstanding init containers\\nA Pod can have multiple containers running apps within it, but it can also have one or more init\\ncontainers, which are run before the app containers are started.\\nInit containers are exactly like regular containers, except:\\nInit containers always run to completion.\", metadata={'source': './PDFS/Concepts.pdf', 'page': 99}),\n",
       " Document(page_content=\"Init containers always run to completion.\\nEach init container must complete successfully before the next one starts.\\nIf a Pod's init container fails, the kubelet repeatedly restarts that init container until it succeeds.\\nHowever, if the Pod has a restartPolicy  of Never, and an init container fails during startup of\\nthat Pod, Kubernetes treats the overall Pod as failed.\\nTo specify an init container for a Pod, add the initContainers  field into the Pod specification , as\\nan array of container  items (similar to the app containers  field and its contents). See Container\\nin the API reference for more details.\\nThe status of the init containers is returned in .status.initContainerStatuses  field as an array of\\nthe container statuses (similar to the .status.containerStatuses  field).\\nDifferences from regular containers\\nInit containers support all the fields and features of app containers, including resource limits,\", metadata={'source': './PDFS/Concepts.pdf', 'page': 99}),\n",
       " Document(page_content='Init containers support all the fields and features of app containers, including resource limits, \\nvolumes , and security settings. However, the resource requests and limits for an init container\\nare handled differently, as documented in Resource sharing within containers .\\nAlso, init containers do not support lifecycle , livenessProbe , readinessProbe , or startupProbe\\nbecause they must run to completion before the Pod can be ready.• \\n• \\n• \\n• \\n• \\n•', metadata={'source': './PDFS/Concepts.pdf', 'page': 99}),\n",
       " Document(page_content='If you specify multiple init containers for a Pod, kubelet runs each init container sequentially.\\nEach init container must succeed before the next can run. When all of the init containers have\\nrun to completion, kubelet initializes the application containers for the Pod and runs them as\\nusual.\\nUsing init containers\\nBecause init containers have separate images from app containers, they have some advantages\\nfor start-up related code:\\nInit containers can contain utilities or custom code for setup that are not present in an\\napp image. For example, there is no need to make an image FROM  another image just to\\nuse a tool like sed, awk, python , or dig during setup.\\nThe application image builder and deployer roles can work independently without the\\nneed to jointly build a single app image.\\nInit containers can run with a different view of the filesystem than app containers in the\\nsame Pod. Consequently, they can be given access to Secrets  that app containers cannot\\naccess.', metadata={'source': './PDFS/Concepts.pdf', 'page': 100}),\n",
       " Document(page_content='same Pod. Consequently, they can be given access to Secrets  that app containers cannot\\naccess.\\nBecause init containers run to completion before any app containers start, init containers\\noffer a mechanism to block or delay app container startup until a set of preconditions are\\nmet. Once preconditions are met, all of the app containers in a Pod can start in parallel.\\nInit containers can securely run utilities or custom code that would otherwise make an\\napp container image less secure. By keeping unnecessary tools separate you can limit the\\nattack surface of your app container image.\\nExamples\\nHere are some ideas for how to use init containers:\\nWait for a Service  to be created, using a shell one-line command like:\\nfor i in {1..100 }; do sleep 1; if nslookup myservice; then exit 0; fi; done ; exit 1\\nRegister this Pod with a remote server from the downward API with a command like:\\ncurl -X POST http:// $MANAGEMENT_SERVICE_HOST :$MANAGEMENT_SERVICE_PO', metadata={'source': './PDFS/Concepts.pdf', 'page': 100}),\n",
       " Document(page_content=\"curl -X POST http:// $MANAGEMENT_SERVICE_HOST :$MANAGEMENT_SERVICE_PO\\nRT/register -d 'instance=$(<POD_NAME>)&ip=$(<POD_IP>)'\\nWait for some time before starting the app container with a command like\\nsleep 60\\nClone a Git repository into a Volume\\nPlace values into a configuration file and run a template tool to dynamically generate a\\nconfiguration file for the main app container. For example, place the POD_IP  value in a\\nconfiguration and generate the main app configuration file using Jinja.\\nInit containers in use\\nThis example defines a simple Pod that has two init containers. The first waits for myservice ,\\nand the second waits for mydb . Once both init containers complete, the Pod runs the app\\ncontainer from its spec section.• \\n• \\n• \\n• \\n• \\n• \\n• \\n• \\n• \\n•\", metadata={'source': './PDFS/Concepts.pdf', 'page': 100}),\n",
       " Document(page_content='apiVersion : v1\\nkind: Pod\\nmetadata :\\n  name : myapp-pod\\n  labels :\\n    app.kubernetes.io/name : MyApp\\nspec:\\n  containers :\\n  - name : myapp-container\\n    image : busybox:1.28\\n    command : [\\'sh\\', \\'-c\\', \\'echo The app is running! && sleep 3600\\' ]\\n  initContainers :\\n  - name : init-myservice\\n    image : busybox:1.28\\n    command : [\\'sh\\', \\'-c\\', \"until nslookup myservice.$(cat /var/run/secrets/kubernetes.io/\\nserviceaccount/namespace).svc.cluster.local; do echo waiting for myservice; sleep 2; done\" ]\\n  - name : init-mydb\\n    image : busybox:1.28\\n    command : [\\'sh\\', \\'-c\\', \"until nslookup mydb.$(cat /var/run/secrets/kubernetes.io/\\nserviceaccount/namespace).svc.cluster.local; do echo waiting for mydb; sleep 2; done\" ]\\nYou can start this Pod by running:\\nkubectl apply -f myapp.yaml\\nThe output is similar to this:\\npod/myapp-pod created\\nAnd check on its status with:\\nkubectl get -f myapp.yaml\\nThe output is similar to this:\\nNAME        READY     STATUS     RESTARTS   AGE', metadata={'source': './PDFS/Concepts.pdf', 'page': 101}),\n",
       " Document(page_content='The output is similar to this:\\nNAME        READY     STATUS     RESTARTS   AGE\\nmyapp-pod   0/1       Init:0/2   0          6m\\nor for more details:\\nkubectl describe -f myapp.yaml\\nThe output is similar to this:\\nName:          myapp-pod\\nNamespace:     default\\n[...]\\nLabels:        app.kubernetes.io/name=MyApp\\nStatus:        Pending\\n[...]\\nInit Containers:\\n  init-myservice:\\n[...]\\n    State:         Running', metadata={'source': './PDFS/Concepts.pdf', 'page': 101}),\n",
       " Document(page_content='[...]\\n  init-mydb:\\n[...]\\n    State:         Waiting\\n      Reason:      PodInitializing\\n    Ready:         False\\n[...]\\nContainers:\\n  myapp-container:\\n[...]\\n    State:         Waiting\\n      Reason:      PodInitializing\\n    Ready:         False\\n[...]\\nEvents:\\n  FirstSeen    LastSeen    Count    From                      SubObjectPath                           Type          \\nReason        Message\\n  ---------    --------    -----    ----                      -------------                           --------      ------        \\n-------\\n  16s          16s         1        {default-scheduler }                                              Normal        \\nScheduled     Successfully assigned myapp-pod to 172.17.4.201\\n  16s          16s         1        {kubelet 172.17.4.201}    spec.initContainers{init-myservice}     \\nNormal        Pulling       pulling image \"busybox\"\\n  13s          13s         1        {kubelet 172.17.4.201}    spec.initContainers{init-myservice}', metadata={'source': './PDFS/Concepts.pdf', 'page': 102}),\n",
       " Document(page_content='Normal        Pulled        Successfully pulled image \"busybox\"\\n  13s          13s         1        {kubelet 172.17.4.201}    spec.initContainers{init-myservice}     \\nNormal        Created       Created container init-myservice\\n  13s          13s         1        {kubelet 172.17.4.201}    spec.initContainers{init-myservice}     \\nNormal        Started       Started container init-myservice\\nTo see logs for the init containers in this Pod, run:\\nkubectl logs myapp-pod -c init-myservice # Inspect the first init container\\nkubectl logs myapp-pod -c init-mydb      # Inspect the second init container\\nAt this point, those init containers will be waiting to discover Services  named mydb  and \\nmyservice .\\nHere\\'s a configuration you can use to make those Services appear:\\n---\\napiVersion : v1\\nkind: Service\\nmetadata :\\n  name : myservice\\nspec:\\n  ports :\\n  - protocol : TCP\\n    port: 80\\n    targetPort : 9376\\n---\\napiVersion : v1\\nkind: Service', metadata={'source': './PDFS/Concepts.pdf', 'page': 102}),\n",
       " Document(page_content=\"metadata :\\n  name : mydb\\nspec:\\n  ports :\\n  - protocol : TCP\\n    port: 80\\n    targetPort : 9377\\nTo create the mydb  and myservice  services:\\nkubectl apply -f services.yaml\\nThe output is similar to this:\\nservice/myservice created\\nservice/mydb created\\nYou'll then see that those init containers complete, and that the myapp-pod  Pod moves into the\\nRunning state:\\nkubectl get -f myapp.yaml\\nThe output is similar to this:\\nNAME        READY     STATUS    RESTARTS   AGE\\nmyapp-pod   1/1       Running   0          9m\\nThis simple example should provide some inspiration for you to create your own init\\ncontainers. What's next  contains a link to a more detailed example.\\nDetailed behavior\\nDuring Pod startup, the kubelet delays running init containers until the networking and storage\\nare ready. Then the kubelet runs the Pod's init containers in the order they appear in the Pod's\\nspec.\\nEach init container must exit successfully before the next container starts. If a container fails to\", metadata={'source': './PDFS/Concepts.pdf', 'page': 103}),\n",
       " Document(page_content='Each init container must exit successfully before the next container starts. If a container fails to\\nstart due to the runtime or exits with failure, it is retried according to the Pod restartPolicy .\\nHowever, if the Pod restartPolicy  is set to Always, the init containers use restartPolicy\\nOnFailure.\\nA Pod cannot be Ready  until all init containers have succeeded. The ports on an init container\\nare not aggregated under a Service. A Pod that is initializing is in the Pending  state but should\\nhave a condition Initialized  set to false.\\nIf the Pod restarts , or is restarted, all init containers must execute again.\\nChanges to the init container spec are limited to the container image field. Altering an init\\ncontainer image field is equivalent to restarting the Pod.\\nBecause init containers can be restarted, retried, or re-executed, init container code should be\\nidempotent. In particular, code that writes to files on EmptyDirs  should be prepared for the', metadata={'source': './PDFS/Concepts.pdf', 'page': 103}),\n",
       " Document(page_content='idempotent. In particular, code that writes to files on EmptyDirs  should be prepared for the\\npossibility that an output file already exists.', metadata={'source': './PDFS/Concepts.pdf', 'page': 103}),\n",
       " Document(page_content='Init containers have all of the fields of an app container. However, Kubernetes prohibits \\nreadinessProbe  from being used because init containers cannot define readiness distinct from\\ncompletion. This is enforced during validation.\\nUse activeDeadlineSeconds  on the Pod to prevent init containers from failing forever. The active\\ndeadline includes init containers. However it is recommended to use activeDeadlineSeconds\\nonly if teams deploy their application as a Job, because activeDeadlineSeconds  has an effect\\neven after initContainer finished. The Pod which is already running correctly would be killed by\\nactiveDeadlineSeconds  if you set.\\nThe name of each app and init container in a Pod must be unique; a validation error is thrown\\nfor any container sharing a name with another.\\nAPI for sidecar containers\\nFEATURE STATE:  Kubernetes v1.28 [alpha]\\nStarting with Kubernetes 1.28 in alpha, a feature gate named SidecarContainers  allows you to', metadata={'source': './PDFS/Concepts.pdf', 'page': 104}),\n",
       " Document(page_content='Starting with Kubernetes 1.28 in alpha, a feature gate named SidecarContainers  allows you to\\nspecify a restartPolicy  for init containers which is independent of the Pod and other init\\ncontainers. Container probes  can also be added to control their lifecycle.\\nIf an init container is created with its restartPolicy  set to Always , it will start and remain\\nrunning during the entire life of the Pod, which is useful for running supporting services\\nseparated from the main application containers.\\nIf a readinessProbe  is specified for this init container, its result will be used to determine the \\nready  state of the Pod.\\nSince these containers are defined as init containers, they benefit from the same ordering and\\nsequential guarantees as other init containers, allowing them to be mixed with other init\\ncontainers into complex Pod initialization flows.\\nCompared to regular init containers, sidecar-style init containers continue to run and the next', metadata={'source': './PDFS/Concepts.pdf', 'page': 104}),\n",
       " Document(page_content=\"Compared to regular init containers, sidecar-style init containers continue to run and the next\\ninit container can begin starting once the kubelet has set the started  container status for the\\nsidecar-style init container to true. That status either becomes true because there is a process\\nrunning in the container and no startup probe defined, or as a result of its startupProbe\\nsucceeding.\\nThis feature can be used to implement the sidecar container pattern in a more robust way, as\\nthe kubelet always restarts a sidecar container if it fails.\\nHere's an example of a Deployment with two containers, one of which is a sidecar:\\napplication/deployment-sidecar.yaml  \\napiVersion : apps/v1\\nkind: Deployment\\nmetadata :\\n  name : myapp\\n  labels :\\n    app: myapp\\nspec:\\n  replicas : 1\\n  selector :\", metadata={'source': './PDFS/Concepts.pdf', 'page': 104}),\n",
       " Document(page_content='matchLabels :\\n      app: myapp\\n  template :\\n    metadata :\\n      labels :\\n        app: myapp\\n    spec:\\n      containers :\\n        - name : myapp\\n          image : alpine:latest\\n          command : [\\'sh\\', \\'-c\\', \\'while true; do echo \"logging\" >> /opt/logs.txt; sleep 1; done\\' ]\\n          volumeMounts :\\n            - name : data\\n              mountPath : /opt\\n      initContainers :\\n        - name : logshipper\\n          image : alpine:latest\\n          restartPolicy : Always\\n          command : [\\'sh\\', \\'-c\\', \\'tail -F /opt/logs.txt\\' ]\\n          volumeMounts :\\n            - name : data\\n              mountPath : /opt\\n      volumes :\\n        - name : data\\n          emptyDir : {}\\nThis feature is also useful for running Jobs with sidecars, as the sidecar container will not\\nprevent the Job from completing after the main container has finished.\\nHere\\'s an example of a Job with two containers, one of which is a sidecar:\\napplication/job/job-sidecar.yaml  \\napiVersion : batch/v1\\nkind: Job\\nmetadata :', metadata={'source': './PDFS/Concepts.pdf', 'page': 105}),\n",
       " Document(page_content='application/job/job-sidecar.yaml  \\napiVersion : batch/v1\\nkind: Job\\nmetadata :\\n  name : myjob\\nspec:\\n  template :\\n    spec:\\n      containers :\\n        - name : myjob\\n          image : alpine:latest\\n          command : [\\'sh\\', \\'-c\\', \\'echo \"logging\" > /opt/logs.txt\\' ]\\n          volumeMounts :\\n            - name : data\\n              mountPath : /opt\\n      initContainers :\\n        - name : logshipper\\n          image : alpine:latest\\n          restartPolicy : Always\\n          command : [\\'sh\\', \\'-c\\', \\'tail -F /opt/logs.txt\\' ]\\n          volumeMounts :', metadata={'source': './PDFS/Concepts.pdf', 'page': 105}),\n",
       " Document(page_content=\"- name : data\\n              mountPath : /opt\\n      restartPolicy : Never\\n      volumes :\\n        - name : data\\n          emptyDir : {}\\nResource sharing within containers\\nGiven the ordering and execution for init containers, the following rules for resource usage\\napply:\\nThe highest of any particular resource request or limit defined on all init containers is the\\neffective init request/limit . If any resource has no resource limit specified this is considered\\nas the highest limit.\\nThe Pod's effective request/limit  for a resource is the higher of:\\nthe sum of all app containers request/limit for a resource\\nthe effective init request/limit for a resource\\nScheduling is done based on effective requests/limits, which means init containers can\\nreserve resources for initialization that are not used during the life of the Pod.\\nThe QoS (quality of service) tier of the Pod's effective QoS tier  is the QoS tier for init\\ncontainers and app containers alike.\", metadata={'source': './PDFS/Concepts.pdf', 'page': 106}),\n",
       " Document(page_content=\"containers and app containers alike.\\nQuota and limits are applied based on the effective Pod request and limit.\\nPod level control groups (cgroups) are based on the effective Pod request and limit, the same as\\nthe scheduler.\\nPod restart reasons\\nA Pod can restart, causing re-execution of init containers, for the following reasons:\\nThe Pod infrastructure container is restarted. This is uncommon and would have to be\\ndone by someone with root access to nodes.\\nAll containers in a Pod are terminated while restartPolicy  is set to Always, forcing a\\nrestart, and the init container completion record has been lost due to garbage collection .\\nThe Pod will not be restarted when the init container image is changed, or the init container\\ncompletion record has been lost due to garbage collection. This applies for Kubernetes v1.20 and\\nlater. If you are using an earlier version of Kubernetes, consult the documentation for the\\nversion you are using.\\nWhat's next\", metadata={'source': './PDFS/Concepts.pdf', 'page': 106}),\n",
       " Document(page_content=\"version you are using.\\nWhat's next\\nRead about creating a Pod that has an init container\\nLearn how to debug init containers\\nRead about an overview of kubelet  and kubectl\\nLearn about the types of probes : liveness, readiness, startup probe.• \\n• \\n◦ \\n◦ \\n• \\n• \\n• \\n• \\n• \\n• \\n• \\n•\", metadata={'source': './PDFS/Concepts.pdf', 'page': 106}),\n",
       " Document(page_content='Disruptions\\nThis guide is for application owners who want to build highly available applications, and thus\\nneed to understand what types of disruptions can happen to Pods.\\nIt is also for cluster administrators who want to perform automated cluster actions, like\\nupgrading and autoscaling clusters.\\nVoluntary and involuntary disruptions\\nPods do not disappear until someone (a person or a controller) destroys them, or there is an\\nunavoidable hardware or system software error.\\nWe call these unavoidable cases involuntary disruptions  to an application. Examples are:\\na hardware failure of the physical machine backing the node\\ncluster administrator deletes VM (instance) by mistake\\ncloud provider or hypervisor failure makes VM disappear\\na kernel panic\\nthe node disappears from the cluster due to cluster network partition\\neviction of a pod due to the node being out-of-resources .\\nExcept for the out-of-resources condition, all these conditions should be familiar to most users;', metadata={'source': './PDFS/Concepts.pdf', 'page': 107}),\n",
       " Document(page_content=\"Except for the out-of-resources condition, all these conditions should be familiar to most users;\\nthey are not specific to Kubernetes.\\nWe call other cases voluntary disruptions . These include both actions initiated by the application\\nowner and those initiated by a Cluster Administrator. Typical application owner actions\\ninclude:\\ndeleting the deployment or other controller that manages the pod\\nupdating a deployment's pod template causing a restart\\ndirectly deleting a pod (e.g. by accident)\\nCluster administrator actions include:\\nDraining a node  for repair or upgrade.\\nDraining a node from a cluster to scale the cluster down (learn about Cluster\\nAutoscaling  ).\\nRemoving a pod from a node to permit something else to fit on that node.\\nThese actions might be taken directly by the cluster administrator, or by automation run by the\\ncluster administrator, or by your cluster hosting provider.\\nAsk your cluster administrator or consult your cloud provider or distribution documentation to\", metadata={'source': './PDFS/Concepts.pdf', 'page': 107}),\n",
       " Document(page_content='Ask your cluster administrator or consult your cloud provider or distribution documentation to\\ndetermine if any sources of voluntary disruptions are enabled for your cluster. If none are\\nenabled, you can skip creating Pod Disruption Budgets.\\nCaution:  Not all voluntary disruptions are constrained by Pod Disruption Budgets. For\\nexample, deleting deployments or pods bypasses Pod Disruption Budgets.• \\n• \\n• \\n• \\n• \\n• \\n• \\n• \\n• \\n• \\n• \\n•', metadata={'source': './PDFS/Concepts.pdf', 'page': 107}),\n",
       " Document(page_content='Dealing with disruptions\\nHere are some ways to mitigate involuntary disruptions:\\nEnsure your pod requests the resources  it needs.\\nReplicate your application if you need higher availability. (Learn about running replicated\\nstateless  and stateful  applications.)\\nFor even higher availability when running replicated applications, spread applications\\nacross racks (using anti-affinity ) or across zones (if using a multi-zone cluster .)\\nThe frequency of voluntary disruptions varies. On a basic Kubernetes cluster, there are no\\nautomated voluntary disruptions (only user-triggered ones). However, your cluster\\nadministrator or hosting provider may run some additional services which cause voluntary\\ndisruptions. For example, rolling out node software updates can cause voluntary disruptions.\\nAlso, some implementations of cluster (node) autoscaling may cause voluntary disruptions to\\ndefragment and compact nodes. Your cluster administrator or hosting provider should have', metadata={'source': './PDFS/Concepts.pdf', 'page': 108}),\n",
       " Document(page_content='defragment and compact nodes. Your cluster administrator or hosting provider should have\\ndocumented what level of voluntary disruptions, if any, to expect. Certain configuration\\noptions, such as using PriorityClasses  in your pod spec can also cause voluntary (and\\ninvoluntary) disruptions.\\nPod disruption budgets\\nFEATURE STATE:  Kubernetes v1.21 [stable]\\nKubernetes offers features to help you run highly available applications even when you\\nintroduce frequent voluntary disruptions.\\nAs an application owner, you can create a PodDisruptionBudget (PDB) for each application. A\\nPDB limits the number of Pods of a replicated application that are down simultaneously from\\nvoluntary disruptions. For example, a quorum-based application would like to ensure that the\\nnumber of replicas running is never brought below the number needed for a quorum. A web\\nfront end might want to ensure that the number of replicas serving load never falls below a\\ncertain percentage of the total.', metadata={'source': './PDFS/Concepts.pdf', 'page': 108}),\n",
       " Document(page_content=\"certain percentage of the total.\\nCluster managers and hosting providers should use tools which respect PodDisruptionBudgets\\nby calling the Eviction API  instead of directly deleting pods or deployments.\\nFor example, the kubectl drain  subcommand lets you mark a node as going out of service.\\nWhen you run kubectl drain , the tool tries to evict all of the Pods on the Node you're taking out\\nof service. The eviction request that kubectl  submits on your behalf may be temporarily\\nrejected, so the tool periodically retries all failed requests until all Pods on the target node are\\nterminated, or until a configurable timeout is reached.\\nA PDB specifies the number of replicas that an application can tolerate having, relative to how\\nmany it is intended to have. For example, a Deployment which has a .spec.replicas: 5  is\\nsupposed to have 5 pods at any given time. If its PDB allows for there to be 4 at a time, then the\", metadata={'source': './PDFS/Concepts.pdf', 'page': 108}),\n",
       " Document(page_content=\"supposed to have 5 pods at any given time. If its PDB allows for there to be 4 at a time, then the\\nEviction API will allow voluntary disruption of one (but not two) pods at a time.\\nThe group of pods that comprise the application is specified using a label selector, the same as\\nthe one used by the application's controller (deployment, stateful-set, etc).• \\n• \\n•\", metadata={'source': './PDFS/Concepts.pdf', 'page': 108}),\n",
       " Document(page_content='The \"intended\" number of pods is computed from the .spec.replicas  of the workload resource\\nthat is managing those pods. The control plane discovers the owning workload resource by\\nexamining the .metadata.ownerReferences  of the Pod.\\nInvoluntary disruptions  cannot be prevented by PDBs; however they do count against the\\nbudget.\\nPods which are deleted or unavailable due to a rolling upgrade to an application do count\\nagainst the disruption budget, but workload resources (such as Deployment and StatefulSet) are\\nnot limited by PDBs when doing rolling upgrades. Instead, the handling of failures during\\napplication updates is configured in the spec for the specific workload resource.\\nIt is recommended to set AlwaysAllow  Unhealthy Pod Eviction Policy  to your\\nPodDisruptionBudgets to support eviction of misbehaving applications during a node drain.\\nThe default behavior is to wait for the application pods to become healthy  before the drain can\\nproceed.', metadata={'source': './PDFS/Concepts.pdf', 'page': 109}),\n",
       " Document(page_content='proceed.\\nWhen a pod is evicted using the eviction API, it is gracefully terminated , honoring the \\nterminationGracePeriodSeconds  setting in its PodSpec .\\nPodDisruptionBudget example\\nConsider a cluster with 3 nodes, node-1  through node-3 . The cluster is running several\\napplications. One of them has 3 replicas initially called pod-a , pod-b , and pod-c . Another,\\nunrelated pod without a PDB, called pod-x , is also shown. Initially, the pods are laid out as\\nfollows:\\nnode-1 node-2 node-3\\npod-a available pod-b available pod-c available\\npod-x available\\nAll 3 pods are part of a deployment, and they collectively have a PDB which requires there be at\\nleast 2 of the 3 pods to be available at all times.\\nFor example, assume the cluster administrator wants to reboot into a new kernel version to fix\\na bug in the kernel. The cluster administrator first tries to drain node-1  using the kubectl drain\\ncommand. That tool tries to evict pod-a  and pod-x . This succeeds immediately. Both pods go', metadata={'source': './PDFS/Concepts.pdf', 'page': 109}),\n",
       " Document(page_content='command. That tool tries to evict pod-a  and pod-x . This succeeds immediately. Both pods go\\ninto the terminating  state at the same time. This puts the cluster in this state:\\nnode-1 draining node-2 node-3\\npod-a terminating pod-b available pod-c available\\npod-x terminating\\nThe deployment notices that one of the pods is terminating, so it creates a replacement called \\npod-d . Since node-1  is cordoned, it lands on another node. Something has also created pod-y  as\\na replacement for pod-x .\\n(Note: for a StatefulSet, pod-a , which would be called something like pod-0 , would need to\\nterminate completely before its replacement, which is also called pod-0  but has a different UID,\\ncould be created. Otherwise, the example applies to a StatefulSet as well.)\\nNow the cluster is in this state:', metadata={'source': './PDFS/Concepts.pdf', 'page': 109}),\n",
       " Document(page_content='node-1 draining node-2 node-3\\npod-a terminating pod-b available pod-c available\\npod-x terminating pod-d starting pod-y\\nAt some point, the pods terminate, and the cluster looks like this:\\nnode-1 drained node-2 node-3\\npod-b available pod-c available\\npod-d starting pod-y\\nAt this point, if an impatient cluster administrator tries to drain node-2  or node-3 , the drain\\ncommand will block, because there are only 2 available pods for the deployment, and its PDB\\nrequires at least 2. After some time passes, pod-d  becomes available.\\nThe cluster state now looks like this:\\nnode-1 drained node-2 node-3\\npod-b available pod-c available\\npod-d available pod-y\\nNow, the cluster administrator tries to drain node-2 . The drain command will try to evict the\\ntwo pods in some order, say pod-b  first and then pod-d . It will succeed at evicting pod-b . But,\\nwhen it tries to evict pod-d , it will be refused because that would leave only one pod available\\nfor the deployment.', metadata={'source': './PDFS/Concepts.pdf', 'page': 110}),\n",
       " Document(page_content=\"for the deployment.\\nThe deployment creates a replacement for pod-b  called pod-e . Because there are not enough\\nresources in the cluster to schedule pod-e  the drain will again block. The cluster may end up in\\nthis state:\\nnode-1 drained node-2 node-3 no node\\npod-b terminating pod-c available pod-e pending\\npod-d available pod-y\\nAt this point, the cluster administrator needs to add a node back to the cluster to proceed with\\nthe upgrade.\\nYou can see how Kubernetes varies the rate at which disruptions can happen, according to:\\nhow many replicas an application needs\\nhow long it takes to gracefully shutdown an instance\\nhow long it takes a new instance to start up\\nthe type of controller\\nthe cluster's resource capacity\\nPod disruption conditions\\nFEATURE STATE:  Kubernetes v1.26 [beta]\\nNote:  In order to use this behavior, you must have the PodDisruptionConditions  feature gate\\nenabled in your cluster.• \\n• \\n• \\n• \\n•\", metadata={'source': './PDFS/Concepts.pdf', 'page': 110}),\n",
       " Document(page_content='When enabled, a dedicated Pod DisruptionTarget  condition  is added to indicate that the Pod is\\nabout to be deleted due to a disruption . The reason  field of the condition additionally indicates\\none of the following reasons for the Pod termination:\\nPreemptionByScheduler\\nPod is due to be preempted  by a scheduler in order to accommodate a new Pod with a\\nhigher priority. For more information, see Pod priority preemption .\\nDeletionByTaintManager\\nPod is due to be deleted by Taint Manager (which is part of the node lifecycle controller\\nwithin kube-controller-manager ) due to a NoExecute  taint that the Pod does not tolerate;\\nsee taint -based evictions.\\nEvictionByEvictionAPI\\nPod has been marked for eviction using the Kubernetes API  .\\nDeletionByPodGC\\nPod, that is bound to a no longer existing Node, is due to be deleted by Pod garbage\\ncollection .\\nTerminationByKubelet\\nPod has been terminated by the kubelet, because of either node pressure eviction  or the \\ngraceful node shutdown .', metadata={'source': './PDFS/Concepts.pdf', 'page': 111}),\n",
       " Document(page_content=\"graceful node shutdown .\\nNote:  A Pod disruption might be interrupted. The control plane might re-attempt to continue\\nthe disruption of the same Pod, but it is not guaranteed. As a result, the DisruptionTarget\\ncondition might be added to a Pod, but that Pod might then not actually be deleted. In such a\\nsituation, after some time, the Pod disruption condition will be cleared.\\nWhen the PodDisruptionConditions  feature gate is enabled, along with cleaning up the pods,\\nthe Pod garbage collector (PodGC) will also mark them as failed if they are in a non-terminal\\nphase (see also Pod garbage collection ).\\nWhen using a Job (or CronJob), you may want to use these Pod disruption conditions as part of\\nyour Job's Pod failure policy .\\nSeparating Cluster Owner and Application Owner Roles\\nOften, it is useful to think of the Cluster Manager and Application Owner as separate roles with\\nlimited knowledge of each other. This separation of responsibilities may make sense in these\\nscenarios:\", metadata={'source': './PDFS/Concepts.pdf', 'page': 111}),\n",
       " Document(page_content='scenarios:\\nwhen there are many application teams sharing a Kubernetes cluster, and there is natural\\nspecialization of roles\\nwhen third-party tools or services are used to automate cluster management\\nPod Disruption Budgets support this separation of roles by providing an interface between the\\nroles.\\nIf you do not have such a separation of responsibilities in your organization, you may not need\\nto use Pod Disruption Budgets.• \\n•', metadata={'source': './PDFS/Concepts.pdf', 'page': 111}),\n",
       " Document(page_content=\"How to perform Disruptive Actions on your Cluster\\nIf you are a Cluster Administrator, and you need to perform a disruptive action on all the nodes\\nin your cluster, such as a node or system software upgrade, here are some options:\\nAccept downtime during the upgrade.\\nFailover to another complete replica cluster.\\nNo downtime, but may be costly both for the duplicated nodes and for human\\neffort to orchestrate the switchover.\\nWrite disruption tolerant applications and use PDBs.\\nNo downtime.\\nMinimal resource duplication.\\nAllows more automation of cluster administration.\\nWriting disruption-tolerant applications is tricky, but the work to tolerate\\nvoluntary disruptions largely overlaps with work to support autoscaling and\\ntolerating involuntary disruptions.\\nWhat's next\\nFollow steps to protect your application by configuring a Pod Disruption Budget .\\nLearn more about draining nodes\\nLearn about updating a deployment  including steps to maintain its availability during the\\nrollout.\", metadata={'source': './PDFS/Concepts.pdf', 'page': 112}),\n",
       " Document(page_content=\"Learn about updating a deployment  including steps to maintain its availability during the\\nrollout.\\nEphemeral Containers\\nFEATURE STATE:  Kubernetes v1.25 [stable]\\nThis page provides an overview of ephemeral containers: a special type of container that runs\\ntemporarily in an existing Pod to accomplish user-initiated actions such as troubleshooting. You\\nuse ephemeral containers to inspect services rather than to build applications.\\nUnderstanding ephemeral containers\\nPods  are the fundamental building block of Kubernetes applications. Since Pods are intended to\\nbe disposable and replaceable, you cannot add a container to a Pod once it has been created.\\nInstead, you usually delete and replace Pods in a controlled fashion using deployments .\\nSometimes it's necessary to inspect the state of an existing Pod, however, for example to\\ntroubleshoot a hard-to-reproduce bug. In these cases you can run an ephemeral container in an\\nexisting Pod to inspect its state and run arbitrary commands.\", metadata={'source': './PDFS/Concepts.pdf', 'page': 112}),\n",
       " Document(page_content='existing Pod to inspect its state and run arbitrary commands.\\nWhat is an ephemeral container?\\nEphemeral containers differ from other containers in that they lack guarantees for resources or\\nexecution, and they will never be automatically restarted, so they are not appropriate for• \\n• \\n◦ \\n• \\n◦ \\n◦ \\n◦ \\n◦ \\n• \\n• \\n•', metadata={'source': './PDFS/Concepts.pdf', 'page': 112}),\n",
       " Document(page_content=\"building applications. Ephemeral containers are described using the same ContainerSpec  as\\nregular containers, but many fields are incompatible and disallowed for ephemeral containers.\\nEphemeral containers may not have ports, so fields such as ports , livenessProbe , \\nreadinessProbe  are disallowed.\\nPod resource allocations are immutable, so setting resources  is disallowed.\\nFor a complete list of allowed fields, see the EphemeralContainer reference\\ndocumentation .\\nEphemeral containers are created using a special ephemeralcontainers  handler in the API rather\\nthan by adding them directly to pod.spec , so it's not possible to add an ephemeral container\\nusing kubectl edit .\\nLike regular containers, you may not change or remove an ephemeral container after you have\\nadded it to a Pod.\\nNote:  Ephemeral containers are not supported by static pods .\\nUses for ephemeral containers\\nEphemeral containers are useful for interactive troubleshooting when kubectl exec  is\", metadata={'source': './PDFS/Concepts.pdf', 'page': 113}),\n",
       " Document(page_content=\"Ephemeral containers are useful for interactive troubleshooting when kubectl exec  is\\ninsufficient because a container has crashed or a container image doesn't include debugging\\nutilities.\\nIn particular, distroless images  enable you to deploy minimal container images that reduce\\nattack surface and exposure to bugs and vulnerabilities. Since distroless images do not include a\\nshell or any debugging utilities, it's difficult to troubleshoot distroless images using kubectl exec\\nalone.\\nWhen using ephemeral containers, it's helpful to enable process namespace sharing  so you can\\nview processes in other containers.\\nWhat's next\\nLearn how to debug pods using ephemeral containers .\\nPod Quality of Service Classes\\nThis page introduces Quality of Service (QoS) classes  in Kubernetes, and explains how\\nKubernetes assigns a QoS class to each Pod as a consequence of the resource constraints that\\nyou specify for the containers in that Pod. Kubernetes relies on this classification to make\", metadata={'source': './PDFS/Concepts.pdf', 'page': 113}),\n",
       " Document(page_content='you specify for the containers in that Pod. Kubernetes relies on this classification to make\\ndecisions about which Pods to evict when there are not enough available resources on a Node.\\nQuality of Service classes\\nKubernetes classifies the Pods that you run and allocates each Pod into a specific quality of\\nservice (QoS) class . Kubernetes uses that classification to influence how different pods are\\nhandled. Kubernetes does this classification based on the resource requests  of the Containers  in\\nthat Pod, along with how those requests relate to resource limits. This is known as Quality of\\nService  (QoS) class. Kubernetes assigns every Pod a QoS class based on the resource requests• \\n• \\n• \\n•', metadata={'source': './PDFS/Concepts.pdf', 'page': 113}),\n",
       " Document(page_content='and limits of its component Containers. QoS classes are used by Kubernetes to decide which\\nPods to evict from a Node experiencing Node Pressure . The possible QoS classes are \\nGuaranteed , Burstable , and BestEffort . When a Node runs out of resources, Kubernetes will first\\nevict BestEffort  Pods running on that Node, followed by Burstable  and finally Guaranteed  Pods.\\nWhen this eviction is due to resource pressure, only Pods exceeding resource requests are\\ncandidates for eviction.\\nGuaranteed\\nPods that are Guaranteed  have the strictest resource limits and are least likely to face eviction.\\nThey are guaranteed not to be killed until they exceed their limits or there are no lower-priority\\nPods that can be preempted from the Node. They may not acquire resources beyond their\\nspecified limits. These Pods can also make use of exclusive CPUs using the static  CPU\\nmanagement policy.\\nCriteria\\nFor a Pod to be given a QoS class of Guaranteed :', metadata={'source': './PDFS/Concepts.pdf', 'page': 114}),\n",
       " Document(page_content='management policy.\\nCriteria\\nFor a Pod to be given a QoS class of Guaranteed :\\nEvery Container in the Pod must have a memory limit and a memory request.\\nFor every Container in the Pod, the memory limit must equal the memory request.\\nEvery Container in the Pod must have a CPU limit and a CPU request.\\nFor every Container in the Pod, the CPU limit must equal the CPU request.\\nBurstable\\nPods that are Burstable  have some lower-bound resource guarantees based on the request, but\\ndo not require a specific limit. If a limit is not specified, it defaults to a limit equivalent to the\\ncapacity of the Node, which allows the Pods to flexibly increase their resources if resources are\\navailable. In the event of Pod eviction due to Node resource pressure, these Pods are evicted\\nonly after all BestEffort  Pods are evicted. Because a Burstable  Pod can include a Container that\\nhas no resource limits or requests, a Pod that is Burstable  can try to use any amount of node\\nresources.\\nCriteria', metadata={'source': './PDFS/Concepts.pdf', 'page': 114}),\n",
       " Document(page_content=\"resources.\\nCriteria\\nA Pod is given a QoS class of Burstable  if:\\nThe Pod does not meet the criteria for QoS class Guaranteed .\\nAt least one Container in the Pod has a memory or CPU request or limit.\\nBestEffort\\nPods in the BestEffort  QoS class can use node resources that aren't specifically assigned to Pods\\nin other QoS classes. For example, if you have a node with 16 CPU cores available to the\\nkubelet, and you assign 4 CPU cores to a Guaranteed  Pod, then a Pod in the BestEffort  QoS\\nclass can try to use any amount of the remaining 12 CPU cores.\\nThe kubelet prefers to evict BestEffort  Pods if the node comes under resource pressure.• \\n• \\n• \\n• \\n• \\n•\", metadata={'source': './PDFS/Concepts.pdf', 'page': 114}),\n",
       " Document(page_content=\"Criteria\\nA Pod has a QoS class of BestEffort  if it doesn't meet the criteria for either Guaranteed  or \\nBurstable . In other words, a Pod is BestEffort  only if none of the Containers in the Pod have a\\nmemory limit or a memory request, and none of the Containers in the Pod have a CPU limit or\\na CPU request. Containers in a Pod can request other resources (not CPU or memory) and still\\nbe classified as BestEffort .\\nMemory QoS with cgroup v2\\nFEATURE STATE:  Kubernetes v1.22 [alpha]\\nMemory QoS uses the memory controller of cgroup v2 to guarantee memory resources in\\nKubernetes. Memory requests and limits of containers in pod are used to set specific interfaces \\nmemory.min  and memory.high  provided by the memory controller. When memory.min  is set to\\nmemory requests, memory resources are reserved and never reclaimed by the kernel; this is\\nhow Memory QoS ensures memory availability for Kubernetes pods. And if memory limits are\", metadata={'source': './PDFS/Concepts.pdf', 'page': 115}),\n",
       " Document(page_content='how Memory QoS ensures memory availability for Kubernetes pods. And if memory limits are\\nset in the container, this means that the system needs to limit container memory usage;\\nMemory QoS uses memory.high  to throttle workload approaching its memory limit, ensuring\\nthat the system is not overwhelmed by instantaneous memory allocation.\\nMemory QoS relies on QoS class to determine which settings to apply; however, these are\\ndifferent mechanisms that both provide controls over quality of service.\\nSome behavior is independent of QoS class\\nCertain behavior is independent of the QoS class assigned by Kubernetes. For example:\\nAny Container exceeding a resource limit will be killed and restarted by the kubelet\\nwithout affecting other Containers in that Pod.\\nIf a Container exceeds its resource request and the node it runs on faces resource\\npressure, the Pod it is in becomes a candidate for eviction . If this occurs, all Containers in', metadata={'source': './PDFS/Concepts.pdf', 'page': 115}),\n",
       " Document(page_content=\"pressure, the Pod it is in becomes a candidate for eviction . If this occurs, all Containers in\\nthe Pod will be terminated. Kubernetes may create a replacement Pod, usually on a\\ndifferent node.\\nThe resource request of a Pod is equal to the sum of the resource requests of its\\ncomponent Containers, and the resource limit of a Pod is equal to the sum of the resource\\nlimits of its component Containers.\\nThe kube-scheduler does not consider QoS class when selecting which Pods to preempt .\\nPreemption can occur when a cluster does not have enough resources to run all the Pods\\nyou defined.\\nWhat's next\\nLearn about resource management for Pods and Containers .\\nLearn about Node-pressure eviction .\\nLearn about Pod priority and preemption .\\nLearn about Pod disruptions .\\nLearn how to assign memory resources to containers and pods .• \\n• \\n• \\n• \\n• \\n• \\n• \\n• \\n•\", metadata={'source': './PDFS/Concepts.pdf', 'page': 115}),\n",
       " Document(page_content='Learn how to assign CPU resources to containers and pods .\\nLearn how to configure Quality of Service for Pods .\\nUser Namespaces\\nFEATURE STATE:  Kubernetes v1.25 [alpha]\\nThis page explains how user namespaces are used in Kubernetes pods. A user namespace\\nisolates the user running inside the container from the one in the host.\\nA process running as root in a container can run as a different (non-root) user in the host; in\\nother words, the process has full privileges for operations inside the user namespace, but is\\nunprivileged for operations outside the namespace.\\nYou can use this feature to reduce the damage a compromised container can do to the host or\\nother pods in the same node. There are several security vulnerabilities  rated either HIGH  or \\nCRITICAL  that were not exploitable when user namespaces is active. It is expected user\\nnamespace will mitigate some future vulnerabilities too.\\nBefore you begin', metadata={'source': './PDFS/Concepts.pdf', 'page': 116}),\n",
       " Document(page_content=\"namespace will mitigate some future vulnerabilities too.\\nBefore you begin\\nNote:  This section links to third party projects that provide functionality required by\\nKubernetes. The Kubernetes project authors aren't responsible for these projects, which are\\nlisted alphabetically. To add a project to this list, read the content guide  before submitting a\\nchange. More information.\\nThis is a Linux-only feature and support is needed in Linux for idmap mounts on the\\nfilesystems used. This means:\\nOn the node, the filesystem you use for /var/lib/kubelet/pods/ , or the custom directory\\nyou configure for this, needs idmap mount support.\\nAll the filesystems used in the pod's volumes must support idmap mounts.\\nIn practice this means you need at least Linux 6.3, as tmpfs started supporting idmap mounts in\\nthat version. This is usually needed as several Kubernetes features use tmpfs (the service\\naccount token that is mounted by default uses a tmpfs, Secrets use a tmpfs, etc.)\", metadata={'source': './PDFS/Concepts.pdf', 'page': 116}),\n",
       " Document(page_content='account token that is mounted by default uses a tmpfs, Secrets use a tmpfs, etc.)\\nSome popular filesystems that support idmap mounts in Linux 6.3 are: btrfs, ext4, xfs, fat, tmpfs,\\noverlayfs.\\nIn addition, support is needed in the container runtime  to use this feature with Kubernetes\\npods:\\nCRI-O: version 1.25 (and later) supports user namespaces for containers.\\ncontainerd v1.7 is not compatible with the userns support in Kubernetes v1.27 to v1.28.\\nKubernetes v1.25 and v1.26 used an earlier implementation that is compatible with containerd\\nv1.7, in terms of userns support. If you are using a version of Kubernetes other than 1.28, check\\nthe documentation for that version of Kubernetes for the most relevant information. If there is a\\nnewer release of containerd than v1.7 available for use, also check the containerd\\ndocumentation for compatibility information.• \\n• \\n• \\n• \\n•', metadata={'source': './PDFS/Concepts.pdf', 'page': 116}),\n",
       " Document(page_content='You can see the status of user namespaces support in cri-dockerd tracked in an issue  on GitHub.\\nIntroduction\\nUser namespaces is a Linux feature that allows to map users in the container to different users\\nin the host. Furthermore, the capabilities granted to a pod in a user namespace are valid only in\\nthe namespace and void outside of it.\\nA pod can opt-in to use user namespaces by setting the pod.spec.hostUsers  field to false.\\nThe kubelet will pick host UIDs/GIDs a pod is mapped to, and will do so in a way to guarantee\\nthat no two pods on the same node use the same mapping.\\nThe runAsUser , runAsGroup , fsGroup , etc. fields in the pod.spec  always refer to the user inside\\nthe container.\\nThe valid UIDs/GIDs when this feature is enabled is the range 0-65535. This applies to files and\\nprocesses ( runAsUser , runAsGroup , etc.).\\nFiles using a UID/GID outside this range will be seen as belonging to the overflow ID, usually', metadata={'source': './PDFS/Concepts.pdf', 'page': 117}),\n",
       " Document(page_content=\"Files using a UID/GID outside this range will be seen as belonging to the overflow ID, usually\\n65534 (configured in /proc/sys/kernel/overflowuid  and /proc/sys/kernel/overflowgid ). However,\\nit is not possible to modify those files, even by running as the 65534 user/group.\\nMost applications that need to run as root but don't access other host namespaces or resources,\\nshould continue to run fine without any changes needed if user namespaces is activated.\\nUnderstanding user namespaces for pods\\nSeveral container runtimes with their default configuration (like Docker Engine, containerd,\\nCRI-O) use Linux namespaces for isolation. Other technologies exist and can be used with those\\nruntimes too (e.g. Kata Containers uses VMs instead of Linux namespaces). This page is\\napplicable for container runtimes using Linux namespaces for isolation.\\nWhen creating a pod, by default, several new namespaces are used for isolation: a network\", metadata={'source': './PDFS/Concepts.pdf', 'page': 117}),\n",
       " Document(page_content=\"When creating a pod, by default, several new namespaces are used for isolation: a network\\nnamespace to isolate the network of the container, a PID namespace to isolate the view of\\nprocesses, etc. If a user namespace is used, this will isolate the users in the container from the\\nusers in the node.\\nThis means containers can run as root and be mapped to a non-root user on the host. Inside the\\ncontainer the process will think it is running as root (and therefore tools like apt, yum, etc.\\nwork fine), while in reality the process doesn't have privileges on the host. You can verify this,\\nfor example, if you check which user the container process is running by executing ps aux  from\\nthe host. The user ps shows is not the same as the user you see if you execute inside the\\ncontainer the command id.\\nThis abstraction limits what can happen, for example, if the container manages to escape to the\\nhost. Given that the container is running as a non-privileged user on the host, it is limited what\", metadata={'source': './PDFS/Concepts.pdf', 'page': 117}),\n",
       " Document(page_content='host. Given that the container is running as a non-privileged user on the host, it is limited what\\nit can do to the host.\\nFurthermore, as users on each pod will be mapped to different non-overlapping users in the\\nhost, it is limited what they can do to other pods too.', metadata={'source': './PDFS/Concepts.pdf', 'page': 117}),\n",
       " Document(page_content=\"Capabilities granted to a pod are also limited to the pod user namespace and mostly invalid out\\nof it, some are even completely void. Here are two examples:\\nCAP_SYS_MODULE  does not have any effect if granted to a pod using user namespaces,\\nthe pod isn't able to load kernel modules.\\nCAP_SYS_ADMIN  is limited to the pod's user namespace and invalid outside of it.\\nWithout using a user namespace a container running as root, in the case of a container\\nbreakout, has root privileges on the node. And if some capability were granted to the container,\\nthe capabilities are valid on the host too. None of this is true when we use user namespaces.\\nIf you want to know more details about what changes when user namespaces are in use, see \\nman 7 user_namespaces .\\nSet up a node to support user namespaces\\nIt is recommended that the host's files and host's processes use UIDs/GIDs in the range of\\n0-65535.\\nThe kubelet will assign UIDs/GIDs higher than that to pods. Therefore, to guarantee as much\", metadata={'source': './PDFS/Concepts.pdf', 'page': 118}),\n",
       " Document(page_content=\"0-65535.\\nThe kubelet will assign UIDs/GIDs higher than that to pods. Therefore, to guarantee as much\\nisolation as possible, the UIDs/GIDs used by the host's files and host's processes should be in\\nthe range 0-65535.\\nNote that this recommendation is important to mitigate the impact of CVEs like \\nCVE-2021-25741 , where a pod can potentially read arbitrary files in the hosts. If the UIDs/GIDs\\nof the pod and the host don't overlap, it is limited what a pod would be able to do: the pod UID/\\nGID won't match the host's file owner/group.\\nLimitations\\nWhen using a user namespace for the pod, it is disallowed to use other host namespaces. In\\nparticular, if you set hostUsers: false  then you are not allowed to set any of:\\nhostNetwork: true\\nhostIPC: true\\nhostPID: true\\nWhat's next\\nTake a look at Use a User Namespace With a Pod\\nDownward API\\nThere are two ways to expose Pod and container fields to a running container: environment\", metadata={'source': './PDFS/Concepts.pdf', 'page': 118}),\n",
       " Document(page_content='There are two ways to expose Pod and container fields to a running container: environment\\nvariables, and as files that are populated by a special volume type. Together, these two ways of\\nexposing Pod and container fields are called the downward API.\\nIt is sometimes useful for a container to have information about itself, without being overly\\ncoupled to Kubernetes. The downward API  allows containers to consume information about\\nthemselves or the cluster without using the Kubernetes client or API server.• \\n• \\n• \\n• \\n• \\n•', metadata={'source': './PDFS/Concepts.pdf', 'page': 118}),\n",
       " Document(page_content=\"An example is an existing application that assumes a particular well-known environment\\nvariable holds a unique identifier. One possibility is to wrap the application, but that is tedious\\nand error-prone, and it violates the goal of low coupling. A better option would be to use the\\nPod's name as an identifier, and inject the Pod's name into the well-known environment\\nvariable.\\nIn Kubernetes, there are two ways to expose Pod and container fields to a running container:\\nas environment variables\\nas files in a downwardAPI  volume\\nTogether, these two ways of exposing Pod and container fields are called the downward API .\\nAvailable fields\\nOnly some Kubernetes API fields are available through the downward API. This section lists\\nwhich fields you can make available.\\nYou can pass information from available Pod-level fields using fieldRef . At the API level, the \\nspec for a Pod always defines at least one Container . You can pass information from available\", metadata={'source': './PDFS/Concepts.pdf', 'page': 119}),\n",
       " Document(page_content=\"spec for a Pod always defines at least one Container . You can pass information from available\\nContainer-level fields using resourceFieldRef .\\nInformation available via fieldRef\\nFor some Pod-level fields, you can provide them to a container either as an environment\\nvariable or using a downwardAPI  volume. The fields available via either mechanism are:\\nmetadata.name\\nthe pod's name\\nmetadata.namespace\\nthe pod's namespace\\nmetadata.uid\\nthe pod's unique ID\\nmetadata.annotations['<KEY>']\\nthe value of the pod's annotation  named <KEY>  (for example, \\nmetadata.annotations['myannotation'] )\\nmetadata.labels['<KEY>']\\nthe text value of the pod's label  named <KEY>  (for example, metadata.labels['mylabel'] )\\nThe following information is available through environment variables but not as a\\ndownwardAPI volume fieldRef :\\nspec.serviceAccountName\\nthe name of the pod's service account\\nspec.nodeName\\nthe name of the node  where the Pod is executing\\nstatus.hostIP\", metadata={'source': './PDFS/Concepts.pdf', 'page': 119}),\n",
       " Document(page_content=\"spec.nodeName\\nthe name of the node  where the Pod is executing\\nstatus.hostIP\\nthe primary IP address of the node to which the Pod is assigned\\nstatus.hostIPs\\nthe IP addresses is a dual-stack version of status.hostIP , the first is always the same as \\nstatus.hostIP . The field is available if you enable the PodHostIPs  feature gate .\\nstatus.podIP\\nthe pod's primary IP address (usually, its IPv4 address)• \\n•\", metadata={'source': './PDFS/Concepts.pdf', 'page': 119}),\n",
       " Document(page_content='status.podIPs\\nthe IP addresses is a dual-stack version of status.podIP , the first is always the same as \\nstatus.podIP\\nThe following information is available through a downwardAPI  volume fieldRef , but not as\\nenvironment variables :\\nmetadata.labels\\nall of the pod\\'s labels, formatted as label-key=\"escaped-label-value\"  with one label per line\\nmetadata.annotations\\nall of the pod\\'s annotations, formatted as annotation-key=\"escaped-annotation-value\"\\nwith one annotation per line\\nInformation available via resourceFieldRef\\nThese container-level fields allow you to provide information about requests and limits  for\\nresources such as CPU and memory.\\nresource: limits.cpu\\nA container\\'s CPU limit\\nresource: requests.cpu\\nA container\\'s CPU request\\nresource: limits.memory\\nA container\\'s memory limit\\nresource: requests.memory\\nA container\\'s memory request\\nresource: limits.hugepages-*\\nA container\\'s hugepages limit\\nresource: requests.hugepages-*\\nA container\\'s hugepages request', metadata={'source': './PDFS/Concepts.pdf', 'page': 120}),\n",
       " Document(page_content=\"A container's hugepages limit\\nresource: requests.hugepages-*\\nA container's hugepages request\\nresource: limits.ephemeral-storage\\nA container's ephemeral-storage limit\\nresource: requests.ephemeral-storage\\nA container's ephemeral-storage request\\nFallback information for resource limits\\nIf CPU and memory limits are not specified for a container, and you use the downward API to\\ntry to expose that information, then the kubelet defaults to exposing the maximum allocatable\\nvalue for CPU and memory based on the node allocatable  calculation.\\nWhat's next\\nYou can read about downwardAPI  volumes .\\nYou can try using the downward API to expose container- or Pod-level information:\\nas environment variables\\nas files in downwardAPI  volume• \\n•\", metadata={'source': './PDFS/Concepts.pdf', 'page': 120}),\n",
       " Document(page_content='Workload Resources\\nKubernetes provides several built-in APIs for declarative management of your workloads  and\\nthe components of those workloads.\\nUltimately, your applications run as containers inside Pods ; however, managing individual Pods\\nwould be a lot of effort. For example, if a Pod fails, you probably want to run a new Pod to\\nreplace it. Kubernetes can do that for you.\\nYou use the Kubernetes API to create a workload object  that represents a higher abstraction\\nlevel than a Pod, and then the Kubernetes control plane  automatically manages Pod objects on\\nyour behalf, based on the specification for the workload object you defined.\\nThe built-in APIs for managing workloads are:\\nDeployment  (and, indirectly, ReplicaSet ), the most common way to run an application on your\\ncluster. Deployment is a good fit for managing a stateless application workload on your cluster,\\nwhere any Pod in the Deployment is interchangeable and can be replaced if needed.', metadata={'source': './PDFS/Concepts.pdf', 'page': 121}),\n",
       " Document(page_content='where any Pod in the Deployment is interchangeable and can be replaced if needed.\\n(Deployments are a replacement for the legacy ReplicationController  API).\\nA StatefulSet  lets you manage one or more Pods – all running the same application code –\\nwhere the Pods rely on having a distinct identity. This is different from a Deployment where the\\nPods are expected to be interchangeable. The most common use for a StatefulSet is to be able to\\nmake a link between its Pods and their persistent storage. For example, you can run a\\nStatefulSet that associates each Pod with a PersistentVolume . If one of the Pods in the\\nStatefulSet fails, Kubernetes makes a replacement Pod that is connected to the same\\nPersistentVolume.\\nA DaemonSet  defines Pods that provide facilities that are local to a specific node ; for example, a\\ndriver that lets containers on that node access a storage system. You use a DaemonSet when the', metadata={'source': './PDFS/Concepts.pdf', 'page': 121}),\n",
       " Document(page_content=\"driver that lets containers on that node access a storage system. You use a DaemonSet when the\\ndriver, or other node-level service, has to run on the node where it's useful. Each Pod in a\\nDaemonSet performs a role similar to a system daemon on a classic Unix / POSIX server. A\\nDaemonSet might be fundamental to the operation of your cluster, such as a plugin to let that\\nnode access cluster networking , it might help you to manage the node, or it could provide less\\nessential facilities that enhance the container platform you are running. You can run\\nDaemonSets (and their pods) across every node in your cluster, or across just a subset (for\\nexample, only install the GPU accelerator driver on nodes that have a GPU installed).\\nYou can use a Job and / or a CronJob  to define tasks that run to completion and then stop. A Job\\nrepresents a one-off task, whereas each CronJob repeats according to a schedule.\\nOther topics in this section:\\nAutomatic Cleanup for Finished Jobs\\nReplicationController\", metadata={'source': './PDFS/Concepts.pdf', 'page': 121}),\n",
       " Document(page_content=\"Other topics in this section:\\nAutomatic Cleanup for Finished Jobs\\nReplicationController\\nDeployments\\nA Deployment manages a set of Pods to run an application workload, usually one that doesn't\\nmaintain state.\\nA Deployment  provides declarative updates for Pods  and ReplicaSets .• \\n•\", metadata={'source': './PDFS/Concepts.pdf', 'page': 121}),\n",
       " Document(page_content='You describe a desired state  in a Deployment, and the Deployment Controller  changes the actual\\nstate to the desired state at a controlled rate. You can define Deployments to create new\\nReplicaSets, or to remove existing Deployments and adopt all their resources with new\\nDeployments.\\nNote:  Do not manage ReplicaSets owned by a Deployment. Consider opening an issue in the\\nmain Kubernetes repository if your use case is not covered below.\\nUse Case\\nThe following are typical use cases for Deployments:\\nCreate a Deployment to rollout a ReplicaSet . The ReplicaSet creates Pods in the\\nbackground. Check the status of the rollout to see if it succeeds or not.\\nDeclare the new state of the Pods  by updating the PodTemplateSpec of the Deployment.\\nA new ReplicaSet is created and the Deployment manages moving the Pods from the old\\nReplicaSet to the new one at a controlled rate. Each new ReplicaSet updates the revision\\nof the Deployment.', metadata={'source': './PDFS/Concepts.pdf', 'page': 122}),\n",
       " Document(page_content=\"of the Deployment.\\nRollback to an earlier Deployment revision  if the current state of the Deployment is not\\nstable. Each rollback updates the revision of the Deployment.\\nScale up the Deployment to facilitate more load .\\nPause the rollout of a Deployment  to apply multiple fixes to its PodTemplateSpec and\\nthen resume it to start a new rollout.\\nUse the status of the Deployment  as an indicator that a rollout has stuck.\\nClean up older ReplicaSets  that you don't need anymore.\\nCreating a Deployment\\nThe following is an example of a Deployment. It creates a ReplicaSet to bring up three nginx\\nPods:\\ncontrollers/nginx-deployment.yaml  \\napiVersion : apps/v1\\nkind: Deployment\\nmetadata :\\n  name : nginx-deployment\\n  labels :\\n    app: nginx\\nspec:\\n  replicas : 3\\n  selector :\\n    matchLabels :\\n      app: nginx\\n  template :\\n    metadata :\\n      labels :\\n        app: nginx\\n    spec:\\n      containers :\\n      - name : nginx\\n        image : nginx:1.14.2• \\n• \\n• \\n• \\n• \\n• \\n•\", metadata={'source': './PDFS/Concepts.pdf', 'page': 122}),\n",
       " Document(page_content='ports :\\n        - containerPort : 80\\nIn this example:\\nA Deployment named nginx-deployment  is created, indicated by the .metadata.name\\nfield. This name will become the basis for the ReplicaSets and Pods which are created\\nlater. See Writing a Deployment Spec  for more details.\\nThe Deployment creates a ReplicaSet that creates three replicated Pods, indicated by the\\n.spec.replicas  field.\\nThe .spec.selector  field defines how the created ReplicaSet finds which Pods to manage. In\\nthis case, you select a label that is defined in the Pod template ( app: nginx ). However,\\nmore sophisticated selection rules are possible, as long as the Pod template itself satisfies\\nthe rule.\\nNote:  The .spec.selector.matchLabels  field is a map of {key,value} pairs. A single\\n{key,value} in the matchLabels  map is equivalent to an element of matchExpressions ,\\nwhose key field is \"key\", the operator  is \"In\", and the values  array contains only \"value\".', metadata={'source': './PDFS/Concepts.pdf', 'page': 123}),\n",
       " Document(page_content='whose key field is \"key\", the operator  is \"In\", and the values  array contains only \"value\".\\nAll of the requirements, from both matchLabels  and matchExpressions , must be satisfied\\nin order to match.\\nThe template  field contains the following sub-fields:\\nThe Pods are labeled app: nginx using the .metadata.labels  field.\\nThe Pod template\\'s specification, or .template.spec  field, indicates that the Pods run\\none container, nginx , which runs the nginx  Docker Hub  image at version 1.14.2.\\nCreate one container and name it nginx  using\\nthe .spec.template.spec.containers[0].name  field.\\nBefore you begin, make sure your Kubernetes cluster is up and running. Follow the steps given\\nbelow to create the above Deployment:\\nCreate the Deployment by running the following command:\\nkubectl apply -f https://k8s.io/examples/controllers/nginx-deployment.yaml\\nRun kubectl get deployments  to check if the Deployment was created.', metadata={'source': './PDFS/Concepts.pdf', 'page': 123}),\n",
       " Document(page_content='Run kubectl get deployments  to check if the Deployment was created.\\nIf the Deployment is still being created, the output is similar to the following:\\nNAME               READY   UP-TO-DATE   AVAILABLE   AGE\\nnginx-deployment   0/3     0            0           1s\\nWhen you inspect the Deployments in your cluster, the following fields are displayed:\\nNAME  lists the names of the Deployments in the namespace.\\nREADY  displays how many replicas of the application are available to your users. It\\nfollows the pattern ready/desired.\\nUP-TO-DATE  displays the number of replicas that have been updated to achieve\\nthe desired state.\\nAVAILABLE  displays how many replicas of the application are available to your\\nusers.\\nAGE  displays the amount of time that the application has been running.• \\n• \\n• \\n• \\n◦ \\n◦ \\n◦ \\n1. \\n2. \\n◦ \\n◦ \\n◦ \\n◦ \\n◦', metadata={'source': './PDFS/Concepts.pdf', 'page': 123}),\n",
       " Document(page_content='Notice how the number of desired replicas is 3 according to .spec.replicas  field.\\nTo see the Deployment rollout status, run kubectl rollout status deployment/nginx-\\ndeployment .\\nThe output is similar to:\\nWaiting for rollout to finish: 2 out of 3 new replicas have been updated...\\ndeployment \"nginx-deployment\" successfully rolled out\\nRun the kubectl get deployments  again a few seconds later. The output is similar to this:\\nNAME               READY   UP-TO-DATE   AVAILABLE   AGE\\nnginx-deployment   3/3     3            3           18s\\nNotice that the Deployment has created all three replicas, and all replicas are up-to-date\\n(they contain the latest Pod template) and available.\\nTo see the ReplicaSet ( rs) created by the Deployment, run kubectl get rs . The output is\\nsimilar to this:\\nNAME                          DESIRED   CURRENT   READY   AGE\\nnginx-deployment-75675f5897   3         3         3       18s\\nReplicaSet output shows the following fields:', metadata={'source': './PDFS/Concepts.pdf', 'page': 124}),\n",
       " Document(page_content='ReplicaSet output shows the following fields:\\nNAME  lists the names of the ReplicaSets in the namespace.\\nDESIRED  displays the desired number of replicas  of the application, which you\\ndefine when you create the Deployment. This is the desired state .\\nCURRENT  displays how many replicas are currently running.\\nREADY  displays how many replicas of the application are available to your users.\\nAGE  displays the amount of time that the application has been running.\\nNotice that the name of the ReplicaSet is always formatted as [DEPLOYMENT-NAME]-\\n[HASH] . This name will become the basis for the Pods which are created.\\nThe HASH  string is the same as the pod-template-hash  label on the ReplicaSet.\\nTo see the labels automatically generated for each Pod, run kubectl get pods --show-\\nlabels . The output is similar to:\\nNAME                                READY     STATUS    RESTARTS   AGE       LABELS\\nnginx-deployment-75675f5897-7ci7o   1/1       Running   0          18s       app=nginx,pod-', metadata={'source': './PDFS/Concepts.pdf', 'page': 124}),\n",
       " Document(page_content='nginx-deployment-75675f5897-7ci7o   1/1       Running   0          18s       app=nginx,pod-\\ntemplate-hash=75675f5897\\nnginx-deployment-75675f5897-kzszj   1/1       Running   0          18s       app=nginx,pod-\\ntemplate-hash=75675f5897\\nnginx-deployment-75675f5897-qqcnn   1/1       Running   0          18s       app=nginx,pod-\\ntemplate-hash=75675f5897\\nThe created ReplicaSet ensures that there are three nginx  Pods.\\nNote:\\nYou must specify an appropriate selector and Pod template labels in a Deployment (in this case, \\napp: nginx ).3. \\n4. \\n5. \\n◦ \\n◦ \\n◦ \\n◦ \\n◦ \\n6.', metadata={'source': './PDFS/Concepts.pdf', 'page': 124}),\n",
       " Document(page_content=\"Do not overlap labels or selectors with other controllers (including other Deployments and\\nStatefulSets). Kubernetes doesn't stop you from overlapping, and if multiple controllers have\\noverlapping selectors those controllers might conflict and behave unexpectedly.\\nPod-template-hash label\\nCaution:  Do not change this label.\\nThe pod-template-hash  label is added by the Deployment controller to every ReplicaSet that a\\nDeployment creates or adopts.\\nThis label ensures that child ReplicaSets of a Deployment do not overlap. It is generated by\\nhashing the PodTemplate  of the ReplicaSet and using the resulting hash as the label value that\\nis added to the ReplicaSet selector, Pod template labels, and in any existing Pods that the\\nReplicaSet might have.\\nUpdating a Deployment\\nNote:  A Deployment's rollout is triggered if and only if the Deployment's Pod template (that\\nis, .spec.template ) is changed, for example if the labels or container images of the template are\", metadata={'source': './PDFS/Concepts.pdf', 'page': 125}),\n",
       " Document(page_content=\"is, .spec.template ) is changed, for example if the labels or container images of the template are\\nupdated. Other updates, such as scaling the Deployment, do not trigger a rollout.\\nFollow the steps given below to update your Deployment:\\nLet's update the nginx Pods to use the nginx:1.16.1  image instead of the nginx:1.14.2\\nimage.\\nkubectl set image deployment.v1.apps/nginx-deployment nginx =nginx:1.16.1\\nor use the following command:\\nkubectl set image deployment/nginx-deployment nginx =nginx:1.16.1\\nwhere deployment/nginx-deployment  indicates the Deployment, nginx  indicates the\\nContainer the update will take place and nginx:1.16.1  indicates the new image and its tag.\\nThe output is similar to:\\ndeployment.apps/nginx-deployment image updated\\nAlternatively, you can edit the Deployment and\\nchange .spec.template.spec.containers[0].image  from nginx:1.14.2  to nginx:1.16.1 :\\nkubectl edit deployment/nginx-deployment\\nThe output is similar to:\\ndeployment.apps/nginx-deployment edited\", metadata={'source': './PDFS/Concepts.pdf', 'page': 125}),\n",
       " Document(page_content='The output is similar to:\\ndeployment.apps/nginx-deployment edited\\nTo see the rollout status, run:\\nkubectl rollout status deployment/nginx-deployment\\nThe output is similar to this:1. \\n2.', metadata={'source': './PDFS/Concepts.pdf', 'page': 125}),\n",
       " Document(page_content='Waiting for rollout to finish: 2 out of 3 new replicas have been updated...\\nor\\ndeployment \"nginx-deployment\" successfully rolled out\\nGet more details on your updated Deployment:\\nAfter the rollout succeeds, you can view the Deployment by running kubectl get \\ndeployments . The output is similar to this:\\nNAME               READY   UP-TO-DATE   AVAILABLE   AGE\\nnginx-deployment   3/3     3            3           36s\\nRun kubectl get rs  to see that the Deployment updated the Pods by creating a new\\nReplicaSet and scaling it up to 3 replicas, as well as scaling down the old ReplicaSet to 0\\nreplicas.\\nkubectl get rs\\nThe output is similar to this:\\nNAME                          DESIRED   CURRENT   READY   AGE\\nnginx-deployment-1564180365   3         3         3       6s\\nnginx-deployment-2035384211   0         0         0       36s\\nRunning get pods  should now show only the new Pods:\\nkubectl get pods\\nThe output is similar to this:', metadata={'source': './PDFS/Concepts.pdf', 'page': 126}),\n",
       " Document(page_content=\"Running get pods  should now show only the new Pods:\\nkubectl get pods\\nThe output is similar to this:\\nNAME                                READY     STATUS    RESTARTS   AGE\\nnginx-deployment-1564180365-khku8   1/1       Running   0          14s\\nnginx-deployment-1564180365-nacti   1/1       Running   0          14s\\nnginx-deployment-1564180365-z9gth   1/1       Running   0          14s\\nNext time you want to update these Pods, you only need to update the Deployment's Pod\\ntemplate again.\\nDeployment ensures that only a certain number of Pods are down while they are being\\nupdated. By default, it ensures that at least 75% of the desired number of Pods are up (25%\\nmax unavailable).\\nDeployment also ensures that only a certain number of Pods are created above the\\ndesired number of Pods. By default, it ensures that at most 125% of the desired number of\\nPods are up (25% max surge).\\nFor example, if you look at the above Deployment closely, you will see that it first creates\", metadata={'source': './PDFS/Concepts.pdf', 'page': 126}),\n",
       " Document(page_content='For example, if you look at the above Deployment closely, you will see that it first creates\\na new Pod, then deletes an old Pod, and creates another new one. It does not kill old Pods\\nuntil a sufficient number of new Pods have come up, and does not create new Pods until a\\nsufficient number of old Pods have been killed. It makes sure that at least 3 Pods are\\navailable and that at max 4 Pods in total are available. In case of a Deployment with 4\\nreplicas, the number of Pods would be between 3 and 5.• \\n• \\n•', metadata={'source': './PDFS/Concepts.pdf', 'page': 126}),\n",
       " Document(page_content='Get details of your Deployment:\\nkubectl describe deployments\\nThe output is similar to this:\\nName:                   nginx-deployment\\nNamespace:              default\\nCreationTimestamp:      Thu, 30 Nov 2017 10:56:25 +0000\\nLabels:                 app=nginx\\nAnnotations:            deployment.kubernetes.io/revision=2\\nSelector:               app=nginx\\nReplicas:               3 desired | 3 updated | 3 total | 3 available | 0 unavailable\\nStrategyType:           RollingUpdate\\nMinReadySeconds:        0\\nRollingUpdateStrategy:  25% max unavailable, 25% max surge\\nPod Template:\\n  Labels:  app=nginx\\n   Containers:\\n    nginx:\\n      Image:        nginx:1.16.1\\n      Port:         80/TCP\\n      Environment:  <none>\\n      Mounts:       <none>\\n    Volumes:        <none>\\n  Conditions:\\n    Type           Status  Reason\\n    ----           ------  ------\\n    Available      True    MinimumReplicasAvailable\\n    Progressing    True    NewReplicaSetAvailable\\n  OldReplicaSets:  <none>', metadata={'source': './PDFS/Concepts.pdf', 'page': 127}),\n",
       " Document(page_content='Progressing    True    NewReplicaSetAvailable\\n  OldReplicaSets:  <none>\\n  NewReplicaSet:   nginx-deployment-1564180365 (3/3 replicas created)\\n  Events:\\n    Type    Reason             Age   From                   Message\\n    ----    ------             ----  ----                   -------\\n    Normal  ScalingReplicaSet  2m    deployment-controller  Scaled up replica set nginx-\\ndeployment-2035384211 to 3\\n    Normal  ScalingReplicaSet  24s   deployment-controller  Scaled up replica set nginx-\\ndeployment-1564180365 to 1\\n    Normal  ScalingReplicaSet  22s   deployment-controller  Scaled down replica set nginx-\\ndeployment-2035384211 to 2\\n    Normal  ScalingReplicaSet  22s   deployment-controller  Scaled up replica set nginx-\\ndeployment-1564180365 to 2\\n    Normal  ScalingReplicaSet  19s   deployment-controller  Scaled down replica set nginx-\\ndeployment-2035384211 to 1\\n    Normal  ScalingReplicaSet  19s   deployment-controller  Scaled up replica set nginx-\\ndeployment-1564180365 to 3', metadata={'source': './PDFS/Concepts.pdf', 'page': 127}),\n",
       " Document(page_content='deployment-1564180365 to 3\\n    Normal  ScalingReplicaSet  14s   deployment-controller  Scaled down replica set nginx-\\ndeployment-2035384211 to 0\\nHere you see that when you first created the Deployment, it created a ReplicaSet (nginx-\\ndeployment-2035384211) and scaled it up to 3 replicas directly. When you updated the\\nDeployment, it created a new ReplicaSet (nginx-deployment-1564180365) and scaled it up•', metadata={'source': './PDFS/Concepts.pdf', 'page': 127}),\n",
       " Document(page_content=\"to 1 and waited for it to come up. Then it scaled down the old ReplicaSet to 2 and scaled\\nup the new ReplicaSet to 2 so that at least 3 Pods were available and at most 4 Pods were\\ncreated at all times. It then continued scaling up and down the new and the old\\nReplicaSet, with the same rolling update strategy. Finally, you'll have 3 available replicas\\nin the new ReplicaSet, and the old ReplicaSet is scaled down to 0.\\nNote:  Kubernetes doesn't count terminating Pods when calculating the number of \\navailableReplicas , which must be between replicas - maxUnavailable  and replicas + maxSurge .\\nAs a result, you might notice that there are more Pods than expected during a rollout, and that\\nthe total resources consumed by the Deployment is more than replicas + maxSurge  until the \\nterminationGracePeriodSeconds  of the terminating Pods expires.\\nRollover (aka multiple updates in-flight)\\nEach time a new Deployment is observed by the Deployment controller, a ReplicaSet is created\", metadata={'source': './PDFS/Concepts.pdf', 'page': 128}),\n",
       " Document(page_content='Each time a new Deployment is observed by the Deployment controller, a ReplicaSet is created\\nto bring up the desired Pods. If the Deployment is updated, the existing ReplicaSet that controls\\nPods whose labels match .spec.selector  but whose template does not match .spec.template  are\\nscaled down. Eventually, the new ReplicaSet is scaled to .spec.replicas  and all old ReplicaSets is\\nscaled to 0.\\nIf you update a Deployment while an existing rollout is in progress, the Deployment creates a\\nnew ReplicaSet as per the update and start scaling that up, and rolls over the ReplicaSet that it\\nwas scaling up previously -- it will add it to its list of old ReplicaSets and start scaling it down.\\nFor example, suppose you create a Deployment to create 5 replicas of nginx:1.14.2 , but then\\nupdate the Deployment to create 5 replicas of nginx:1.16.1 , when only 3 replicas of nginx:1.14.2\\nhad been created. In that case, the Deployment immediately starts killing the 3 nginx:1.14.2', metadata={'source': './PDFS/Concepts.pdf', 'page': 128}),\n",
       " Document(page_content=\"had been created. In that case, the Deployment immediately starts killing the 3 nginx:1.14.2\\nPods that it had created, and starts creating nginx:1.16.1  Pods. It does not wait for the 5 replicas\\nof nginx:1.14.2  to be created before changing course.\\nLabel selector updates\\nIt is generally discouraged to make label selector updates and it is suggested to plan your\\nselectors up front. In any case, if you need to perform a label selector update, exercise great\\ncaution and make sure you have grasped all of the implications.\\nNote:  In API version apps/v1 , a Deployment's label selector is immutable after it gets created.\\nSelector additions require the Pod template labels in the Deployment spec to be updated\\nwith the new label too, otherwise a validation error is returned. This change is a non-\\noverlapping one, meaning that the new selector does not select ReplicaSets and Pods\\ncreated with the old selector, resulting in orphaning all old ReplicaSets and creating a\\nnew ReplicaSet.\", metadata={'source': './PDFS/Concepts.pdf', 'page': 128}),\n",
       " Document(page_content='new ReplicaSet.\\nSelector updates changes the existing value in a selector key -- result in the same\\nbehavior as additions.\\nSelector removals removes an existing key from the Deployment selector -- do not\\nrequire any changes in the Pod template labels. Existing ReplicaSets are not orphaned,\\nand a new ReplicaSet is not created, but note that the removed label still exists in any\\nexisting Pods and ReplicaSets.• \\n• \\n•', metadata={'source': './PDFS/Concepts.pdf', 'page': 128}),\n",
       " Document(page_content=\"Rolling Back a Deployment\\nSometimes, you may want to rollback a Deployment; for example, when the Deployment is not\\nstable, such as crash looping. By default, all of the Deployment's rollout history is kept in the\\nsystem so that you can rollback anytime you want (you can change that by modifying revision\\nhistory limit).\\nNote:  A Deployment's revision is created when a Deployment's rollout is triggered. This means\\nthat the new revision is created if and only if the Deployment's Pod template ( .spec.template ) is\\nchanged, for example if you update the labels or container images of the template. Other\\nupdates, such as scaling the Deployment, do not create a Deployment revision, so that you can\\nfacilitate simultaneous manual- or auto-scaling. This means that when you roll back to an\\nearlier revision, only the Deployment's Pod template part is rolled back.\\nSuppose that you made a typo while updating the Deployment, by putting the image\\nname as nginx:1.161  instead of nginx:1.16.1 :\", metadata={'source': './PDFS/Concepts.pdf', 'page': 129}),\n",
       " Document(page_content='name as nginx:1.161  instead of nginx:1.16.1 :\\nkubectl set image deployment/nginx-deployment nginx =nginx:1.161\\nThe output is similar to this:\\ndeployment.apps/nginx-deployment image updated\\nThe rollout gets stuck. You can verify it by checking the rollout status:\\nkubectl rollout status deployment/nginx-deployment\\nThe output is similar to this:\\nWaiting for rollout to finish: 1 out of 3 new replicas have been updated...\\nPress Ctrl-C to stop the above rollout status watch. For more information on stuck\\nrollouts, read more here .\\nYou see that the number of old replicas (adding the replica count from nginx-\\ndeployment-1564180365  and nginx-deployment-2035384211 ) is 3, and the number of new\\nreplicas (from nginx-deployment-3066724191 ) is 1.\\nkubectl get rs\\nThe output is similar to this:\\nNAME                          DESIRED   CURRENT   READY   AGE\\nnginx-deployment-1564180365   3         3         3       25s\\nnginx-deployment-2035384211   0         0         0       36s', metadata={'source': './PDFS/Concepts.pdf', 'page': 129}),\n",
       " Document(page_content='nginx-deployment-2035384211   0         0         0       36s\\nnginx-deployment-3066724191   1         1         0       6s\\nLooking at the Pods created, you see that 1 Pod created by new ReplicaSet is stuck in an\\nimage pull loop.\\nkubectl get pods\\nThe output is similar to this:• \\n• \\n• \\n• \\n•', metadata={'source': './PDFS/Concepts.pdf', 'page': 129}),\n",
       " Document(page_content='NAME                                READY     STATUS             RESTARTS   AGE\\nnginx-deployment-1564180365-70iae   1/1       Running            0          25s\\nnginx-deployment-1564180365-jbqqo   1/1       Running            0          25s\\nnginx-deployment-1564180365-hysrc   1/1       Running            0          25s\\nnginx-deployment-3066724191-08mng   0/1       ImagePullBackOff   0          6s\\nNote:  The Deployment controller stops the bad rollout automatically, and stops scaling\\nup the new ReplicaSet. This depends on the rollingUpdate parameters ( maxUnavailable\\nspecifically) that you have specified. Kubernetes by default sets the value to 25%.\\nGet the description of the Deployment:\\nkubectl describe deployment\\nThe output is similar to this:\\nName:           nginx-deployment\\nNamespace:      default\\nCreationTimestamp:  Tue, 15 Mar 2016 14:48:04 -0700\\nLabels:         app=nginx\\nSelector:       app=nginx\\nReplicas:       3 desired | 1 updated | 4 total | 3 available | 1 unavailable', metadata={'source': './PDFS/Concepts.pdf', 'page': 130}),\n",
       " Document(page_content='Replicas:       3 desired | 1 updated | 4 total | 3 available | 1 unavailable\\nStrategyType:       RollingUpdate\\nMinReadySeconds:    0\\nRollingUpdateStrategy:  25% max unavailable, 25% max surge\\nPod Template:\\n  Labels:  app=nginx\\n  Containers:\\n   nginx:\\n    Image:        nginx:1.161\\n    Port:         80/TCP\\n    Host Port:    0/TCP\\n    Environment:  <none>\\n    Mounts:       <none>\\n  Volumes:        <none>\\nConditions:\\n  Type           Status  Reason\\n  ----           ------  ------\\n  Available      True    MinimumReplicasAvailable\\n  Progressing    True    ReplicaSetUpdated\\nOldReplicaSets:     nginx-deployment-1564180365 (3/3 replicas created)\\nNewReplicaSet:      nginx-deployment-3066724191 (1/1 replicas created)\\nEvents:\\n  FirstSeen LastSeen    Count   From                    SubObjectPath   Type        Reason              \\nMessage\\n  --------- --------    -----   ----                    -------------   --------    ------              -------', metadata={'source': './PDFS/Concepts.pdf', 'page': 130}),\n",
       " Document(page_content='1m        1m          1       {deployment-controller }                Normal      ScalingReplicaSet   \\nScaled up replica set nginx-deployment-2035384211 to 3\\n  22s       22s         1       {deployment-controller }                Normal      ScalingReplicaSet   \\nScaled up replica set nginx-deployment-1564180365 to 1\\n  22s       22s         1       {deployment-controller }                Normal      ScalingReplicaSet   \\nScaled down replica set nginx-deployment-2035384211 to 2\\n  22s       22s         1       {deployment-controller }                Normal      ScalingReplicaSet   •', metadata={'source': './PDFS/Concepts.pdf', 'page': 130}),\n",
       " Document(page_content='Scaled up replica set nginx-deployment-1564180365 to 2\\n  21s       21s         1       {deployment-controller }                Normal      ScalingReplicaSet   \\nScaled down replica set nginx-deployment-2035384211 to 1\\n  21s       21s         1       {deployment-controller }                Normal      ScalingReplicaSet   \\nScaled up replica set nginx-deployment-1564180365 to 3\\n  13s       13s         1       {deployment-controller }                Normal      ScalingReplicaSet   \\nScaled down replica set nginx-deployment-2035384211 to 0\\n  13s       13s         1       {deployment-controller }                Normal      ScalingReplicaSet   \\nScaled up replica set nginx-deployment-3066724191 to 1\\nTo fix this, you need to rollback to a previous revision of Deployment that is stable.\\nChecking Rollout History of a Deployment\\nFollow the steps given below to check the rollout history:\\nFirst, check the revisions of this Deployment:\\nkubectl rollout history  deployment/nginx-deployment', metadata={'source': './PDFS/Concepts.pdf', 'page': 131}),\n",
       " Document(page_content='First, check the revisions of this Deployment:\\nkubectl rollout history  deployment/nginx-deployment\\nThe output is similar to this:\\ndeployments \"nginx-deployment\"\\nREVISION    CHANGE-CAUSE\\n1           kubectl apply --filename=https://k8s.io/examples/controllers/nginx-\\ndeployment.yaml\\n2           kubectl set image deployment/nginx-deployment nginx=nginx:1.16.1\\n3           kubectl set image deployment/nginx-deployment nginx=nginx:1.161\\nCHANGE-CAUSE  is copied from the Deployment annotation kubernetes.io/change-cause\\nto its revisions upon creation. You can specify the CHANGE-CAUSE  message by:\\nAnnotating the Deployment with kubectl annotate deployment/nginx-deployment \\nkubernetes.io/change-cause=\"image updated to 1.16.1\"\\nManually editing the manifest of the resource.\\nTo see the details of each revision, run:\\nkubectl rollout history  deployment/nginx-deployment --revision =2\\nThe output is similar to this:\\ndeployments \"nginx-deployment\" revision 2\\n  Labels:       app=nginx', metadata={'source': './PDFS/Concepts.pdf', 'page': 131}),\n",
       " Document(page_content='The output is similar to this:\\ndeployments \"nginx-deployment\" revision 2\\n  Labels:       app=nginx\\n          pod-template-hash=1159050644\\n  Annotations:  kubernetes.io/change-cause=kubectl set image deployment/nginx-\\ndeployment nginx=nginx:1.16.1\\n  Containers:\\n   nginx:\\n    Image:      nginx:1.16.1\\n    Port:       80/TCP\\n     QoS Tier:\\n        cpu:      BestEffort\\n        memory:   BestEffort1. \\n◦ \\n◦ \\n2.', metadata={'source': './PDFS/Concepts.pdf', 'page': 131}),\n",
       " Document(page_content=\"Environment Variables:      <none>\\n  No volumes.\\nRolling Back to a Previous Revision\\nFollow the steps given below to rollback the Deployment from the current version to the\\nprevious version, which is version 2.\\nNow you've decided to undo the current rollout and rollback to the previous revision:\\nkubectl rollout undo deployment/nginx-deployment\\nThe output is similar to this:\\ndeployment.apps/nginx-deployment rolled back\\nAlternatively, you can rollback to a specific revision by specifying it with --to-revision :\\nkubectl rollout undo deployment/nginx-deployment --to-revision =2\\nThe output is similar to this:\\ndeployment.apps/nginx-deployment rolled back\\nFor more details about rollout related commands, read kubectl rollout .\\nThe Deployment is now rolled back to a previous stable revision. As you can see, a \\nDeploymentRollback  event for rolling back to revision 2 is generated from Deployment\\ncontroller.\\nCheck if the rollback was successful and the Deployment is running as expected, run:\", metadata={'source': './PDFS/Concepts.pdf', 'page': 132}),\n",
       " Document(page_content='controller.\\nCheck if the rollback was successful and the Deployment is running as expected, run:\\nkubectl get deployment nginx-deployment\\nThe output is similar to this:\\nNAME               READY   UP-TO-DATE   AVAILABLE   AGE\\nnginx-deployment   3/3     3            3           30m\\nGet the description of the Deployment:\\nkubectl describe deployment nginx-deployment\\nThe output is similar to this:\\nName:                   nginx-deployment\\nNamespace:              default\\nCreationTimestamp:      Sun, 02 Sep 2018 18:17:55 -0500\\nLabels:                 app=nginx\\nAnnotations:            deployment.kubernetes.io/revision=4\\n                        kubernetes.io/change-cause=kubectl set image deployment/nginx-\\ndeployment nginx=nginx:1.16.1\\nSelector:               app=nginx\\nReplicas:               3 desired | 3 updated | 3 total | 3 available | 0 unavailable\\nStrategyType:           RollingUpdate1. \\n2. \\n3.', metadata={'source': './PDFS/Concepts.pdf', 'page': 132}),\n",
       " Document(page_content='MinReadySeconds:        0\\nRollingUpdateStrategy:  25% max unavailable, 25% max surge\\nPod Template:\\n  Labels:  app=nginx\\n  Containers:\\n   nginx:\\n    Image:        nginx:1.16.1\\n    Port:         80/TCP\\n    Host Port:    0/TCP\\n    Environment:  <none>\\n    Mounts:       <none>\\n  Volumes:        <none>\\nConditions:\\n  Type           Status  Reason\\n  ----           ------  ------\\n  Available      True    MinimumReplicasAvailable\\n  Progressing    True    NewReplicaSetAvailable\\nOldReplicaSets:  <none>\\nNewReplicaSet:   nginx-deployment-c4747d96c (3/3 replicas created)\\nEvents:\\n  Type    Reason              Age   From                   Message\\n  ----    ------              ----  ----                   -------\\n  Normal  ScalingReplicaSet   12m   deployment-controller  Scaled up replica set nginx-\\ndeployment-75675f5897 to 3\\n  Normal  ScalingReplicaSet   11m   deployment-controller  Scaled up replica set nginx-\\ndeployment-c4747d96c to 1', metadata={'source': './PDFS/Concepts.pdf', 'page': 133}),\n",
       " Document(page_content='deployment-c4747d96c to 1\\n  Normal  ScalingReplicaSet   11m   deployment-controller  Scaled down replica set nginx-\\ndeployment-75675f5897 to 2\\n  Normal  ScalingReplicaSet   11m   deployment-controller  Scaled up replica set nginx-\\ndeployment-c4747d96c to 2\\n  Normal  ScalingReplicaSet   11m   deployment-controller  Scaled down replica set nginx-\\ndeployment-75675f5897 to 1\\n  Normal  ScalingReplicaSet   11m   deployment-controller  Scaled up replica set nginx-\\ndeployment-c4747d96c to 3\\n  Normal  ScalingReplicaSet   11m   deployment-controller  Scaled down replica set nginx-\\ndeployment-75675f5897 to 0\\n  Normal  ScalingReplicaSet   11m   deployment-controller  Scaled up replica set nginx-\\ndeployment-595696685f to 1\\n  Normal  DeploymentRollback  15s   deployment-controller  Rolled back deployment \\n\"nginx-deployment\" to revision 2\\n  Normal  ScalingReplicaSet   15s   deployment-controller  Scaled down replica set nginx-\\ndeployment-595696685f to 0\\nScaling a Deployment', metadata={'source': './PDFS/Concepts.pdf', 'page': 133}),\n",
       " Document(page_content='deployment-595696685f to 0\\nScaling a Deployment\\nYou can scale a Deployment by using the following command:\\nkubectl scale deployment/nginx-deployment --replicas =10\\nThe output is similar to this:\\ndeployment.apps/nginx-deployment scaled', metadata={'source': './PDFS/Concepts.pdf', 'page': 133}),\n",
       " Document(page_content='Assuming horizontal Pod autoscaling  is enabled in your cluster, you can set up an autoscaler for\\nyour Deployment and choose the minimum and maximum number of Pods you want to run\\nbased on the CPU utilization of your existing Pods.\\nkubectl autoscale deployment/nginx-deployment --min =10 --max =15 --cpu-percent =80\\nThe output is similar to this:\\ndeployment.apps/nginx-deployment scaled\\nProportional scaling\\nRollingUpdate Deployments support running multiple versions of an application at the same\\ntime. When you or an autoscaler scales a RollingUpdate Deployment that is in the middle of a\\nrollout (either in progress or paused), the Deployment controller balances the additional\\nreplicas in the existing active ReplicaSets (ReplicaSets with Pods) in order to mitigate risk. This\\nis called proportional scaling .\\nFor example, you are running a Deployment with 10 replicas, maxSurge =3, and \\nmaxUnavailable =2.\\nEnsure that the 10 replicas in your Deployment are running.\\nkubectl get deploy', metadata={'source': './PDFS/Concepts.pdf', 'page': 134}),\n",
       " Document(page_content=\"maxUnavailable =2.\\nEnsure that the 10 replicas in your Deployment are running.\\nkubectl get deploy\\nThe output is similar to this:\\nNAME                 DESIRED   CURRENT   UP-TO-DATE   AVAILABLE   AGE\\nnginx-deployment     10        10        10           10          50s\\nYou update to a new image which happens to be unresolvable from inside the cluster.\\nkubectl set image deployment/nginx-deployment nginx =nginx:sometag\\nThe output is similar to this:\\ndeployment.apps/nginx-deployment image updated\\nThe image update starts a new rollout with ReplicaSet nginx-deployment-1989198191, but\\nit's blocked due to the maxUnavailable  requirement that you mentioned above. Check out\\nthe rollout status:\\nkubectl get rs\\nThe output is similar to this:\\nNAME                          DESIRED   CURRENT   READY     AGE\\nnginx-deployment-1989198191   5         5         0         9s\\nnginx-deployment-618515232    8         8         8         1m\", metadata={'source': './PDFS/Concepts.pdf', 'page': 134}),\n",
       " Document(page_content=\"nginx-deployment-618515232    8         8         8         1m\\nThen a new scaling request for the Deployment comes along. The autoscaler increments\\nthe Deployment replicas to 15. The Deployment controller needs to decide where to add\\nthese new 5 replicas. If you weren't using proportional scaling, all 5 of them would be\\nadded in the new ReplicaSet. With proportional scaling, you spread the additional\\nreplicas across all ReplicaSets. Bigger proportions go to the ReplicaSets with the most• \\n• \\n• \\n•\", metadata={'source': './PDFS/Concepts.pdf', 'page': 134}),\n",
       " Document(page_content='replicas and lower proportions go to ReplicaSets with less replicas. Any leftovers are\\nadded to the ReplicaSet with the most replicas. ReplicaSets with zero replicas are not\\nscaled up.\\nIn our example above, 3 replicas are added to the old ReplicaSet and 2 replicas are added to the\\nnew ReplicaSet. The rollout process should eventually move all replicas to the new ReplicaSet,\\nassuming the new replicas become healthy. To confirm this, run:\\nkubectl get deploy\\nThe output is similar to this:\\nNAME                 DESIRED   CURRENT   UP-TO-DATE   AVAILABLE   AGE\\nnginx-deployment     15        18        7            8           7m\\nThe rollout status confirms how the replicas were added to each ReplicaSet.\\nkubectl get rs\\nThe output is similar to this:\\nNAME                          DESIRED   CURRENT   READY     AGE\\nnginx-deployment-1989198191   7         7         0         7m\\nnginx-deployment-618515232    11        11        11        7m\\nPausing and Resuming a rollout of a Deployment', metadata={'source': './PDFS/Concepts.pdf', 'page': 135}),\n",
       " Document(page_content=\"Pausing and Resuming a rollout of a Deployment\\nWhen you update a Deployment, or plan to, you can pause rollouts for that Deployment before\\nyou trigger one or more updates. When you're ready to apply those changes, you resume\\nrollouts for the Deployment. This approach allows you to apply multiple fixes in between\\npausing and resuming without triggering unnecessary rollouts.\\nFor example, with a Deployment that was created:\\nGet the Deployment details:\\nkubectl get deploy\\nThe output is similar to this:\\nNAME      DESIRED   CURRENT   UP-TO-DATE   AVAILABLE   AGE\\nnginx     3         3         3            3           1m\\nGet the rollout status:\\nkubectl get rs\\nThe output is similar to this:\\nNAME               DESIRED   CURRENT   READY     AGE\\nnginx-2142116321   3         3         3         1m\\nPause by running the following command:• \\n•\", metadata={'source': './PDFS/Concepts.pdf', 'page': 135}),\n",
       " Document(page_content='kubectl rollout pause deployment/nginx-deployment\\nThe output is similar to this:\\ndeployment.apps/nginx-deployment paused\\nThen update the image of the Deployment:\\nkubectl set image deployment/nginx-deployment nginx =nginx:1.16.1\\nThe output is similar to this:\\ndeployment.apps/nginx-deployment image updated\\nNotice that no new rollout started:\\nkubectl rollout history  deployment/nginx-deployment\\nThe output is similar to this:\\ndeployments \"nginx\"\\nREVISION  CHANGE-CAUSE\\n1   <none>\\nGet the rollout status to verify that the existing ReplicaSet has not changed:\\nkubectl get rs\\nThe output is similar to this:\\nNAME               DESIRED   CURRENT   READY     AGE\\nnginx-2142116321   3         3         3         2m\\nYou can make as many updates as you wish, for example, update the resources that will\\nbe used:\\nkubectl set resources deployment/nginx-deployment -c =nginx --limits =cpu=200m,memor\\ny=512Mi\\nThe output is similar to this:\\ndeployment.apps/nginx-deployment resource requirements updated', metadata={'source': './PDFS/Concepts.pdf', 'page': 136}),\n",
       " Document(page_content='The output is similar to this:\\ndeployment.apps/nginx-deployment resource requirements updated\\nThe initial state of the Deployment prior to pausing its rollout will continue its function,\\nbut new updates to the Deployment will not have any effect as long as the Deployment\\nrollout is paused.\\nEventually, resume the Deployment rollout and observe a new ReplicaSet coming up with\\nall the new updates:\\nkubectl rollout resume deployment/nginx-deployment\\nThe output is similar to this:\\ndeployment.apps/nginx-deployment resumed• \\n• \\n• \\n• \\n•', metadata={'source': './PDFS/Concepts.pdf', 'page': 136}),\n",
       " Document(page_content=\"Watch the status of the rollout until it's done.\\nkubectl get rs -w\\nThe output is similar to this:\\nNAME               DESIRED   CURRENT   READY     AGE\\nnginx-2142116321   2         2         2         2m\\nnginx-3926361531   2         2         0         6s\\nnginx-3926361531   2         2         1         18s\\nnginx-2142116321   1         2         2         2m\\nnginx-2142116321   1         2         2         2m\\nnginx-3926361531   3         2         1         18s\\nnginx-3926361531   3         2         1         18s\\nnginx-2142116321   1         1         1         2m\\nnginx-3926361531   3         3         1         18s\\nnginx-3926361531   3         3         2         19s\\nnginx-2142116321   0         1         1         2m\\nnginx-2142116321   0         1         1         2m\\nnginx-2142116321   0         0         0         2m\\nnginx-3926361531   3         3         3         20s\\nGet the status of the latest rollout:\\nkubectl get rs\\nThe output is similar to this:\", metadata={'source': './PDFS/Concepts.pdf', 'page': 137}),\n",
       " Document(page_content=\"Get the status of the latest rollout:\\nkubectl get rs\\nThe output is similar to this:\\nNAME               DESIRED   CURRENT   READY     AGE\\nnginx-2142116321   0         0         0         2m\\nnginx-3926361531   3         3         3         28s\\nNote:  You cannot rollback a paused Deployment until you resume it.\\nDeployment status\\nA Deployment enters various states during its lifecycle. It can be progressing  while rolling out a\\nnew ReplicaSet, it can be complete , or it can fail to progress .\\nProgressing Deployment\\nKubernetes marks a Deployment as progressing  when one of the following tasks is performed:\\nThe Deployment creates a new ReplicaSet.\\nThe Deployment is scaling up its newest ReplicaSet.\\nThe Deployment is scaling down its older ReplicaSet(s).\\nNew Pods become ready or available (ready for at least MinReadySeconds ).\\nWhen the rollout becomes “progressing”, the Deployment controller adds a condition with the\\nfollowing attributes to the Deployment's .status.conditions :\", metadata={'source': './PDFS/Concepts.pdf', 'page': 137}),\n",
       " Document(page_content='following attributes to the Deployment\\'s .status.conditions :\\ntype: Progressing\\nstatus: \"True\"• \\n• \\n• \\n• \\n• \\n• \\n• \\n•', metadata={'source': './PDFS/Concepts.pdf', 'page': 137}),\n",
       " Document(page_content='reason: NewReplicaSetCreated  | reason: FoundNewReplicaSet  | reason: ReplicaSetUpdated\\nYou can monitor the progress for a Deployment by using kubectl rollout status .\\nComplete Deployment\\nKubernetes marks a Deployment as complete  when it has the following characteristics:\\nAll of the replicas associated with the Deployment have been updated to the latest\\nversion you\\'ve specified, meaning any updates you\\'ve requested have been completed.\\nAll of the replicas associated with the Deployment are available.\\nNo old replicas for the Deployment are running.\\nWhen the rollout becomes “complete”, the Deployment controller sets a condition with the\\nfollowing attributes to the Deployment\\'s .status.conditions :\\ntype: Progressing\\nstatus: \"True\"\\nreason: NewReplicaSetAvailable\\nThis Progressing  condition will retain a status value of \"True\"  until a new rollout is initiated.\\nThe condition holds even when availability of replicas changes (which does instead affect the \\nAvailable  condition).', metadata={'source': './PDFS/Concepts.pdf', 'page': 138}),\n",
       " Document(page_content='Available  condition).\\nYou can check if a Deployment has completed by using kubectl rollout status . If the rollout\\ncompleted successfully, kubectl rollout status  returns a zero exit code.\\nkubectl rollout status deployment/nginx-deployment\\nThe output is similar to this:\\nWaiting for rollout to finish: 2 of 3 updated replicas are available...\\ndeployment \"nginx-deployment\" successfully rolled out\\nand the exit status from kubectl rollout  is 0 (success):\\necho  $?\\n0\\nFailed Deployment\\nYour Deployment may get stuck trying to deploy its newest ReplicaSet without ever\\ncompleting. This can occur due to some of the following factors:\\nInsufficient quota\\nReadiness probe failures\\nImage pull errors\\nInsufficient permissions\\nLimit ranges\\nApplication runtime misconfiguration\\nOne way you can detect this condition is to specify a deadline parameter in your Deployment\\nspec: ( .spec.progressDeadlineSeconds ). .spec.progressDeadlineSeconds  denotes the number of• \\n• \\n• \\n• \\n• \\n• \\n• \\n• \\n• \\n• \\n• \\n• \\n•', metadata={'source': './PDFS/Concepts.pdf', 'page': 138}),\n",
       " Document(page_content='seconds the Deployment controller waits before indicating (in the Deployment status) that the\\nDeployment progress has stalled.\\nThe following kubectl  command sets the spec with progressDeadlineSeconds  to make the\\ncontroller report lack of progress of a rollout for a Deployment after 10 minutes:\\nkubectl patch deployment/nginx-deployment -p \\'{\"spec\":{\"progressDeadlineSeconds\":600}}\\'\\nThe output is similar to this:\\ndeployment.apps/nginx-deployment patched\\nOnce the deadline has been exceeded, the Deployment controller adds a DeploymentCondition\\nwith the following attributes to the Deployment\\'s .status.conditions :\\ntype: Progressing\\nstatus: \"False\"\\nreason: ProgressDeadlineExceeded\\nThis condition can also fail early and is then set to status value of \"False\"  due to reasons as \\nReplicaSetCreateError . Also, the deadline is not taken into account anymore once the\\nDeployment rollout completes.\\nSee the Kubernetes API conventions  for more information on status conditions.', metadata={'source': './PDFS/Concepts.pdf', 'page': 139}),\n",
       " Document(page_content=\"See the Kubernetes API conventions  for more information on status conditions.\\nNote:  Kubernetes takes no action on a stalled Deployment other than to report a status\\ncondition with reason: ProgressDeadlineExceeded . Higher level orchestrators can take\\nadvantage of it and act accordingly, for example, rollback the Deployment to its previous\\nversion.\\nNote:  If you pause a Deployment rollout, Kubernetes does not check progress against your\\nspecified deadline. You can safely pause a Deployment rollout in the middle of a rollout and\\nresume without triggering the condition for exceeding the deadline.\\nYou may experience transient errors with your Deployments, either due to a low timeout that\\nyou have set or due to any other kind of error that can be treated as transient. For example, let's\\nsuppose you have insufficient quota. If you describe the Deployment you will notice the\\nfollowing section:\\nkubectl describe deployment nginx-deployment\\nThe output is similar to this:\\n<...>\\nConditions:\", metadata={'source': './PDFS/Concepts.pdf', 'page': 139}),\n",
       " Document(page_content='kubectl describe deployment nginx-deployment\\nThe output is similar to this:\\n<...>\\nConditions:\\n  Type            Status  Reason\\n  ----            ------  ------\\n  Available       True    MinimumReplicasAvailable\\n  Progressing     True    ReplicaSetUpdated\\n  ReplicaFailure  True    FailedCreate\\n<...>\\nIf you run kubectl get deployment nginx-deployment -o yaml , the Deployment status is similar\\nto this:• \\n• \\n•', metadata={'source': './PDFS/Concepts.pdf', 'page': 139}),\n",
       " Document(page_content='status:\\n  availableReplicas: 2\\n  conditions:\\n  - lastTransitionTime: 2016-10-04T12:25:39Z\\n    lastUpdateTime: 2016-10-04T12:25:39Z\\n    message: Replica set \"nginx-deployment-4262182780\" is progressing.\\n    reason: ReplicaSetUpdated\\n    status: \"True\"\\n    type: Progressing\\n  - lastTransitionTime: 2016-10-04T12:25:42Z\\n    lastUpdateTime: 2016-10-04T12:25:42Z\\n    message: Deployment has minimum availability.\\n    reason: MinimumReplicasAvailable\\n    status: \"True\"\\n    type: Available\\n  - lastTransitionTime: 2016-10-04T12:25:39Z\\n    lastUpdateTime: 2016-10-04T12:25:39Z\\n    message: \\'Error creating: pods \"nginx-deployment-4262182780-\" is forbidden: exceeded quota:\\n      object-counts, requested: pods=1, used: pods=3, limited: pods=2\\'\\n    reason: FailedCreate\\n    status: \"True\"\\n    type: ReplicaFailure\\n  observedGeneration: 3\\n  replicas: 2\\n  unavailableReplicas: 2\\nEventually, once the Deployment progress deadline is exceeded, Kubernetes updates the status', metadata={'source': './PDFS/Concepts.pdf', 'page': 140}),\n",
       " Document(page_content='Eventually, once the Deployment progress deadline is exceeded, Kubernetes updates the status\\nand the reason for the Progressing condition:\\nConditions:\\n  Type            Status  Reason\\n  ----            ------  ------\\n  Available       True    MinimumReplicasAvailable\\n  Progressing     False   ProgressDeadlineExceeded\\n  ReplicaFailure  True    FailedCreate\\nYou can address an issue of insufficient quota by scaling down your Deployment, by scaling\\ndown other controllers you may be running, or by increasing quota in your namespace. If you\\nsatisfy the quota conditions and the Deployment controller then completes the Deployment\\nrollout, you\\'ll see the Deployment\\'s status update with a successful condition ( status: \"True\"  and \\nreason: NewReplicaSetAvailable ).\\nConditions:\\n  Type          Status  Reason\\n  ----          ------  ------\\n  Available     True    MinimumReplicasAvailable\\n  Progressing   True    NewReplicaSetAvailable', metadata={'source': './PDFS/Concepts.pdf', 'page': 140}),\n",
       " Document(page_content='Available     True    MinimumReplicasAvailable\\n  Progressing   True    NewReplicaSetAvailable\\ntype: Available  with status: \"True\"  means that your Deployment has minimum availability.\\nMinimum availability is dictated by the parameters specified in the deployment strategy. type: \\nProgressing  with status: \"True\"  means that your Deployment is either in the middle of a rollout\\nand it is progressing or that it has successfully completed its progress and the minimum', metadata={'source': './PDFS/Concepts.pdf', 'page': 140}),\n",
       " Document(page_content='required new replicas are available (see the Reason of the condition for the particulars - in our\\ncase reason: NewReplicaSetAvailable  means that the Deployment is complete).\\nYou can check if a Deployment has failed to progress by using kubectl rollout status . kubectl \\nrollout status  returns a non-zero exit code if the Deployment has exceeded the progression\\ndeadline.\\nkubectl rollout status deployment/nginx-deployment\\nThe output is similar to this:\\nWaiting for rollout to finish: 2 out of 3 new replicas have been updated...\\nerror: deployment \"nginx\" exceeded its progress deadline\\nand the exit status from kubectl rollout  is 1 (indicating an error):\\necho  $?\\n1\\nOperating on a failed deployment\\nAll actions that apply to a complete Deployment also apply to a failed Deployment. You can\\nscale it up/down, roll back to a previous revision, or even pause it if you need to apply multiple\\ntweaks in the Deployment Pod template.\\nClean up Policy', metadata={'source': './PDFS/Concepts.pdf', 'page': 141}),\n",
       " Document(page_content='tweaks in the Deployment Pod template.\\nClean up Policy\\nYou can set .spec.revisionHistoryLimit  field in a Deployment to specify how many old\\nReplicaSets for this Deployment you want to retain. The rest will be garbage-collected in the\\nbackground. By default, it is 10.\\nNote:  Explicitly setting this field to 0, will result in cleaning up all the history of your\\nDeployment thus that Deployment will not be able to roll back.\\nCanary Deployment\\nIf you want to roll out releases to a subset of users or servers using the Deployment, you can\\ncreate multiple Deployments, one for each release, following the canary pattern described in \\nmanaging resources .\\nWriting a Deployment Spec\\nAs with all other Kubernetes configs, a Deployment needs .apiVersion , .kind , and .metadata\\nfields. For general information about working with config files, see deploying applications ,\\nconfiguring containers, and using kubectl to manage resources  documents.', metadata={'source': './PDFS/Concepts.pdf', 'page': 141}),\n",
       " Document(page_content='configuring containers, and using kubectl to manage resources  documents.\\nWhen the control plane creates new Pods for a Deployment, the .metadata.name  of the\\nDeployment is part of the basis for naming those Pods. The name of a Deployment must be a\\nvalid DNS subdomain  value, but this can produce unexpected results for the Pod hostnames.\\nFor best compatibility, the name should follow the more restrictive rules for a DNS label .', metadata={'source': './PDFS/Concepts.pdf', 'page': 141}),\n",
       " Document(page_content='A Deployment also needs a .spec  section .\\nPod Template\\nThe .spec.template  and .spec.selector  are the only required fields of the .spec .\\nThe .spec.template  is a Pod template . It has exactly the same schema as a Pod, except it is nested\\nand does not have an apiVersion  or kind.\\nIn addition to required fields for a Pod, a Pod template in a Deployment must specify\\nappropriate labels and an appropriate restart policy. For labels, make sure not to overlap with\\nother controllers. See selector .\\nOnly a .spec.template.spec.restartPolicy  equal to Always  is allowed, which is the default if not\\nspecified.\\nReplicas\\n.spec.replicas  is an optional field that specifies the number of desired Pods. It defaults to 1.\\nShould you manually scale a Deployment, example via kubectl scale deployment deployment --\\nreplicas=X , and then you update that Deployment based on a manifest (for example: by\\nrunning kubectl apply -f deployment.yaml ), then applying that manifest overwrites the manual', metadata={'source': './PDFS/Concepts.pdf', 'page': 142}),\n",
       " Document(page_content=\"running kubectl apply -f deployment.yaml ), then applying that manifest overwrites the manual\\nscaling that you previously did.\\nIf a HorizontalPodAutoscaler  (or any similar API for horizontal scaling) is managing scaling for\\na Deployment, don't set .spec.replicas .\\nInstead, allow the Kubernetes control plane  to manage the .spec.replicas  field automatically.\\nSelector\\n.spec.selector  is a required field that specifies a label selector  for the Pods targeted by this\\nDeployment.\\n.spec.selector  must match .spec.template.metadata.labels , or it will be rejected by the API.\\nIn API version apps/v1 , .spec.selector  and .metadata.labels  do not default\\nto .spec.template.metadata.labels  if not set. So they must be set explicitly. Also note\\nthat .spec.selector  is immutable after creation of the Deployment in apps/v1 .\\nA Deployment may terminate Pods whose labels match the selector if their template is different\", metadata={'source': './PDFS/Concepts.pdf', 'page': 142}),\n",
       " Document(page_content=\"A Deployment may terminate Pods whose labels match the selector if their template is different\\nfrom .spec.template  or if the total number of such Pods exceeds .spec.replicas . It brings up new\\nPods with .spec.template  if the number of Pods is less than the desired number.\\nNote:  You should not create other Pods whose labels match this selector, either directly, by\\ncreating another Deployment, or by creating another controller such as a ReplicaSet or a\\nReplicationController. If you do so, the first Deployment thinks that it created these other Pods.\\nKubernetes does not stop you from doing this.\\nIf you have multiple controllers that have overlapping selectors, the controllers will fight with\\neach other and won't behave correctly.\", metadata={'source': './PDFS/Concepts.pdf', 'page': 142}),\n",
       " Document(page_content='Strategy\\n.spec.strategy  specifies the strategy used to replace old Pods by new ones. .spec.strategy.type\\ncan be \"Recreate\" or \"RollingUpdate\". \"RollingUpdate\" is the default value.\\nRecreate Deployment\\nAll existing Pods are killed before new ones are created when .spec.strategy.type==Recreate .\\nNote:  This will only guarantee Pod termination previous to creation for upgrades. If you\\nupgrade a Deployment, all Pods of the old revision will be terminated immediately. Successful\\nremoval is awaited before any Pod of the new revision is created. If you manually delete a Pod,\\nthe lifecycle is controlled by the ReplicaSet and the replacement will be created immediately\\n(even if the old Pod is still in a Terminating state). If you need an \"at most\" guarantee for your\\nPods, you should consider using a StatefulSet .\\nRolling Update Deployment\\nThe Deployment updates Pods in a rolling update fashion when\\n.spec.strategy.type==RollingUpdate . You can specify maxUnavailable  and maxSurge  to control', metadata={'source': './PDFS/Concepts.pdf', 'page': 143}),\n",
       " Document(page_content='.spec.strategy.type==RollingUpdate . You can specify maxUnavailable  and maxSurge  to control\\nthe rolling update process.\\nMax Unavailable\\n.spec.strategy.rollingUpdate.maxUnavailable  is an optional field that specifies the maximum\\nnumber of Pods that can be unavailable during the update process. The value can be an absolute\\nnumber (for example, 5) or a percentage of desired Pods (for example, 10%). The absolute\\nnumber is calculated from percentage by rounding down. The value cannot be 0 if\\n.spec.strategy.rollingUpdate.maxSurge  is 0. The default value is 25%.\\nFor example, when this value is set to 30%, the old ReplicaSet can be scaled down to 70% of\\ndesired Pods immediately when the rolling update starts. Once new Pods are ready, old\\nReplicaSet can be scaled down further, followed by scaling up the new ReplicaSet, ensuring that\\nthe total number of Pods available at all times during the update is at least 70% of the desired\\nPods.\\nMax Surge', metadata={'source': './PDFS/Concepts.pdf', 'page': 143}),\n",
       " Document(page_content='Pods.\\nMax Surge\\n.spec.strategy.rollingUpdate.maxSurge  is an optional field that specifies the maximum number\\nof Pods that can be created over the desired number of Pods. The value can be an absolute\\nnumber (for example, 5) or a percentage of desired Pods (for example, 10%). The value cannot be\\n0 if MaxUnavailable  is 0. The absolute number is calculated from the percentage by rounding\\nup. The default value is 25%.\\nFor example, when this value is set to 30%, the new ReplicaSet can be scaled up immediately\\nwhen the rolling update starts, such that the total number of old and new Pods does not exceed\\n130% of desired Pods. Once old Pods have been killed, the new ReplicaSet can be scaled up\\nfurther, ensuring that the total number of Pods running at any time during the update is at\\nmost 130% of desired Pods.', metadata={'source': './PDFS/Concepts.pdf', 'page': 143}),\n",
       " Document(page_content='Here are some Rolling Update Deployment examples that use the maxUnavailable  and \\nmaxSurge :\\nMax Unavailable\\nMax Surge\\nHybrid\\napiVersion : apps/v1\\nkind: Deployment\\nmetadata :\\n name : nginx-deployment\\n labels :\\n   app: nginx\\nspec:\\n replicas : 3\\n selector :\\n   matchLabels :\\n     app: nginx\\n template :\\n   metadata :\\n     labels :\\n       app: nginx\\n   spec:\\n     containers :\\n     - name : nginx\\n       image : nginx:1.14.2\\n       ports :\\n       - containerPort : 80\\n strategy :\\n   type: RollingUpdate\\n   rollingUpdate :\\n     maxUnavailable : 1\\napiVersion : apps/v1\\nkind: Deployment\\nmetadata :\\n name : nginx-deployment\\n labels :\\n   app: nginx\\nspec:\\n replicas : 3\\n selector :\\n   matchLabels :\\n     app: nginx\\n template :\\n   metadata :\\n     labels :\\n       app: nginx\\n   spec:\\n     containers :\\n     - name : nginx\\n       image : nginx:1.14.2\\n       ports :• \\n• \\n•', metadata={'source': './PDFS/Concepts.pdf', 'page': 144}),\n",
       " Document(page_content='- containerPort : 80\\n strategy :\\n   type: RollingUpdate\\n   rollingUpdate :\\n     maxSurge : 1\\napiVersion : apps/v1\\nkind: Deployment\\nmetadata :\\n name : nginx-deployment\\n labels :\\n   app: nginx\\nspec:\\n replicas : 3\\n selector :\\n   matchLabels :\\n     app: nginx\\n template :\\n   metadata :\\n     labels :\\n       app: nginx\\n   spec:\\n     containers :\\n     - name : nginx\\n       image : nginx:1.14.2\\n       ports :\\n       - containerPort : 80\\n strategy :\\n   type: RollingUpdate\\n   rollingUpdate :\\n     maxSurge : 1\\n     maxUnavailable : 1\\nProgress Deadline Seconds\\n.spec.progressDeadlineSeconds  is an optional field that specifies the number of seconds you\\nwant to wait for your Deployment to progress before the system reports back that the\\nDeployment has failed progressing  - surfaced as a condition with type: Progressing , status: \\n\"False\" . and reason: ProgressDeadlineExceeded  in the status of the resource. The Deployment', metadata={'source': './PDFS/Concepts.pdf', 'page': 145}),\n",
       " Document(page_content='\"False\" . and reason: ProgressDeadlineExceeded  in the status of the resource. The Deployment\\ncontroller will keep retrying the Deployment. This defaults to 600. In the future, once automatic\\nrollback will be implemented, the Deployment controller will roll back a Deployment as soon as\\nit observes such a condition.\\nIf specified, this field needs to be greater than .spec.minReadySeconds .\\nMin Ready Seconds\\n.spec.minReadySeconds  is an optional field that specifies the minimum number of seconds for\\nwhich a newly created Pod should be ready without any of its containers crashing, for it to be\\nconsidered available. This defaults to 0 (the Pod will be considered available as soon as it is\\nready). To learn more about when a Pod is considered ready, see Container Probes .', metadata={'source': './PDFS/Concepts.pdf', 'page': 145}),\n",
       " Document(page_content=\"Revision History Limit\\nA Deployment's revision history is stored in the ReplicaSets it controls.\\n.spec.revisionHistoryLimit  is an optional field that specifies the number of old ReplicaSets to\\nretain to allow rollback. These old ReplicaSets consume resources in etcd and crowd the output\\nof kubectl get rs . The configuration of each Deployment revision is stored in its ReplicaSets;\\ntherefore, once an old ReplicaSet is deleted, you lose the ability to rollback to that revision of\\nDeployment. By default, 10 old ReplicaSets will be kept, however its ideal value depends on the\\nfrequency and stability of new Deployments.\\nMore specifically, setting this field to zero means that all old ReplicaSets with 0 replicas will be\\ncleaned up. In this case, a new Deployment rollout cannot be undone, since its revision history\\nis cleaned up.\\nPaused\\n.spec.paused  is an optional boolean field for pausing and resuming a Deployment. The only\", metadata={'source': './PDFS/Concepts.pdf', 'page': 146}),\n",
       " Document(page_content=\"Paused\\n.spec.paused  is an optional boolean field for pausing and resuming a Deployment. The only\\ndifference between a paused Deployment and one that is not paused, is that any changes into\\nthe PodTemplateSpec of the paused Deployment will not trigger new rollouts as long as it is\\npaused. A Deployment is not paused by default when it is created.\\nWhat's next\\nLearn more about Pods .\\nRun a stateless application using a Deployment .\\nRead the Deployment  to understand the Deployment API.\\nRead about PodDisruptionBudget  and how you can use it to manage application\\navailability during disruptions.\\nUse kubectl to create a Deployment .\\nReplicaSet\\nA ReplicaSet's purpose is to maintain a stable set of replica Pods running at any given time.\\nUsually, you define a Deployment and let that Deployment manage ReplicaSets automatically.\\nA ReplicaSet's purpose is to maintain a stable set of replica Pods running at any given time. As\", metadata={'source': './PDFS/Concepts.pdf', 'page': 146}),\n",
       " Document(page_content=\"A ReplicaSet's purpose is to maintain a stable set of replica Pods running at any given time. As\\nsuch, it is often used to guarantee the availability of a specified number of identical Pods.\\nHow a ReplicaSet works\\nA ReplicaSet is defined with fields, including a selector that specifies how to identify Pods it can\\nacquire, a number of replicas indicating how many Pods it should be maintaining, and a pod\\ntemplate specifying the data of new Pods it should create to meet the number of replicas\\ncriteria. A ReplicaSet then fulfills its purpose by creating and deleting Pods as needed to reach\\nthe desired number. When a ReplicaSet needs to create new Pods, it uses its Pod template.\\nA ReplicaSet is linked to its Pods via the Pods' metadata.ownerReferences  field, which specifies\\nwhat resource the current object is owned by. All Pods acquired by a ReplicaSet have their\\nowning ReplicaSet's identifying information within their ownerReferences field. It's through• \\n• \\n• \\n• \\n•\", metadata={'source': './PDFS/Concepts.pdf', 'page': 146}),\n",
       " Document(page_content=\"this link that the ReplicaSet knows of the state of the Pods it is maintaining and plans\\naccordingly.\\nA ReplicaSet identifies new Pods to acquire by using its selector. If there is a Pod that has no\\nOwnerReference or the OwnerReference is not a Controller  and it matches a ReplicaSet's\\nselector, it will be immediately acquired by said ReplicaSet.\\nWhen to use a ReplicaSet\\nA ReplicaSet ensures that a specified number of pod replicas are running at any given time.\\nHowever, a Deployment is a higher-level concept that manages ReplicaSets and provides\\ndeclarative updates to Pods along with a lot of other useful features. Therefore, we recommend\\nusing Deployments instead of directly using ReplicaSets, unless you require custom update\\norchestration or don't require updates at all.\\nThis actually means that you may never need to manipulate ReplicaSet objects: use a\\nDeployment instead, and define your application in the spec section.\\nExample\\ncontrollers/frontend.yaml  \\napiVersion : apps/v1\", metadata={'source': './PDFS/Concepts.pdf', 'page': 147}),\n",
       " Document(page_content='Example\\ncontrollers/frontend.yaml  \\napiVersion : apps/v1\\nkind: ReplicaSet\\nmetadata :\\n  name : frontend\\n  labels :\\n    app: guestbook\\n    tier: frontend\\nspec:\\n  # modify replicas according to your case\\n  replicas : 3\\n  selector :\\n    matchLabels :\\n      tier: frontend\\n  template :\\n    metadata :\\n      labels :\\n        tier: frontend\\n    spec:\\n      containers :\\n      - name : php-redis\\n        image : gcr.io/google_samples/gb-frontend:v3\\nSaving this manifest into frontend.yaml  and submitting it to a Kubernetes cluster will create the\\ndefined ReplicaSet and the Pods that it manages.\\nkubectl apply -f https://kubernetes.io/examples/controllers/frontend.yaml\\nYou can then get the current ReplicaSets deployed:\\nkubectl get rs', metadata={'source': './PDFS/Concepts.pdf', 'page': 147}),\n",
       " Document(page_content='And see the frontend one you created:\\nNAME       DESIRED   CURRENT   READY   AGE\\nfrontend   3         3         3       6s\\nYou can also check on the state of the ReplicaSet:\\nkubectl describe rs/frontend\\nAnd you will see output similar to:\\nName:         frontend\\nNamespace:    default\\nSelector:     tier=frontend\\nLabels:       app=guestbook\\n              tier=frontend\\nAnnotations:  kubectl.kubernetes.io/last-applied-configuration:\\n                {\"apiVersion\":\"apps/v1\",\"kind\":\"ReplicaSet\",\"metadata\":{\"annotations\":{},\"labels\":\\n{\"app\":\"guestbook\",\"tier\":\"frontend\"},\"name\":\"frontend\",...\\nReplicas:     3 current / 3 desired\\nPods Status:  3 Running / 0 Waiting / 0 Succeeded / 0 Failed\\nPod Template:\\n  Labels:  tier=frontend\\n  Containers:\\n   php-redis:\\n    Image:        gcr.io/google_samples/gb-frontend:v3\\n    Port:         <none>\\n    Host Port:    <none>\\n    Environment:  <none>\\n    Mounts:       <none>\\n  Volumes:        <none>\\nEvents:', metadata={'source': './PDFS/Concepts.pdf', 'page': 148}),\n",
       " Document(page_content='Environment:  <none>\\n    Mounts:       <none>\\n  Volumes:        <none>\\nEvents:\\n  Type    Reason            Age   From                   Message\\n  ----    ------            ----  ----                   -------\\n  Normal  SuccessfulCreate  117s  replicaset-controller  Created pod: frontend-wtsmm\\n  Normal  SuccessfulCreate  116s  replicaset-controller  Created pod: frontend-b2zdv\\n  Normal  SuccessfulCreate  116s  replicaset-controller  Created pod: frontend-vcmts\\nAnd lastly you can check for the Pods brought up:\\nkubectl get pods\\nYou should see Pod information similar to:\\nNAME             READY   STATUS    RESTARTS   AGE\\nfrontend-b2zdv   1/1     Running   0          6m36s\\nfrontend-vcmts   1/1     Running   0          6m36s\\nfrontend-wtsmm   1/1     Running   0          6m36s\\nYou can also verify that the owner reference of these pods is set to the frontend ReplicaSet. To\\ndo this, get the yaml of one of the Pods running:\\nkubectl get pods frontend-b2zdv -o yaml', metadata={'source': './PDFS/Concepts.pdf', 'page': 148}),\n",
       " Document(page_content='The output will look similar to this, with the frontend ReplicaSet\\'s info set in the metadata\\'s\\nownerReferences field:\\napiVersion : v1\\nkind: Pod\\nmetadata :\\n  creationTimestamp : \"2020-02-12T07:06:16Z\"\\n  generateName : frontend-\\n  labels :\\n    tier: frontend\\n  name : frontend-b2zdv\\n  namespace : default\\n  ownerReferences :\\n  - apiVersion : apps/v1\\n    blockOwnerDeletion : true\\n    controller : true\\n    kind: ReplicaSet\\n    name : frontend\\n    uid: f391f6db-bb9b-4c09-ae74-6a1f77f3d5cf\\n...\\nNon-Template Pod acquisitions\\nWhile you can create bare Pods with no problems, it is strongly recommended to make sure\\nthat the bare Pods do not have labels which match the selector of one of your ReplicaSets. The\\nreason for this is because a ReplicaSet is not limited to owning Pods specified by its template--\\nit can acquire other Pods in the manner specified in the previous sections.\\nTake the previous frontend ReplicaSet example, and the Pods specified in the following\\nmanifest:\\npods/pod-rs.yaml', metadata={'source': './PDFS/Concepts.pdf', 'page': 149}),\n",
       " Document(page_content='manifest:\\npods/pod-rs.yaml  \\napiVersion : v1\\nkind: Pod\\nmetadata :\\n  name : pod1\\n  labels :\\n    tier: frontend\\nspec:\\n  containers :\\n  - name : hello1\\n    image : gcr.io/google-samples/hello-app:2.0\\n---\\napiVersion : v1\\nkind: Pod\\nmetadata :\\n  name : pod2\\n  labels :\\n    tier: frontend\\nspec:', metadata={'source': './PDFS/Concepts.pdf', 'page': 149}),\n",
       " Document(page_content='containers :\\n  - name : hello2\\n    image : gcr.io/google-samples/hello-app:1.0\\nAs those Pods do not have a Controller (or any object) as their owner reference and match the\\nselector of the frontend ReplicaSet, they will immediately be acquired by it.\\nSuppose you create the Pods after the frontend ReplicaSet has been deployed and has set up its\\ninitial Pod replicas to fulfill its replica count requirement:\\nkubectl apply -f https://kubernetes.io/examples/pods/pod-rs.yaml\\nThe new Pods will be acquired by the ReplicaSet, and then immediately terminated as the\\nReplicaSet would be over its desired count.\\nFetching the Pods:\\nkubectl get pods\\nThe output shows that the new Pods are either already terminated, or in the process of being\\nterminated:\\nNAME             READY   STATUS        RESTARTS   AGE\\nfrontend-b2zdv   1/1     Running       0          10m\\nfrontend-vcmts   1/1     Running       0          10m\\nfrontend-wtsmm   1/1     Running       0          10m', metadata={'source': './PDFS/Concepts.pdf', 'page': 150}),\n",
       " Document(page_content='frontend-wtsmm   1/1     Running       0          10m\\npod1             0/1     Terminating   0          1s\\npod2             0/1     Terminating   0          1s\\nIf you create the Pods first:\\nkubectl apply -f https://kubernetes.io/examples/pods/pod-rs.yaml\\nAnd then create the ReplicaSet however:\\nkubectl apply -f https://kubernetes.io/examples/controllers/frontend.yaml\\nYou shall see that the ReplicaSet has acquired the Pods and has only created new ones\\naccording to its spec until the number of its new Pods and the original matches its desired\\ncount. As fetching the Pods:\\nkubectl get pods\\nWill reveal in its output:\\nNAME             READY   STATUS    RESTARTS   AGE\\nfrontend-hmmj2   1/1     Running   0          9s\\npod1             1/1     Running   0          36s\\npod2             1/1     Running   0          36s\\nIn this manner, a ReplicaSet can own a non-homogenous set of Pods', metadata={'source': './PDFS/Concepts.pdf', 'page': 150}),\n",
       " Document(page_content=\"Writing a ReplicaSet manifest\\nAs with all other Kubernetes API objects, a ReplicaSet needs the apiVersion , kind, and metadata\\nfields. For ReplicaSets, the kind is always a ReplicaSet.\\nWhen the control plane creates new Pods for a ReplicaSet, the .metadata.name  of the ReplicaSet\\nis part of the basis for naming those Pods. The name of a ReplicaSet must be a valid DNS\\nsubdomain  value, but this can produce unexpected results for the Pod hostnames. For best\\ncompatibility, the name should follow the more restrictive rules for a DNS label .\\nA ReplicaSet also needs a .spec  section .\\nPod Template\\nThe .spec.template  is a pod template  which is also required to have labels in place. In our \\nfrontend.yaml  example we had one label: tier: frontend . Be careful not to overlap with the\\nselectors of other controllers, lest they try to adopt this Pod.\\nFor the template's restart policy  field, .spec.template.spec.restartPolicy , the only allowed value\\nis Always , which is the default.\\nPod Selector\", metadata={'source': './PDFS/Concepts.pdf', 'page': 151}),\n",
       " Document(page_content='is Always , which is the default.\\nPod Selector\\nThe .spec.selector  field is a label selector . As discussed earlier  these are the labels used to\\nidentify potential Pods to acquire. In our frontend.yaml  example, the selector was:\\nmatchLabels :\\n  tier: frontend\\nIn the ReplicaSet, .spec.template.metadata.labels  must match spec.selector , or it will be rejected\\nby the API.\\nNote:  For 2 ReplicaSets specifying the same .spec.selector  but\\ndifferent .spec.template.metadata.labels  and .spec.template.spec  fields, each ReplicaSet ignores\\nthe Pods created by the other ReplicaSet.\\nReplicas\\nYou can specify how many Pods should run concurrently by setting .spec.replicas . The\\nReplicaSet will create/delete its Pods to match this number.\\nIf you do not specify .spec.replicas , then it defaults to 1.\\nWorking with ReplicaSets\\nDeleting a ReplicaSet and its Pods\\nTo delete a ReplicaSet and all of its Pods, use kubectl delete . The Garbage collector', metadata={'source': './PDFS/Concepts.pdf', 'page': 151}),\n",
       " Document(page_content='To delete a ReplicaSet and all of its Pods, use kubectl delete . The Garbage collector\\nautomatically deletes all of the dependent Pods by default.\\nWhen using the REST API or the client-go  library, you must set propagationPolicy  to \\nBackground  or Foreground  in the -d option. For example:', metadata={'source': './PDFS/Concepts.pdf', 'page': 151}),\n",
       " Document(page_content='kubectl proxy --port =8080\\ncurl -X DELETE  \\'localhost:8080/apis/apps/v1/namespaces/default/replicasets/frontend\\'  \\\\\\n  -d \\'{\"kind\":\"DeleteOptions\",\"apiVersion\":\"v1\",\"propagationPolicy\":\"Foreground\"}\\'  \\\\\\n  -H \"Content-Type: application/json\"\\nDeleting just a ReplicaSet\\nYou can delete a ReplicaSet without affecting any of its Pods using kubectl delete  with the --\\ncascade=orphan  option. When using the REST API or the client-go  library, you must set \\npropagationPolicy  to Orphan . For example:\\nkubectl proxy --port =8080\\ncurl -X DELETE  \\'localhost:8080/apis/apps/v1/namespaces/default/replicasets/frontend\\'  \\\\\\n  -d \\'{\"kind\":\"DeleteOptions\",\"apiVersion\":\"v1\",\"propagationPolicy\":\"Orphan\"}\\'  \\\\\\n  -H \"Content-Type: application/json\"\\nOnce the original is deleted, you can create a new ReplicaSet to replace it. As long as the old\\nand new .spec.selector  are the same, then the new one will adopt the old Pods. However, it will', metadata={'source': './PDFS/Concepts.pdf', 'page': 152}),\n",
       " Document(page_content='and new .spec.selector  are the same, then the new one will adopt the old Pods. However, it will\\nnot make any effort to make existing Pods match a new, different pod template. To update Pods\\nto a new spec in a controlled way, use a Deployment , as ReplicaSets do not support a rolling\\nupdate directly.\\nIsolating Pods from a ReplicaSet\\nYou can remove Pods from a ReplicaSet by changing their labels. This technique may be used to\\nremove Pods from service for debugging, data recovery, etc. Pods that are removed in this way\\nwill be replaced automatically ( assuming that the number of replicas is not also changed).\\nScaling a ReplicaSet\\nA ReplicaSet can be easily scaled up or down by simply updating the .spec.replicas  field. The\\nReplicaSet controller ensures that a desired number of Pods with a matching label selector are\\navailable and operational.\\nWhen scaling down, the ReplicaSet controller chooses which pods to delete by sorting the', metadata={'source': './PDFS/Concepts.pdf', 'page': 152}),\n",
       " Document(page_content=\"When scaling down, the ReplicaSet controller chooses which pods to delete by sorting the\\navailable pods to prioritize scaling down pods based on the following general algorithm:\\nPending (and unschedulable) pods are scaled down first\\nIf controller.kubernetes.io/pod-deletion-cost  annotation is set, then the pod with the\\nlower value will come first.\\nPods on nodes with more replicas come before pods on nodes with fewer replicas.\\nIf the pods' creation times differ, the pod that was created more recently comes before the\\nolder pod (the creation times are bucketed on an integer log scale when the \\nLogarithmicScaleDown  feature gate  is enabled)\\nIf all of the above match, then selection is random.\\nPod deletion cost\\nFEATURE STATE:  Kubernetes v1.22 [beta]\\nUsing the controller.kubernetes.io/pod-deletion-cost  annotation, users can set a preference\\nregarding which pods to remove first when downscaling a ReplicaSet.1. \\n2. \\n3. \\n4.\", metadata={'source': './PDFS/Concepts.pdf', 'page': 152}),\n",
       " Document(page_content=\"The annotation should be set on the pod, the range is [-2147483648, 2147483647]. It represents\\nthe cost of deleting a pod compared to other pods belonging to the same ReplicaSet. Pods with\\nlower deletion cost are preferred to be deleted before pods with higher deletion cost.\\nThe implicit value for this annotation for pods that don't set it is 0; negative values are\\npermitted. Invalid values will be rejected by the API server.\\nThis feature is beta and enabled by default. You can disable it using the feature gate  \\nPodDeletionCost  in both kube-apiserver and kube-controller-manager.\\nNote:\\nThis is honored on a best-effort basis, so it does not offer any guarantees on pod deletion\\norder.\\nUsers should avoid updating the annotation frequently, such as updating it based on a\\nmetric value, because doing so will generate a significant number of pod updates on the\\napiserver.\\nExample Use Case\\nThe different pods of an application could have different utilization levels. On scale down, the\", metadata={'source': './PDFS/Concepts.pdf', 'page': 153}),\n",
       " Document(page_content='The different pods of an application could have different utilization levels. On scale down, the\\napplication may prefer to remove the pods with lower utilization. To avoid frequently updating\\nthe pods, the application should update controller.kubernetes.io/pod-deletion-cost  once before\\nissuing a scale down (setting the annotation to a value proportional to pod utilization level).\\nThis works if the application itself controls the down scaling; for example, the driver pod of a\\nSpark deployment.\\nReplicaSet as a Horizontal Pod Autoscaler Target\\nA ReplicaSet can also be a target for Horizontal Pod Autoscalers (HPA) . That is, a ReplicaSet can\\nbe auto-scaled by an HPA. Here is an example HPA targeting the ReplicaSet we created in the\\nprevious example.\\ncontrollers/hpa-rs.yaml  \\napiVersion : autoscaling/v1\\nkind: HorizontalPodAutoscaler\\nmetadata :\\n  name : frontend-scaler\\nspec:\\n  scaleTargetRef :\\n    kind: ReplicaSet\\n    name : frontend\\n  minReplicas : 3\\n  maxReplicas : 10', metadata={'source': './PDFS/Concepts.pdf', 'page': 153}),\n",
       " Document(page_content='scaleTargetRef :\\n    kind: ReplicaSet\\n    name : frontend\\n  minReplicas : 3\\n  maxReplicas : 10\\n  targetCPUUtilizationPercentage : 50\\nSaving this manifest into hpa-rs.yaml  and submitting it to a Kubernetes cluster should create\\nthe defined HPA that autoscales the target ReplicaSet depending on the CPU usage of the\\nreplicated Pods.\\nkubectl apply -f https://k8s.io/examples/controllers/hpa-rs.yaml• \\n•', metadata={'source': './PDFS/Concepts.pdf', 'page': 153}),\n",
       " Document(page_content=\"Alternatively, you can use the kubectl autoscale  command to accomplish the same (and it's\\neasier!)\\nkubectl autoscale rs frontend --max =10 --min =3 --cpu-percent =50\\nAlternatives to ReplicaSet\\nDeployment (recommended)\\nDeployment  is an object which can own ReplicaSets and update them and their Pods via\\ndeclarative, server-side rolling updates. While ReplicaSets can be used independently, today\\nthey're mainly used by Deployments as a mechanism to orchestrate Pod creation, deletion and\\nupdates. When you use Deployments you don't have to worry about managing the ReplicaSets\\nthat they create. Deployments own and manage their ReplicaSets. As such, it is recommended\\nto use Deployments when you want ReplicaSets.\\nBare Pods\\nUnlike the case where a user directly created Pods, a ReplicaSet replaces Pods that are deleted\\nor terminated for any reason, such as in the case of node failure or disruptive node\\nmaintenance, such as a kernel upgrade. For this reason, we recommend that you use a\", metadata={'source': './PDFS/Concepts.pdf', 'page': 154}),\n",
       " Document(page_content='maintenance, such as a kernel upgrade. For this reason, we recommend that you use a\\nReplicaSet even if your application requires only a single Pod. Think of it similarly to a process\\nsupervisor, only it supervises multiple Pods across multiple nodes instead of individual\\nprocesses on a single node. A ReplicaSet delegates local container restarts to some agent on the\\nnode such as Kubelet.\\nJob\\nUse a Job instead of a ReplicaSet for Pods that are expected to terminate on their own (that is,\\nbatch jobs).\\nDaemonSet\\nUse a DaemonSet  instead of a ReplicaSet for Pods that provide a machine-level function, such as\\nmachine monitoring or machine logging. These Pods have a lifetime that is tied to a machine\\nlifetime: the Pod needs to be running on the machine before other Pods start, and are safe to\\nterminate when the machine is otherwise ready to be rebooted/shutdown.\\nReplicationController\\nReplicaSets are the successors to ReplicationControllers . The two serve the same purpose, and', metadata={'source': './PDFS/Concepts.pdf', 'page': 154}),\n",
       " Document(page_content=\"ReplicaSets are the successors to ReplicationControllers . The two serve the same purpose, and\\nbehave similarly, except that a ReplicationController does not support set-based selector\\nrequirements as described in the labels user guide . As such, ReplicaSets are preferred over\\nReplicationControllers\\nWhat's next\\nLearn about Pods .\\nLearn about Deployments .\\nRun a Stateless Application Using a Deployment , which relies on ReplicaSets to work.• \\n• \\n•\", metadata={'source': './PDFS/Concepts.pdf', 'page': 154}),\n",
       " Document(page_content='ReplicaSet  is a top-level resource in the Kubernetes REST API. Read the ReplicaSet  object\\ndefinition to understand the API for replica sets.\\nRead about PodDisruptionBudget  and how you can use it to manage application\\navailability during disruptions.\\nStatefulSets\\nA StatefulSet runs a group of Pods, and maintains a sticky identity for each of those Pods. This\\nis useful for managing applications that need persistent storage or a stable, unique network\\nidentity.\\nStatefulSet is the workload API object used to manage stateful applications.\\nManages the deployment and scaling of a set of Pods , and provides guarantees about the ordering\\nand uniqueness  of these Pods.\\nLike a Deployment , a StatefulSet manages Pods that are based on an identical container spec.\\nUnlike a Deployment, a StatefulSet maintains a sticky identity for each of its Pods. These pods\\nare created from the same spec, but are not interchangeable: each has a persistent identifier that\\nit maintains across any rescheduling.', metadata={'source': './PDFS/Concepts.pdf', 'page': 155}),\n",
       " Document(page_content=\"it maintains across any rescheduling.\\nIf you want to use storage volumes to provide persistence for your workload, you can use a\\nStatefulSet as part of the solution. Although individual Pods in a StatefulSet are susceptible to\\nfailure, the persistent Pod identifiers make it easier to match existing volumes to the new Pods\\nthat replace any that have failed.\\nUsing StatefulSets\\nStatefulSets are valuable for applications that require one or more of the following.\\nStable, unique network identifiers.\\nStable, persistent storage.\\nOrdered, graceful deployment and scaling.\\nOrdered, automated rolling updates.\\nIn the above, stable is synonymous with persistence across Pod (re)scheduling. If an application\\ndoesn't require any stable identifiers or ordered deployment, deletion, or scaling, you should\\ndeploy your application using a workload object that provides a set of stateless replicas. \\nDeployment  or ReplicaSet  may be better suited to your stateless needs.\\nLimitations\", metadata={'source': './PDFS/Concepts.pdf', 'page': 155}),\n",
       " Document(page_content='Deployment  or ReplicaSet  may be better suited to your stateless needs.\\nLimitations\\nThe storage for a given Pod must either be provisioned by a PersistentVolume Provisioner\\nbased on the requested storage class , or pre-provisioned by an admin.\\nDeleting and/or scaling a StatefulSet down will not delete the volumes associated with the\\nStatefulSet. This is done to ensure data safety, which is generally more valuable than an\\nautomatic purge of all related StatefulSet resources.\\nStatefulSets currently require a Headless Service  to be responsible for the network\\nidentity of the Pods. You are responsible for creating this Service.• \\n• \\n• \\n• \\n• \\n• \\n• \\n• \\n•', metadata={'source': './PDFS/Concepts.pdf', 'page': 155}),\n",
       " Document(page_content='StatefulSets do not provide any guarantees on the termination of pods when a StatefulSet\\nis deleted. To achieve ordered and graceful termination of the pods in the StatefulSet, it is\\npossible to scale the StatefulSet down to 0 prior to deletion.\\nWhen using Rolling Updates  with the default Pod Management Policy  (OrderedReady ),\\nit\\'s possible to get into a broken state that requires manual intervention to repair .\\nComponents\\nThe example below demonstrates the components of a StatefulSet.\\napiVersion : v1\\nkind: Service\\nmetadata :\\n  name : nginx\\n  labels :\\n    app: nginx\\nspec:\\n  ports :\\n  - port: 80\\n    name : web\\n  clusterIP : None\\n  selector :\\n    app: nginx\\n---\\napiVersion : apps/v1\\nkind: StatefulSet\\nmetadata :\\n  name : web\\nspec:\\n  selector :\\n    matchLabels :\\n      app: nginx  # has to match .spec.template.metadata.labels\\n  serviceName : \"nginx\"\\n  replicas : 3 # by default is 1\\n  minReadySeconds : 10 # by default is 0\\n  template :\\n    metadata :\\n      labels :', metadata={'source': './PDFS/Concepts.pdf', 'page': 156}),\n",
       " Document(page_content='minReadySeconds : 10 # by default is 0\\n  template :\\n    metadata :\\n      labels :\\n        app: nginx  # has to match .spec.selector.matchLabels\\n    spec:\\n      terminationGracePeriodSeconds : 10\\n      containers :\\n      - name : nginx\\n        image : registry.k8s.io/nginx-slim:0.8\\n        ports :\\n        - containerPort : 80\\n          name : web\\n        volumeMounts :\\n        - name : www\\n          mountPath : /usr/share/nginx/html\\n  volumeClaimTemplates :\\n  - metadata :• \\n•', metadata={'source': './PDFS/Concepts.pdf', 'page': 156}),\n",
       " Document(page_content='name : www\\n    spec:\\n      accessModes : [ \"ReadWriteOnce\"  ]\\n      storageClassName : \"my-storage-class\"\\n      resources :\\n        requests :\\n          storage : 1Gi\\nIn the above example:\\nA Headless Service, named nginx , is used to control the network domain.\\nThe StatefulSet, named web, has a Spec that indicates that 3 replicas of the nginx\\ncontainer will be launched in unique Pods.\\nThe volumeClaimTemplates  will provide stable storage using PersistentVolumes\\nprovisioned by a PersistentVolume Provisioner.\\nThe name of a StatefulSet object must be a valid DNS label .\\nPod Selector\\nYou must set the .spec.selector  field of a StatefulSet to match the labels of\\nits .spec.template.metadata.labels . Failing to specify a matching Pod Selector will result in a\\nvalidation error during StatefulSet creation.\\nVolume Claim Templates\\nYou can set the .spec.volumeClaimTemplates  which can provide stable storage using \\nPersistentVolumes  provisioned by a PersistentVolume Provisioner.', metadata={'source': './PDFS/Concepts.pdf', 'page': 157}),\n",
       " Document(page_content=\"PersistentVolumes  provisioned by a PersistentVolume Provisioner.\\nMinimum ready seconds\\nFEATURE STATE:  Kubernetes v1.25 [stable]\\n.spec.minReadySeconds  is an optional field that specifies the minimum number of seconds for\\nwhich a newly created Pod should be running and ready without any of its containers crashing,\\nfor it to be considered available. This is used to check progression of a rollout when using a \\nRolling Update  strategy. This field defaults to 0 (the Pod will be considered available as soon as\\nit is ready). To learn more about when a Pod is considered ready, see Container Probes .\\nPod Identity\\nStatefulSet Pods have a unique identity that consists of an ordinal, a stable network identity,\\nand stable storage. The identity sticks to the Pod, regardless of which node it's (re)scheduled on.\\nOrdinal Index\\nFor a StatefulSet with N replicas , each Pod in the StatefulSet will be assigned an integer ordinal,\", metadata={'source': './PDFS/Concepts.pdf', 'page': 157}),\n",
       " Document(page_content='For a StatefulSet with N replicas , each Pod in the StatefulSet will be assigned an integer ordinal,\\nthat is unique over the Set. By default, pods will be assigned ordinals from 0 up through N-1.\\nThe StatefulSet controller will also add a pod label with this index: apps.kubernetes.io/pod-\\nindex .• \\n• \\n•', metadata={'source': './PDFS/Concepts.pdf', 'page': 157}),\n",
       " Document(page_content='Start ordinal\\nFEATURE STATE:  Kubernetes v1.27 [beta]\\n.spec.ordinals  is an optional field that allows you to configure the integer ordinals assigned to\\neach Pod. It defaults to nil. You must enable the StatefulSetStartOrdinal  feature gate  to use this\\nfield. Once enabled, you can configure the following options:\\n.spec.ordinals.start : If the .spec.ordinals.start  field is set, Pods will be assigned ordinals\\nfrom .spec.ordinals.start  up through .spec.ordinals.start + .spec.replicas - 1 .\\nStable Network ID\\nEach Pod in a StatefulSet derives its hostname from the name of the StatefulSet and the ordinal\\nof the Pod. The pattern for the constructed hostname is $(statefulset name)-$(ordinal) . The\\nexample above will create three Pods named web-0,web-1,web-2 . A StatefulSet can use a \\nHeadless Service  to control the domain of its Pods. The domain managed by this Service takes\\nthe form: $(service name).$(namespace).svc.cluster.local , where \"cluster.local\" is the cluster', metadata={'source': './PDFS/Concepts.pdf', 'page': 158}),\n",
       " Document(page_content='the form: $(service name).$(namespace).svc.cluster.local , where \"cluster.local\" is the cluster\\ndomain. As each Pod is created, it gets a matching DNS subdomain, taking the form: $\\n(podname).$(governing service domain) , where the governing service is defined by the \\nserviceName  field on the StatefulSet.\\nDepending on how DNS is configured in your cluster, you may not be able to look up the DNS\\nname for a newly-run Pod immediately. This behavior can occur when other clients in the\\ncluster have already sent queries for the hostname of the Pod before it was created. Negative\\ncaching (normal in DNS) means that the results of previous failed lookups are remembered and\\nreused, even after the Pod is running, for at least a few seconds.\\nIf you need to discover Pods promptly after they are created, you have a few options:\\nQuery the Kubernetes API directly (for example, using a watch) rather than relying on\\nDNS lookups.', metadata={'source': './PDFS/Concepts.pdf', 'page': 158}),\n",
       " Document(page_content=\"Query the Kubernetes API directly (for example, using a watch) rather than relying on\\nDNS lookups.\\nDecrease the time of caching in your Kubernetes DNS provider (typically this means\\nediting the config map for CoreDNS, which currently caches for 30 seconds).\\nAs mentioned in the limitations  section, you are responsible for creating the Headless Service\\nresponsible for the network identity of the pods.\\nHere are some examples of choices for Cluster Domain, Service name, StatefulSet name, and\\nhow that affects the DNS names for the StatefulSet's Pods.\\nCluster\\nDomainService\\n(ns/\\nname)StatefulSet\\n(ns/name)StatefulSet Domain Pod DNSPod\\nHostname\\ncluster.localdefault/\\nnginxdefault/web nginx.default.svc.cluster.localweb-\\n{0..N-1}.nginx.default.svc.cluster.localweb-\\n{0..N-1}\\ncluster.localfoo/\\nnginxfoo/web nginx.foo.svc.cluster.localweb-\\n{0..N-1}.nginx.foo.svc.cluster.localweb-\\n{0..N-1}\\nkube.localfoo/\\nnginxfoo/web nginx.foo.svc.kube.local web-{0..N-1}.nginx.foo.svc.kube.localweb-\\n{0..N-1}\", metadata={'source': './PDFS/Concepts.pdf', 'page': 158}),\n",
       " Document(page_content='nginxfoo/web nginx.foo.svc.kube.local web-{0..N-1}.nginx.foo.svc.kube.localweb-\\n{0..N-1}\\nNote:  Cluster Domain will be set to cluster.local  unless otherwise configured .• \\n• \\n•', metadata={'source': './PDFS/Concepts.pdf', 'page': 158}),\n",
       " Document(page_content=\"Stable Storage\\nFor each VolumeClaimTemplate entry defined in a StatefulSet, each Pod receives one\\nPersistentVolumeClaim. In the nginx example above, each Pod receives a single\\nPersistentVolume with a StorageClass of my-storage-class  and 1 GiB of provisioned storage. If\\nno StorageClass is specified, then the default StorageClass will be used. When a Pod is\\n(re)scheduled onto a node, its volumeMounts  mount the PersistentVolumes associated with its\\nPersistentVolume Claims. Note that, the PersistentVolumes associated with the Pods'\\nPersistentVolume Claims are not deleted when the Pods, or StatefulSet are deleted. This must be\\ndone manually.\\nPod Name Label\\nWhen the StatefulSet controller  creates a Pod, it adds a label, statefulset.kubernetes.io/pod-\\nname , that is set to the name of the Pod. This label allows you to attach a Service to a specific\\nPod in the StatefulSet.\\nPod index label\\nFEATURE STATE:  Kubernetes v1.28 [beta]\", metadata={'source': './PDFS/Concepts.pdf', 'page': 159}),\n",
       " Document(page_content='Pod in the StatefulSet.\\nPod index label\\nFEATURE STATE:  Kubernetes v1.28 [beta]\\nWhen the StatefulSet controller  creates a Pod, the new Pod is labelled with apps.kubernetes.io/\\npod-index . The value of this label is the ordinal index of the Pod. This label allows you to route\\ntraffic to a particular pod index, filter logs/metrics using the pod index label, and more. Note the\\nfeature gate PodIndexLabel  must be enabled for this feature, and it is enabled by default.\\nDeployment and Scaling Guarantees\\nFor a StatefulSet with N replicas, when Pods are being deployed, they are created\\nsequentially, in order from {0..N-1}.\\nWhen Pods are being deleted, they are terminated in reverse order, from {N-1..0}.\\nBefore a scaling operation is applied to a Pod, all of its predecessors must be Running and\\nReady.\\nBefore a Pod is terminated, all of its successors must be completely shutdown.\\nThe StatefulSet should not specify a pod.Spec.TerminationGracePeriodSeconds  of 0. This', metadata={'source': './PDFS/Concepts.pdf', 'page': 159}),\n",
       " Document(page_content=\"The StatefulSet should not specify a pod.Spec.TerminationGracePeriodSeconds  of 0. This\\npractice is unsafe and strongly discouraged. For further explanation, please refer to force\\ndeleting StatefulSet Pods .\\nWhen the nginx example above is created, three Pods will be deployed in the order web-0,\\nweb-1, web-2. web-1 will not be deployed before web-0 is Running and Ready , and web-2 will\\nnot be deployed until web-1 is Running and Ready. If web-0 should fail, after web-1 is Running\\nand Ready, but before web-2 is launched, web-2 will not be launched until web-0 is successfully\\nrelaunched and becomes Running and Ready.\\nIf a user were to scale the deployed example by patching the StatefulSet such that replicas=1 ,\\nweb-2 would be terminated first. web-1 would not be terminated until web-2 is fully shutdown\\nand deleted. If web-0 were to fail after web-2 has been terminated and is completely shutdown,\\nbut prior to web-1's termination, web-1 would not be terminated until web-0 is Running and\", metadata={'source': './PDFS/Concepts.pdf', 'page': 159}),\n",
       " Document(page_content=\"but prior to web-1's termination, web-1 would not be terminated until web-0 is Running and\\nReady.• \\n• \\n• \\n•\", metadata={'source': './PDFS/Concepts.pdf', 'page': 159}),\n",
       " Document(page_content=\"Pod Management Policies\\nStatefulSet allows you to relax its ordering guarantees while preserving its uniqueness and\\nidentity guarantees via its .spec.podManagementPolicy  field.\\nOrderedReady Pod Management\\nOrderedReady  pod management is the default for StatefulSets. It implements the behavior\\ndescribed above .\\nParallel Pod Management\\nParallel  pod management tells the StatefulSet controller to launch or terminate all Pods in\\nparallel, and to not wait for Pods to become Running and Ready or completely terminated prior\\nto launching or terminating another Pod. This option only affects the behavior for scaling\\noperations. Updates are not affected.\\nUpdate strategies\\nA StatefulSet's .spec.updateStrategy  field allows you to configure and disable automated rolling\\nupdates for containers, labels, resource request/limits, and annotations for the Pods in a\\nStatefulSet. There are two possible values:\\nOnDelete\\nWhen a StatefulSet's .spec.updateStrategy.type  is set to OnDelete , the StatefulSet\", metadata={'source': './PDFS/Concepts.pdf', 'page': 160}),\n",
       " Document(page_content=\"OnDelete\\nWhen a StatefulSet's .spec.updateStrategy.type  is set to OnDelete , the StatefulSet\\ncontroller will not automatically update the Pods in a StatefulSet. Users must manually\\ndelete Pods to cause the controller to create new Pods that reflect modifications made to a\\nStatefulSet's .spec.template .\\nRollingUpdate\\nThe RollingUpdate  update strategy implements automated, rolling updates for the Pods in\\na StatefulSet. This is the default update strategy.\\nRolling Updates\\nWhen a StatefulSet's .spec.updateStrategy.type  is set to RollingUpdate , the StatefulSet controller\\nwill delete and recreate each Pod in the StatefulSet. It will proceed in the same order as Pod\\ntermination (from the largest ordinal to the smallest), updating each Pod one at a time.\\nThe Kubernetes control plane waits until an updated Pod is Running and Ready prior to\\nupdating its predecessor. If you have set .spec.minReadySeconds  (see Minimum Ready Seconds ),\", metadata={'source': './PDFS/Concepts.pdf', 'page': 160}),\n",
       " Document(page_content=\"updating its predecessor. If you have set .spec.minReadySeconds  (see Minimum Ready Seconds ),\\nthe control plane additionally waits that amount of time after the Pod turns ready, before\\nmoving on.\\nPartitioned rolling updates\\nThe RollingUpdate  update strategy can be partitioned, by specifying\\na .spec.updateStrategy.rollingUpdate.partition . If a partition is specified, all Pods with an ordinal\\nthat is greater than or equal to the partition will be updated when the StatefulSet's\\n.spec.template  is updated. All Pods with an ordinal that is less than the partition will not be\\nupdated, and, even if they are deleted, they will be recreated at the previous version. If a\\nStatefulSet's .spec.updateStrategy.rollingUpdate.partition  is greater than its .spec.replicas ,\", metadata={'source': './PDFS/Concepts.pdf', 'page': 160}),\n",
       " Document(page_content='updates to its .spec.template  will not be propagated to its Pods. In most cases you will not need\\nto use a partition, but they are useful if you want to stage an update, roll out a canary, or\\nperform a phased roll out.\\nMaximum unavailable Pods\\nFEATURE STATE:  Kubernetes v1.24 [alpha]\\nYou can control the maximum number of Pods that can be unavailable during an update by\\nspecifying the .spec.updateStrategy.rollingUpdate.maxUnavailable  field. The value can be an\\nabsolute number (for example, 5) or a percentage of desired Pods (for example, 10%). Absolute\\nnumber is calculated from the percentage value by rounding it up. This field cannot be 0. The\\ndefault setting is 1.\\nThis field applies to all Pods in the range 0 to replicas - 1 . If there is any unavailable Pod in the\\nrange 0 to replicas - 1 , it will be counted towards maxUnavailable .\\nNote:  The maxUnavailable  field is in Alpha stage and it is honored only by API servers that are', metadata={'source': './PDFS/Concepts.pdf', 'page': 161}),\n",
       " Document(page_content=\"Note:  The maxUnavailable  field is in Alpha stage and it is honored only by API servers that are\\nrunning with the MaxUnavailableStatefulSet  feature gate  enabled.\\nForced rollback\\nWhen using Rolling Updates  with the default Pod Management Policy  (OrderedReady ), it's\\npossible to get into a broken state that requires manual intervention to repair.\\nIf you update the Pod template to a configuration that never becomes Running and Ready (for\\nexample, due to a bad binary or application-level configuration error), StatefulSet will stop the\\nrollout and wait.\\nIn this state, it's not enough to revert the Pod template to a good configuration. Due to a known\\nissue , StatefulSet will continue to wait for the broken Pod to become Ready (which never\\nhappens) before it will attempt to revert it back to the working configuration.\\nAfter reverting the template, you must also delete any Pods that StatefulSet had already\", metadata={'source': './PDFS/Concepts.pdf', 'page': 161}),\n",
       " Document(page_content='After reverting the template, you must also delete any Pods that StatefulSet had already\\nattempted to run with the bad configuration. StatefulSet will then begin to recreate the Pods\\nusing the reverted template.\\nPersistentVolumeClaim retention\\nFEATURE STATE:  Kubernetes v1.27 [beta]\\nThe optional .spec.persistentVolumeClaimRetentionPolicy  field controls if and how PVCs are\\ndeleted during the lifecycle of a StatefulSet. You must enable the StatefulSetAutoDeletePVC  \\nfeature gate  on the API server and the controller manager to use this field. Once enabled, there\\nare two policies you can configure for each StatefulSet:\\nwhenDeleted\\nconfigures the volume retention behavior that applies when the StatefulSet is deleted\\nwhenScaled\\nconfigures the volume retention behavior that applies when the replica count of the\\nStatefulSet is reduced; for example, when scaling down the set.\\nFor each policy that you can configure, you can set the value to either Delete  or Retain .', metadata={'source': './PDFS/Concepts.pdf', 'page': 161}),\n",
       " Document(page_content='Delete\\nThe PVCs created from the StatefulSet volumeClaimTemplate  are deleted for each Pod\\naffected by the policy. With the whenDeleted  policy all PVCs from the \\nvolumeClaimTemplate  are deleted after their Pods have been deleted. With the \\nwhenScaled  policy, only PVCs corresponding to Pod replicas being scaled down are\\ndeleted, after their Pods have been deleted.\\nRetain  (default)\\nPVCs from the volumeClaimTemplate  are not affected when their Pod is deleted. This is\\nthe behavior before this new feature.\\nBear in mind that these policies only  apply when Pods are being removed due to the\\nStatefulSet being deleted or scaled down. For example, if a Pod associated with a StatefulSet\\nfails due to node failure, and the control plane creates a replacement Pod, the StatefulSet retains\\nthe existing PVC. The existing volume is unaffected, and the cluster will attach it to the node\\nwhere the new Pod is about to launch.', metadata={'source': './PDFS/Concepts.pdf', 'page': 162}),\n",
       " Document(page_content='where the new Pod is about to launch.\\nThe default for policies is Retain , matching the StatefulSet behavior before this new feature.\\nHere is an example policy.\\napiVersion : apps/v1\\nkind: StatefulSet\\n...\\nspec:\\n  persistentVolumeClaimRetentionPolicy :\\n    whenDeleted : Retain\\n    whenScaled : Delete\\n...\\nThe StatefulSet controller  adds owner references  to its PVCs, which are then deleted by the \\ngarbage collector  after the Pod is terminated. This enables the Pod to cleanly unmount all\\nvolumes before the PVCs are deleted (and before the backing PV and volume are deleted,\\ndepending on the retain policy). When you set the whenDeleted  policy to Delete , an owner\\nreference to the StatefulSet instance is placed on all PVCs associated with that StatefulSet.\\nThe whenScaled  policy must delete PVCs only when a Pod is scaled down, and not when a Pod\\nis deleted for another reason. When reconciling, the StatefulSet controller compares its desired', metadata={'source': './PDFS/Concepts.pdf', 'page': 162}),\n",
       " Document(page_content='is deleted for another reason. When reconciling, the StatefulSet controller compares its desired\\nreplica count to the actual Pods present on the cluster. Any StatefulSet Pod whose id greater\\nthan the replica count is condemned and marked for deletion. If the whenScaled  policy is \\nDelete , the condemned Pods are first set as owners to the associated StatefulSet template PVCs,\\nbefore the Pod is deleted. This causes the PVCs to be garbage collected after only the\\ncondemned Pods have terminated.\\nThis means that if the controller crashes and restarts, no Pod will be deleted before its owner\\nreference has been updated appropriate to the policy. If a condemned Pod is force-deleted while\\nthe controller is down, the owner reference may or may not have been set up, depending on\\nwhen the controller crashed. It may take several reconcile loops to update the owner references,\\nso some condemned Pods may have set up owner references and others may not. For this', metadata={'source': './PDFS/Concepts.pdf', 'page': 162}),\n",
       " Document(page_content='so some condemned Pods may have set up owner references and others may not. For this\\nreason we recommend waiting for the controller to come back up, which will verify owner\\nreferences before terminating Pods. If that is not possible, the operator should verify the owner\\nreferences on PVCs to ensure the expected objects are deleted when Pods are force-deleted.', metadata={'source': './PDFS/Concepts.pdf', 'page': 162}),\n",
       " Document(page_content=\"Replicas\\n.spec.replicas  is an optional field that specifies the number of desired Pods. It defaults to 1.\\nShould you manually scale a deployment, example via kubectl scale statefulset statefulset --\\nreplicas=X , and then you update that StatefulSet based on a manifest (for example: by running \\nkubectl apply -f statefulset.yaml ), then applying that manifest overwrites the manual scaling\\nthat you previously did.\\nIf a HorizontalPodAutoscaler  (or any similar API for horizontal scaling) is managing scaling for\\na Statefulset, don't set .spec.replicas . Instead, allow the Kubernetes control plane  to manage\\nthe .spec.replicas  field automatically.\\nWhat's next\\nLearn about Pods .\\nFind out how to use StatefulSets\\nFollow an example of deploying a stateful application .\\nFollow an example of deploying Cassandra with Stateful Sets .\\nFollow an example of running a replicated stateful application .\\nLearn how to scale a StatefulSet .\\nLearn what's involved when you delete a StatefulSet .\", metadata={'source': './PDFS/Concepts.pdf', 'page': 163}),\n",
       " Document(page_content=\"Learn how to scale a StatefulSet .\\nLearn what's involved when you delete a StatefulSet .\\nLearn how to configure a Pod to use a volume for storage .\\nLearn how to configure a Pod to use a PersistentVolume for storage .\\nStatefulSet  is a top-level resource in the Kubernetes REST API. Read the StatefulSet  object\\ndefinition to understand the API for stateful sets.\\nRead about PodDisruptionBudget  and how you can use it to manage application\\navailability during disruptions.\\nDaemonSet\\nA DaemonSet defines Pods that provide node-local facilities. These might be fundamental to the\\noperation of your cluster, such as a networking helper tool, or be part of an add-on.\\nA DaemonSet  ensures that all (or some) Nodes run a copy of a Pod. As nodes are added to the\\ncluster, Pods are added to them. As nodes are removed from the cluster, those Pods are garbage\\ncollected. Deleting a DaemonSet will clean up the Pods it created.\\nSome typical uses of a DaemonSet are:\", metadata={'source': './PDFS/Concepts.pdf', 'page': 163}),\n",
       " Document(page_content='Some typical uses of a DaemonSet are:\\nrunning a cluster storage daemon on every node\\nrunning a logs collection daemon on every node\\nrunning a node monitoring daemon on every node\\nIn a simple case, one DaemonSet, covering all nodes, would be used for each type of daemon. A\\nmore complex setup might use multiple DaemonSets for a single type of daemon, but with\\ndifferent flags and/or different memory and cpu requests for different hardware types.• \\n• \\n◦ \\n◦ \\n◦ \\n◦ \\n◦ \\n◦ \\n◦ \\n• \\n• \\n• \\n• \\n•', metadata={'source': './PDFS/Concepts.pdf', 'page': 163}),\n",
       " Document(page_content='Writing a DaemonSet Spec\\nCreate a DaemonSet\\nYou can describe a DaemonSet in a YAML file. For example, the daemonset.yaml  file below\\ndescribes a DaemonSet that runs the fluentd-elasticsearch Docker image:\\ncontrollers/daemonset.yaml  \\napiVersion : apps/v1\\nkind: DaemonSet\\nmetadata :\\n  name : fluentd-elasticsearch\\n  namespace : kube-system\\n  labels :\\n    k8s-app : fluentd-logging\\nspec:\\n  selector :\\n    matchLabels :\\n      name : fluentd-elasticsearch\\n  template :\\n    metadata :\\n      labels :\\n        name : fluentd-elasticsearch\\n    spec:\\n      tolerations :\\n      # these tolerations are to have the daemonset runnable on control plane nodes\\n      # remove them if your control plane nodes should not run pods\\n      - key: node-role.kubernetes.io/control-plane\\n        operator : Exists\\n        effect : NoSchedule\\n      - key: node-role.kubernetes.io/master\\n        operator : Exists\\n        effect : NoSchedule\\n      containers :\\n      - name : fluentd-elasticsearch', metadata={'source': './PDFS/Concepts.pdf', 'page': 164}),\n",
       " Document(page_content='effect : NoSchedule\\n      containers :\\n      - name : fluentd-elasticsearch\\n        image : quay.io/fluentd_elasticsearch/fluentd:v2.5.2\\n        resources :\\n          limits :\\n            memory : 200Mi\\n          requests :\\n            cpu: 100m\\n            memory : 200Mi\\n        volumeMounts :\\n        - name : varlog\\n          mountPath : /var/log\\n      # it may be desirable to set a high priority class to ensure that a DaemonSet Pod\\n      # preempts running Pods\\n      # priorityClassName: important\\n      terminationGracePeriodSeconds : 30\\n      volumes :\\n      - name : varlog', metadata={'source': './PDFS/Concepts.pdf', 'page': 164}),\n",
       " Document(page_content='hostPath :\\n          path: /var/log\\nCreate a DaemonSet based on the YAML file:\\nkubectl apply -f https://k8s.io/examples/controllers/daemonset.yaml\\nRequired Fields\\nAs with all other Kubernetes config, a DaemonSet needs apiVersion , kind, and metadata  fields.\\nFor general information about working with config files, see running stateless applications  and \\nobject management using kubectl .\\nThe name of a DaemonSet object must be a valid DNS subdomain name .\\nA DaemonSet also needs a .spec  section.\\nPod Template\\nThe .spec.template  is one of the required fields in .spec .\\nThe .spec.template  is a pod template . It has exactly the same schema as a Pod, except it is nested\\nand does not have an apiVersion  or kind.\\nIn addition to required fields for a Pod, a Pod template in a DaemonSet has to specify\\nappropriate labels (see pod selector ).\\nA Pod Template in a DaemonSet must have a RestartPolicy  equal to Always , or be unspecified,\\nwhich defaults to Always .\\nPod Selector', metadata={'source': './PDFS/Concepts.pdf', 'page': 165}),\n",
       " Document(page_content='which defaults to Always .\\nPod Selector\\nThe .spec.selector  field is a pod selector. It works the same as the .spec.selector  of a Job.\\nYou must specify a pod selector that matches the labels of the .spec.template . Also, once a\\nDaemonSet is created, its .spec.selector  can not be mutated. Mutating the pod selector can lead\\nto the unintentional orphaning of Pods, and it was found to be confusing to users.\\nThe .spec.selector  is an object consisting of two fields:\\nmatchLabels  - works the same as the .spec.selector  of a ReplicationController .\\nmatchExpressions  - allows to build more sophisticated selectors by specifying key, list of\\nvalues and an operator that relates the key and values.\\nWhen the two are specified the result is ANDed.\\nThe .spec.selector  must match the .spec.template.metadata.labels . Config with these two not\\nmatching will be rejected by the API.\\nRunning Pods on select Nodes\\nIf you specify a .spec.template.spec.nodeSelector , then the DaemonSet controller will create', metadata={'source': './PDFS/Concepts.pdf', 'page': 165}),\n",
       " Document(page_content='If you specify a .spec.template.spec.nodeSelector , then the DaemonSet controller will create\\nPods on nodes which match that node selector . Likewise if you specify• \\n•', metadata={'source': './PDFS/Concepts.pdf', 'page': 165}),\n",
       " Document(page_content=\"a .spec.template.spec.affinity , then DaemonSet controller will create Pods on nodes which\\nmatch that node affinity . If you do not specify either, then the DaemonSet controller will create\\nPods on all nodes.\\nHow Daemon Pods are scheduled\\nA DaemonSet can be used to ensure that all eligible nodes run a copy of a Pod. The DaemonSet\\ncontroller creates a Pod for each eligible node and adds the spec.affinity.nodeAffinity  field of the\\nPod to match the target host. After the Pod is created, the default scheduler typically takes over\\nand then binds the Pod to the target host by setting the .spec.nodeName  field. If the new Pod\\ncannot fit on the node, the default scheduler may preempt (evict) some of the existing Pods\\nbased on the priority  of the new Pod.\\nNote:  If it's important that the DaemonSet pod run on each node, it's often desirable to set\\nthe .spec.template.spec.priorityClassName  of the DaemonSet to a PriorityClass  with a higher\\npriority to ensure that this eviction occurs.\", metadata={'source': './PDFS/Concepts.pdf', 'page': 166}),\n",
       " Document(page_content='priority to ensure that this eviction occurs.\\nThe user can specify a different scheduler for the Pods of the DaemonSet, by setting the\\n.spec.template.spec.schedulerName  field of the DaemonSet.\\nThe original node affinity specified at the .spec.template.spec.affinity.nodeAffinity  field (if\\nspecified) is taken into consideration by the DaemonSet controller when evaluating the eligible\\nnodes, but is replaced on the created Pod with the node affinity that matches the name of the\\neligible node.\\nnodeAffinity :\\n  requiredDuringSchedulingIgnoredDuringExecution :\\n    nodeSelectorTerms :\\n    - matchFields :\\n      - key: metadata.name\\n        operator : In\\n        values :\\n        - target-host-name\\nTaints and tolerations\\nThe DaemonSet controller automatically adds a set of tolerations  to DaemonSet Pods:\\nTolerations for DaemonSet pods\\nToleration key Effect Details\\nnode.kubernetes.io/\\nnot-readyNoExecuteDaemonSet Pods can be scheduled onto nodes that are not', metadata={'source': './PDFS/Concepts.pdf', 'page': 166}),\n",
       " Document(page_content='node.kubernetes.io/\\nnot-readyNoExecuteDaemonSet Pods can be scheduled onto nodes that are not\\nhealthy or ready to accept Pods. Any DaemonSet Pods\\nrunning on such nodes will not be evicted.\\nnode.kubernetes.io/\\nunreachableNoExecuteDaemonSet Pods can be scheduled onto nodes that are\\nunreachable from the node controller. Any DaemonSet\\nPods running on such nodes will not be evicted.\\nnode.kubernetes.io/\\ndisk-pressureNoScheduleDaemonSet Pods can be scheduled onto nodes with disk\\npressure issues.\\nnode.kubernetes.io/\\nmemory-pressureNoScheduleDaemonSet Pods can be scheduled onto nodes with\\nmemory pressure issues.\\nnode.kubernetes.io/pid-\\npressureNoScheduleDaemonSet Pods can be scheduled onto nodes with\\nprocess pressure issues.', metadata={'source': './PDFS/Concepts.pdf', 'page': 166}),\n",
       " Document(page_content='Toleration key Effect Details\\nnode.kubernetes.io/\\nunschedulableNoScheduleDaemonSet Pods can be scheduled onto nodes that are\\nunschedulable.\\nnode.kubernetes.io/\\nnetwork-unavailableNoScheduleOnly added for DaemonSet Pods that request host\\nnetworking , i.e., Pods having spec.hostNetwork: true .\\nSuch DaemonSet Pods can be scheduled onto nodes with\\nunavailable network.\\nYou can add your own tolerations to the Pods of a DaemonSet as well, by defining these in the\\nPod template of the DaemonSet.\\nBecause the DaemonSet controller sets the node.kubernetes.io/unschedulable:NoSchedule\\ntoleration automatically, Kubernetes can run DaemonSet Pods on nodes that are marked as \\nunschedulable .\\nIf you use a DaemonSet to provide an important node-level function, such as cluster\\nnetworking , it is helpful that Kubernetes places DaemonSet Pods on nodes before they are\\nready. For example, without that special toleration, you could end up in a deadlock situation', metadata={'source': './PDFS/Concepts.pdf', 'page': 167}),\n",
       " Document(page_content='ready. For example, without that special toleration, you could end up in a deadlock situation\\nwhere the node is not marked as ready because the network plugin is not running there, and at\\nthe same time the network plugin is not running on that node because the node is not yet\\nready.\\nCommunicating with Daemon Pods\\nSome possible patterns for communicating with Pods in a DaemonSet are:\\nPush : Pods in the DaemonSet are configured to send updates to another service, such as\\na stats database. They do not have clients.\\nNodeIP and Known Port : Pods in the DaemonSet can use a hostPort , so that the pods\\nare reachable via the node IPs. Clients know the list of node IPs somehow, and know the\\nport by convention.\\nDNS : Create a headless service  with the same pod selector, and then discover\\nDaemonSets using the endpoints  resource or retrieve multiple A records from DNS.\\nService : Create a service with the same Pod selector, and use the service to reach a', metadata={'source': './PDFS/Concepts.pdf', 'page': 167}),\n",
       " Document(page_content='Service : Create a service with the same Pod selector, and use the service to reach a\\ndaemon on a random node. (No way to reach specific node.)\\nUpdating a DaemonSet\\nIf node labels are changed, the DaemonSet will promptly add Pods to newly matching nodes\\nand delete Pods from newly not-matching nodes.\\nYou can modify the Pods that a DaemonSet creates. However, Pods do not allow all fields to be\\nupdated. Also, the DaemonSet controller will use the original template the next time a node\\n(even with the same name) is created.\\nYou can delete a DaemonSet. If you specify --cascade=orphan  with kubectl , then the Pods will\\nbe left on the nodes. If you subsequently create a new DaemonSet with the same selector, the\\nnew DaemonSet adopts the existing Pods. If any Pods need replacing the DaemonSet replaces\\nthem according to its updateStrategy .\\nYou can perform a rolling update  on a DaemonSet.• \\n• \\n• \\n•', metadata={'source': './PDFS/Concepts.pdf', 'page': 167}),\n",
       " Document(page_content='Alternatives to DaemonSet\\nInit scripts\\nIt is certainly possible to run daemon processes by directly starting them on a node (e.g. using \\ninit, upstartd , or systemd ). This is perfectly fine. However, there are several advantages to\\nrunning such processes via a DaemonSet:\\nAbility to monitor and manage logs for daemons in the same way as applications.\\nSame config language and tools (e.g. Pod templates, kubectl ) for daemons and\\napplications.\\nRunning daemons in containers with resource limits increases isolation between daemons\\nfrom app containers. However, this can also be accomplished by running the daemons in\\na container but not in a Pod.\\nBare Pods\\nIt is possible to create Pods directly which specify a particular node to run on. However, a\\nDaemonSet replaces Pods that are deleted or terminated for any reason, such as in the case of\\nnode failure or disruptive node maintenance, such as a kernel upgrade. For this reason, you\\nshould use a DaemonSet rather than creating individual Pods.', metadata={'source': './PDFS/Concepts.pdf', 'page': 168}),\n",
       " Document(page_content='should use a DaemonSet rather than creating individual Pods.\\nStatic Pods\\nIt is possible to create Pods by writing a file to a certain directory watched by Kubelet. These\\nare called static pods . Unlike DaemonSet, static Pods cannot be managed with kubectl or other\\nKubernetes API clients. Static Pods do not depend on the apiserver, making them useful in\\ncluster bootstrapping cases. Also, static Pods may be deprecated in the future.\\nDeployments\\nDaemonSets are similar to Deployments  in that they both create Pods, and those Pods have\\nprocesses which are not expected to terminate (e.g. web servers, storage servers).\\nUse a Deployment for stateless services, like frontends, where scaling up and down the number\\nof replicas and rolling out updates are more important than controlling exactly which host the\\nPod runs on. Use a DaemonSet when it is important that a copy of a Pod always run on all or\\ncertain hosts, if the DaemonSet provides node-level functionality that allows other Pods to run', metadata={'source': './PDFS/Concepts.pdf', 'page': 168}),\n",
       " Document(page_content=\"certain hosts, if the DaemonSet provides node-level functionality that allows other Pods to run\\ncorrectly on that particular node.\\nFor example, network plugins  often include a component that runs as a DaemonSet. The\\nDaemonSet component makes sure that the node where it's running has working cluster\\nnetworking.\\nWhat's next\\nLearn about Pods .\\nLearn about static Pods , which are useful for running Kubernetes control plane\\ncomponents.\\nFind out how to use DaemonSets\\nPerform a rolling update on a DaemonSet• \\n• \\n• \\n• \\n◦ \\n• \\n◦\", metadata={'source': './PDFS/Concepts.pdf', 'page': 168}),\n",
       " Document(page_content=\"Perform a rollback on a DaemonSet  (for example, if a roll out didn't work how you\\nexpected).\\nUnderstand how Kubernetes assigns Pods to Nodes .\\nLearn about device plugins  and add ons , which often run as DaemonSets.\\nDaemonSet  is a top-level resource in the Kubernetes REST API. Read the DaemonSet\\nobject definition to understand the API for daemon sets.\\nJobs\\nJobs represent one-off tasks that run to completion and then stop.\\nA Job creates one or more Pods and will continue to retry execution of the Pods until a\\nspecified number of them successfully terminate. As pods successfully complete, the Job tracks\\nthe successful completions. When a specified number of successful completions is reached, the\\ntask (ie, Job) is complete. Deleting a Job will clean up the Pods it created. Suspending a Job will\\ndelete its active Pods until the Job is resumed again.\\nA simple case is to create one Job object in order to reliably run one Pod to completion. The Job\", metadata={'source': './PDFS/Concepts.pdf', 'page': 169}),\n",
       " Document(page_content='A simple case is to create one Job object in order to reliably run one Pod to completion. The Job\\nobject will start a new Pod if the first Pod fails or is deleted (for example due to a node\\nhardware failure or a node reboot).\\nYou can also use a Job to run multiple Pods in parallel.\\nIf you want to run a Job (either a single task, or several in parallel) on a schedule, see CronJob .\\nRunning an example Job\\nHere is an example Job config. It computes π to 2000 places and prints it out. It takes around 10s\\nto complete.\\ncontrollers/job.yaml  \\napiVersion : batch/v1\\nkind: Job\\nmetadata :\\n  name : pi\\nspec:\\n  template :\\n    spec:\\n      containers :\\n      - name : pi\\n        image : perl:5.34.0\\n        command : [\"perl\" ,  \"-Mbignum=bpi\" , \"-wle\" , \"print bpi(2000)\" ]\\n      restartPolicy : Never\\n  backoffLimit : 4\\nYou can run the example with this command:\\nkubectl apply -f https://kubernetes.io/examples/controllers/job.yaml\\nThe output is similar to this:◦ \\n• \\n• \\n•', metadata={'source': './PDFS/Concepts.pdf', 'page': 169}),\n",
       " Document(page_content='job.batch/pi created\\nCheck on the status of the Job with kubectl :\\nkubectl describe job pi\\nkubectl get job pi -o yaml\\nName:           pi\\nNamespace:      default\\nSelector:       batch.kubernetes.io/controller-uid =c9948307-e56d-4b5d-8302-ae2d7b7da67c\\nLabels:         batch.kubernetes.io/controller-uid =c9948307-e56d-4b5d-8302-ae2d7b7da67c\\n                batch.kubernetes.io/job-name =pi\\n                ...\\nAnnotations:    batch.kubernetes.io/job-tracking: \"\"\\nParallelism:    1\\nCompletions:    1\\nStart Time:     Mon, 02 Dec 2019 15:20:11 +0200\\nCompleted At:   Mon, 02 Dec 2019 15:21:16 +0200\\nDuration:       65s\\nPods Statuses:  0 Running / 1 Succeeded / 0 Failed\\nPod Template:\\n  Labels:  batch.kubernetes.io/controller-uid =c9948307-e56d-4b5d-8302-ae2d7b7da67c\\n           batch.kubernetes.io/job-name =pi\\n  Containers:\\n   pi:\\n    Image:      perl:5.34.0\\n    Port:       <none>\\n    Host Port:  <none>\\n    Command:\\n      perl\\n      -Mbignum =bpi\\n      -wle\\n      print bpi (2000)', metadata={'source': './PDFS/Concepts.pdf', 'page': 170}),\n",
       " Document(page_content='Host Port:  <none>\\n    Command:\\n      perl\\n      -Mbignum =bpi\\n      -wle\\n      print bpi (2000)\\n    Environment:  <none>\\n    Mounts:       <none>\\n  Volumes:        <none>\\nEvents:\\n  Type    Reason            Age   From            Message\\n  ----    ------            ----  ----            -------\\n  Normal  SuccessfulCreate  21s   job-controller  Created pod: pi-xf9p4\\n  Normal  Completed         18s   job-controller  Job completed\\napiVersion: batch/v1\\nkind: Job\\nmetadata:\\n  annotations: batch.kubernetes.io/job-tracking: \"\"\\n             ...  \\n  creationTimestamp: \"2022-11-10T17:53:53Z\"\\n  generation: 1\\n  labels:\\n    batch.kubernetes.io/controller-uid: 863452e6-270d-420e-9b94-53a54146c223• \\n•', metadata={'source': './PDFS/Concepts.pdf', 'page': 170}),\n",
       " Document(page_content='batch.kubernetes.io/job-name: pi\\n  name: pi\\n  namespace: default\\n  resourceVersion: \"4751\"\\n  uid: 204fb678-040b-497f-9266-35ffa8716d14\\nspec:\\n  backoffLimit: 4\\n  completionMode: NonIndexed\\n  completions: 1\\n  parallelism: 1\\n  selector:\\n    matchLabels:\\n      batch.kubernetes.io/controller-uid: 863452e6-270d-420e-9b94-53a54146c223\\n  suspend: false\\n  template:\\n    metadata:\\n      creationTimestamp: null\\n      labels:\\n        batch.kubernetes.io/controller-uid: 863452e6-270d-420e-9b94-53a54146c223\\n        batch.kubernetes.io/job-name: pi\\n    spec:\\n      containers:\\n      - command:\\n        - perl\\n        - -Mbignum =bpi\\n        - -wle\\n        - print bpi (2000)\\n        image: perl:5.34.0\\n        imagePullPolicy: IfNotPresent\\n        name: pi\\n        resources: {}\\n        terminationMessagePath: /dev/termination-log\\n        terminationMessagePolicy: File\\n      dnsPolicy: ClusterFirst\\n      restartPolicy: Never\\n      schedulerName: default-scheduler\\n      securityContext: {}', metadata={'source': './PDFS/Concepts.pdf', 'page': 171}),\n",
       " Document(page_content='restartPolicy: Never\\n      schedulerName: default-scheduler\\n      securityContext: {}\\n      terminationGracePeriodSeconds: 30\\nstatus:\\n  active: 1\\n  ready: 0\\n  startTime: \"2022-11-10T17:53:57Z\"\\n  uncountedTerminatedPods: {}\\nTo view completed Pods of a Job, use kubectl get pods .\\nTo list all the Pods that belong to a Job in a machine readable form, you can use a command like\\nthis:\\npods =$(kubectl get pods --selector =batch.kubernetes.io/job-name =pi --output =jsonpath =\\'{.items\\n[*].metadata.name}\\' )\\necho  $pods', metadata={'source': './PDFS/Concepts.pdf', 'page': 171}),\n",
       " Document(page_content='The output is similar to this:\\npi-5rwd7\\nHere, the selector is the same as the selector for the Job. The --output=jsonpath  option specifies\\nan expression with the name from each Pod in the returned list.\\nView the standard output of one of the pods:\\nkubectl logs $pods\\nAnother way to view the logs of a Job:\\nkubectl logs jobs/pi\\nThe output is similar to this:\\n3.1415926535897932384626433832795028841971693993751058209749445923078164062862089986\\n280348253421170679821480865132823066470938446095505822317253594081284811174502841027\\n019385211055596446229489549303819644288109756659334461284756482337867831652712019091\\n456485669234603486104543266482133936072602491412737245870066063155881748815209209628\\n292540917153643678925903600113305305488204665213841469519415116094330572703657595919\\n530921861173819326117931051185480744623799627495673518857527248912279381830119491298\\n336733624406566430860213949463952247371907021798609437027705392171762931767523846748', metadata={'source': './PDFS/Concepts.pdf', 'page': 172}),\n",
       " Document(page_content='336733624406566430860213949463952247371907021798609437027705392171762931767523846748\\n184676694051320005681271452635608277857713427577896091736371787214684409012249534301\\n465495853710507922796892589235420199561121290219608640344181598136297747713099605187\\n072113499999983729780499510597317328160963185950244594553469083026425223082533446850\\n352619311881710100031378387528865875332083814206171776691473035982534904287554687311\\n595628638823537875937519577818577805321712268066130019278766111959092164201989380952\\n572010654858632788659361533818279682303019520353018529689957736225994138912497217752\\n834791315155748572424541506959508295331168617278558890750983817546374649393192550604\\n009277016711390098488240128583616035637076601047101819429555961989467678374494482553\\n797747268471040475346462080466842590694912933136770289891521047521620569660240580381\\n501935112533824300355876402474964732639141992726042699227967823547816360093417216412', metadata={'source': './PDFS/Concepts.pdf', 'page': 172}),\n",
       " Document(page_content='501935112533824300355876402474964732639141992726042699227967823547816360093417216412\\n199245863150302861829745557067498385054945885869269956909272107975093029553211653449\\n872027559602364806654991198818347977535663698074265425278625518184175746728909777727\\n938000816470600161452491921732172147723501414419735685481613611573525521334757418494\\n684385233239073941433345477624168625189835694855620992192221842725502542568876717904\\n946016534668049886272327917860857843838279679766814541009538837863609506800642251252\\n051173929848960841284886269456042419652850222106611863067442786220391949450471237137\\n869609563643719172874677646575739624138908658326459958133904780275901\\nWriting a Job spec\\nAs with all other Kubernetes config, a Job needs apiVersion , kind, and metadata  fields.\\nWhen the control plane creates new Pods for a Job, the .metadata.name  of the Job is part of the\\nbasis for naming those Pods. The name of a Job must be a valid DNS subdomain  value, but this', metadata={'source': './PDFS/Concepts.pdf', 'page': 172}),\n",
       " Document(page_content='basis for naming those Pods. The name of a Job must be a valid DNS subdomain  value, but this\\ncan produce unexpected results for the Pod hostnames. For best compatibility, the name should\\nfollow the more restrictive rules for a DNS label . Even when the name is a DNS subdomain, the\\nname must be no longer than 63 characters.\\nA Job also needs a .spec  section .', metadata={'source': './PDFS/Concepts.pdf', 'page': 172}),\n",
       " Document(page_content='Job Labels\\nJob labels will have batch.kubernetes.io/  prefix for job-name  and controller-uid .\\nPod Template\\nThe .spec.template  is the only required field of the .spec .\\nThe .spec.template  is a pod template . It has exactly the same schema as a Pod, except it is nested\\nand does not have an apiVersion  or kind.\\nIn addition to required fields for a Pod, a pod template in a Job must specify appropriate labels\\n(see pod selector ) and an appropriate restart policy.\\nOnly a RestartPolicy  equal to Never  or OnFailure  is allowed.\\nPod selector\\nThe .spec.selector  field is optional. In almost all cases you should not specify it. See section \\nspecifying your own pod selector .\\nParallel execution for Jobs\\nThere are three main types of task suitable to run as a Job:\\nNon-parallel Jobs\\nnormally, only one Pod is started, unless the Pod fails.\\nthe Job is complete as soon as its Pod terminates successfully.\\nParallel Jobs with a fixed completion count :', metadata={'source': './PDFS/Concepts.pdf', 'page': 173}),\n",
       " Document(page_content='Parallel Jobs with a fixed completion count :\\nspecify a non-zero positive value for .spec.completions .\\nthe Job represents the overall task, and is complete when there are\\n.spec.completions  successful Pods.\\nwhen using .spec.completionMode=\"Indexed\" , each Pod gets a different index in the\\nrange 0 to .spec.completions-1 .\\nParallel Jobs with a work queue :\\ndo not specify .spec.completions , default to .spec.parallelism .\\nthe Pods must coordinate amongst themselves or an external service to determine\\nwhat each should work on. For example, a Pod might fetch a batch of up to N items\\nfrom the work queue.\\neach Pod is independently capable of determining whether or not all its peers are\\ndone, and thus that the entire Job is done.\\nwhen any Pod from the Job terminates with success, no new Pods are created.\\nonce at least one Pod has terminated with success and all Pods are terminated, then\\nthe Job is completed with success.', metadata={'source': './PDFS/Concepts.pdf', 'page': 173}),\n",
       " Document(page_content='the Job is completed with success.\\nonce any Pod has exited with success, no other Pod should still be doing any work\\nfor this task or writing any output. They should all be in the process of exiting.\\nFor a non-parallel  Job, you can leave both .spec.completions  and .spec.parallelism  unset. When\\nboth are unset, both are defaulted to 1.\\nFor a fixed completion count  Job, you should set .spec.completions  to the number of completions\\nneeded. You can set .spec.parallelism , or leave it unset and it will default to 1.1. \\n◦ \\n◦ \\n2. \\n◦ \\n◦ \\n◦ \\n3. \\n◦ \\n◦ \\n◦ \\n◦ \\n◦ \\n◦', metadata={'source': './PDFS/Concepts.pdf', 'page': 173}),\n",
       " Document(page_content='For a work queue  Job, you must leave .spec.completions  unset, and set .spec.parallelism  to a\\nnon-negative integer.\\nFor more information about how to make use of the different types of job, see the job patterns\\nsection.\\nControlling parallelism\\nThe requested parallelism ( .spec.parallelism ) can be set to any non-negative value. If it is\\nunspecified, it defaults to 1. If it is specified as 0, then the Job is effectively paused until it is\\nincreased.\\nActual parallelism (number of pods running at any instant) may be more or less than requested\\nparallelism, for a variety of reasons:\\nFor fixed completion count  Jobs, the actual number of pods running in parallel will not\\nexceed the number of remaining completions. Higher values of .spec.parallelism  are\\neffectively ignored.\\nFor work queue  Jobs, no new Pods are started after any Pod has succeeded -- remaining\\nPods are allowed to complete, however.\\nIf the Job Controller  has not had time to react.', metadata={'source': './PDFS/Concepts.pdf', 'page': 174}),\n",
       " Document(page_content='Pods are allowed to complete, however.\\nIf the Job Controller  has not had time to react.\\nIf the Job controller failed to create Pods for any reason (lack of ResourceQuota , lack of\\npermission, etc.), then there may be fewer pods than requested.\\nThe Job controller may throttle new Pod creation due to excessive previous pod failures in\\nthe same Job.\\nWhen a Pod is gracefully shut down, it takes time to stop.\\nCompletion mode\\nFEATURE STATE:  Kubernetes v1.24 [stable]\\nJobs with fixed completion count  - that is, jobs that have non null .spec.completions  - can have a\\ncompletion mode that is specified in .spec.completionMode :\\nNonIndexed  (default): the Job is considered complete when there have\\nbeen .spec.completions  successfully completed Pods. In other words, each Pod completion\\nis homologous to each other. Note that Jobs that have null .spec.completions  are\\nimplicitly NonIndexed .\\nIndexed : the Pods of a Job get an associated completion index from 0', metadata={'source': './PDFS/Concepts.pdf', 'page': 174}),\n",
       " Document(page_content='implicitly NonIndexed .\\nIndexed : the Pods of a Job get an associated completion index from 0\\nto .spec.completions-1 . The index is available through four mechanisms:\\nThe Pod annotation batch.kubernetes.io/job-completion-index .\\nThe Pod label batch.kubernetes.io/job-completion-index  (for v1.28 and later). Note\\nthe feature gate PodIndexLabel  must be enabled to use this label, and it is enabled\\nby default.\\nAs part of the Pod hostname, following the pattern $(job-name)-$(index) . When\\nyou use an Indexed Job in combination with a Service , Pods within the Job can use\\nthe deterministic hostnames to address each other via DNS. For more information\\nabout how to configure this, see Job with Pod-to-Pod Communication .\\nFrom the containerized task, in the environment variable \\nJOB_COMPLETION_INDEX .• \\n• \\n• \\n• \\n• \\n• \\n• \\n• \\n◦ \\n◦ \\n◦ \\n◦', metadata={'source': './PDFS/Concepts.pdf', 'page': 174}),\n",
       " Document(page_content='The Job is considered complete when there is one successfully completed Pod for each\\nindex. For more information about how to use this mode, see Indexed Job for Parallel\\nProcessing with Static Work Assignment .\\nNote:  Although rare, more than one Pod could be started for the same index (due to various\\nreasons such as node failures, kubelet restarts, or Pod evictions). In this case, only the first Pod\\nthat completes successfully will count towards the completion count and update the status of\\nthe Job. The other Pods that are running or completed for the same index will be deleted by the\\nJob controller once they are detected.\\nHandling Pod and container failures\\nA container in a Pod may fail for a number of reasons, such as because the process in it exited\\nwith a non-zero exit code, or the container was killed for exceeding a memory limit, etc. If this\\nhappens, and the .spec.template.spec.restartPolicy = \"OnFailure\" , then the Pod stays on the', metadata={'source': './PDFS/Concepts.pdf', 'page': 175}),\n",
       " Document(page_content='happens, and the .spec.template.spec.restartPolicy = \"OnFailure\" , then the Pod stays on the\\nnode, but the container is re-run. Therefore, your program needs to handle the case when it is\\nrestarted locally, or else specify .spec.template.spec.restartPolicy = \"Never\" . See pod lifecycle  for\\nmore information on restartPolicy .\\nAn entire Pod can also fail, for a number of reasons, such as when the pod is kicked off the\\nnode (node is upgraded, rebooted, deleted, etc.), or if a container of the Pod fails and the\\n.spec.template.spec.restartPolicy = \"Never\" . When a Pod fails, then the Job controller starts a\\nnew Pod. This means that your application needs to handle the case when it is restarted in a\\nnew pod. In particular, it needs to handle temporary files, locks, incomplete output and the like\\ncaused by previous runs.\\nBy default, each pod failure is counted towards the .spec.backoffLimit  limit, see pod backoff', metadata={'source': './PDFS/Concepts.pdf', 'page': 175}),\n",
       " Document(page_content='By default, each pod failure is counted towards the .spec.backoffLimit  limit, see pod backoff\\nfailure policy . However, you can customize handling of pod failures by setting the Job\\'s pod\\nfailure policy .\\nAdditionally, you can choose to count the pod failures independently for each index of an \\nIndexed  Job by setting the .spec.backoffLimitPerIndex  field (for more information, see backoff\\nlimit per index ).\\nNote that even if you specify .spec.parallelism = 1  and .spec.completions = 1\\nand .spec.template.spec.restartPolicy = \"Never\" , the same program may sometimes be started\\ntwice.\\nIf you do specify .spec.parallelism  and .spec.completions  both greater than 1, then there may be\\nmultiple pods running at once. Therefore, your pods must also be tolerant of concurrency.\\nWhen the feature gates  PodDisruptionConditions  and JobPodFailurePolicy  are both enabled,\\nand the .spec.podFailurePolicy  field is set, the Job controller does not consider a terminating', metadata={'source': './PDFS/Concepts.pdf', 'page': 175}),\n",
       " Document(page_content='and the .spec.podFailurePolicy  field is set, the Job controller does not consider a terminating\\nPod (a pod that has a .metadata.deletionTimestamp  field set) as a failure until that Pod is\\nterminal (its .status.phase  is Failed  or Succeeded ). However, the Job controller creates a\\nreplacement Pod as soon as the termination becomes apparent. Once the pod terminates, the\\nJob controller evaluates .backoffLimit  and .podFailurePolicy  for the relevant Job, taking this\\nnow-terminated Pod into consideration.\\nIf either of these requirements is not satisfied, the Job controller counts a terminating Pod as an\\nimmediate failure, even if that Pod later terminates with phase: \"Succeeded\" .', metadata={'source': './PDFS/Concepts.pdf', 'page': 175}),\n",
       " Document(page_content='Pod backoff failure policy\\nThere are situations where you want to fail a Job after some amount of retries due to a logical\\nerror in configuration etc. To do so, set .spec.backoffLimit  to specify the number of retries\\nbefore considering a Job as failed. The back-off limit is set by default to 6. Failed Pods associated\\nwith the Job are recreated by the Job controller with an exponential back-off delay (10s, 20s, 40s\\n...) capped at six minutes.\\nThe number of retries is calculated in two ways:\\nThe number of Pods with .status.phase = \"Failed\" .\\nWhen using restartPolicy = \"OnFailure\" , the number of retries in all the containers of\\nPods with .status.phase  equal to Pending  or Running .\\nIf either of the calculations reaches the .spec.backoffLimit , the Job is considered failed.\\nNote:  If your job has restartPolicy = \"OnFailure\" , keep in mind that your Pod running the Job\\nwill be terminated once the job backoff limit has been reached. This can make debugging the', metadata={'source': './PDFS/Concepts.pdf', 'page': 176}),\n",
       " Document(page_content='will be terminated once the job backoff limit has been reached. This can make debugging the\\nJob\\'s executable more difficult. We suggest setting restartPolicy = \"Never\"  when debugging the\\nJob or using a logging system to ensure output from failed Jobs is not lost inadvertently.\\nBackoff limit per index\\nFEATURE STATE:  Kubernetes v1.28 [alpha]\\nNote:  You can only configure the backoff limit per index for an Indexed  Job, if you have the \\nJobBackoffLimitPerIndex  feature gate  enabled in your cluster.\\nWhen you run an indexed  Job, you can choose to handle retries for pod failures independently\\nfor each index. To do so, set the .spec.backoffLimitPerIndex  to specify the maximal number of\\npod failures per index.\\nWhen the per-index backoff limit is exceeded for an index, Kuberentes considers the index as\\nfailed and adds it to the .status.failedIndexes  field. The succeeded indexes, those with a\\nsuccessfully executed pods, are recorded in the .status.completedIndexes  field, regardless of', metadata={'source': './PDFS/Concepts.pdf', 'page': 176}),\n",
       " Document(page_content='successfully executed pods, are recorded in the .status.completedIndexes  field, regardless of\\nwhether you set the backoffLimitPerIndex  field.\\nNote that a failing index does not interrupt execution of other indexes. Once all indexes finish\\nfor a Job where you specified a backoff limit per index, if at least one of those indexes did fail,\\nthe Job controller marks the overall Job as failed, by setting the Failed condition in the status.\\nThe Job gets marked as failed even if some, potentially nearly all, of the indexes were processed\\nsuccessfully.\\nYou can additionally limit the maximal number of indexes marked failed by setting the\\n.spec.maxFailedIndexes  field. When the number of failed indexes exceeds the maxFailedIndexes\\nfield, the Job controller triggers termination of all remaining running Pods for that Job. Once all\\npods are terminated, the entire Job is marked failed by the Job controller, by setting the Failed\\ncondition in the Job status.', metadata={'source': './PDFS/Concepts.pdf', 'page': 176}),\n",
       " Document(page_content='condition in the Job status.\\nHere is an example manifest for a Job that defines a backoffLimitPerIndex :\\n/controllers/job-backoff-limit-per-index-example.yaml  \\napiVersion : batch/v1\\nkind: Job• \\n•', metadata={'source': './PDFS/Concepts.pdf', 'page': 176}),\n",
       " Document(page_content='metadata :\\n  name : job-backoff-limit-per-index-example\\nspec:\\n  completions : 10\\n  parallelism : 3\\n  completionMode : Indexed  # required for the feature\\n  backoffLimitPerIndex : 1  # maximal number of failures per index\\n  maxFailedIndexes : 5      # maximal number of failed indexes before terminating the Job \\nexecution\\n  template :\\n    spec:\\n      restartPolicy : Never  # required for the feature\\n      containers :\\n      - name : example\\n        image : python\\n        command :           # The jobs fails as there is at least one failed index\\n                           # (all even indexes fail in here), yet all indexes\\n                           # are executed as maxFailedIndexes is not exceeded.\\n        - python3\\n        - -c\\n        - |\\n          import os, sys\\n          print(\"Hello world\")\\n          if int(os.environ.get(\"JOB_COMPLETION_INDEX\")) % 2 == 0:\\n            sys.exit(1)', metadata={'source': './PDFS/Concepts.pdf', 'page': 177}),\n",
       " Document(page_content='sys.exit(1)           \\nIn the example above, the Job controller allows for one restart for each of the indexes. When the\\ntotal number of failed indexes exceeds 5, then the entire Job is terminated.\\nOnce the job is finished, the Job status looks as follows:\\nkubectl get -o yaml job job-backoff-limit-per-index-example\\n  status :\\n    completedIndexes : 1,3,5,7,9\\n    failedIndexes : 0,2,4,6,8\\n    succeeded : 5          # 1 succeeded pod for each of 5 succeeded indexes\\n    failed : 10            # 2 failed pods (1 retry) for each of 5 failed indexes\\n    conditions :\\n    - message : Job has failed indexes\\n      reason : FailedIndexes\\n      status : \"True\"\\n      type: Failed\\nAdditionally, you may want to use the per-index backoff along with a pod failure policy . When\\nusing per-index backoff, there is a new FailIndex  action available which allows you to avoid\\nunnecessary retries within an index.\\nPod failure policy\\nFEATURE STATE:  Kubernetes v1.26 [beta]', metadata={'source': './PDFS/Concepts.pdf', 'page': 177}),\n",
       " Document(page_content='unnecessary retries within an index.\\nPod failure policy\\nFEATURE STATE:  Kubernetes v1.26 [beta]\\nNote:  You can only configure a Pod failure policy for a Job if you have the JobPodFailurePolicy  \\nfeature gate  enabled in your cluster. Additionally, it is recommended to enable the', metadata={'source': './PDFS/Concepts.pdf', 'page': 177}),\n",
       " Document(page_content=\"PodDisruptionConditions  feature gate in order to be able to detect and handle Pod disruption\\nconditions in the Pod failure policy (see also: Pod disruption conditions ). Both feature gates are\\navailable in Kubernetes 1.28.\\nA Pod failure policy, defined with the .spec.podFailurePolicy  field, enables your cluster to\\nhandle Pod failures based on the container exit codes and the Pod conditions.\\nIn some situations, you may want to have a better control when handling Pod failures than the\\ncontrol provided by the Pod backoff failure policy , which is based on the\\nJob's .spec.backoffLimit . These are some examples of use cases:\\nTo optimize costs of running workloads by avoiding unnecessary Pod restarts, you can\\nterminate a Job as soon as one of its Pods fails with an exit code indicating a software\\nbug.\\nTo guarantee that your Job finishes even if there are disruptions, you can ignore Pod\\nfailures caused by disruptions (such as preemption , API-initiated eviction  or taint -based\", metadata={'source': './PDFS/Concepts.pdf', 'page': 178}),\n",
       " Document(page_content='failures caused by disruptions (such as preemption , API-initiated eviction  or taint -based\\neviction) so that they don\\'t count towards the .spec.backoffLimit  limit of retries.\\nYou can configure a Pod failure policy, in the .spec.podFailurePolicy  field, to meet the above use\\ncases. This policy can handle Pod failures based on the container exit codes and the Pod\\nconditions.\\nHere is a manifest for a Job that defines a podFailurePolicy :\\n/controllers/job-pod-failure-policy-example.yaml  \\napiVersion : batch/v1\\nkind: Job\\nmetadata :\\n  name : job-pod-failure-policy-example\\nspec:\\n  completions : 12\\n  parallelism : 3\\n  template :\\n    spec:\\n      restartPolicy : Never\\n      containers :\\n      - name : main\\n        image : docker.io/library/bash:5\\n        command : [\"bash\" ]        # example command simulating a bug which triggers the FailJob \\naction\\n        args:\\n        - -c\\n        - echo \"Hello world!\" && sleep 5 && exit 42\\n  backoffLimit : 6\\n  podFailurePolicy :\\n    rules :', metadata={'source': './PDFS/Concepts.pdf', 'page': 178}),\n",
       " Document(page_content='backoffLimit : 6\\n  podFailurePolicy :\\n    rules :\\n    - action : FailJob\\n      onExitCodes :\\n        containerName : main      # optional\\n        operator: In             # one of : In, NotIn\\n        values : [42]\\n    - action: Ignore             # one of : Ignore, FailJob, Count• \\n•', metadata={'source': './PDFS/Concepts.pdf', 'page': 178}),\n",
       " Document(page_content='onPodConditions :\\n      - type: DisruptionTarget   # indicates Pod disruption\\nIn the example above, the first rule of the Pod failure policy specifies that the Job should be\\nmarked failed if the main  container fails with the 42 exit code. The following are the rules for\\nthe main  container specifically:\\nan exit code of 0 means that the container succeeded\\nan exit code of 42 means that the entire Job  failed\\nany other exit code represents that the container failed, and hence the entire Pod. The\\nPod will be re-created if the total number of restarts is below backoffLimit . If the \\nbackoffLimit  is reached the entire Job  failed.\\nNote:  Because the Pod template specifies a restartPolicy: Never , the kubelet does not restart\\nthe main  container in that particular Pod.\\nThe second rule of the Pod failure policy, specifying the Ignore  action for failed Pods with\\ncondition DisruptionTarget  excludes Pod disruptions from being counted towards\\nthe .spec.backoffLimit  limit of retries.', metadata={'source': './PDFS/Concepts.pdf', 'page': 179}),\n",
       " Document(page_content=\"the .spec.backoffLimit  limit of retries.\\nNote:  If the Job failed, either by the Pod failure policy or Pod backoff failure policy, and the Job\\nis running multiple Pods, Kubernetes terminates all the Pods in that Job that are still Pending or\\nRunning.\\nThese are some requirements and semantics of the API:\\nif you want to use a .spec.podFailurePolicy  field for a Job, you must also define that Job's\\npod template with .spec.restartPolicy  set to Never .\\nthe Pod failure policy rules you specify under spec.podFailurePolicy.rules  are evaluated in\\norder. Once a rule matches a Pod failure, the remaining rules are ignored. When no rule\\nmatches the Pod failure, the default handling applies.\\nyou may want to restrict a rule to a specific container by specifying its name\\ninspec.podFailurePolicy.rules[*].onExitCodes.containerName . When not specified the rule\\napplies to all containers. When specified, it should match one the container or \\ninitContainer  names in the Pod template.\", metadata={'source': './PDFS/Concepts.pdf', 'page': 179}),\n",
       " Document(page_content=\"initContainer  names in the Pod template.\\nyou may specify the action taken when a Pod failure policy is matched by \\nspec.podFailurePolicy.rules[*].action . Possible values are:\\nFailJob : use to indicate that the Pod's job should be marked as Failed and all\\nrunning Pods should be terminated.\\nIgnore : use to indicate that the counter towards the .spec.backoffLimit  should not\\nbe incremented and a replacement Pod should be created.\\nCount : use to indicate that the Pod should be handled in the default way. The\\ncounter towards the .spec.backoffLimit  should be incremented.\\nFailIndex : use this action along with backoff limit per index  to avoid unnecessary\\nretries within the index of a failed pod.\\nNote:  When you use a podFailurePolicy , the job controller only matches Pods in the Failed\\nphase. Pods with a deletion timestamp that are not in a terminal phase ( Failed  or Succeeded ) are\\nconsidered still terminating. This implies that terminating pods retain a tracking finalizer  until\", metadata={'source': './PDFS/Concepts.pdf', 'page': 179}),\n",
       " Document(page_content='considered still terminating. This implies that terminating pods retain a tracking finalizer  until\\nthey reach a terminal phase. Since Kubernetes 1.27, Kubelet transitions deleted pods to a\\nterminal phase (see: Pod Phase ). This ensures that deleted pods have their finalizers removed by\\nthe Job controller.• \\n• \\n• \\n• \\n• \\n• \\n• \\n◦ \\n◦ \\n◦ \\n◦', metadata={'source': './PDFS/Concepts.pdf', 'page': 179}),\n",
       " Document(page_content='Note:  Starting with Kubernetes v1.28, when Pod failure policy is used, the Job controller\\nrecreates terminating Pods only once these Pods reach the terminal Failed  phase. This behavior\\nis similar to podReplacementPolicy: Failed . For more information, see Pod replacement policy .\\nJob termination and cleanup\\nWhen a Job completes, no more Pods are created, but the Pods are usually  not deleted either.\\nKeeping them around allows you to still view the logs of completed pods to check for errors,\\nwarnings, or other diagnostic output. The job object also remains after it is completed so that\\nyou can view its status. It is up to the user to delete old jobs after noting their status. Delete the\\njob with kubectl  (e.g. kubectl delete jobs/pi  or kubectl delete -f ./job.yaml ). When you delete the\\njob using kubectl , all the pods it created are deleted too.\\nBy default, a Job will run uninterrupted unless a Pod fails ( restartPolicy=Never ) or a Container', metadata={'source': './PDFS/Concepts.pdf', 'page': 180}),\n",
       " Document(page_content=\"By default, a Job will run uninterrupted unless a Pod fails ( restartPolicy=Never ) or a Container\\nexits in error ( restartPolicy=OnFailure ), at which point the Job defers to the .spec.backoffLimit\\ndescribed above. Once .spec.backoffLimit  has been reached the Job will be marked as failed and\\nany running Pods will be terminated.\\nAnother way to terminate a Job is by setting an active deadline. Do this by setting the\\n.spec.activeDeadlineSeconds  field of the Job to a number of seconds. The activeDeadlineSeconds\\napplies to the duration of the job, no matter how many Pods are created. Once a Job reaches \\nactiveDeadlineSeconds , all of its running Pods are terminated and the Job status will become \\ntype: Failed  with reason: DeadlineExceeded .\\nNote that a Job's .spec.activeDeadlineSeconds  takes precedence over its .spec.backoffLimit .\\nTherefore, a Job that is retrying one or more failed Pods will not deploy additional Pods once it\", metadata={'source': './PDFS/Concepts.pdf', 'page': 180}),\n",
       " Document(page_content='Therefore, a Job that is retrying one or more failed Pods will not deploy additional Pods once it\\nreaches the time limit specified by activeDeadlineSeconds , even if the backoffLimit  is not yet\\nreached.\\nExample:\\napiVersion : batch/v1\\nkind: Job\\nmetadata :\\n  name : pi-with-timeout\\nspec:\\n  backoffLimit : 5\\n  activeDeadlineSeconds : 100\\n  template :\\n    spec:\\n      containers :\\n      - name : pi\\n        image : perl:5.34.0\\n        command : [\"perl\" , \"-Mbignum=bpi\" , \"-wle\" , \"print bpi(2000)\" ]\\n      restartPolicy : Never\\nNote that both the Job spec and the Pod template spec  within the Job have an \\nactiveDeadlineSeconds  field. Ensure that you set this field at the proper level.\\nKeep in mind that the restartPolicy  applies to the Pod, and not to the Job itself: there is no\\nautomatic Job restart once the Job status is type: Failed . That is, the Job termination\\nmechanisms activated with .spec.activeDeadlineSeconds  and .spec.backoffLimit  result in a', metadata={'source': './PDFS/Concepts.pdf', 'page': 180}),\n",
       " Document(page_content='mechanisms activated with .spec.activeDeadlineSeconds  and .spec.backoffLimit  result in a\\npermanent Job failure that requires manual intervention to resolve.', metadata={'source': './PDFS/Concepts.pdf', 'page': 180}),\n",
       " Document(page_content='Clean up finished jobs automatically\\nFinished Jobs are usually no longer needed in the system. Keeping them around in the system\\nwill put pressure on the API server. If the Jobs are managed directly by a higher level controller,\\nsuch as CronJobs , the Jobs can be cleaned up by CronJobs based on the specified capacity-based\\ncleanup policy.\\nTTL mechanism for finished Jobs\\nFEATURE STATE:  Kubernetes v1.23 [stable]\\nAnother way to clean up finished Jobs (either Complete  or Failed ) automatically is to use a TTL\\nmechanism provided by a TTL controller  for finished resources, by specifying\\nthe .spec.ttlSecondsAfterFinished  field of the Job.\\nWhen the TTL controller cleans up the Job, it will delete the Job cascadingly, i.e. delete its\\ndependent objects, such as Pods, together with the Job. Note that when the Job is deleted, its\\nlifecycle guarantees, such as finalizers, will be honored.\\nFor example:\\napiVersion : batch/v1\\nkind: Job\\nmetadata :\\n  name : pi-with-ttl\\nspec:', metadata={'source': './PDFS/Concepts.pdf', 'page': 181}),\n",
       " Document(page_content='For example:\\napiVersion : batch/v1\\nkind: Job\\nmetadata :\\n  name : pi-with-ttl\\nspec:\\n  ttlSecondsAfterFinished : 100\\n  template :\\n    spec:\\n      containers :\\n      - name : pi\\n        image : perl:5.34.0\\n        command : [\"perl\" , \"-Mbignum=bpi\" , \"-wle\" , \"print bpi(2000)\" ]\\n      restartPolicy : Never\\nThe Job pi-with-ttl  will be eligible to be automatically deleted, 100 seconds after it finishes.\\nIf the field is set to 0, the Job will be eligible to be automatically deleted immediately after it\\nfinishes. If the field is unset, this Job won\\'t be cleaned up by the TTL controller after it finishes.\\nNote:\\nIt is recommended to set ttlSecondsAfterFinished  field because unmanaged jobs (Jobs that you\\ncreated directly, and not indirectly through other workload APIs such as CronJob) have a\\ndefault deletion policy of orphanDependents  causing Pods created by an unmanaged Job to be\\nleft around after that Job is fully deleted. Even though the control plane  eventually garbage', metadata={'source': './PDFS/Concepts.pdf', 'page': 181}),\n",
       " Document(page_content='left around after that Job is fully deleted. Even though the control plane  eventually garbage\\ncollects  the Pods from a deleted Job after they either fail or complete, sometimes those lingering\\npods may cause cluster performance degradation or in worst case cause the cluster to go offline\\ndue to this degradation.\\nYou can use LimitRanges  and ResourceQuotas  to place a cap on the amount of resources that a\\nparticular namespace can consume.', metadata={'source': './PDFS/Concepts.pdf', 'page': 181}),\n",
       " Document(page_content='Job patterns\\nThe Job object can be used to process a set of independent but related work items . These might\\nbe emails to be sent, frames to be rendered, files to be transcoded, ranges of keys in a NoSQL\\ndatabase to scan, and so on.\\nIn a complex system, there may be multiple different sets of work items. Here we are just\\nconsidering one set of work items that the user wants to manage together — a batch job .\\nThere are several different patterns for parallel computation, each with strengths and\\nweaknesses. The tradeoffs are:\\nOne Job object for each work item, versus a single Job object for all work items. One Job\\nper work item creates some overhead for the user and for the system to manage large\\nnumbers of Job objects. A single Job for all work items is better for large numbers of\\nitems.\\nNumber of Pods created equals number of work items, versus each Pod can process\\nmultiple work items. When the number of Pods equals the number of work items, the', metadata={'source': './PDFS/Concepts.pdf', 'page': 182}),\n",
       " Document(page_content='multiple work items. When the number of Pods equals the number of work items, the\\nPods typically requires less modification to existing code and containers. Having each\\nPod process multiple work items is better for large numbers of items.\\nSeveral approaches use a work queue. This requires running a queue service, and\\nmodifications to the existing program or container to make it use the work queue. Other\\napproaches are easier to adapt to an existing containerised application.\\nWhen the Job is associated with a headless Service , you can enable the Pods within a Job\\nto communicate with each other to collaborate in a computation.\\nThe tradeoffs are summarized here, with columns 2 to 4 corresponding to the above tradeoffs.\\nThe pattern names are also links to examples and more detailed description.\\nPatternSingle Job\\nobjectFewer pods than work\\nitems?Use app\\nunmodified?\\nQueue with Pod Per Work Item sometimes\\nQueue with Variable Pod Count \\nIndexed Job with Static Work\\nAssignment', metadata={'source': './PDFS/Concepts.pdf', 'page': 182}),\n",
       " Document(page_content='Queue with Variable Pod Count \\nIndexed Job with Static Work\\nAssignment \\nJob with Pod-to-Pod\\nCommunication sometimes sometimes\\nJob Template Expansion \\nWhen you specify completions with .spec.completions , each Pod created by the Job controller\\nhas an identical spec. This means that all pods for a task will have the same command line and\\nthe same image, the same volumes, and (almost) the same environment variables. These\\npatterns are different ways to arrange for pods to work on different things.\\nThis table shows the required settings for .spec.parallelism  and .spec.completions  for each of the\\npatterns. Here, W is the number of work items.\\nPattern .spec.completions .spec.parallelism\\nQueue with Pod Per Work Item W any\\nQueue with Variable Pod Count null any\\nIndexed Job with Static Work Assignment W any• \\n• \\n• \\n•', metadata={'source': './PDFS/Concepts.pdf', 'page': 182}),\n",
       " Document(page_content=\"Pattern .spec.completions .spec.parallelism\\nJob with Pod-to-Pod Communication W W\\nJob Template Expansion 1 should be 1\\nAdvanced usage\\nSuspending a Job\\nFEATURE STATE:  Kubernetes v1.24 [stable]\\nWhen a Job is created, the Job controller will immediately begin creating Pods to satisfy the\\nJob's requirements and will continue to do so until the Job is complete. However, you may want\\nto temporarily suspend a Job's execution and resume it later, or start Jobs in suspended state\\nand have a custom controller decide later when to start them.\\nTo suspend a Job, you can update the .spec.suspend  field of the Job to true; later, when you\\nwant to resume it again, update it to false. Creating a Job with .spec.suspend  set to true will\\ncreate it in the suspended state.\\nWhen a Job is resumed from suspension, its .status.startTime  field will be reset to the current\\ntime. This means that the .spec.activeDeadlineSeconds  timer will be stopped and reset when a\\nJob is suspended and resumed.\", metadata={'source': './PDFS/Concepts.pdf', 'page': 183}),\n",
       " Document(page_content='Job is suspended and resumed.\\nWhen you suspend a Job, any running Pods that don\\'t have a status of Completed  will be \\nterminated . with a SIGTERM signal. The Pod\\'s graceful termination period will be honored and\\nyour Pod must handle this signal in this period. This may involve saving progress for later or\\nundoing changes. Pods terminated this way will not count towards the Job\\'s completions  count.\\nAn example Job definition in the suspended state can be like so:\\nkubectl get job myjob -o yaml\\napiVersion : batch/v1\\nkind: Job\\nmetadata :\\n  name : myjob\\nspec:\\n  suspend : true\\n  parallelism : 1\\n  completions : 5\\n  template :\\n    spec:\\n      ...\\nYou can also toggle Job suspension by patching the Job using the command line.\\nSuspend an active Job:\\nkubectl patch job/myjob --type =strategic --patch \\'{\"spec\":{\"suspend\":true}}\\'\\nResume a suspended Job:\\nkubectl patch job/myjob --type =strategic --patch \\'{\"spec\":{\"suspend\":false}}\\'', metadata={'source': './PDFS/Concepts.pdf', 'page': 183}),\n",
       " Document(page_content='The Job\\'s status can be used to determine if a Job is suspended or has been suspended in the\\npast:\\nkubectl get jobs/myjob -o yaml\\napiVersion : batch/v1\\nkind: Job\\n# .metadata and .spec omitted\\nstatus :\\n  conditions :\\n  - lastProbeTime : \"2021-02-05T13:14:33Z\"\\n    lastTransitionTime : \"2021-02-05T13:14:33Z\"\\n    status : \"True\"\\n    type: Suspended\\n  startTime : \"2021-02-05T13:13:48Z\"\\nThe Job condition of type \"Suspended\" with status \"True\" means the Job is suspended; the \\nlastTransitionTime  field can be used to determine how long the Job has been suspended for. If\\nthe status of that condition is \"False\", then the Job was previously suspended and is now\\nrunning. If such a condition does not exist in the Job\\'s status, the Job has never been stopped.\\nEvents are also created when the Job is suspended and resumed:\\nkubectl describe jobs/myjob\\nName:           myjob\\n...\\nEvents:\\n  Type    Reason            Age   From            Message\\n  ----    ------            ----  ----            -------', metadata={'source': './PDFS/Concepts.pdf', 'page': 184}),\n",
       " Document(page_content='----    ------            ----  ----            -------\\n  Normal  SuccessfulCreate  12m   job-controller  Created pod: myjob-hlrpl\\n  Normal  SuccessfulDelete  11m   job-controller  Deleted pod: myjob-hlrpl\\n  Normal  Suspended         11m   job-controller  Job suspended\\n  Normal  SuccessfulCreate  3s    job-controller  Created pod: myjob-jvb44\\n  Normal  Resumed           3s    job-controller  Job resumed\\nThe last four events, particularly the \"Suspended\" and \"Resumed\" events, are directly a result of\\ntoggling the .spec.suspend  field. In the time between these two events, we see that no Pods\\nwere created, but Pod creation restarted as soon as the Job was resumed.\\nMutable Scheduling Directives\\nFEATURE STATE:  Kubernetes v1.27 [stable]\\nIn most cases, a parallel job will want the pods to run with constraints, like all in the same zone,\\nor all either on GPU model x or y but not a mix of both.\\nThe suspend  field is the first step towards achieving those semantics. Suspend allows a custom', metadata={'source': './PDFS/Concepts.pdf', 'page': 184}),\n",
       " Document(page_content=\"The suspend  field is the first step towards achieving those semantics. Suspend allows a custom\\nqueue controller to decide when a job should start; However, once a job is unsuspended, a\\ncustom queue controller has no influence on where the pods of a job will actually land.\\nThis feature allows updating a Job's scheduling directives before it starts, which gives custom\\nqueue controllers the ability to influence pod placement while at the same time offloading\", metadata={'source': './PDFS/Concepts.pdf', 'page': 184}),\n",
       " Document(page_content=\"actual pod-to-node assignment to kube-scheduler. This is allowed only for suspended Jobs that\\nhave never been unsuspended before.\\nThe fields in a Job's pod template that can be updated are node affinity, node selector,\\ntolerations, labels, annotations and scheduling gates .\\nSpecifying your own Pod selector\\nNormally, when you create a Job object, you do not specify .spec.selector . The system defaulting\\nlogic adds this field when the Job is created. It picks a selector value that will not overlap with\\nany other jobs.\\nHowever, in some cases, you might need to override this automatically set selector. To do this,\\nyou can specify the .spec.selector  of the Job.\\nBe very careful when doing this. If you specify a label selector which is not unique to the pods\\nof that Job, and which matches unrelated Pods, then pods of the unrelated job may be deleted,\\nor this Job may count other Pods as completing it, or one or both Jobs may refuse to create Pods\", metadata={'source': './PDFS/Concepts.pdf', 'page': 185}),\n",
       " Document(page_content='or this Job may count other Pods as completing it, or one or both Jobs may refuse to create Pods\\nor run to completion. If a non-unique selector is chosen, then other controllers (e.g.\\nReplicationController) and their Pods may behave in unpredictable ways too. Kubernetes will\\nnot stop you from making a mistake when specifying .spec.selector .\\nHere is an example of a case when you might want to use this feature.\\nSay Job old is already running. You want existing Pods to keep running, but you want the rest\\nof the Pods it creates to use a different pod template and for the Job to have a new name. You\\ncannot update the Job because these fields are not updatable. Therefore, you delete Job old but \\nleave its pods running , using kubectl delete jobs/old --cascade=orphan . Before deleting it, you\\nmake a note of what selector it uses:\\nkubectl get job old -o yaml\\nThe output is similar to this:\\nkind: Job\\nmetadata :\\n  name : old\\n  ...\\nspec:\\n  selector :\\n    matchLabels :', metadata={'source': './PDFS/Concepts.pdf', 'page': 185}),\n",
       " Document(page_content='kind: Job\\nmetadata :\\n  name : old\\n  ...\\nspec:\\n  selector :\\n    matchLabels :\\n      batch.kubernetes.io/controller-uid : a8f3d00d-c6d2-11e5-9f87-42010af00002\\n  ...\\nThen you create a new Job with name new and you explicitly specify the same selector. Since\\nthe existing Pods have label batch.kubernetes.io/controller-uid=a8f3d00d-\\nc6d2-11e5-9f87-42010af00002 , they are controlled by Job new as well.\\nYou need to specify manualSelector: true  in the new Job since you are not using the selector\\nthat the system normally generates for you automatically.\\nkind: Job\\nmetadata :\\n  name : new', metadata={'source': './PDFS/Concepts.pdf', 'page': 185}),\n",
       " Document(page_content='...\\nspec:\\n  manualSelector : true\\n  selector :\\n    matchLabels :\\n      batch.kubernetes.io/controller-uid : a8f3d00d-c6d2-11e5-9f87-42010af00002\\n  ...\\nThe new Job itself will have a different uid from a8f3d00d-c6d2-11e5-9f87-42010af00002 . Setting \\nmanualSelector: true  tells the system that you know what you are doing and to allow this\\nmismatch.\\nJob tracking with finalizers\\nFEATURE STATE:  Kubernetes v1.26 [stable]\\nThe control plane keeps track of the Pods that belong to any Job and notices if any such Pod is\\nremoved from the API server. To do that, the Job controller creates Pods with the finalizer \\nbatch.kubernetes.io/job-tracking . The controller removes the finalizer only after the Pod has\\nbeen accounted for in the Job status, allowing the Pod to be removed by other controllers or\\nusers.\\nNote:  See My pod stays terminating  if you observe that pods from a Job are stucked with the\\ntracking finalizer.\\nElastic Indexed Jobs\\nFEATURE STATE:  Kubernetes v1.27 [beta]', metadata={'source': './PDFS/Concepts.pdf', 'page': 186}),\n",
       " Document(page_content='tracking finalizer.\\nElastic Indexed Jobs\\nFEATURE STATE:  Kubernetes v1.27 [beta]\\nYou can scale Indexed Jobs up or down by mutating both .spec.parallelism\\nand .spec.completions  together such that .spec.parallelism == .spec.completions . When the \\nElasticIndexedJob feature gate  on the API server  is disabled, .spec.completions  is immutable.\\nUse cases for elastic Indexed Jobs include batch workloads which require scaling an indexed\\nJob, such as MPI, Horovord, Ray, and PyTorch training jobs.\\nDelayed creation of replacement pods\\nFEATURE STATE:  Kubernetes v1.28 [alpha]\\nNote:  You can only set podReplacementPolicy  on Jobs if you enable the \\nJobPodReplacementPolicy  feature gate .\\nBy default, the Job controller recreates Pods as soon they either fail or are terminating (have a\\ndeletion timestamp). This means that, at a given time, when some of the Pods are terminating,\\nthe number of running Pods for a Job can be greater than parallelism  or greater than one Pod', metadata={'source': './PDFS/Concepts.pdf', 'page': 186}),\n",
       " Document(page_content='the number of running Pods for a Job can be greater than parallelism  or greater than one Pod\\nper index (if you are using an Indexed Job).\\nYou may choose to create replacement Pods only when the terminating Pod is fully terminal\\n(has status.phase: Failed ). To do this, set the .spec.podReplacementPolicy: Failed . The default\\nreplacement policy depends on whether the Job has a podFailurePolicy  set. With no Pod failure\\npolicy defined for a Job, omitting the podReplacementPolicy  field selects the \\nTerminatingOrFailed  replacement policy: the control plane creates replacement Pods\\nimmediately upon Pod deletion (as soon as the control plane sees that a Pod for this Job has', metadata={'source': './PDFS/Concepts.pdf', 'page': 186}),\n",
       " Document(page_content='deletionTimestamp  set). For Jobs with a Pod failure policy set, the default \\npodReplacementPolicy  is Failed , and no other value is permitted. See Pod failure policy  to learn\\nmore about Pod failure policies for Jobs.\\nkind: Job\\nmetadata :\\n  name : new\\n  ...\\nspec:\\n  podReplacementPolicy : Failed\\n  ...\\nProvided your cluster has the feature gate enabled, you can inspect the .status.terminating  field\\nof a Job. The value of the field is the number of Pods owned by the Job that are currently\\nterminating.\\nkubectl get jobs/myjob -o yaml\\napiVersion : batch/v1\\nkind: Job\\n# .metadata and .spec omitted\\nstatus :\\n  terminating : 3 # three Pods are terminating and have not yet reached the Failed phase\\nAlternatives\\nBare Pods\\nWhen the node that a Pod is running on reboots or fails, the pod is terminated and will not be\\nrestarted. However, a Job will create new Pods to replace terminated ones. For this reason, we', metadata={'source': './PDFS/Concepts.pdf', 'page': 187}),\n",
       " Document(page_content='restarted. However, a Job will create new Pods to replace terminated ones. For this reason, we\\nrecommend that you use a Job rather than a bare Pod, even if your application requires only a\\nsingle Pod.\\nReplication Controller\\nJobs are complementary to Replication Controllers . A Replication Controller manages Pods\\nwhich are not expected to terminate (e.g. web servers), and a Job manages Pods that are\\nexpected to terminate (e.g. batch tasks).\\nAs discussed in Pod Lifecycle , Job is only appropriate for pods with RestartPolicy  equal to \\nOnFailure  or Never . (Note: If RestartPolicy  is not set, the default value is Always .)\\nSingle Job starts controller Pod\\nAnother pattern is for a single Job to create a Pod which then creates other Pods, acting as a\\nsort of custom controller for those Pods. This allows the most flexibility, but may be somewhat\\ncomplicated to get started with and offers less integration with Kubernetes.', metadata={'source': './PDFS/Concepts.pdf', 'page': 187}),\n",
       " Document(page_content='complicated to get started with and offers less integration with Kubernetes.\\nOne example of this pattern would be a Job which starts a Pod which runs a script that in turn\\nstarts a Spark master controller (see spark example ), runs a spark driver, and then cleans up.', metadata={'source': './PDFS/Concepts.pdf', 'page': 187}),\n",
       " Document(page_content=\"An advantage of this approach is that the overall process gets the completion guarantee of a Job\\nobject, but maintains complete control over what Pods are created and how work is assigned to\\nthem.\\nWhat's next\\nLearn about Pods .\\nRead about different ways of running Jobs:\\nCoarse Parallel Processing Using a Work Queue\\nFine Parallel Processing Using a Work Queue\\nUse an indexed Job for parallel processing with static work assignment\\nCreate multiple Jobs based on a template: Parallel Processing using Expansions\\nFollow the links within Clean up finished jobs automatically  to learn more about how\\nyour cluster can clean up completed and / or failed tasks.\\nJob is part of the Kubernetes REST API. Read the Job object definition to understand the\\nAPI for jobs.\\nRead about CronJob , which you can use to define a series of Jobs that will run based on a\\nschedule, similar to the UNIX tool cron.\\nPractice how to configure handling of retriable and non-retriable pod failures using\", metadata={'source': './PDFS/Concepts.pdf', 'page': 188}),\n",
       " Document(page_content=\"Practice how to configure handling of retriable and non-retriable pod failures using \\npodFailurePolicy , based on the step-by-step examples .\\nAutomatic Cleanup for Finished Jobs\\nA time-to-live mechanism to clean up old Jobs that have finished execution.\\nFEATURE STATE:  Kubernetes v1.23 [stable]\\nWhen your Job has finished, it's useful to keep that Job in the API (and not immediately delete\\nthe Job) so that you can tell whether the Job succeeded or failed.\\nKubernetes' TTL-after-finished controller  provides a TTL (time to live) mechanism to limit the\\nlifetime of Job objects that have finished execution.\\nCleanup for finished Jobs\\nThe TTL-after-finished controller is only supported for Jobs. You can use this mechanism to\\nclean up finished Jobs (either Complete  or Failed ) automatically by specifying\\nthe .spec.ttlSecondsAfterFinished  field of a Job, as in this example .\\nThe TTL-after-finished controller assumes that a Job is eligible to be cleaned up TTL seconds\", metadata={'source': './PDFS/Concepts.pdf', 'page': 188}),\n",
       " Document(page_content='The TTL-after-finished controller assumes that a Job is eligible to be cleaned up TTL seconds\\nafter the Job has finished. The timer starts once the status condition of the Job changes to show\\nthat the Job is either Complete  or Failed ; once the TTL has expired, that Job becomes eligible\\nfor cascading  removal. When the TTL-after-finished controller cleans up a job, it will delete it\\ncascadingly, that is to say it will delete its dependent objects together with it.\\nKubernetes honors object lifecycle guarantees on the Job, such as waiting for finalizers .\\nYou can set the TTL seconds at any time. Here are some examples for setting the\\n.spec.ttlSecondsAfterFinished  field of a Job:\\nSpecify this field in the Job manifest, so that a Job can be cleaned up automatically some\\ntime after it finishes.• \\n• \\n◦ \\n◦ \\n◦ \\n◦ \\n• \\n• \\n• \\n• \\n•', metadata={'source': './PDFS/Concepts.pdf', 'page': 188}),\n",
       " Document(page_content=\"Manually set this field of existing, already finished Jobs, so that they become eligible for\\ncleanup.\\nUse a mutating admission webhook  to set this field dynamically at Job creation time.\\nCluster administrators can use this to enforce a TTL policy for finished jobs.\\nUse a mutating admission webhook  to set this field dynamically after the Job has finished,\\nand choose different TTL values based on job status, labels. For this case, the webhook\\nneeds to detect changes to the .status  of the Job and only set a TTL when the Job is being\\nmarked as completed.\\nWrite your own controller to manage the cleanup TTL for Jobs that match a particular \\nselector-selector .\\nCaveats\\nUpdating TTL for finished Jobs\\nYou can modify the TTL period, e.g. .spec.ttlSecondsAfterFinished  field of Jobs, after the job is\\ncreated or has finished. If you extend the TTL period after the existing ttlSecondsAfterFinished\\nperiod has expired, Kubernetes doesn't guarantee to retain that Job, even if an update to extend\", metadata={'source': './PDFS/Concepts.pdf', 'page': 189}),\n",
       " Document(page_content=\"period has expired, Kubernetes doesn't guarantee to retain that Job, even if an update to extend\\nthe TTL returns a successful API response.\\nTime skew\\nBecause the TTL-after-finished controller uses timestamps stored in the Kubernetes jobs to\\ndetermine whether the TTL has expired or not, this feature is sensitive to time skew in your\\ncluster, which may cause the control plane to clean up Job objects at the wrong time.\\nClocks aren't always correct, but the difference should be very small. Please be aware of this\\nrisk when setting a non-zero TTL.\\nWhat's next\\nRead Clean up Jobs automatically\\nRefer to the Kubernetes Enhancement Proposal  (KEP) for adding this mechanism.\\nCronJob\\nA CronJob starts one-time Jobs on a repeating schedule.\\nFEATURE STATE:  Kubernetes v1.21 [stable]\\nA CronJob  creates Jobs on a repeating schedule.\\nCronJob is meant for performing regular scheduled actions such as backups, report generation,\", metadata={'source': './PDFS/Concepts.pdf', 'page': 189}),\n",
       " Document(page_content='CronJob is meant for performing regular scheduled actions such as backups, report generation,\\nand so on. One CronJob object is like one line of a crontab  (cron table) file on a Unix system. It\\nruns a job periodically on a given schedule, written in Cron  format.\\nCronJobs have limitations and idiosyncrasies. For example, in certain circumstances, a single\\nCronJob can create multiple concurrent Jobs. See the limitations  below.• \\n• \\n• \\n• \\n• \\n•', metadata={'source': './PDFS/Concepts.pdf', 'page': 189}),\n",
       " Document(page_content='When the control plane creates new Jobs and (indirectly) Pods for a CronJob, the\\n.metadata.name  of the CronJob is part of the basis for naming those Pods. The name of a\\nCronJob must be a valid DNS subdomain  value, but this can produce unexpected results for the\\nPod hostnames. For best compatibility, the name should follow the more restrictive rules for a \\nDNS label . Even when the name is a DNS subdomain, the name must be no longer than 52\\ncharacters. This is because the CronJob controller will automatically append 11 characters to\\nthe name you provide and there is a constraint that the length of a Job name is no more than 63\\ncharacters.\\nExample\\nThis example CronJob manifest prints the current time and a hello message every minute:\\napplication/job/cronjob.yaml  \\napiVersion : batch/v1\\nkind: CronJob\\nmetadata :\\n  name : hello\\nspec:\\n  schedule : \"* * * * *\"\\n  jobTemplate :\\n    spec:\\n      template :\\n        spec:\\n          containers :\\n          - name : hello', metadata={'source': './PDFS/Concepts.pdf', 'page': 190}),\n",
       " Document(page_content='spec:\\n      template :\\n        spec:\\n          containers :\\n          - name : hello\\n            image : busybox:1.28\\n            imagePullPolicy : IfNotPresent\\n            command :\\n            - /bin/sh\\n            - -c\\n            - date; echo Hello from the Kubernetes cluster\\n          restartPolicy : OnFailure\\n(Running Automated Tasks with a CronJob  takes you through this example in more detail).\\nWriting a CronJob spec\\nSchedule syntax\\nThe .spec.schedule  field is required. The value of that field follows the Cron  syntax:\\n# ┌───────────── minute (0 - 59)\\n# │ ┌───────────── hour (0 - 23)\\n# │ │ ┌───────────── day of the month (1 - 31)\\n# │ │ │ ┌───────────── month (1 - 12)\\n# │ │ │ │ ┌───────────── day of the week (0 - 6) (Sunday to Saturday;\\n# │ │ │ │ │                                   7 is also Sunday on some systems)\\n# │ │ │ │ │                                   OR sun, mon, tue, wed, thu, fri, sat\\n# │ │ │ │ │\\n# * * * * *', metadata={'source': './PDFS/Concepts.pdf', 'page': 190}),\n",
       " Document(page_content='For example, 0 0 13 * 5  states that the task must be started every Friday at midnight, as well as\\non the 13th of each month at midnight.\\nThe format also includes extended \"Vixie cron\" step values. As explained in the FreeBSD\\nmanual :\\nStep values can be used in conjunction with ranges. Following a range with /\\n<number>  specifies skips of the number\\'s value through the range. For example, \\n0-23/2  can be used in the hours field to specify command execution every other\\nhour (the alternative in the V7 standard is 0,2,4,6,8,10,12,14,16,18,20,22 ). Steps are\\nalso permitted after an asterisk, so if you want to say \"every two hours\", just use */\\n2.\\nNote:  A question mark ( ?) in the schedule has the same meaning as an asterisk *, that is, it\\nstands for any of available value for a given field.\\nOther than the standard syntax, some macros like @monthly  can also be used:\\nEntry DescriptionEquivalent\\nto\\n@yearly (or\\n@annually)Run once a year at midnight of 1 January 0 0 1 1 *', metadata={'source': './PDFS/Concepts.pdf', 'page': 191}),\n",
       " Document(page_content='to\\n@yearly (or\\n@annually)Run once a year at midnight of 1 January 0 0 1 1 *\\n@monthlyRun once a month at midnight of the first day of the\\nmonth0 0 1 * *\\n@weekly Run once a week at midnight on Sunday morning 0 0 * * 0\\n@daily (or @midnight) Run once a day at midnight 0 0 * * *\\n@hourly Run once an hour at the beginning of the hour 0 * * * *\\nTo generate CronJob schedule expressions, you can also use web tools like crontab.guru .\\nJob template\\nThe .spec.jobTemplate  defines a template for the Jobs that the CronJob creates, and it is\\nrequired. It has exactly the same schema as a Job, except that it is nested and does not have an \\napiVersion  or kind. You can specify common metadata for the templated Jobs, such as labels  or \\nannotations . For information about writing a Job .spec , see Writing a Job Spec .\\nDeadline for delayed job start\\nThe .spec.startingDeadlineSeconds  field is optional. This field defines a deadline (in whole', metadata={'source': './PDFS/Concepts.pdf', 'page': 191}),\n",
       " Document(page_content=\"The .spec.startingDeadlineSeconds  field is optional. This field defines a deadline (in whole\\nseconds) for starting the Job, if that Job misses its scheduled time for any reason.\\nAfter missing the deadline, the CronJob skips that instance of the Job (future occurrences are\\nstill scheduled). For example, if you have a backup job that runs twice a day, you might allow it\\nto start up to 8 hours late, but no later, because a backup taken any later wouldn't be useful: you\\nwould instead prefer to wait for the next scheduled run.\\nFor Jobs that miss their configured deadline, Kubernetes treats them as failed Jobs. If you don't\\nspecify startingDeadlineSeconds  for a CronJob, the Job occurrences have no deadline.\", metadata={'source': './PDFS/Concepts.pdf', 'page': 191}),\n",
       " Document(page_content=\"If the .spec.startingDeadlineSeconds  field is set (not null), the CronJob controller measures the\\ntime between when a job is expected to be created and now. If the difference is higher than that\\nlimit, it will skip this execution.\\nFor example, if it is set to 200, it allows a job to be created for up to 200 seconds after the actual\\nschedule.\\nConcurrency policy\\nThe .spec.concurrencyPolicy  field is also optional. It specifies how to treat concurrent\\nexecutions of a job that is created by this CronJob. The spec may specify only one of the\\nfollowing concurrency policies:\\nAllow  (default): The CronJob allows concurrently running jobs\\nForbid : The CronJob does not allow concurrent runs; if it is time for a new job run and\\nthe previous job run hasn't finished yet, the CronJob skips the new job run\\nReplace : If it is time for a new job run and the previous job run hasn't finished yet, the\\nCronJob replaces the currently running job run with a new job run\", metadata={'source': './PDFS/Concepts.pdf', 'page': 192}),\n",
       " Document(page_content='CronJob replaces the currently running job run with a new job run\\nNote that concurrency policy only applies to the jobs created by the same cron job. If there are\\nmultiple CronJobs, their respective jobs are always allowed to run concurrently.\\nSchedule suspension\\nYou can suspend execution of Jobs for a CronJob, by setting the optional .spec.suspend  field to\\ntrue. The field defaults to false.\\nThis setting does not affect Jobs that the CronJob has already started.\\nIf you do set that field to true, all subsequent executions are suspended (they remain scheduled,\\nbut the CronJob controller does not start the Jobs to run the tasks) until you unsuspend the\\nCronJob.\\nCaution:  Executions that are suspended during their scheduled time count as missed jobs.\\nWhen .spec.suspend  changes from true to false on an existing CronJob without a starting\\ndeadline , the missed jobs are scheduled immediately.\\nJobs history limits', metadata={'source': './PDFS/Concepts.pdf', 'page': 192}),\n",
       " Document(page_content='deadline , the missed jobs are scheduled immediately.\\nJobs history limits\\nThe .spec.successfulJobsHistoryLimit  and .spec.failedJobsHistoryLimit  fields are optional. These\\nfields specify how many completed and failed jobs should be kept. By default, they are set to 3\\nand 1 respectively. Setting a limit to 0 corresponds to keeping none of the corresponding kind\\nof jobs after they finish.\\nFor another way to clean up jobs automatically, see Clean up finished jobs automatically .\\nTime zones\\nFEATURE STATE:  Kubernetes v1.27 [stable]\\nFor CronJobs with no time zone specified, the kube-controller-manager  interprets schedules\\nrelative to its local time zone.• \\n• \\n•', metadata={'source': './PDFS/Concepts.pdf', 'page': 192}),\n",
       " Document(page_content='You can specify a time zone for a CronJob by setting .spec.timeZone  to the name of a valid time\\nzone . For example, setting .spec.timeZone: \"Etc/UTC\"  instructs Kubernetes to interpret the\\nschedule relative to Coordinated Universal Time.\\nA time zone database from the Go standard library is included in the binaries and used as a\\nfallback in case an external database is not available on the system.\\nCronJob limitations\\nUnsupported TimeZone specification\\nThe implementation of the CronJob API in Kubernetes 1.28 lets you set the .spec.schedule  field\\nto include a timezone; for example: CRON_TZ=UTC * * * * *  or TZ=UTC * * * * * .\\nSpecifying a timezone that way is not officially supported  (and never has been).\\nIf you try to set a schedule that includes TZ or CRON_TZ  timezone specification, Kubernetes\\nreports a warning  to the client. Future versions of Kubernetes will prevent setting the unofficial\\ntimezone mechanism entirely.\\nModifying a CronJob', metadata={'source': './PDFS/Concepts.pdf', 'page': 193}),\n",
       " Document(page_content='timezone mechanism entirely.\\nModifying a CronJob\\nBy design, a CronJob contains a template for new Jobs. If you modify an existing CronJob, the\\nchanges you make will apply to new Jobs that start to run after your modification is complete.\\nJobs (and their Pods) that have already started continue to run without changes. That is, the\\nCronJob does not update existing Jobs, even if those remain running.\\nJob creation\\nA CronJob creates a Job object approximately once per execution time of its schedule. The\\nscheduling is approximate because there are certain circumstances where two Jobs might be\\ncreated, or no Job might be created. Kubernetes tries to avoid those situations, but does not\\ncompletely prevent them. Therefore, the Jobs that you define should be idempotent .\\nIf startingDeadlineSeconds  is set to a large value or left unset (the default) and if \\nconcurrencyPolicy  is set to Allow , the jobs will always run at least once.', metadata={'source': './PDFS/Concepts.pdf', 'page': 193}),\n",
       " Document(page_content='concurrencyPolicy  is set to Allow , the jobs will always run at least once.\\nCaution:  If startingDeadlineSeconds  is set to a value less than 10 seconds, the CronJob may not\\nbe scheduled. This is because the CronJob controller checks things every 10 seconds.\\nFor every CronJob, the CronJob Controller  checks how many schedules it missed in the\\nduration from its last scheduled time until now. If there are more than 100 missed schedules,\\nthen it does not start the job and logs the error.\\nCannot determine if job needs to be started. Too many missed start time (> 100). Set or \\ndecrease .spec.startingDeadlineSeconds or check clock skew.\\nIt is important to note that if the startingDeadlineSeconds  field is set (not nil), the controller\\ncounts how many missed jobs occurred from the value of startingDeadlineSeconds  until now\\nrather than from the last scheduled time until now. For example, if startingDeadlineSeconds  is', metadata={'source': './PDFS/Concepts.pdf', 'page': 193}),\n",
       " Document(page_content='rather than from the last scheduled time until now. For example, if startingDeadlineSeconds  is \\n200, the controller counts how many missed jobs occurred in the last 200 seconds.', metadata={'source': './PDFS/Concepts.pdf', 'page': 193}),\n",
       " Document(page_content='A CronJob is counted as missed if it has failed to be created at its scheduled time. For example,\\nif concurrencyPolicy  is set to Forbid  and a CronJob was attempted to be scheduled when there\\nwas a previous schedule still running, then it would count as missed.\\nFor example, suppose a CronJob is set to schedule a new Job every one minute beginning at \\n08:30:00 , and its startingDeadlineSeconds  field is not set. If the CronJob controller happens to be\\ndown from 08:29:00  to 10:21:00 , the job will not start as the number of missed jobs which\\nmissed their schedule is greater than 100.\\nTo illustrate this concept further, suppose a CronJob is set to schedule a new Job every one\\nminute beginning at 08:30:00 , and its startingDeadlineSeconds  is set to 200 seconds. If the\\nCronJob controller happens to be down for the same period as the previous example ( 08:29:00\\nto 10:21:00 ,) the Job will still start at 10:22:00. This happens as the controller now checks how', metadata={'source': './PDFS/Concepts.pdf', 'page': 194}),\n",
       " Document(page_content=\"to 10:21:00 ,) the Job will still start at 10:22:00. This happens as the controller now checks how\\nmany missed schedules happened in the last 200 seconds (i.e., 3 missed schedules), rather than\\nfrom the last scheduled time until now.\\nThe CronJob is only responsible for creating Jobs that match its schedule, and the Job in turn is\\nresponsible for the management of the Pods it represents.\\nWhat's next\\nLearn about Pods  and Jobs, two concepts that CronJobs rely upon.\\nRead about the detailed format  of CronJob .spec.schedule  fields.\\nFor instructions on creating and working with CronJobs, and for an example of a CronJob\\nmanifest, see Running automated tasks with CronJobs .\\nCronJob  is part of the Kubernetes REST API. Read the CronJob  API reference for more\\ndetails.\\nReplicationController\\nLegacy API for managing workloads that can scale horizontally. Superseded by the Deployment\\nand ReplicaSet APIs.\\nNote:  A Deployment  that configures a ReplicaSet  is now the recommended way to set up\", metadata={'source': './PDFS/Concepts.pdf', 'page': 194}),\n",
       " Document(page_content='Note:  A Deployment  that configures a ReplicaSet  is now the recommended way to set up\\nreplication.\\nA ReplicationController  ensures that a specified number of pod replicas are running at any one\\ntime. In other words, a ReplicationController makes sure that a pod or a homogeneous set of\\npods is always up and available.\\nHow a ReplicationController works\\nIf there are too many pods, the ReplicationController terminates the extra pods. If there are too\\nfew, the ReplicationController starts more pods. Unlike manually created pods, the pods\\nmaintained by a ReplicationController are automatically replaced if they fail, are deleted, or are\\nterminated. For example, your pods are re-created on a node after disruptive maintenance such\\nas a kernel upgrade. For this reason, you should use a ReplicationController even if your\\napplication requires only a single pod. A ReplicationController is similar to a process\\nsupervisor, but instead of supervising individual processes on a single node, the', metadata={'source': './PDFS/Concepts.pdf', 'page': 194}),\n",
       " Document(page_content='supervisor, but instead of supervising individual processes on a single node, the\\nReplicationController supervises multiple pods across multiple nodes.• \\n• \\n• \\n•', metadata={'source': './PDFS/Concepts.pdf', 'page': 194}),\n",
       " Document(page_content='ReplicationController is often abbreviated to \"rc\" in discussion, and as a shortcut in kubectl\\ncommands.\\nA simple case is to create one ReplicationController object to reliably run one instance of a Pod\\nindefinitely. A more complex use case is to run several identical replicas of a replicated service,\\nsuch as web servers.\\nRunning an example ReplicationController\\nThis example ReplicationController config runs three copies of the nginx web server.\\ncontrollers/replication.yaml  \\napiVersion : v1\\nkind: ReplicationController\\nmetadata :\\n  name : nginx\\nspec:\\n  replicas : 3\\n  selector :\\n    app: nginx\\n  template :\\n    metadata :\\n      name : nginx\\n      labels :\\n        app: nginx\\n    spec:\\n      containers :\\n      - name : nginx\\n        image : nginx\\n        ports :\\n        - containerPort : 80\\nRun the example job by downloading the example file and then running this command:\\nkubectl apply -f https://k8s.io/examples/controllers/replication.yaml\\nThe output is similar to this:', metadata={'source': './PDFS/Concepts.pdf', 'page': 195}),\n",
       " Document(page_content='kubectl apply -f https://k8s.io/examples/controllers/replication.yaml\\nThe output is similar to this:\\nreplicationcontroller/nginx created\\nCheck on the status of the ReplicationController using this command:\\nkubectl describe replicationcontrollers/nginx\\nThe output is similar to this:\\nName:        nginx\\nNamespace:   default\\nSelector:    app=nginx\\nLabels:      app=nginx\\nAnnotations:    <none>\\nReplicas:    3 current / 3 desired', metadata={'source': './PDFS/Concepts.pdf', 'page': 195}),\n",
       " Document(page_content='Pods Status: 0 Running / 3 Waiting / 0 Succeeded / 0 Failed\\nPod Template:\\n  Labels:       app=nginx\\n  Containers:\\n   nginx:\\n    Image:              nginx\\n    Port:               80/TCP\\n    Environment:        <none>\\n    Mounts:             <none>\\n  Volumes:              <none>\\nEvents:\\n  FirstSeen       LastSeen     Count    From                        SubobjectPath    Type      Reason              \\nMessage\\n  ---------       --------     -----    ----                        -------------    ----      ------              -------\\n  20s             20s          1        {replication-controller }                    Normal    SuccessfulCreate    \\nCreated pod: nginx-qrm3m\\n  20s             20s          1        {replication-controller }                    Normal    SuccessfulCreate    \\nCreated pod: nginx-3ntk0\\n  20s             20s          1        {replication-controller }                    Normal    SuccessfulCreate    \\nCreated pod: nginx-4ok8v', metadata={'source': './PDFS/Concepts.pdf', 'page': 196}),\n",
       " Document(page_content='Created pod: nginx-4ok8v\\nHere, three pods are created, but none is running yet, perhaps because the image is being\\npulled. A little later, the same command may show:\\nPods Status:    3 Running / 0 Waiting / 0 Succeeded / 0 Failed\\nTo list all the pods that belong to the ReplicationController in a machine readable form, you can\\nuse a command like this:\\npods =$(kubectl get pods --selector =app=nginx --output =jsonpath ={.items..metadata.name })\\necho  $pods\\nThe output is similar to this:\\nnginx-3ntk0 nginx-4ok8v nginx-qrm3m\\nHere, the selector is the same as the selector for the ReplicationController (seen in the kubectl \\ndescribe  output), and in a different form in replication.yaml . The --output=jsonpath  option\\nspecifies an expression with the name from each pod in the returned list.\\nWriting a ReplicationController Manifest\\nAs with all other Kubernetes config, a ReplicationController needs apiVersion , kind, and \\nmetadata  fields.', metadata={'source': './PDFS/Concepts.pdf', 'page': 196}),\n",
       " Document(page_content='metadata  fields.\\nWhen the control plane creates new Pods for a ReplicationController, the .metadata.name  of the\\nReplicationController is part of the basis for naming those Pods. The name of a\\nReplicationController must be a valid DNS subdomain  value, but this can produce unexpected\\nresults for the Pod hostnames. For best compatibility, the name should follow the more\\nrestrictive rules for a DNS label .\\nFor general information about working with configuration files, see object management .', metadata={'source': './PDFS/Concepts.pdf', 'page': 196}),\n",
       " Document(page_content='A ReplicationController also needs a .spec  section .\\nPod Template\\nThe .spec.template  is the only required field of the .spec .\\nThe .spec.template  is a pod template . It has exactly the same schema as a Pod, except it is nested\\nand does not have an apiVersion  or kind.\\nIn addition to required fields for a Pod, a pod template in a ReplicationController must specify\\nappropriate labels and an appropriate restart policy. For labels, make sure not to overlap with\\nother controllers. See pod selector .\\nOnly a .spec.template.spec.restartPolicy  equal to Always  is allowed, which is the default if not\\nspecified.\\nFor local container restarts, ReplicationControllers delegate to an agent on the node, for\\nexample the Kubelet .\\nLabels on the ReplicationController\\nThe ReplicationController can itself have labels ( .metadata.labels ). Typically, you would set\\nthese the same as the .spec.template.metadata.labels ; if .metadata.labels  is not specified then it', metadata={'source': './PDFS/Concepts.pdf', 'page': 197}),\n",
       " Document(page_content='these the same as the .spec.template.metadata.labels ; if .metadata.labels  is not specified then it\\ndefaults to .spec.template.metadata.labels . However, they are allowed to be different, and\\nthe .metadata.labels  do not affect the behavior of the ReplicationController.\\nPod Selector\\nThe .spec.selector  field is a label selector . A ReplicationController manages all the pods with\\nlabels that match the selector. It does not distinguish between pods that it created or deleted\\nand pods that another person or process created or deleted. This allows the\\nReplicationController to be replaced without affecting the running pods.\\nIf specified, the .spec.template.metadata.labels  must be equal to the .spec.selector , or it will be\\nrejected by the API. If .spec.selector  is unspecified, it will be defaulted\\nto .spec.template.metadata.labels .\\nAlso you should not normally create any pods whose labels match this selector, either directly,', metadata={'source': './PDFS/Concepts.pdf', 'page': 197}),\n",
       " Document(page_content='Also you should not normally create any pods whose labels match this selector, either directly,\\nwith another ReplicationController, or with another controller such as Job. If you do so, the\\nReplicationController thinks that it created the other pods. Kubernetes does not stop you from\\ndoing this.\\nIf you do end up with multiple controllers that have overlapping selectors, you will have to\\nmanage the deletion yourself (see below ).\\nMultiple Replicas\\nYou can specify how many pods should run concurrently by setting .spec.replicas  to the\\nnumber of pods you would like to have running concurrently. The number running at any time\\nmay be higher or lower, such as if the replicas were just increased or decreased, or if a pod is\\ngracefully shutdown, and a replacement starts early.\\nIf you do not specify .spec.replicas , then it defaults to 1.', metadata={'source': './PDFS/Concepts.pdf', 'page': 197}),\n",
       " Document(page_content='Working with ReplicationControllers\\nDeleting a ReplicationController and its Pods\\nTo delete a ReplicationController and all its pods, use kubectl delete . Kubectl will scale the\\nReplicationController to zero and wait for it to delete each pod before deleting the\\nReplicationController itself. If this kubectl command is interrupted, it can be restarted.\\nWhen using the REST API or client library , you need to do the steps explicitly (scale replicas to\\n0, wait for pod deletions, then delete the ReplicationController).\\nDeleting only a ReplicationController\\nYou can delete a ReplicationController without affecting any of its pods.\\nUsing kubectl, specify the --cascade=orphan  option to kubectl delete .\\nWhen using the REST API or client library , you can delete the ReplicationController object.\\nOnce the original is deleted, you can create a new ReplicationController to replace it. As long as\\nthe old and new .spec.selector  are the same, then the new one will adopt the old pods. However,', metadata={'source': './PDFS/Concepts.pdf', 'page': 198}),\n",
       " Document(page_content=\"the old and new .spec.selector  are the same, then the new one will adopt the old pods. However,\\nit will not make any effort to make existing pods match a new, different pod template. To\\nupdate pods to a new spec in a controlled way, use a rolling update .\\nIsolating pods from a ReplicationController\\nPods may be removed from a ReplicationController's target set by changing their labels. This\\ntechnique may be used to remove pods from service for debugging and data recovery. Pods that\\nare removed in this way will be replaced automatically (assuming that the number of replicas is\\nnot also changed).\\nCommon usage patterns\\nRescheduling\\nAs mentioned above, whether you have 1 pod you want to keep running, or 1000, a\\nReplicationController will ensure that the specified number of pods exists, even in the event of\\nnode failure or pod termination (for example, due to an action by another control agent).\\nScaling\", metadata={'source': './PDFS/Concepts.pdf', 'page': 198}),\n",
       " Document(page_content='node failure or pod termination (for example, due to an action by another control agent).\\nScaling\\nThe ReplicationController enables scaling the number of replicas up or down, either manually\\nor by an auto-scaling control agent, by updating the replicas  field.\\nRolling updates\\nThe ReplicationController is designed to facilitate rolling updates to a service by replacing pods\\none-by-one.\\nAs explained in #1353 , the recommended approach is to create a new ReplicationController\\nwith 1 replica, scale the new (+1) and old (-1) controllers one by one, and then delete the old', metadata={'source': './PDFS/Concepts.pdf', 'page': 198}),\n",
       " Document(page_content=\"controller after it reaches 0 replicas. This predictably updates the set of pods regardless of\\nunexpected failures.\\nIdeally, the rolling update controller would take application readiness into account, and would\\nensure that a sufficient number of pods were productively serving at any given time.\\nThe two ReplicationControllers would need to create pods with at least one differentiating label,\\nsuch as the image tag of the primary container of the pod, since it is typically image updates\\nthat motivate rolling updates.\\nMultiple release tracks\\nIn addition to running multiple releases of an application while a rolling update is in progress,\\nit's common to run multiple releases for an extended period of time, or even continuously, using\\nmultiple release tracks. The tracks would be differentiated by labels.\\nFor instance, a service might target all pods with tier in (frontend), environment in (prod) . Now\", metadata={'source': './PDFS/Concepts.pdf', 'page': 199}),\n",
       " Document(page_content=\"For instance, a service might target all pods with tier in (frontend), environment in (prod) . Now\\nsay you have 10 replicated pods that make up this tier. But you want to be able to 'canary' a\\nnew version of this component. You could set up a ReplicationController with replicas  set to 9\\nfor the bulk of the replicas, with labels tier=frontend, environment=prod, track=stable , and\\nanother ReplicationController with replicas  set to 1 for the canary, with labels tier=frontend, \\nenvironment=prod, track=canary . Now the service is covering both the canary and non-canary\\npods. But you can mess with the ReplicationControllers separately to test things out, monitor\\nthe results, etc.\\nUsing ReplicationControllers with Services\\nMultiple ReplicationControllers can sit behind a single service, so that, for example, some traffic\\ngoes to the old version, and some goes to the new version.\\nA ReplicationController will never terminate on its own, but it isn't expected to be as long-lived\", metadata={'source': './PDFS/Concepts.pdf', 'page': 199}),\n",
       " Document(page_content=\"A ReplicationController will never terminate on its own, but it isn't expected to be as long-lived\\nas services. Services may be composed of pods controlled by multiple ReplicationControllers,\\nand it is expected that many ReplicationControllers may be created and destroyed over the\\nlifetime of a service (for instance, to perform an update of pods that run the service). Both\\nservices themselves and their clients should remain oblivious to the ReplicationControllers that\\nmaintain the pods of the services.\\nWriting programs for Replication\\nPods created by a ReplicationController are intended to be fungible and semantically identical,\\nthough their configurations may become heterogeneous over time. This is an obvious fit for\\nreplicated stateless servers, but ReplicationControllers can also be used to maintain availability\\nof master-elected, sharded, and worker-pool applications. Such applications should use dynamic\", metadata={'source': './PDFS/Concepts.pdf', 'page': 199}),\n",
       " Document(page_content='of master-elected, sharded, and worker-pool applications. Such applications should use dynamic\\nwork assignment mechanisms, such as the RabbitMQ work queues , as opposed to static/one-\\ntime customization of the configuration of each pod, which is considered an anti-pattern. Any\\npod customization performed, such as vertical auto-sizing of resources (for example, cpu or\\nmemory), should be performed by another online controller process, not unlike the\\nReplicationController itself.', metadata={'source': './PDFS/Concepts.pdf', 'page': 199}),\n",
       " Document(page_content='Responsibilities of the ReplicationController\\nThe ReplicationController ensures that the desired number of pods matches its label selector\\nand are operational. Currently, only terminated pods are excluded from its count. In the future, \\nreadiness  and other information available from the system may be taken into account, we may\\nadd more controls over the replacement policy, and we plan to emit events that could be used\\nby external clients to implement arbitrarily sophisticated replacement and/or scale-down\\npolicies.\\nThe ReplicationController is forever constrained to this narrow responsibility. It itself will not\\nperform readiness nor liveness probes. Rather than performing auto-scaling, it is intended to be\\ncontrolled by an external auto-scaler (as discussed in #492), which would change its replicas\\nfield. We will not add scheduling policies (for example, spreading ) to the ReplicationController.', metadata={'source': './PDFS/Concepts.pdf', 'page': 200}),\n",
       " Document(page_content='field. We will not add scheduling policies (for example, spreading ) to the ReplicationController.\\nNor should it verify that the pods controlled match the currently specified template, as that\\nwould obstruct auto-sizing and other automated processes. Similarly, completion deadlines,\\nordering dependencies, configuration expansion, and other features belong elsewhere. We even\\nplan to factor out the mechanism for bulk pod creation ( #170).\\nThe ReplicationController is intended to be a composable building-block primitive. We expect\\nhigher-level APIs and/or tools to be built on top of it and other complementary primitives for\\nuser convenience in the future. The \"macro\" operations currently supported by kubectl (run,\\nscale) are proof-of-concept examples of this. For instance, we could imagine something like \\nAsgard  managing ReplicationControllers, auto-scalers, services, scheduling policies, canaries,\\netc.\\nAPI Object', metadata={'source': './PDFS/Concepts.pdf', 'page': 200}),\n",
       " Document(page_content=\"etc.\\nAPI Object\\nReplication controller is a top-level resource in the Kubernetes REST API. More details about\\nthe API object can be found at: ReplicationController API object .\\nAlternatives to ReplicationController\\nReplicaSet\\nReplicaSet  is the next-generation ReplicationController that supports the new set-based label\\nselector . It's mainly used by Deployment  as a mechanism to orchestrate pod creation, deletion\\nand updates. Note that we recommend using Deployments instead of directly using Replica\\nSets, unless you require custom update orchestration or don't require updates at all.\\nDeployment (Recommended)\\nDeployment  is a higher-level API object that updates its underlying Replica Sets and their Pods.\\nDeployments are recommended if you want the rolling update functionality, because they are\\ndeclarative, server-side, and have additional features.\\nBare Pods\\nUnlike in the case where a user directly created pods, a ReplicationController replaces pods that\", metadata={'source': './PDFS/Concepts.pdf', 'page': 200}),\n",
       " Document(page_content='Unlike in the case where a user directly created pods, a ReplicationController replaces pods that\\nare deleted or terminated for any reason, such as in the case of node failure or disruptive node\\nmaintenance, such as a kernel upgrade. For this reason, we recommend that you use a', metadata={'source': './PDFS/Concepts.pdf', 'page': 200}),\n",
       " Document(page_content=\"ReplicationController even if your application requires only a single pod. Think of it similarly\\nto a process supervisor, only it supervises multiple pods across multiple nodes instead of\\nindividual processes on a single node. A ReplicationController delegates local container restarts\\nto some agent on the node, such as the kubelet.\\nJob\\nUse a Job instead of a ReplicationController for pods that are expected to terminate on their\\nown (that is, batch jobs).\\nDaemonSet\\nUse a DaemonSet  instead of a ReplicationController for pods that provide a machine-level\\nfunction, such as machine monitoring or machine logging. These pods have a lifetime that is\\ntied to a machine lifetime: the pod needs to be running on the machine before other pods start,\\nand are safe to terminate when the machine is otherwise ready to be rebooted/shutdown.\\nWhat's next\\nLearn about Pods .\\nLearn about Deployment , the replacement for ReplicationController.\", metadata={'source': './PDFS/Concepts.pdf', 'page': 201}),\n",
       " Document(page_content=\"What's next\\nLearn about Pods .\\nLearn about Deployment , the replacement for ReplicationController.\\nReplicationController  is part of the Kubernetes REST API. Read the ReplicationController\\nobject definition to understand the API for replication controllers.\\nServices, Load Balancing, and Networking\\nConcepts and resources behind networking in Kubernetes.\\nThe Kubernetes network model\\nEvery Pod in a cluster gets its own unique cluster-wide IP address. This means you do not need\\nto explicitly create links between Pods  and you almost never need to deal with mapping\\ncontainer ports to host ports.\\nThis creates a clean, backwards-compatible model where Pods  can be treated much like VMs or\\nphysical hosts from the perspectives of port allocation, naming, service discovery, load\\nbalancing , application configuration, and migration.\\nKubernetes imposes the following fundamental requirements on any networking\\nimplementation (barring any intentional network segmentation policies):\", metadata={'source': './PDFS/Concepts.pdf', 'page': 201}),\n",
       " Document(page_content='implementation (barring any intentional network segmentation policies):\\npods can communicate with all other pods on any other node  without NAT\\nagents on a node (e.g. system daemons, kubelet) can communicate with all pods on that\\nnode\\nNote: For those platforms that support Pods  running in the host network (e.g. Linux), when\\npods are attached to the host network of a node they can still communicate with all pods on all\\nnodes without NAT.\\nThis model is not only less complex overall, but it is principally compatible with the desire for\\nKubernetes to enable low-friction porting of apps from VMs to containers. If your job• \\n• \\n• \\n• \\n•', metadata={'source': './PDFS/Concepts.pdf', 'page': 201}),\n",
       " Document(page_content='previously ran in a VM, your VM had an IP and could talk to other VMs in your project. This is\\nthe same basic model.\\nKubernetes IP addresses exist at the Pod scope - containers within a Pod share their network\\nnamespaces - including their IP address and MAC address. This means that containers within a \\nPod can all reach each other\\'s ports on localhost . This also means that containers within a Pod\\nmust coordinate port usage, but this is no different from processes in a VM. This is called the\\n\"IP-per-pod\" model.\\nHow this is implemented is a detail of the particular container runtime in use.\\nIt is possible to request ports on the Node  itself which forward to your Pod (called host ports),\\nbut this is a very niche operation. How that forwarding is implemented is also a detail of the\\ncontainer runtime. The Pod itself is blind to the existence or non-existence of host ports.\\nKubernetes networking addresses four concerns:\\nContainers within a Pod use networking to communicate  via loopback.', metadata={'source': './PDFS/Concepts.pdf', 'page': 202}),\n",
       " Document(page_content='Containers within a Pod use networking to communicate  via loopback.\\nCluster networking provides communication between different Pods.\\nThe Service  API lets you expose an application running in Pods  to be reachable from\\noutside your cluster.\\nIngress  provides extra functionality specifically for exposing HTTP applications,\\nwebsites and APIs.\\nGateway API  is an add-on  that provides an expressive, extensible, and role-oriented\\nfamily of API kinds for modeling service networking.\\nYou can also use Services to publish services only for consumption inside your cluster .\\nThe Connecting Applications with Services  tutorial lets you learn about Services and\\nKubernetes networking with a hands-on example.\\nCluster Networking  explains how to set up networking for your cluster, and also provides an\\noverview of the technologies involved.\\nService\\nExpose an application running in your cluster behind a single outward-facing endpoint, even\\nwhen the workload is split across multiple backends.\\nIngress', metadata={'source': './PDFS/Concepts.pdf', 'page': 202}),\n",
       " Document(page_content='when the workload is split across multiple backends.\\nIngress\\nMake your HTTP (or HTTPS) network service available using a protocol-aware configuration\\nmechanism, that understands web concepts like URIs, hostnames, paths, and more. The Ingress\\nconcept lets you map traffic to different backends based on rules you define via the Kubernetes\\nAPI.\\nIngress Controllers\\nIn order for an Ingress  to work in your cluster, there must be an ingress controller  running. You\\nneed to select at least one ingress controller and make sure it is set up in your cluster. This page\\nlists common ingress controllers that you can deploy.• \\n• \\n• \\n◦ \\n◦ \\n•', metadata={'source': './PDFS/Concepts.pdf', 'page': 202}),\n",
       " Document(page_content='Gateway API\\nGateway API is a family of API kinds that provide dynamic infrastructure provisioning and\\nadvanced traffic routing.\\nEndpointSlices\\nThe EndpointSlice API is the mechanism that Kubernetes uses to let your Service scale to\\nhandle large numbers of backends, and allows the cluster to update its list of healthy backends\\nefficiently.\\nNetwork Policies\\nIf you want to control traffic flow at the IP address or port level (OSI layer 3 or 4),\\nNetworkPolicies allow you to specify rules for traffic flow within your cluster, and also between\\nPods and the outside world. Your cluster must use a network plugin that supports\\nNetworkPolicy enforcement.\\nDNS for Services and Pods\\nYour workload can discover Services within your cluster using DNS; this page explains how\\nthat works.\\nIPv4/IPv6 dual-stack\\nKubernetes lets you configure single-stack IPv4 networking, single-stack IPv6 networking, or\\ndual stack networking with both network families active. This page explains how.\\nTopology Aware Routing', metadata={'source': './PDFS/Concepts.pdf', 'page': 203}),\n",
       " Document(page_content='Topology Aware Routing\\nTopology Aware Routing  provides a mechanism to help keep network traffic within the zone\\nwhere it originated. Preferring same-zone traffic between Pods in your cluster can help with\\nreliability, performance (network latency and throughput), or cost.\\nNetworking on Windows\\nService ClusterIP allocation\\nService Internal Traffic Policy\\nIf two Pods in your cluster want to communicate, and both Pods are actually running on the\\nsame node, use Service Internal Traffic Policy  to keep network traffic within that node. Avoiding\\na round trip via the cluster network can help with reliability, performance (network latency and\\nthroughput), or cost.\\nService\\nExpose an application running in your cluster behind a single outward-facing endpoint, even\\nwhen the workload is split across multiple backends.', metadata={'source': './PDFS/Concepts.pdf', 'page': 203}),\n",
       " Document(page_content=\"In Kubernetes, a Service is a method for exposing a network application that is running as one\\nor more Pods  in your cluster.\\nA key aim of Services in Kubernetes is that you don't need to modify your existing application\\nto use an unfamiliar service discovery mechanism. You can run code in Pods, whether this is a\\ncode designed for a cloud-native world, or an older app you've containerized. You use a Service\\nto make that set of Pods available on the network so that clients can interact with it.\\nIf you use a Deployment  to run your app, that Deployment can create and destroy Pods\\ndynamically. From one moment to the next, you don't know how many of those Pods are\\nworking and healthy; you might not even know what those healthy Pods are named.\\nKubernetes Pods  are created and destroyed to match the desired state of your cluster. Pods are\\nephemeral resources (you should not expect that an individual Pod is reliable and durable).\", metadata={'source': './PDFS/Concepts.pdf', 'page': 204}),\n",
       " Document(page_content='ephemeral resources (you should not expect that an individual Pod is reliable and durable).\\nEach Pod gets its own IP address (Kubernetes expects network plugins to ensure this). For a\\ngiven Deployment in your cluster, the set of Pods running in one moment in time could be\\ndifferent from the set of Pods running that application a moment later.\\nThis leads to a problem: if some set of Pods (call them \"backends\") provides functionality to\\nother Pods (call them \"frontends\") inside your cluster, how do the frontends find out and keep\\ntrack of which IP address to connect to, so that the frontend can use the backend part of the\\nworkload?\\nEnter Services .\\nServices in Kubernetes\\nThe Service API, part of Kubernetes, is an abstraction to help you expose groups of Pods over a\\nnetwork. Each Service object defines a logical set of endpoints (usually these endpoints are\\nPods) along with a policy about how to make those pods accessible.', metadata={'source': './PDFS/Concepts.pdf', 'page': 204}),\n",
       " Document(page_content='Pods) along with a policy about how to make those pods accessible.\\nFor example, consider a stateless image-processing backend which is running with 3 replicas.\\nThose replicas are fungible—frontends do not care which backend they use. While the actual\\nPods that compose the backend set may change, the frontend clients should not need to be\\naware of that, nor should they need to keep track of the set of backends themselves.\\nThe Service abstraction enables this decoupling.\\nThe set of Pods targeted by a Service is usually determined by a selector  that you define. To\\nlearn about other ways to define Service endpoints, see Services without  selectors .\\nIf your workload speaks HTTP, you might choose to use an Ingress  to control how web traffic\\nreaches that workload. Ingress is not a Service type, but it acts as the entry point for your\\ncluster. An Ingress lets you consolidate your routing rules into a single resource, so that you', metadata={'source': './PDFS/Concepts.pdf', 'page': 204}),\n",
       " Document(page_content='cluster. An Ingress lets you consolidate your routing rules into a single resource, so that you\\ncan expose multiple components of your workload, running separately in your cluster, behind a\\nsingle listener.\\nThe Gateway  API for Kubernetes provides extra capabilities beyond Ingress and Service. You\\ncan add Gateway to your cluster - it is a family of extension APIs, implemented using \\nCustomResourceDefinitions  - and then use these to configure access to network services that\\nare running in your cluster.', metadata={'source': './PDFS/Concepts.pdf', 'page': 204}),\n",
       " Document(page_content=\"Cloud-native service discovery\\nIf you're able to use Kubernetes APIs for service discovery in your application, you can query\\nthe API server  for matching EndpointSlices. Kubernetes updates the EndpointSlices for a\\nService whenever the set of Pods in a Service changes.\\nFor non-native applications, Kubernetes offers ways to place a network port or load balancer in\\nbetween your application and the backend Pods.\\nEither way, your workload can use these service discovery  mechanisms to find the target it\\nwants to connect to.\\nDefining a Service\\nA Service is an object  (the same way that a Pod or a ConfigMap is an object). You can create,\\nview or modify Service definitions using the Kubernetes API. Usually you use a tool such as \\nkubectl  to make those API calls for you.\\nFor example, suppose you have a set of Pods that each listen on TCP port 9376 and are labelled\\nas app.kubernetes.io/name=MyApp . You can define a Service to publish that TCP listener:\\napiVersion : v1\\nkind: Service\\nmetadata :\", metadata={'source': './PDFS/Concepts.pdf', 'page': 205}),\n",
       " Document(page_content='apiVersion : v1\\nkind: Service\\nmetadata :\\n  name : my-service\\nspec:\\n  selector :\\n    app.kubernetes.io/name : MyApp\\n  ports :\\n    - protocol : TCP\\n      port: 80\\n      targetPort : 9376\\nApplying this manifest creates a new Service named \"my-service\" with the default ClusterIP \\nservice type . The Service targets TCP port 9376 on any Pod with the app.kubernetes.io/name: \\nMyApp  label.\\nKubernetes assigns this Service an IP address (the cluster IP ), that is used by the virtual IP\\naddress mechanism. For more details on that mechanism, read Virtual IPs and Service Proxies .\\nThe controller for that Service continuously scans for Pods that match its selector, and then\\nmakes any necessary updates to the set of EndpointSlices for the Service.\\nThe name of a Service object must be a valid RFC 1035 label name .\\nNote:  A Service can map any incoming port to a targetPort . By default and for convenience,\\nthe targetPort  is set to the same value as the port field.\\nPort definitions', metadata={'source': './PDFS/Concepts.pdf', 'page': 205}),\n",
       " Document(page_content='the targetPort  is set to the same value as the port field.\\nPort definitions\\nPort definitions in Pods have names, and you can reference these names in the targetPort\\nattribute of a Service. For example, we can bind the targetPort  of the Service to the Pod port in\\nthe following way:', metadata={'source': './PDFS/Concepts.pdf', 'page': 205}),\n",
       " Document(page_content='apiVersion : v1\\nkind: Pod\\nmetadata :\\n  name : nginx\\n  labels :\\n    app.kubernetes.io/name : proxy\\nspec:\\n  containers :\\n  - name : nginx\\n    image : nginx:stable\\n    ports :\\n      - containerPort : 80\\n        name : http-web-svc\\n---\\napiVersion : v1\\nkind: Service\\nmetadata :\\n  name : nginx-service\\nspec:\\n  selector :\\n    app.kubernetes.io/name : proxy\\n  ports :\\n  - name : name-of-service-port\\n    protocol : TCP\\n    port: 80\\n    targetPort : http-web-svc\\nThis works even if there is a mixture of Pods in the Service using a single configured name,\\nwith the same network protocol available via different port numbers. This offers a lot of\\nflexibility for deploying and evolving your Services. For example, you can change the port\\nnumbers that Pods expose in the next version of your backend software, without breaking\\nclients.\\nThe default protocol for Services is TCP; you can also use any other supported protocol .', metadata={'source': './PDFS/Concepts.pdf', 'page': 206}),\n",
       " Document(page_content='clients.\\nThe default protocol for Services is TCP; you can also use any other supported protocol .\\nBecause many Services need to expose more than one port, Kubernetes supports multiple port\\ndefinitions  for a single Service. Each port definition can have the same protocol , or a different\\none.\\nServices without selectors\\nServices most commonly abstract access to Kubernetes Pods thanks to the selector, but when\\nused with a corresponding set of EndpointSlices  objects and without a selector, the Service can\\nabstract other kinds of backends, including ones that run outside the cluster.\\nFor example:\\nYou want to have an external database cluster in production, but in your test\\nenvironment you use your own databases.\\nYou want to point your Service to a Service in a different Namespace  or on another\\ncluster.• \\n•', metadata={'source': './PDFS/Concepts.pdf', 'page': 206}),\n",
       " Document(page_content='You are migrating a workload to Kubernetes. While evaluating the approach, you run\\nonly a portion of your backends in Kubernetes.\\nIn any of these scenarios you can define a Service without  specifying a selector to match Pods.\\nFor example:\\napiVersion : v1\\nkind: Service\\nmetadata :\\n  name : my-service\\nspec:\\n  ports :\\n    - protocol : TCP\\n      port: 80\\n      targetPort : 9376\\nBecause this Service has no selector, the corresponding EndpointSlice (and legacy Endpoints)\\nobjects are not created automatically. You can map the Service to the network address and port\\nwhere it\\'s running, by adding an EndpointSlice object manually. For example:\\napiVersion : discovery.k8s.io/v1\\nkind: EndpointSlice\\nmetadata :\\n  name : my-service-1  # by convention, use the name of the Service\\n                     # as a prefix for the name of the EndpointSlice\\n  labels :\\n    # You should set the \"kubernetes.io/service-name\" label.\\n    # Set its value to match the name of the Service', metadata={'source': './PDFS/Concepts.pdf', 'page': 207}),\n",
       " Document(page_content='# Set its value to match the name of the Service\\n    kubernetes.io/service-name : my-service\\naddressType : IPv4\\nports :\\n  - name : \\'\\' # empty because port 9376 is not assigned as a well-known\\n             # port (by IANA)\\n    appProtocol : http\\n    protocol : TCP\\n    port: 9376\\nendpoints :\\n  - addresses :\\n      - \"10.4.5.6\"\\n  - addresses :\\n      - \"10.1.2.3\"\\nCustom EndpointSlices\\nWhen you create an EndpointSlice  object for a Service, you can use any name for the\\nEndpointSlice. Each EndpointSlice in a namespace must have a unique name. You link an\\nEndpointSlice to a Service by setting the kubernetes.io/service-name  label  on that\\nEndpointSlice.\\nNote:\\nThe endpoint IPs must not  be: loopback (127.0.0.0/8 for IPv4, ::1/128 for IPv6), or link-local\\n(169.254.0.0/16 and 224.0.0.0/24 for IPv4, fe80::/64 for IPv6).•', metadata={'source': './PDFS/Concepts.pdf', 'page': 207}),\n",
       " Document(page_content='The endpoint IP addresses cannot be the cluster IPs of other Kubernetes Services, because kube-\\nproxy  doesn\\'t support virtual IPs as a destination.\\nFor an EndpointSlice that you create yourself, or in your own code, you should also pick a value\\nto use for the label endpointslice.kubernetes.io/managed-by . If you create your own controller\\ncode to manage EndpointSlices, consider using a value similar to \"my-domain.example/name-\\nof-controller\" . If you are using a third party tool, use the name of the tool in all-lowercase and\\nchange spaces and other punctuation to dashes ( -). If people are directly using a tool such as \\nkubectl  to manage EndpointSlices, use a name that describes this manual management, such as \\n\"staff\"  or \"cluster-admins\" . You should avoid using the reserved value \"controller\" , which\\nidentifies EndpointSlices managed by Kubernetes\\' own control plane.\\nAccessing a Service without a selector', metadata={'source': './PDFS/Concepts.pdf', 'page': 208}),\n",
       " Document(page_content='Accessing a Service without a selector\\nAccessing a Service without a selector works the same as if it had a selector. In the example  for\\na Service without a selector, traffic is routed to one of the two endpoints defined in the\\nEndpointSlice manifest: a TCP connection to 10.1.2.3 or 10.4.5.6, on port 9376.\\nNote:  The Kubernetes API server does not allow proxying to endpoints that are not mapped to\\npods. Actions such as kubectl proxy <service-name>  where the service has no selector will fail\\ndue to this constraint. This prevents the Kubernetes API server from being used as a proxy to\\nendpoints the caller may not be authorized to access.\\nAn ExternalName  Service is a special case of Service that does not have selectors and uses DNS\\nnames instead. For more information, see the ExternalName  section.\\nEndpointSlices\\nFEATURE STATE:  Kubernetes v1.21 [stable]\\nEndpointSlices  are objects that represent a subset (a slice) of the backing network endpoints for\\na Service.', metadata={'source': './PDFS/Concepts.pdf', 'page': 208}),\n",
       " Document(page_content='a Service.\\nYour Kubernetes cluster tracks how many endpoints each EndpointSlice represents. If there are\\nso many endpoints for a Service that a threshold is reached, then Kubernetes adds another\\nempty EndpointSlice and stores new endpoint information there. By default, Kubernetes makes\\na new EndpointSlice once the existing EndpointSlices all contain at least 100 endpoints.\\nKubernetes does not make the new EndpointSlice until an extra endpoint needs to be added.\\nSee EndpointSlices  for more information about this API.\\nEndpoints\\nIn the Kubernetes API, an Endpoints  (the resource kind is plural) defines a list of network\\nendpoints, typically referenced by a Service to define which Pods the traffic can be sent to.\\nThe EndpointSlice API is the recommended replacement for Endpoints.\\nOver-capacity endpoints\\nKubernetes limits the number of endpoints that can fit in a single Endpoints object. When there', metadata={'source': './PDFS/Concepts.pdf', 'page': 208}),\n",
       " Document(page_content='Kubernetes limits the number of endpoints that can fit in a single Endpoints object. When there\\nare over 1000 backing endpoints for a Service, Kubernetes truncates the data in the Endpoints', metadata={'source': './PDFS/Concepts.pdf', 'page': 208}),\n",
       " Document(page_content='object. Because a Service can be linked with more than one EndpointSlice, the 1000 backing\\nendpoint limit only affects the legacy Endpoints API.\\nIn that case, Kubernetes selects at most 1000 possible backend endpoints to store into the\\nEndpoints object, and sets an annotation  on the Endpoints: endpoints.kubernetes.io/over-\\ncapacity: truncated . The control plane also removes that annotation if the number of backend\\nPods drops below 1000.\\nTraffic is still sent to backends, but any load balancing mechanism that relies on the legacy\\nEndpoints API only sends traffic to at most 1000 of the available backing endpoints.\\nThe same API limit means that you cannot manually update an Endpoints to have more than\\n1000 endpoints.\\nApplication protocol\\nFEATURE STATE:  Kubernetes v1.20 [stable]\\nThe appProtocol  field provides a way to specify an application protocol for each Service port.\\nThis is used as a hint for implementations to offer richer behavior for protocols that they', metadata={'source': './PDFS/Concepts.pdf', 'page': 209}),\n",
       " Document(page_content='This is used as a hint for implementations to offer richer behavior for protocols that they\\nunderstand. The value of this field is mirrored by the corresponding Endpoints and\\nEndpointSlice objects.\\nThis field follows standard Kubernetes label syntax. Valid values are one of:\\nIANA standard service names .\\nImplementation-defined prefixed names such as mycompany.com/my-custom-protocol .\\nKubernetes-defined prefixed names:\\nProtocol Description\\nkubernetes.io/h2c HTTP/2 over cleartext as described in RFC 7540\\nMulti-port Services\\nFor some Services, you need to expose more than one port. Kubernetes lets you configure\\nmultiple port definitions on a Service object. When using multiple ports for a Service, you must\\ngive all of your ports names so that these are unambiguous. For example:\\napiVersion : v1\\nkind: Service\\nmetadata :\\n  name : my-service\\nspec:\\n  selector :\\n    app.kubernetes.io/name : MyApp\\n  ports :\\n    - name : http\\n      protocol : TCP\\n      port: 80\\n      targetPort : 9376', metadata={'source': './PDFS/Concepts.pdf', 'page': 209}),\n",
       " Document(page_content='ports :\\n    - name : http\\n      protocol : TCP\\n      port: 80\\n      targetPort : 9376\\n    - name : https• \\n• \\n•', metadata={'source': './PDFS/Concepts.pdf', 'page': 209}),\n",
       " Document(page_content=\"protocol : TCP\\n      port: 443\\n      targetPort : 9377\\nNote:\\nAs with Kubernetes names  in general, names for ports must only contain lowercase\\nalphanumeric characters and -. Port names must also start and end with an alphanumeric\\ncharacter.\\nFor example, the names 123-abc  and web are valid, but 123_abc  and -web  are not.\\nService type\\nFor some parts of your application (for example, frontends) you may want to expose a Service\\nonto an external IP address, one that's accessible from outside of your cluster.\\nKubernetes Service types allow you to specify what kind of Service you want.\\nThe available type values and their behaviors are:\\nClusterIP\\nExposes the Service on a cluster-internal IP. Choosing this value makes the Service only\\nreachable from within the cluster. This is the default that is used if you don't explicitly\\nspecify a type for a Service. You can expose the Service to the public internet using an \\nIngress  or a Gateway .\\nNodePort\", metadata={'source': './PDFS/Concepts.pdf', 'page': 210}),\n",
       " Document(page_content=\"Ingress  or a Gateway .\\nNodePort\\nExposes the Service on each Node's IP at a static port (the NodePort ). To make the node\\nport available, Kubernetes sets up a cluster IP address, the same as if you had requested a\\nService of type: ClusterIP .\\nLoadBalancer\\nExposes the Service externally using an external load balancer. Kubernetes does not\\ndirectly offer a load balancing component; you must provide one, or you can integrate\\nyour Kubernetes cluster with a cloud provider.\\nExternalName\\nMaps the Service to the contents of the externalName  field (for example, to the hostname \\napi.foo.bar.example ). The mapping configures your cluster's DNS server to return a \\nCNAME  record with that external hostname value. No proxying of any kind is set up.\\nThe type field in the Service API is designed as nested functionality - each level adds to the\\nprevious. However there is an exception to this nested design. You can define a LoadBalancer\\nService by disabling the load balancer NodePort  allocation .\", metadata={'source': './PDFS/Concepts.pdf', 'page': 210}),\n",
       " Document(page_content='Service by disabling the load balancer NodePort  allocation .\\ntype: ClusterIP\\nThis default Service type assigns an IP address from a pool of IP addresses that your cluster has\\nreserved for that purpose.\\nSeveral of the other types for Service build on the ClusterIP  type as a foundation.\\nIf you define a Service that has the .spec.clusterIP  set to \"None\"  then Kubernetes does not\\nassign an IP address. See headless Services  for more information.', metadata={'source': './PDFS/Concepts.pdf', 'page': 210}),\n",
       " Document(page_content=\"Choosing your own IP address\\nYou can specify your own cluster IP address as part of a Service  creation request. To do this, set\\nthe .spec.clusterIP  field. For example, if you already have an existing DNS entry that you wish\\nto reuse, or legacy systems that are configured for a specific IP address and difficult to re-\\nconfigure.\\nThe IP address that you choose must be a valid IPv4 or IPv6 address from within the service-\\ncluster-ip-range  CIDR range that is configured for the API server. If you try to create a Service\\nwith an invalid clusterIP  address value, the API server will return a 422 HTTP status code to\\nindicate that there's a problem.\\nRead avoiding collisions  to learn how Kubernetes helps reduce the risk and impact of two\\ndifferent Services both trying to use the same IP address.\\ntype: NodePort\\nIf you set the type field to NodePort , the Kubernetes control plane allocates a port from a range\", metadata={'source': './PDFS/Concepts.pdf', 'page': 211}),\n",
       " Document(page_content=\"If you set the type field to NodePort , the Kubernetes control plane allocates a port from a range\\nspecified by --service-node-port-range  flag (default: 30000-32767). Each node proxies that port\\n(the same port number on every Node) into your Service. Your Service reports the allocated\\nport in its .spec.ports[*].nodePort  field.\\nUsing a NodePort gives you the freedom to set up your own load balancing solution, to\\nconfigure environments that are not fully supported by Kubernetes, or even to expose one or\\nmore nodes' IP addresses directly.\\nFor a node port Service, Kubernetes additionally allocates a port (TCP, UDP or SCTP to match\\nthe protocol of the Service). Every node in the cluster configures itself to listen on that assigned\\nport and to forward traffic to one of the ready endpoints associated with that Service. You'll be\\nable to contact the type: NodePort  Service, from outside the cluster, by connecting to any node\", metadata={'source': './PDFS/Concepts.pdf', 'page': 211}),\n",
       " Document(page_content=\"able to contact the type: NodePort  Service, from outside the cluster, by connecting to any node\\nusing the appropriate protocol (for example: TCP), and the appropriate port (as assigned to that\\nService).\\nChoosing your own port\\nIf you want a specific port number, you can specify a value in the nodePort  field. The control\\nplane will either allocate you that port or report that the API transaction failed. This means that\\nyou need to take care of possible port collisions yourself. You also have to use a valid port\\nnumber, one that's inside the range configured for NodePort use.\\nHere is an example manifest for a Service of type: NodePort  that specifies a NodePort value\\n(30007, in this example):\\napiVersion : v1\\nkind: Service\\nmetadata :\\n  name : my-service\\nspec:\\n  type: NodePort\\n  selector :\\n    app.kubernetes.io/name : MyApp\\n  ports :\\n    - port: 80\", metadata={'source': './PDFS/Concepts.pdf', 'page': 211}),\n",
       " Document(page_content='# By default and for convenience, the `targetPort` is set to\\n      # the same value as the `port` field.\\n      targetPort : 80\\n      # Optional field\\n      # By default and for convenience, the Kubernetes control plane\\n      # will allocate a port from a range (default: 30000-32767)\\n      nodePort : 30007\\nReserve Nodeport ranges to avoid collisions\\nFEATURE STATE:  Kubernetes v1.28 [beta]\\nThe policy for assigning ports to NodePort services applies to both the auto-assignment and the\\nmanual assignment scenarios. When a user wants to create a NodePort service that uses a\\nspecific port, the target port may conflict with another port that has already been assigned. In\\nthis case, you can enable the feature gate ServiceNodePortStaticSubrange , which allows you to\\nuse a different port allocation strategy for NodePort Services. The port range for NodePort\\nservices is divided into two bands. Dynamic port assignment uses the upper band by default,', metadata={'source': './PDFS/Concepts.pdf', 'page': 212}),\n",
       " Document(page_content='services is divided into two bands. Dynamic port assignment uses the upper band by default,\\nand it may use the lower band once the upper band has been exhausted. Users can then allocate\\nfrom the lower band with a lower risk of port collision.\\nCustom IP address configuration for type: NodePort  Services\\nYou can set up nodes in your cluster to use a particular IP address for serving node port\\nservices. You might want to do this if each node is connected to multiple networks (for\\nexample: one network for application traffic, and another network for traffic between nodes and\\nthe control plane).\\nIf you want to specify particular IP address(es) to proxy the port, you can set the --nodeport-\\naddresses  flag for kube-proxy or the equivalent nodePortAddresses  field of the kube-proxy\\nconfiguration file  to particular IP block(s).\\nThis flag takes a comma-delimited list of IP blocks (e.g. 10.0.0.0/8 , 192.0.2.0/25 ) to specify IP', metadata={'source': './PDFS/Concepts.pdf', 'page': 212}),\n",
       " Document(page_content=\"This flag takes a comma-delimited list of IP blocks (e.g. 10.0.0.0/8 , 192.0.2.0/25 ) to specify IP\\naddress ranges that kube-proxy should consider as local to this node.\\nFor example, if you start kube-proxy with the --nodeport-addresses=127.0.0.0/8  flag, kube-proxy\\nonly selects the loopback interface for NodePort Services. The default for --nodeport-addresses\\nis an empty list. This means that kube-proxy should consider all available network interfaces\\nfor NodePort. (That's also compatible with earlier Kubernetes releases.)\\nNote:  This Service is visible as <NodeIP>:spec.ports[*].nodePort\\nand .spec.clusterIP:spec.ports[*].port . If the --nodeport-addresses  flag for kube-proxy or the\\nequivalent field in the kube-proxy configuration file is set, <NodeIP>  would be a filtered node\\nIP address (or possibly IP addresses).\\ntype: LoadBalancer\\nOn cloud providers which support external load balancers, setting the type field to\", metadata={'source': './PDFS/Concepts.pdf', 'page': 212}),\n",
       " Document(page_content=\"On cloud providers which support external load balancers, setting the type field to \\nLoadBalancer  provisions a load balancer for your Service. The actual creation of the load\\nbalancer happens asynchronously, and information about the provisioned balancer is published\\nin the Service's .status.loadBalancer  field. For example:\", metadata={'source': './PDFS/Concepts.pdf', 'page': 212}),\n",
       " Document(page_content='apiVersion : v1\\nkind: Service\\nmetadata :\\n  name : my-service\\nspec:\\n  selector :\\n    app.kubernetes.io/name : MyApp\\n  ports :\\n    - protocol : TCP\\n      port: 80\\n      targetPort : 9376\\n  clusterIP : 10.0.171.239\\n  type: LoadBalancer\\nstatus :\\n  loadBalancer :\\n    ingress :\\n    - ip: 192.0.2.127\\nTraffic from the external load balancer is directed at the backend Pods. The cloud provider\\ndecides how it is load balanced.\\nTo implement a Service of type: LoadBalancer , Kubernetes typically starts off by making the\\nchanges that are equivalent to you requesting a Service of type: NodePort . The cloud-controller-\\nmanager component then configures the external load balancer to forward traffic to that\\nassigned node port.\\nYou can configure a load balanced Service to omit  assigning a node port, provided that the\\ncloud provider implementation supports this.\\nSome cloud providers allow you to specify the loadBalancerIP . In those cases, the load-balancer', metadata={'source': './PDFS/Concepts.pdf', 'page': 213}),\n",
       " Document(page_content=\"Some cloud providers allow you to specify the loadBalancerIP . In those cases, the load-balancer\\nis created with the user-specified loadBalancerIP . If the loadBalancerIP  field is not specified, the\\nload balancer is set up with an ephemeral IP address. If you specify a loadBalancerIP  but your\\ncloud provider does not support the feature, the loadbalancerIP  field that you set is ignored.\\nNote:\\nThe.spec.loadBalancerIP  field for a Service was deprecated in Kubernetes v1.24.\\nThis field was under-specified and its meaning varies across implementations. It also cannot\\nsupport dual-stack networking. This field may be removed in a future API version.\\nIf you're integrating with a provider that supports specifying the load balancer IP address(es)\\nfor a Service via a (provider specific) annotation, you should switch to doing that.\\nIf you are writing code for a load balancer integration with Kubernetes, avoid using this field.\", metadata={'source': './PDFS/Concepts.pdf', 'page': 213}),\n",
       " Document(page_content='If you are writing code for a load balancer integration with Kubernetes, avoid using this field.\\nYou can integrate with Gateway  rather than Service, or you can define your own (provider\\nspecific) annotations on the Service that specify the equivalent detail.\\nLoad balancers with mixed protocol types\\nFEATURE STATE:  Kubernetes v1.26 [stable]', metadata={'source': './PDFS/Concepts.pdf', 'page': 213}),\n",
       " Document(page_content='By default, for LoadBalancer type of Services, when there is more than one port defined, all\\nports must have the same protocol, and the protocol must be one which is supported by the\\ncloud provider.\\nThe feature gate MixedProtocolLBService  (enabled by default for the kube-apiserver as of v1.24)\\nallows the use of different protocols for LoadBalancer type of Services, when there is more than\\none port defined.\\nNote:  The set of protocols that can be used for load balanced Services is defined by your cloud\\nprovider; they may impose restrictions beyond what the Kubernetes API enforces.\\nDisabling load balancer NodePort allocation\\nFEATURE STATE:  Kubernetes v1.24 [stable]\\nYou can optionally disable node port allocation for a Service of type: LoadBalancer , by setting\\nthe field spec.allocateLoadBalancerNodePorts  to false. This should only be used for load\\nbalancer implementations that route traffic directly to pods as opposed to using node ports. By', metadata={'source': './PDFS/Concepts.pdf', 'page': 214}),\n",
       " Document(page_content=\"balancer implementations that route traffic directly to pods as opposed to using node ports. By\\ndefault, spec.allocateLoadBalancerNodePorts  is true and type LoadBalancer Services will\\ncontinue to allocate node ports. If spec.allocateLoadBalancerNodePorts  is set to false on an\\nexisting Service with allocated node ports, those node ports will not be de-allocated\\nautomatically. You must explicitly remove the nodePorts  entry in every Service port to de-\\nallocate those node ports.\\nSpecifying class of load balancer implementation\\nFEATURE STATE:  Kubernetes v1.24 [stable]\\nFor a Service with type set to LoadBalancer , the .spec.loadBalancerClass  field enables you to use\\na load balancer implementation other than the cloud provider default.\\nBy default, .spec.loadBalancerClass  is not set and a LoadBalancer  type of Service uses the cloud\\nprovider's default load balancer implementation if the cluster is configured with a cloud\\nprovider using the --cloud-provider  component flag.\", metadata={'source': './PDFS/Concepts.pdf', 'page': 214}),\n",
       " Document(page_content='provider using the --cloud-provider  component flag.\\nIf you specify .spec.loadBalancerClass , it is assumed that a load balancer implementation that\\nmatches the specified class is watching for Services. Any default load balancer implementation\\n(for example, the one provided by the cloud provider) will ignore Services that have this field\\nset. spec.loadBalancerClass  can be set on a Service of type LoadBalancer  only. Once set, it\\ncannot be changed. The value of spec.loadBalancerClass  must be a label-style identifier, with an\\noptional prefix such as \" internal-vip \" or \" example.com/internal-vip \". Unprefixed names are\\nreserved for end-users.\\nInternal load balancer\\nIn a mixed environment it is sometimes necessary to route traffic from Services inside the same\\n(virtual) network address block.\\nIn a split-horizon DNS environment you would need two Services to be able to route both\\nexternal and internal traffic to your endpoints.', metadata={'source': './PDFS/Concepts.pdf', 'page': 214}),\n",
       " Document(page_content='To set an internal load balancer, add one of the following annotations to your Service\\ndepending on the cloud service provider you\\'re using:\\nDefault\\nGCP\\nAWS\\nAzure\\nIBM Cloud\\nOpenStack\\nBaidu Cloud\\nTencent Cloud\\nAlibaba Cloud\\nOCI\\nSelect one of the tabs.\\nmetadata :\\n  name : my-service\\n  annotations :\\n      networking.gke.io/load-balancer-type : \"Internal\"\\nmetadata :\\n    name : my-service\\n    annotations :\\n        service.beta.kubernetes.io/aws-load-balancer-internal : \"true\"\\nmetadata :\\n  name : my-service\\n  annotations :\\n      service.beta.kubernetes.io/azure-load-balancer-internal : \"true\"\\nmetadata :\\n  name : my-service\\n  annotations :\\n      service.kubernetes.io/ibm-load-balancer-cloud-provider-ip-type : \"private\"\\nmetadata :\\n  name : my-service\\n  annotations :\\n    service.beta.kubernetes.io/openstack-internal-load-balancer : \"true\"\\nmetadata :\\n  name : my-service\\n  annotations :\\n    service.beta.kubernetes.io/cce-load-balancer-internal-vpc : \"true\"\\nmetadata :\\n  annotations :', metadata={'source': './PDFS/Concepts.pdf', 'page': 215}),\n",
       " Document(page_content='service.beta.kubernetes.io/cce-load-balancer-internal-vpc : \"true\"\\nmetadata :\\n  annotations :\\n    service.kubernetes.io/qcloud-loadbalancer-internal-subnetid : subnet-xxxxx\\nmetadata :\\n  annotations :\\n    service.beta.kubernetes.io/alibaba-cloud-loadbalancer-address-type : \"intranet\"• \\n• \\n• \\n• \\n• \\n• \\n• \\n• \\n• \\n•', metadata={'source': './PDFS/Concepts.pdf', 'page': 215}),\n",
       " Document(page_content='metadata :\\n  name : my-service\\n  annotations :\\n      service.beta.kubernetes.io/oci-load-balancer-internal : true\\ntype: ExternalName\\nServices of type ExternalName map a Service to a DNS name, not to a typical selector such as \\nmy-service  or cassandra . You specify these Services with the spec.externalName  parameter.\\nThis Service definition, for example, maps the my-service  Service in the prod  namespace to \\nmy.database.example.com :\\napiVersion : v1\\nkind: Service\\nmetadata :\\n  name : my-service\\n  namespace : prod\\nspec:\\n  type: ExternalName\\n  externalName : my.database.example.com\\nNote:\\nA Service of type: ExternalName  accepts an IPv4 address string, but treats that string as a DNS\\nname comprised of digits, not as an IP address (the internet does not however allow such names\\nin DNS). Services with external names that resemble IPv4 addresses are not resolved by DNS\\nservers.\\nIf you want to map a Service directly to a specific IP address, consider using headless Services .', metadata={'source': './PDFS/Concepts.pdf', 'page': 216}),\n",
       " Document(page_content=\"If you want to map a Service directly to a specific IP address, consider using headless Services .\\nWhen looking up the host my-service.prod.svc.cluster.local , the cluster DNS Service returns a \\nCNAME  record with the value my.database.example.com . Accessing my-service  works in the\\nsame way as other Services but with the crucial difference that redirection happens at the DNS\\nlevel rather than via proxying or forwarding. Should you later decide to move your database\\ninto your cluster, you can start its Pods, add appropriate selectors or endpoints, and change the\\nService's type.\\nCaution:\\nYou may have trouble using ExternalName for some common protocols, including HTTP and\\nHTTPS. If you use ExternalName then the hostname used by clients inside your cluster is\\ndifferent from the name that the ExternalName references.\\nFor protocols that use hostnames this difference may lead to errors or unexpected responses.\", metadata={'source': './PDFS/Concepts.pdf', 'page': 216}),\n",
       " Document(page_content='For protocols that use hostnames this difference may lead to errors or unexpected responses.\\nHTTP requests will have a Host:  header that the origin server does not recognize; TLS servers\\nwill not be able to provide a certificate matching the hostname that the client connected to.\\nHeadless Services\\nSometimes you don\\'t need load-balancing and a single Service IP. In this case, you can create\\nwhat are termed headless Services , by explicitly specifying \"None\"  for the cluster IP address\\n(.spec.clusterIP ).', metadata={'source': './PDFS/Concepts.pdf', 'page': 216}),\n",
       " Document(page_content=\"You can use a headless Service to interface with other service discovery mechanisms, without\\nbeing tied to Kubernetes' implementation.\\nFor headless Services, a cluster IP is not allocated, kube-proxy does not handle these Services,\\nand there is no load balancing or proxying done by the platform for them. How DNS is\\nautomatically configured depends on whether the Service has selectors defined:\\nWith selectors\\nFor headless Services that define selectors, the endpoints controller creates EndpointSlices in\\nthe Kubernetes API, and modifies the DNS configuration to return A or AAAA records (IPv4 or\\nIPv6 addresses) that point directly to the Pods backing the Service.\\nWithout selectors\\nFor headless Services that do not define selectors, the control plane does not create\\nEndpointSlice objects. However, the DNS system looks for and configures either:\\nDNS CNAME records for type: ExternalName  Services.\\nDNS A / AAAA records for all IP addresses of the Service's ready endpoints, for all\", metadata={'source': './PDFS/Concepts.pdf', 'page': 217}),\n",
       " Document(page_content='DNS A / AAAA records for all IP addresses of the Service\\'s ready endpoints, for all\\nService types other than ExternalName .\\nFor IPv4 endpoints, the DNS system creates A records.\\nFor IPv6 endpoints, the DNS system creates AAAA records.\\nWhen you define a headless Service without a selector, the port must match the targetPort .\\nDiscovering services\\nFor clients running inside your cluster, Kubernetes supports two primary modes of finding a\\nService: environment variables and DNS.\\nEnvironment variables\\nWhen a Pod is run on a Node, the kubelet adds a set of environment variables for each active\\nService. It adds {SVCNAME}_SERVICE_HOST  and {SVCNAME}_SERVICE_PORT  variables,\\nwhere the Service name is upper-cased and dashes are converted to underscores. It also\\nsupports variables (see makeLinkVariables ) that are compatible with Docker Engine\\'s \" legacy\\ncontainer links \" feature.\\nFor example, the Service redis-primary  which exposes TCP port 6379 and has been allocated', metadata={'source': './PDFS/Concepts.pdf', 'page': 217}),\n",
       " Document(page_content='For example, the Service redis-primary  which exposes TCP port 6379 and has been allocated\\ncluster IP address 10.0.0.11, produces the following environment variables:\\nREDIS_PRIMARY_SERVICE_HOST =10.0.0.11\\nREDIS_PRIMARY_SERVICE_PORT =6379\\nREDIS_PRIMARY_PORT =tcp://10.0.0.11:6379\\nREDIS_PRIMARY_PORT_6379_TCP =tcp://10.0.0.11:6379\\nREDIS_PRIMARY_PORT_6379_TCP_PROTO =tcp\\nREDIS_PRIMARY_PORT_6379_TCP_PORT =6379\\nREDIS_PRIMARY_PORT_6379_TCP_ADDR =10.0.0.11\\nNote:• \\n• \\n◦ \\n◦', metadata={'source': './PDFS/Concepts.pdf', 'page': 217}),\n",
       " Document(page_content='When you have a Pod that needs to access a Service, and you are using the environment\\nvariable method to publish the port and cluster IP to the client Pods, you must create the\\nService before  the client Pods come into existence. Otherwise, those client Pods won\\'t have their\\nenvironment variables populated.\\nIf you only use DNS to discover the cluster IP for a Service, you don\\'t need to worry about this\\nordering issue.\\nKubernetes also supports and provides variables that are compatible with Docker Engine\\'s\\n\"legacy container links \" feature. You can read makeLinkVariables  to see how this is implemented\\nin Kubernetes.\\nDNS\\nYou can (and almost always should) set up a DNS service for your Kubernetes cluster using an \\nadd-on .\\nA cluster-aware DNS server, such as CoreDNS, watches the Kubernetes API for new Services\\nand creates a set of DNS records for each one. If DNS has been enabled throughout your cluster\\nthen all Pods should automatically be able to resolve Services by their DNS name.', metadata={'source': './PDFS/Concepts.pdf', 'page': 218}),\n",
       " Document(page_content='then all Pods should automatically be able to resolve Services by their DNS name.\\nFor example, if you have a Service called my-service  in a Kubernetes namespace my-ns , the\\ncontrol plane and the DNS Service acting together create a DNS record for my-service.my-ns .\\nPods in the my-ns  namespace should be able to find the service by doing a name lookup for my-\\nservice  (my-service.my-ns  would also work).\\nPods in other namespaces must qualify the name as my-service.my-ns . These names will resolve\\nto the cluster IP assigned for the Service.\\nKubernetes also supports DNS SRV (Service) records for named ports. If the my-service.my-ns\\nService has a port named http with the protocol set to TCP, you can do a DNS SRV query for \\n_http._tcp.my-service.my-ns  to discover the port number for http, as well as the IP address.\\nThe Kubernetes DNS server is the only way to access ExternalName  Services. You can find more\\ninformation about ExternalName  resolution in DNS for Services and Pods .', metadata={'source': './PDFS/Concepts.pdf', 'page': 218}),\n",
       " Document(page_content='information about ExternalName  resolution in DNS for Services and Pods .\\nVirtual IP addressing mechanism\\nRead Virtual IPs and Service Proxies  explains the mechanism Kubernetes provides to expose a\\nService with a virtual IP address.\\nTraffic policies\\nYou can set the .spec.internalTrafficPolicy  and .spec.externalTrafficPolicy  fields to control how\\nKubernetes routes traffic to healthy (“ready”) backends.\\nSee Traffic Policies  for more details.', metadata={'source': './PDFS/Concepts.pdf', 'page': 218}),\n",
       " Document(page_content='Session stickiness\\nIf you want to make sure that connections from a particular client are passed to the same Pod\\neach time, you can configure session affinity based on the client\\'s IP address. Read session\\naffinity  to learn more.\\nExternal IPs\\nIf there are external IPs that route to one or more cluster nodes, Kubernetes Services can be\\nexposed on those externalIPs . When network traffic arrives into the cluster, with the external IP\\n(as destination IP) and the port matching that Service, rules and routes that Kubernetes has\\nconfigured ensure that the traffic is routed to one of the endpoints for that Service.\\nWhen you define a Service, you can specify externalIPs  for any service type . In the example\\nbelow, the Service named \"my-service\"  can be accessed by clients using TCP, on \\n\"198.51.100.32:80\"  (calculated from .spec.externalIPs[]  and .spec.ports[].port ).\\napiVersion : v1\\nkind: Service\\nmetadata :\\n  name : my-service\\nspec:\\n  selector :\\n    app.kubernetes.io/name : MyApp\\n  ports :', metadata={'source': './PDFS/Concepts.pdf', 'page': 219}),\n",
       " Document(page_content=\"metadata :\\n  name : my-service\\nspec:\\n  selector :\\n    app.kubernetes.io/name : MyApp\\n  ports :\\n    - name : http\\n      protocol : TCP\\n      port: 80\\n      targetPort : 49152\\n  externalIPs :\\n    - 198.51.100.32\\nNote:  Kubernetes does not manage allocation of externalIPs ; these are the responsibility of the\\ncluster administrator.\\nAPI Object\\nService is a top-level resource in the Kubernetes REST API. You can find more details about the \\nService API object .\\nWhat's next\\nLearn more about Services and how they fit into Kubernetes:\\nFollow the Connecting Applications with Services  tutorial.\\nRead about Ingress , which exposes HTTP and HTTPS routes from outside the cluster to\\nServices within your cluster.\\nRead about Gateway , an extension to Kubernetes that provides more flexibility than\\nIngress.• \\n• \\n•\", metadata={'source': './PDFS/Concepts.pdf', 'page': 219}),\n",
       " Document(page_content='For more context, read the following:\\nVirtual IPs and Service Proxies\\nEndpointSlices\\nService API reference\\nEndpointSlice API reference\\nEndpoint API reference (legacy)\\nIngress\\nMake your HTTP (or HTTPS) network service available using a protocol-aware configuration\\nmechanism, that understands web concepts like URIs, hostnames, paths, and more. The Ingress\\nconcept lets you map traffic to different backends based on rules you define via the Kubernetes\\nAPI.\\nFEATURE STATE:  Kubernetes v1.19 [stable]\\nAn API object that manages external access to the services in a cluster, typically HTTP.\\nIngress may provide load balancing, SSL termination and name-based virtual hosting.\\nNote:  Ingress is frozen. New features are being added to the Gateway API .\\nTerminology\\nFor clarity, this guide defines the following terms:\\nNode: A worker machine in Kubernetes, part of a cluster.\\nCluster: A set of Nodes that run containerized applications managed by Kubernetes. For', metadata={'source': './PDFS/Concepts.pdf', 'page': 220}),\n",
       " Document(page_content='Cluster: A set of Nodes that run containerized applications managed by Kubernetes. For\\nthis example, and in most common Kubernetes deployments, nodes in the cluster are not\\npart of the public internet.\\nEdge router: A router that enforces the firewall policy for your cluster. This could be a\\ngateway managed by a cloud provider or a physical piece of hardware.\\nCluster network: A set of links, logical or physical, that facilitate communication within a\\ncluster according to the Kubernetes networking model .\\nService: A Kubernetes Service  that identifies a set of Pods using label  selectors. Unless\\nmentioned otherwise, Services are assumed to have virtual IPs only routable within the\\ncluster network.\\nWhat is Ingress?\\nIngress  exposes HTTP and HTTPS routes from outside the cluster to services  within the cluster.\\nTraffic routing is controlled by rules defined on the Ingress resource.\\nHere is a simple example where an Ingress sends all its traffic to one Service:\\ningress-diagram', metadata={'source': './PDFS/Concepts.pdf', 'page': 220}),\n",
       " Document(page_content='Here is a simple example where an Ingress sends all its traffic to one Service:\\ningress-diagram\\nFigure. Ingress• \\n• \\n• \\n• \\n• \\n• \\n• \\n• \\n• \\n•', metadata={'source': './PDFS/Concepts.pdf', 'page': 220}),\n",
       " Document(page_content='An Ingress may be configured to give Services externally-reachable URLs, load balance traffic,\\nterminate SSL / TLS, and offer name-based virtual hosting. An Ingress controller  is responsible\\nfor fulfilling the Ingress, usually with a load balancer, though it may also configure your edge\\nrouter or additional frontends to help handle the traffic.\\nAn Ingress does not expose arbitrary ports or protocols. Exposing services other than HTTP\\nand HTTPS to the internet typically uses a service of type Service.Type=NodePort  or \\nService.Type=LoadBalancer .\\nPrerequisites\\nYou must have an Ingress controller  to satisfy an Ingress. Only creating an Ingress resource has\\nno effect.\\nYou may need to deploy an Ingress controller such as ingress-nginx . You can choose from a\\nnumber of Ingress controllers .\\nIdeally, all Ingress controllers should fit the reference specification. In reality, the various\\nIngress controllers operate slightly differently.', metadata={'source': './PDFS/Concepts.pdf', 'page': 221}),\n",
       " Document(page_content=\"Ingress controllers operate slightly differently.\\nNote:  Make sure you review your Ingress controller's documentation to understand the caveats\\nof choosing it.\\nThe Ingress resource\\nA minimal Ingress resource example:\\nservice/networking/minimal-ingress.yaml  \\napiVersion : networking.k8s.io/v1\\nkind: Ingress\\nmetadata :\\n  name : minimal-ingress\\n  annotations :\\n    nginx.ingress.kubernetes.io/rewrite-target : /\\nspec:\\n  ingressClassName : nginx-example\\n  rules :\\n  - http:\\n      paths :\\n      - path: /testpath\\n        pathType : Prefix\\n        backend :\\n          service :\\n            name : test\\n            port:\\n              number : 80\\nAn Ingress needs apiVersion , kind, metadata  and spec fields. The name of an Ingress object\\nmust be a valid DNS subdomain name . For general information about working with config files,\\nsee deploying applications , configuring containers , managing resources . Ingress frequently uses\", metadata={'source': './PDFS/Concepts.pdf', 'page': 221}),\n",
       " Document(page_content='see deploying applications , configuring containers , managing resources . Ingress frequently uses\\nannotations to configure some options depending on the Ingress controller, an example of', metadata={'source': './PDFS/Concepts.pdf', 'page': 221}),\n",
       " Document(page_content='which is the rewrite-target annotation . Different Ingress controllers  support different\\nannotations. Review the documentation for your choice of Ingress controller to learn which\\nannotations are supported.\\nThe Ingress spec  has all the information needed to configure a load balancer or proxy server.\\nMost importantly, it contains a list of rules matched against all incoming requests. Ingress\\nresource only supports rules for directing HTTP(S) traffic.\\nIf the ingressClassName  is omitted, a default Ingress class  should be defined.\\nThere are some ingress controllers, that work without the definition of a default IngressClass .\\nFor example, the Ingress-NGINX controller can be configured with a flag --watch-ingress-\\nwithout-class . It is recommended  though, to specify the default IngressClass  as shown below .\\nIngress rules\\nEach HTTP rule contains the following information:\\nAn optional host. In this example, no host is specified, so the rule applies to all inbound', metadata={'source': './PDFS/Concepts.pdf', 'page': 222}),\n",
       " Document(page_content='An optional host. In this example, no host is specified, so the rule applies to all inbound\\nHTTP traffic through the IP address specified. If a host is provided (for example,\\nfoo.bar.com), the rules apply to that host.\\nA list of paths (for example, /testpath ), each of which has an associated backend defined\\nwith a service.name  and a service.port.name  or service.port.number . Both the host and\\npath must match the content of an incoming request before the load balancer directs\\ntraffic to the referenced Service.\\nA backend is a combination of Service and port names as described in the Service doc  or\\na custom resource backend  by way of a CRD . HTTP (and HTTPS) requests to the Ingress\\nthat match the host and path of the rule are sent to the listed backend.\\nA defaultBackend  is often configured in an Ingress controller to service any requests that do not\\nmatch a path in the spec.\\nDefaultBackend', metadata={'source': './PDFS/Concepts.pdf', 'page': 222}),\n",
       " Document(page_content='match a path in the spec.\\nDefaultBackend\\nAn Ingress with no rules sends all traffic to a single default backend and .spec.defaultBackend  is\\nthe backend that should handle requests in that case. The defaultBackend  is conventionally a\\nconfiguration option of the Ingress controller  and is not specified in your Ingress resources. If\\nno .spec.rules  are specified, .spec.defaultBackend  must be specified. If defaultBackend  is not set,\\nthe handling of requests that do not match any of the rules will be up to the ingress controller\\n(consult the documentation for your ingress controller to find out how it handles this case).\\nIf none of the hosts or paths match the HTTP request in the Ingress objects, the traffic is routed\\nto your default backend.\\nResource backends\\nA Resource  backend is an ObjectRef to another Kubernetes resource within the same\\nnamespace as the Ingress object. A Resource  is a mutually exclusive setting with Service, and', metadata={'source': './PDFS/Concepts.pdf', 'page': 222}),\n",
       " Document(page_content='namespace as the Ingress object. A Resource  is a mutually exclusive setting with Service, and\\nwill fail validation if both are specified. A common usage for a Resource  backend is to ingress\\ndata to an object storage backend with static assets.\\nservice/networking/ingress-resource-backend.yaml  • \\n• \\n•', metadata={'source': './PDFS/Concepts.pdf', 'page': 222}),\n",
       " Document(page_content='apiVersion : networking.k8s.io/v1\\nkind: Ingress\\nmetadata :\\n  name : ingress-resource-backend\\nspec:\\n  defaultBackend :\\n    resource :\\n      apiGroup : k8s.example.com\\n      kind: StorageBucket\\n      name : static-assets\\n  rules :\\n    - http:\\n        paths :\\n          - path: /icons\\n            pathType : ImplementationSpecific\\n            backend :\\n              resource :\\n                apiGroup : k8s.example.com\\n                kind: StorageBucket\\n                name : icon-assets\\nAfter creating the Ingress above, you can view it with the following command:\\nkubectl describe ingress ingress-resource-backend\\nName:             ingress-resource-backend\\nNamespace:        default\\nAddress:\\nDefault backend:  APIGroup: k8s.example.com, Kind: StorageBucket, Name: static-assets\\nRules:\\n  Host        Path  Backends\\n  ----        ----  --------\\n  *\\n              /icons   APIGroup: k8s.example.com, Kind: StorageBucket, Name: icon-assets\\nAnnotations:  <none>\\nEvents:       <none>\\nPath types', metadata={'source': './PDFS/Concepts.pdf', 'page': 223}),\n",
       " Document(page_content='Annotations:  <none>\\nEvents:       <none>\\nPath types\\nEach path in an Ingress is required to have a corresponding path type. Paths that do not include\\nan explicit pathType  will fail validation. There are three supported path types:\\nImplementationSpecific : With this path type, matching is up to the IngressClass.\\nImplementations can treat this as a separate pathType  or treat it identically to Prefix  or \\nExact  path types.\\nExact : Matches the URL path exactly and with case sensitivity.\\nPrefix : Matches based on a URL path prefix split by /. Matching is case sensitive and done\\non a path element by element basis. A path element refers to the list of labels in the path\\nsplit by the / separator. A request is a match for path p if every p is an element-wise\\nprefix of p of the request path.• \\n• \\n•', metadata={'source': './PDFS/Concepts.pdf', 'page': 223}),\n",
       " Document(page_content='Note:  If the last element of the path is a substring of the last element in request path, it is\\nnot a match (for example: /foo/bar  matches /foo/bar/baz , but does not match /foo/barbaz ).\\nExamples\\nKind Path(s) Request path(s) Matches?\\nPrefix / (all paths) Yes\\nExact /foo /foo Yes\\nExact /foo /bar No\\nExact /foo /foo/ No\\nExact /foo/ /foo No\\nPrefix /foo /foo, /foo/ Yes\\nPrefix /foo/ /foo, /foo/ Yes\\nPrefix /aaa/bb /aaa/bbb No\\nPrefix /aaa/bbb /aaa/bbb Yes\\nPrefix /aaa/bbb/ /aaa/bbb Yes, ignores trailing slash\\nPrefix /aaa/bbb /aaa/bbb/ Yes, matches trailing slash\\nPrefix /aaa/bbb /aaa/bbb/ccc Yes, matches subpath\\nPrefix /aaa/bbb /aaa/bbbxyz No, does not match string prefix\\nPrefix /, /aaa /aaa/ccc Yes, matches /aaa prefix\\nPrefix /, /aaa, /aaa/bbb /aaa/bbb Yes, matches /aaa/bbb  prefix\\nPrefix /, /aaa, /aaa/bbb /ccc Yes, matches / prefix\\nPrefix /aaa /ccc No, uses default backend\\nMixed /foo (Prefix), /foo (Exact) /foo Yes, prefers Exact\\nMultiple matches', metadata={'source': './PDFS/Concepts.pdf', 'page': 224}),\n",
       " Document(page_content='Mixed /foo (Prefix), /foo (Exact) /foo Yes, prefers Exact\\nMultiple matches\\nIn some cases, multiple paths within an Ingress will match a request. In those cases precedence\\nwill be given first to the longest matching path. If two paths are still equally matched,\\nprecedence will be given to paths with an exact path type over prefix path type.\\nHostname wildcards\\nHosts can be precise matches (for example “ foo.bar.com ”) or a wildcard (for example\\n“*.foo.com ”). Precise matches require that the HTTP host header matches the host field.\\nWildcard matches require the HTTP host header is equal to the suffix of the wildcard rule.\\nHost Host header Match?\\n*.foo.com bar.foo.com Matches based on shared suffix\\n*.foo.com baz.bar.foo.com No match, wildcard only covers a single DNS label\\n*.foo.com foo.com No match, wildcard only covers a single DNS label\\nservice/networking/ingress-wildcard-host.yaml  \\napiVersion : networking.k8s.io/v1\\nkind: Ingress\\nmetadata :\\n  name : ingress-wildcard-host\\nspec:', metadata={'source': './PDFS/Concepts.pdf', 'page': 224}),\n",
       " Document(page_content='rules :\\n  - host: \"foo.bar.com\"\\n    http:\\n      paths :\\n      - pathType : Prefix\\n        path: \"/bar\"\\n        backend :\\n          service :\\n            name : service1\\n            port:\\n              number : 80\\n  - host: \"*.foo.com\"\\n    http:\\n      paths :\\n      - pathType : Prefix\\n        path: \"/foo\"\\n        backend :\\n          service :\\n            name : service2\\n            port:\\n              number : 80\\nIngress class\\nIngresses can be implemented by different controllers, often with different configuration. Each\\nIngress should specify a class, a reference to an IngressClass resource that contains additional\\nconfiguration including the name of the controller that should implement the class.\\nservice/networking/external-lb.yaml  \\napiVersion : networking.k8s.io/v1\\nkind: IngressClass\\nmetadata :\\n  name : external-lb\\nspec:\\n  controller : example.com/ingress-controller\\n  parameters :\\n    apiGroup : k8s.example.com\\n    kind: IngressParameters\\n    name : external-lb', metadata={'source': './PDFS/Concepts.pdf', 'page': 225}),\n",
       " Document(page_content='parameters :\\n    apiGroup : k8s.example.com\\n    kind: IngressParameters\\n    name : external-lb\\nThe .spec.parameters  field of an IngressClass lets you reference another resource that provides\\nconfiguration related to that IngressClass.\\nThe specific type of parameters to use depends on the ingress controller that you specify in the\\n.spec.controller  field of the IngressClass.\\nIngressClass scope\\nDepending on your ingress controller, you may be able to use parameters that you set cluster-\\nwide, or just for one namespace.\\nCluster•', metadata={'source': './PDFS/Concepts.pdf', 'page': 225}),\n",
       " Document(page_content='Namespaced\\nThe default scope for IngressClass parameters is cluster-wide.\\nIf you set the .spec.parameters  field and don\\'t set .spec.parameters.scope , or if you\\nset .spec.parameters.scope  to Cluster , then the IngressClass refers to a cluster-scoped resource.\\nThe kind (in combination the apiGroup ) of the parameters refers to a cluster-scoped API\\n(possibly a custom resource), and the name  of the parameters identifies a specific cluster scoped\\nresource for that API.\\nFor example:\\n---\\napiVersion : networking.k8s.io/v1\\nkind: IngressClass\\nmetadata :\\n  name : external-lb-1\\nspec:\\n  controller : example.com/ingress-controller\\n  parameters :\\n    # The parameters for this IngressClass are specified in a\\n    # ClusterIngressParameter (API group k8s.example.net) named\\n    # \"external-config-1\". This definition tells Kubernetes to\\n    # look for a cluster-scoped parameter resource.\\n    scope : Cluster\\n    apiGroup : k8s.example.net\\n    kind: ClusterIngressParameter\\n    name : external-config-1', metadata={'source': './PDFS/Concepts.pdf', 'page': 226}),\n",
       " Document(page_content=\"apiGroup : k8s.example.net\\n    kind: ClusterIngressParameter\\n    name : external-config-1\\nFEATURE STATE:  Kubernetes v1.23 [stable]\\nIf you set the .spec.parameters  field and set .spec.parameters.scope  to Namespace , then the\\nIngressClass refers to a namespaced-scoped resource. You must also set the namespace  field\\nwithin .spec.parameters  to the namespace that contains the parameters you want to use.\\nThe kind (in combination the apiGroup ) of the parameters refers to a namespaced API (for\\nexample: ConfigMap), and the name  of the parameters identifies a specific resource in the\\nnamespace you specified in namespace .\\nNamespace-scoped parameters help the cluster operator delegate control over the configuration\\n(for example: load balancer settings, API gateway definition) that is used for a workload. If you\\nused a cluster-scoped parameter then either:\\nthe cluster operator team needs to approve a different team's changes every time there's a\\nnew configuration change being applied.\", metadata={'source': './PDFS/Concepts.pdf', 'page': 226}),\n",
       " Document(page_content='new configuration change being applied.\\nthe cluster operator must define specific access controls, such as RBAC  roles and\\nbindings, that let the application team make changes to the cluster-scoped parameters\\nresource.\\nThe IngressClass API itself is always cluster-scoped.\\nHere is an example of an IngressClass that refers to parameters that are namespaced:• \\n• \\n•', metadata={'source': './PDFS/Concepts.pdf', 'page': 226}),\n",
       " Document(page_content='---\\napiVersion : networking.k8s.io/v1\\nkind: IngressClass\\nmetadata :\\n  name : external-lb-2\\nspec:\\n  controller : example.com/ingress-controller\\n  parameters :\\n    # The parameters for this IngressClass are specified in an\\n    # IngressParameter (API group k8s.example.com) named \"external-config\",\\n    # that\\'s in the \"external-configuration\" namespace.\\n    scope : Namespace\\n    apiGroup : k8s.example.com\\n    kind: IngressParameter\\n    namespace : external-configuration\\n    name : external-config\\nDeprecated annotation\\nBefore the IngressClass resource and ingressClassName  field were added in Kubernetes 1.18,\\nIngress classes were specified with a kubernetes.io/ingress.class  annotation on the Ingress. This\\nannotation was never formally defined, but was widely supported by Ingress controllers.\\nThe newer ingressClassName  field on Ingresses is a replacement for that annotation, but is not\\na direct equivalent. While the annotation was generally used to reference the name of the', metadata={'source': './PDFS/Concepts.pdf', 'page': 227}),\n",
       " Document(page_content=\"a direct equivalent. While the annotation was generally used to reference the name of the\\nIngress controller that should implement the Ingress, the field is a reference to an IngressClass\\nresource that contains additional Ingress configuration, including the name of the Ingress\\ncontroller.\\nDefault IngressClass\\nYou can mark a particular IngressClass as default for your cluster. Setting the \\ningressclass.kubernetes.io/is-default-class  annotation to true on an IngressClass resource will\\nensure that new Ingresses without an ingressClassName  field specified will be assigned this\\ndefault IngressClass.\\nCaution:  If you have more than one IngressClass marked as the default for your cluster, the\\nadmission controller prevents creating new Ingress objects that don't have an \\ningressClassName  specified. You can resolve this by ensuring that at most 1 IngressClass is\\nmarked as default in your cluster.\", metadata={'source': './PDFS/Concepts.pdf', 'page': 227}),\n",
       " Document(page_content='marked as default in your cluster.\\nThere are some ingress controllers, that work without the definition of a default IngressClass .\\nFor example, the Ingress-NGINX controller can be configured with a flag --watch-ingress-\\nwithout-class . It is recommended  though, to specify the default IngressClass :\\nservice/networking/default-ingressclass.yaml  \\napiVersion : networking.k8s.io/v1\\nkind: IngressClass\\nmetadata :\\n  labels :\\n    app.kubernetes.io/component : controller\\n  name : nginx-example', metadata={'source': './PDFS/Concepts.pdf', 'page': 227}),\n",
       " Document(page_content='annotations :\\n    ingressclass.kubernetes.io/is-default-class : \"true\"\\nspec:\\n  controller : k8s.io/ingress-nginx\\nTypes of Ingress\\nIngress backed by a single Service\\nThere are existing Kubernetes concepts that allow you to expose a single Service (see \\nalternatives ). You can also do this with an Ingress by specifying a default backend  with no rules.\\nservice/networking/test-ingress.yaml  \\napiVersion : networking.k8s.io/v1\\nkind: Ingress\\nmetadata :\\n  name : test-ingress\\nspec:\\n  defaultBackend :\\n    service :\\n      name : test\\n      port:\\n        number : 80\\nIf you create it using kubectl apply -f  you should be able to view the state of the Ingress you\\nadded:\\nkubectl get ingress test-ingress\\nNAME           CLASS         HOSTS   ADDRESS         PORTS   AGE\\ntest-ingress   external-lb   *       203.0.113.123   80      59s\\nWhere 203.0.113.123  is the IP allocated by the Ingress controller to satisfy this Ingress.', metadata={'source': './PDFS/Concepts.pdf', 'page': 228}),\n",
       " Document(page_content='Where 203.0.113.123  is the IP allocated by the Ingress controller to satisfy this Ingress.\\nNote:  Ingress controllers and load balancers may take a minute or two to allocate an IP address.\\nUntil that time, you often see the address listed as <pending> .\\nSimple fanout\\nA fanout configuration routes traffic from a single IP address to more than one Service, based\\non the HTTP URI being requested. An Ingress allows you to keep the number of load balancers\\ndown to a minimum. For example, a setup like:\\ningress-fanout-diagram\\nFigure. Ingress Fan Out\\nIt would require an Ingress such as:\\nservice/networking/simple-fanout-example.yaml', metadata={'source': './PDFS/Concepts.pdf', 'page': 228}),\n",
       " Document(page_content='apiVersion : networking.k8s.io/v1\\nkind: Ingress\\nmetadata :\\n  name : simple-fanout-example\\nspec:\\n  rules :\\n  - host: foo.bar.com\\n    http:\\n      paths :\\n      - path: /foo\\n        pathType : Prefix\\n        backend :\\n          service :\\n            name : service1\\n            port:\\n              number : 4200\\n      - path: /bar\\n        pathType : Prefix\\n        backend :\\n          service :\\n            name : service2\\n            port:\\n              number : 8080\\nWhen you create the Ingress with kubectl apply -f :\\nkubectl describe ingress simple-fanout-example\\nName:             simple-fanout-example\\nNamespace:        default\\nAddress:          178.91.123.132\\nDefault backend:  default-http-backend:80 (10.8.2.3:8080)\\nRules:\\n  Host         Path  Backends\\n  ----         ----  --------\\n  foo.bar.com\\n               /foo   service1:4200 (10.8.0.90:4200)\\n               /bar   service2:8080 (10.8.0.91:8080)\\nEvents:\\n  Type     Reason  Age                From                     Message', metadata={'source': './PDFS/Concepts.pdf', 'page': 229}),\n",
       " Document(page_content='Events:\\n  Type     Reason  Age                From                     Message\\n  ----     ------  ----               ----                     -------\\n  Normal   ADD     22s                loadbalancer-controller  default/test\\nThe Ingress controller provisions an implementation-specific load balancer that satisfies the\\nIngress, as long as the Services ( service1 , service2 ) exist. When it has done so, you can see the\\naddress of the load balancer at the Address field.\\nNote:  Depending on the Ingress controller  you are using, you may need to create a default-\\nhttp-backend Service .', metadata={'source': './PDFS/Concepts.pdf', 'page': 229}),\n",
       " Document(page_content='Name based virtual hosting\\nName-based virtual hosts support routing HTTP traffic to multiple host names at the same IP\\naddress.\\ningress-namebase-diagram\\nFigure. Ingress Name Based Virtual hosting\\nThe following Ingress tells the backing load balancer to route requests based on the Host\\nheader .\\nservice/networking/name-virtual-host-ingress.yaml  \\napiVersion : networking.k8s.io/v1\\nkind: Ingress\\nmetadata :\\n  name : name-virtual-host-ingress\\nspec:\\n  rules :\\n  - host: foo.bar.com\\n    http:\\n      paths :\\n      - pathType : Prefix\\n        path: \"/\"\\n        backend :\\n          service :\\n            name : service1\\n            port:\\n              number : 80\\n  - host: bar.foo.com\\n    http:\\n      paths :\\n      - pathType : Prefix\\n        path: \"/\"\\n        backend :\\n          service :\\n            name : service2\\n            port:\\n              number : 80\\nIf you create an Ingress resource without any hosts defined in the rules, then any web traffic to', metadata={'source': './PDFS/Concepts.pdf', 'page': 230}),\n",
       " Document(page_content=\"If you create an Ingress resource without any hosts defined in the rules, then any web traffic to\\nthe IP address of your Ingress controller can be matched without a name based virtual host\\nbeing required.\\nFor example, the following Ingress routes traffic requested for first.bar.com  to service1 , \\nsecond.bar.com  to service2 , and any traffic whose request host header doesn't match \\nfirst.bar.com  and second.bar.com  to service3 .\\nservice/networking/name-virtual-host-ingress-no-third-host.yaml  \\napiVersion : networking.k8s.io/v1\\nkind: Ingress\\nmetadata :\", metadata={'source': './PDFS/Concepts.pdf', 'page': 230}),\n",
       " Document(page_content='name : name-virtual-host-ingress-no-third-host\\nspec:\\n  rules :\\n  - host: first.bar.com\\n    http:\\n      paths :\\n      - pathType : Prefix\\n        path: \"/\"\\n        backend :\\n          service :\\n            name : service1\\n            port:\\n              number : 80\\n  - host: second.bar.com\\n    http:\\n      paths :\\n      - pathType : Prefix\\n        path: \"/\"\\n        backend :\\n          service :\\n            name : service2\\n            port:\\n              number : 80\\n  - http:\\n      paths :\\n      - pathType : Prefix\\n        path: \"/\"\\n        backend :\\n          service :\\n            name : service3\\n            port:\\n              number : 80\\nTLS\\nYou can secure an Ingress by specifying a Secret  that contains a TLS private key and certificate.\\nThe Ingress resource only supports a single TLS port, 443, and assumes TLS termination at the\\ningress point (traffic to the Service and its Pods is in plaintext). If the TLS configuration section', metadata={'source': './PDFS/Concepts.pdf', 'page': 231}),\n",
       " Document(page_content='in an Ingress specifies different hosts, they are multiplexed on the same port according to the\\nhostname specified through the SNI TLS extension (provided the Ingress controller supports\\nSNI). The TLS secret must contain keys named tls.crt  and tls.key  that contain the certificate and\\nprivate key to use for TLS. For example:\\napiVersion : v1\\nkind: Secret\\nmetadata :\\n  name : testsecret-tls\\n  namespace : default\\ndata:\\n  tls.crt : base64 encoded cert\\n  tls.key : base64 encoded key\\ntype: kubernetes.io/tls', metadata={'source': './PDFS/Concepts.pdf', 'page': 231}),\n",
       " Document(page_content='Referencing this secret in an Ingress tells the Ingress controller to secure the channel from the\\nclient to the load balancer using TLS. You need to make sure the TLS secret you created came\\nfrom a certificate that contains a Common Name (CN), also known as a Fully Qualified Domain\\nName (FQDN) for https-example.foo.com .\\nNote:  Keep in mind that TLS will not work on the default rule because the certificates would\\nhave to be issued for all the possible sub-domains. Therefore, hosts  in the tls section need to\\nexplicitly match the host in the rules  section.\\nservice/networking/tls-example-ingress.yaml  \\napiVersion : networking.k8s.io/v1\\nkind: Ingress\\nmetadata :\\n  name : tls-example-ingress\\nspec:\\n  tls:\\n  - hosts :\\n      - https-example.foo.com\\n    secretName : testsecret-tls\\n  rules :\\n  - host: https-example.foo.com\\n    http:\\n      paths :\\n      - path: /\\n        pathType : Prefix\\n        backend :\\n          service :\\n            name : service1\\n            port:', metadata={'source': './PDFS/Concepts.pdf', 'page': 232}),\n",
       " Document(page_content=\"backend :\\n          service :\\n            name : service1\\n            port:\\n              number : 80\\nNote:  There is a gap between TLS features supported by various Ingress controllers. Please\\nrefer to documentation on nginx , GCE , or any other platform specific Ingress controller to\\nunderstand how TLS works in your environment.\\nLoad balancing\\nAn Ingress controller is bootstrapped with some load balancing policy settings that it applies to\\nall Ingress, such as the load balancing algorithm, backend weight scheme, and others. More\\nadvanced load balancing concepts (e.g. persistent sessions, dynamic weights) are not yet\\nexposed through the Ingress. You can instead get these features through the load balancer used\\nfor a Service.\\nIt's also worth noting that even though health checks are not exposed directly through the\\nIngress, there exist parallel concepts in Kubernetes such as readiness probes  that allow you to\", metadata={'source': './PDFS/Concepts.pdf', 'page': 232}),\n",
       " Document(page_content='Ingress, there exist parallel concepts in Kubernetes such as readiness probes  that allow you to\\nachieve the same end result. Please review the controller specific documentation to see how\\nthey handle health checks (for example: nginx , or GCE ).\\nUpdating an Ingress\\nTo update an existing Ingress to add a new Host, you can update it by editing the resource:\\nkubectl describe ingress test', metadata={'source': './PDFS/Concepts.pdf', 'page': 232}),\n",
       " Document(page_content='Name:             test\\nNamespace:        default\\nAddress:          178.91.123.132\\nDefault backend:  default-http-backend:80 (10.8.2.3:8080)\\nRules:\\n  Host         Path  Backends\\n  ----         ----  --------\\n  foo.bar.com\\n               /foo   service1:80 (10.8.0.90:80)\\nAnnotations:\\n  nginx.ingress.kubernetes.io/rewrite-target:  /\\nEvents:\\n  Type     Reason  Age                From                     Message\\n  ----     ------  ----               ----                     -------\\n  Normal   ADD     35s                loadbalancer-controller  default/test\\nkubectl edit ingress test\\nThis pops up an editor with the existing configuration in YAML format. Modify it to include the\\nnew Host:\\nspec:\\n  rules :\\n  - host: foo.bar.com\\n    http:\\n      paths :\\n      - backend :\\n          service :\\n            name : service1\\n            port:\\n              number : 80\\n        path: /foo\\n        pathType : Prefix\\n  - host: bar.baz.com\\n    http:\\n      paths :\\n      - backend :\\n          service :', metadata={'source': './PDFS/Concepts.pdf', 'page': 233}),\n",
       " Document(page_content='- host: bar.baz.com\\n    http:\\n      paths :\\n      - backend :\\n          service :\\n            name : service2\\n            port:\\n              number : 80\\n        path: /foo\\n        pathType : Prefix\\n..\\nAfter you save your changes, kubectl updates the resource in the API server, which tells the\\nIngress controller to reconfigure the load balancer.\\nVerify this:\\nkubectl describe ingress test\\nName:             test\\nNamespace:        default', metadata={'source': './PDFS/Concepts.pdf', 'page': 233}),\n",
       " Document(page_content=\"Address:          178.91.123.132\\nDefault backend:  default-http-backend:80 (10.8.2.3:8080)\\nRules:\\n  Host         Path  Backends\\n  ----         ----  --------\\n  foo.bar.com\\n               /foo   service1:80 (10.8.0.90:80)\\n  bar.baz.com\\n               /foo   service2:80 (10.8.0.91:80)\\nAnnotations:\\n  nginx.ingress.kubernetes.io/rewrite-target:  /\\nEvents:\\n  Type     Reason  Age                From                     Message\\n  ----     ------  ----               ----                     -------\\n  Normal   ADD     45s                loadbalancer-controller  default/test\\nYou can achieve the same outcome by invoking kubectl replace -f  on a modified Ingress YAML\\nfile.\\nFailing across availability zones\\nTechniques for spreading traffic across failure domains differ between cloud providers. Please\\ncheck the documentation of the relevant Ingress controller  for details.\\nAlternatives\\nYou can expose a Service in multiple ways that don't directly involve the Ingress resource:\", metadata={'source': './PDFS/Concepts.pdf', 'page': 234}),\n",
       " Document(page_content=\"You can expose a Service in multiple ways that don't directly involve the Ingress resource:\\nUse Service.Type=LoadBalancer\\nUse Service.Type=NodePort\\nWhat's next\\nLearn about the Ingress  API\\nLearn about Ingress controllers\\nSet up Ingress on Minikube with the NGINX Controller\\nIngress Controllers\\nIn order for an Ingress  to work in your cluster, there must be an ingress controller  running. You\\nneed to select at least one ingress controller and make sure it is set up in your cluster. This page\\nlists common ingress controllers that you can deploy.\\nIn order for the Ingress resource to work, the cluster must have an ingress controller running.\\nUnlike other types of controllers which run as part of the kube-controller-manager  binary,\\nIngress controllers are not started automatically with a cluster. Use this page to choose the\\ningress controller implementation that best fits your cluster.\\nKubernetes as a project supports and maintains AWS , GCE , and nginx  ingress controllers.• \\n• \\n• \\n• \\n•\", metadata={'source': './PDFS/Concepts.pdf', 'page': 234}),\n",
       " Document(page_content=\"Additional controllers\\nNote:  This section links to third party projects that provide functionality required by\\nKubernetes. The Kubernetes project authors aren't responsible for these projects, which are\\nlisted alphabetically. To add a project to this list, read the content guide  before submitting a\\nchange. More information.\\nAKS Application Gateway Ingress Controller  is an ingress controller that configures the \\nAzure Application Gateway .\\nAlibaba Cloud MSE Ingress  is an ingress controller that configures the Alibaba Cloud\\nNative Gateway , which is also the commercial version of Higress .\\nApache APISIX ingress controller  is an Apache APISIX -based ingress controller.\\nAvi Kubernetes Operator  provides L4-L7 load-balancing using VMware NSX Advanced\\nLoad Balancer .\\nBFE Ingress Controller  is a BFE-based ingress controller.\\nCilium Ingress Controller  is an ingress controller powered by Cilium .\\nThe Citrix ingress controller  works with Citrix Application Delivery Controller.\", metadata={'source': './PDFS/Concepts.pdf', 'page': 235}),\n",
       " Document(page_content='The Citrix ingress controller  works with Citrix Application Delivery Controller.\\nContour  is an Envoy  based ingress controller.\\nEmissary-Ingress  API Gateway is an Envoy -based ingress controller.\\nEnRoute  is an Envoy  based API gateway that can run as an ingress controller.\\nEasegress IngressController  is an Easegress  based API gateway that can run as an ingress\\ncontroller.\\nF5 BIG-IP Container Ingress Services for Kubernetes  lets you use an Ingress to configure\\nF5 BIG-IP virtual servers.\\nFortiADC Ingress Controller  support the Kubernetes Ingress resources and allows you to\\nmanage FortiADC objects from Kubernetes\\nGloo  is an open-source ingress controller based on Envoy , which offers API gateway\\nfunctionality.\\nHAProxy Ingress  is an ingress controller for HAProxy .\\nHigress  is an Envoy  based API gateway that can run as an ingress controller.\\nThe HAProxy Ingress Controller for Kubernetes  is also an ingress controller for HAProxy .', metadata={'source': './PDFS/Concepts.pdf', 'page': 235}),\n",
       " Document(page_content='The HAProxy Ingress Controller for Kubernetes  is also an ingress controller for HAProxy .\\nIstio Ingress  is an Istio based ingress controller.\\nThe Kong Ingress Controller for Kubernetes  is an ingress controller driving Kong\\nGateway .\\nKusk Gateway  is an OpenAPI-driven ingress controller based on Envoy .\\nThe NGINX Ingress Controller for Kubernetes  works with the NGINX  webserver (as a\\nproxy).\\nThe ngrok Kubernetes Ingress Controller  is an open source controller for adding secure\\npublic access to your K8s services using the ngrok platform .\\nThe OCI Native Ingress Controller  is an Ingress controller for Oracle Cloud Infrastructure\\nwhich allows you to manage the OCI Load Balancer .\\nThe Pomerium Ingress Controller  is based on Pomerium , which offers context-aware\\naccess policy.\\nSkipper  HTTP router and reverse proxy for service composition, including use cases like\\nKubernetes Ingress, designed as a library to build your custom proxy.', metadata={'source': './PDFS/Concepts.pdf', 'page': 235}),\n",
       " Document(page_content='Kubernetes Ingress, designed as a library to build your custom proxy.\\nThe Traefik Kubernetes Ingress provider  is an ingress controller for the Traefik  proxy.\\nTyk Operator  extends Ingress with Custom Resources to bring API Management\\ncapabilities to Ingress. Tyk Operator works with the Open Source Tyk Gateway & Tyk\\nCloud control plane.\\nVoyager  is an ingress controller for HAProxy .\\nWallarm Ingress Controller  is an Ingress Controller that provides WAAP (WAF) and API\\nSecurity capabilities.• \\n• \\n• \\n• \\n• \\n• \\n• \\n• \\n• \\n• \\n• \\n• \\n• \\n• \\n• \\n• \\n• \\n• \\n• \\n• \\n• \\n• \\n• \\n• \\n• \\n• \\n• \\n• \\n•', metadata={'source': './PDFS/Concepts.pdf', 'page': 235}),\n",
       " Document(page_content='Using multiple Ingress controllers\\nYou may deploy any number of ingress controllers using ingress class  within a cluster. Note\\nthe .metadata.name  of your ingress class resource. When you create an ingress you would need\\nthat name to specify the ingressClassName  field on your Ingress object (refer to IngressSpec v1\\nreference ). ingressClassName  is a replacement of the older annotation method .\\nIf you do not specify an IngressClass for an Ingress, and your cluster has exactly one\\nIngressClass marked as default, then Kubernetes applies  the cluster\\'s default IngressClass to the\\nIngress. You mark an IngressClass as default by setting the ingressclass.kubernetes.io/is-default-\\nclass  annotation  on that IngressClass, with the string value \"true\" .\\nIdeally, all ingress controllers should fulfill this specification, but the various ingress controllers\\noperate slightly differently.\\nNote:  Make sure you review your ingress controller\\'s documentation to understand the caveats\\nof choosing it.', metadata={'source': './PDFS/Concepts.pdf', 'page': 236}),\n",
       " Document(page_content=\"of choosing it.\\nWhat's next\\nLearn more about Ingress .\\nSet up Ingress on Minikube with the NGINX Controller .\\nGateway API\\nGateway API is a family of API kinds that provide dynamic infrastructure provisioning and\\nadvanced traffic routing.\\nMake network services available by using an extensible, role-oriented, protocol-aware\\nconfiguration mechanism. Gateway API  is an add-on  containing API kinds  that provide\\ndynamic infrastructure provisioning and advanced traffic routing.\\nDesign principles\\nThe following principles shaped the design and architecture of Gateway API:\\nRole-oriented:  Gateway API kinds are modeled after organizational roles that are\\nresponsible for managing Kubernetes service networking:\\nInfrastructure Provider:  Manages infrastructure that allows multiple isolated\\nclusters to serve multiple tenants, e.g. a cloud provider.\\nCluster Operator:  Manages clusters and is typically concerned with policies,\\nnetwork access, application permissions, etc.\", metadata={'source': './PDFS/Concepts.pdf', 'page': 236}),\n",
       " Document(page_content='network access, application permissions, etc.\\nApplication Developer:  Manages an application running in a cluster and is\\ntypically concerned with application-level configuration and Service  composition.\\nPortable:  Gateway API specifications are defined as custom resources  and are supported\\nby many implementations .\\nExpressive:  Gateway API kinds support functionality for common traffic routing use\\ncases such as header-based matching, traffic weighting, and others that were only\\npossible in Ingress  by using custom annotations.• \\n• \\n• \\n◦ \\n◦ \\n◦ \\n• \\n•', metadata={'source': './PDFS/Concepts.pdf', 'page': 236}),\n",
       " Document(page_content='Extensible:  Gateway allows for custom resources to be linked at various layers of the\\nAPI. This makes granular customization possible at the appropriate places within the API\\nstructure.\\nResource model\\nGateway API has three stable API kinds:\\nGatewayClass:  Defines a set of gateways with common configuration and managed by a\\ncontroller that implements the class.\\nGateway:  Defines an instance of traffic handling infrastructure, such as cloud load\\nbalancer.\\nHTTPRoute:  Defines HTTP-specific rules for mapping traffic from a Gateway listener to\\na representation of backend network endpoints. These endpoints are often represented as\\na Service .\\nGateway API is organized into different API kinds that have interdependent relationships to\\nsupport the role-oriented nature of organizations. A Gateway object is associated with exactly\\none GatewayClass; the GatewayClass describes the gateway controller responsible for', metadata={'source': './PDFS/Concepts.pdf', 'page': 237}),\n",
       " Document(page_content='one GatewayClass; the GatewayClass describes the gateway controller responsible for\\nmanaging Gateways of this class. One or more route kinds such as HTTPRoute, are then\\nassociated to Gateways. A Gateway can filter the routes that may be attached to its listeners ,\\nforming a bidirectional trust model with routes.\\nThe following figure illustrates the relationships of the three stable Gateway API kinds:\\nA figure illustrating the relationships of the three stable Gateway API kinds\\nGatewayClass\\nGateways can be implemented by different controllers, often with different configurations. A\\nGateway must reference a GatewayClass that contains the name of the controller that\\nimplements the class.\\nA minimal GatewayClass example:\\napiVersion : gateway.networking.k8s.io/v1\\nkind: GatewayClass\\nmetadata :\\n  name : example-class\\nspec:\\n  controllerName : example.com/gateway-controller\\nIn this example, a controller that has implemented Gateway API is configured to manage', metadata={'source': './PDFS/Concepts.pdf', 'page': 237}),\n",
       " Document(page_content=\"In this example, a controller that has implemented Gateway API is configured to manage\\nGatewayClasses with the controller name example.com/gateway-controller . Gateways of this\\nclass will be managed by the implementation's controller.\\nSee the GatewayClass  reference for a full definition of this API kind.• \\n• \\n• \\n•\", metadata={'source': './PDFS/Concepts.pdf', 'page': 237}),\n",
       " Document(page_content=\"Gateway\\nA Gateway describes an instance of traffic handling infrastructure. It defines a network\\nendpoint that can be used for processing traffic, i.e. filtering, balancing, splitting, etc. for\\nbackends such as a Service. For example, a Gateway may represent a cloud load balancer or an\\nin-cluster proxy server that is configured to accept HTTP traffic.\\nA minimal Gateway resource example:\\napiVersion : gateway.networking.k8s.io/v1\\nkind: Gateway\\nmetadata :\\n  name : example-gateway\\nspec:\\n  gatewayClassName : example-class\\n  listeners :\\n  - name : http\\n    protocol : HTTP\\n    port: 80\\nIn this example, an instance of traffic handling infrastructure is programmed to listen for HTTP\\ntraffic on port 80. Since the addresses  field is unspecified, an address or hostname is assigned to\\nthe Gateway by the implementation's controller. This address is used as a network endpoint for\\nprocessing traffic of backend network endpoints defined in routes.\", metadata={'source': './PDFS/Concepts.pdf', 'page': 238}),\n",
       " Document(page_content='processing traffic of backend network endpoints defined in routes.\\nSee the Gateway  reference for a full definition of this API kind.\\nHTTPRoute\\nThe HTTPRoute kind specifies routing behavior of HTTP requests from a Gateway listener to\\nbackend network endpoints. For a Service backend, an implementation may represent the\\nbackend network endpoint as a Service IP or the backing Endpoints of the Service. An\\nHTTPRoute represents configuration that is applied to the underlying Gateway\\nimplementation. For example, defining a new HTTPRoute may result in configuring additional\\ntraffic routes in a cloud load balancer or in-cluster proxy server.\\nA minimal HTTPRoute example:\\napiVersion : gateway.networking.k8s.io/v1\\nkind: HTTPRoute\\nmetadata :\\n  name : example-httproute\\nspec:\\n  parentRefs :\\n  - name : example-gateway\\n  hostnames :\\n  - \"www.example.com\"\\n  rules :\\n  - matches :\\n    - path:\\n        type: PathPrefix\\n        value : /login\\n    backendRefs :', metadata={'source': './PDFS/Concepts.pdf', 'page': 238}),\n",
       " Document(page_content=\"- name : example-svc\\n      port: 8080\\nIn this example, HTTP traffic from Gateway example-gateway  with the Host: header set to \\nwww.example.com  and the request path specified as /login  will be routed to Service example-\\nsvc on port 8080.\\nSee the HTTPRoute  reference for a full definition of this API kind.\\nRequest flow\\nHere is a simple example of HTTP traffic being routed to a Service by using a Gateway and an\\nHTTPRoute:\\nA diagram that provides an example of HTTP traffic being routed to a Service by\\nusing a Gateway and an HTTPRoute\\nIn this example, the request flow for a Gateway implemented as a reverse proxy is:\\nThe client starts to prepare an HTTP request for the URL http://www.example.com\\nThe client's DNS resolver queries for the destination name and learns a mapping to one or\\nmore IP addresses associated with the Gateway.\\nThe client sends a request to the Gateway IP address; the reverse proxy receives the\", metadata={'source': './PDFS/Concepts.pdf', 'page': 239}),\n",
       " Document(page_content='The client sends a request to the Gateway IP address; the reverse proxy receives the\\nHTTP request and uses the Host: header to match a configuration that was derived from\\nthe Gateway and attached HTTPRoute.\\nOptionally, the reverse proxy can perform request header and/or path matching based on\\nmatch rules of the HTTPRoute.\\nOptionally, the reverse proxy can modify the request; for example, to add or remove\\nheaders, based on filter rules of the HTTPRoute.\\nLastly, the reverse proxy forwards the request to one or more backends.\\nConformance\\nGateway API covers a broad set of features and is widely implemented. This combination\\nrequires clear conformance definitions and tests to ensure that the API provides a consistent\\nexperience wherever it is used.\\nSee the conformance  documentation to understand details such as release channels, support\\nlevels, and running conformance tests.\\nMigrating from Ingress', metadata={'source': './PDFS/Concepts.pdf', 'page': 239}),\n",
       " Document(page_content='levels, and running conformance tests.\\nMigrating from Ingress\\nGateway API is the successor to the Ingress  API. However, it does not include the Ingress kind.\\nAs a result, a one-time conversion from your existing Ingress resources to Gateway API\\nresources is necessary.\\nRefer to the ingress migration  guide for details on migrating Ingress resources to Gateway API\\nresources.1. \\n2. \\n3. \\n4. \\n5. \\n6.', metadata={'source': './PDFS/Concepts.pdf', 'page': 239}),\n",
       " Document(page_content=\"What's next\\nInstead of Gateway API resources being natively implemented by Kubernetes, the specifications\\nare defined as Custom Resources  supported by a wide range of implementations . Install  the\\nGateway API CRDs or follow the installation instructions of your selected implementation.\\nAfter installing an implementation, use the Getting Started  guide to help you quickly start\\nworking with Gateway API.\\nNote:  Make sure to review the documentation of your selected implementation to understand\\nany caveats.\\nRefer to the API specification  for additional details of all Gateway API kinds.\\nEndpointSlices\\nThe EndpointSlice API is the mechanism that Kubernetes uses to let your Service scale to\\nhandle large numbers of backends, and allows the cluster to update its list of healthy backends\\nefficiently.\\nFEATURE STATE:  Kubernetes v1.21 [stable]\\nKubernetes' EndpointSlice  API provides a way to track network endpoints within a Kubernetes\", metadata={'source': './PDFS/Concepts.pdf', 'page': 240}),\n",
       " Document(page_content=\"Kubernetes' EndpointSlice  API provides a way to track network endpoints within a Kubernetes\\ncluster. EndpointSlices offer a more scalable and extensible alternative to Endpoints .\\nEndpointSlice API\\nIn Kubernetes, an EndpointSlice contains references to a set of network endpoints. The control\\nplane automatically creates EndpointSlices for any Kubernetes Service that has a selector\\nspecified. These EndpointSlices include references to all the Pods that match the Service\\nselector. EndpointSlices group network endpoints together by unique combinations of protocol,\\nport number, and Service name. The name of a EndpointSlice object must be a valid DNS\\nsubdomain name .\\nAs an example, here's a sample EndpointSlice object, that's owned by the example  Kubernetes\\nService.\\napiVersion : discovery.k8s.io/v1\\nkind: EndpointSlice\\nmetadata :\\n  name : example-abc\\n  labels :\\n    kubernetes.io/service-name : example\\naddressType : IPv4\\nports :\\n  - name : http\\n    protocol : TCP\\n    port: 80\\nendpoints :\", metadata={'source': './PDFS/Concepts.pdf', 'page': 240}),\n",
       " Document(page_content='addressType : IPv4\\nports :\\n  - name : http\\n    protocol : TCP\\n    port: 80\\nendpoints :\\n  - addresses :\\n      - \"10.1.2.3\"\\n    conditions :\\n      ready : true', metadata={'source': './PDFS/Concepts.pdf', 'page': 240}),\n",
       " Document(page_content=\"hostname : pod-1\\n    nodeName : node-1\\n    zone : us-west2-a\\nBy default, the control plane creates and manages EndpointSlices to have no more than 100\\nendpoints each. You can configure this with the --max-endpoints-per-slice  kube-controller-\\nmanager  flag, up to a maximum of 1000.\\nEndpointSlices can act as the source of truth for kube-proxy  when it comes to how to route\\ninternal traffic.\\nAddress types\\nEndpointSlices support three address types:\\nIPv4\\nIPv6\\nFQDN (Fully Qualified Domain Name)\\nEach EndpointSlice  object represents a specific IP address type. If you have a Service that is\\navailable via IPv4 and IPv6, there will be at least two EndpointSlice  objects (one for IPv4, and\\none for IPv6).\\nConditions\\nThe EndpointSlice API stores conditions about endpoints that may be useful for consumers. The\\nthree conditions are ready , serving , and terminating .\\nReady\\nready  is a condition that maps to a Pod's Ready  condition. A running Pod with the Ready\", metadata={'source': './PDFS/Concepts.pdf', 'page': 241}),\n",
       " Document(page_content=\"Ready\\nready  is a condition that maps to a Pod's Ready  condition. A running Pod with the Ready\\ncondition set to True  should have this EndpointSlice condition also set to true. For\\ncompatibility reasons, ready  is NEVER true when a Pod is terminating. Consumers should refer\\nto the serving  condition to inspect the readiness of terminating Pods. The only exception to this\\nrule is for Services with spec.publishNotReadyAddresses  set to true. Endpoints for these\\nServices will always have the ready  condition set to true.\\nServing\\nFEATURE STATE:  Kubernetes v1.26 [stable]\\nThe serving  condition is almost identical to the ready  condition. The difference is that\\nconsumers of the EndpointSlice API should check the serving  condition if they care about pod\\nreadiness while the pod is also terminating.\\nNote:  Although serving  is almost identical to ready , it was added to prevent breaking the\\nexisting meaning of ready . It may be unexpected for existing clients if ready  could be true for\", metadata={'source': './PDFS/Concepts.pdf', 'page': 241}),\n",
       " Document(page_content='existing meaning of ready . It may be unexpected for existing clients if ready  could be true for\\nterminating endpoints, since historically terminating endpoints were never included in the\\nEndpoints or EndpointSlice API to begin with. For this reason, ready  is always  false for\\nterminating endpoints, and a new condition serving  was added in v1.20 so that clients can track\\nreadiness for terminating pods independent of the existing semantics for ready .• \\n• \\n•', metadata={'source': './PDFS/Concepts.pdf', 'page': 241}),\n",
       " Document(page_content='Terminating\\nFEATURE STATE:  Kubernetes v1.22 [beta]\\nTerminating  is a condition that indicates whether an endpoint is terminating. For pods, this is\\nany pod that has a deletion timestamp set.\\nTopology information\\nEach endpoint within an EndpointSlice can contain relevant topology information. The\\ntopology information includes the location of the endpoint and information about the\\ncorresponding Node and zone. These are available in the following per endpoint fields on\\nEndpointSlices:\\nnodeName  - The name of the Node this endpoint is on.\\nzone  - The zone this endpoint is in.\\nNote:\\nIn the v1 API, the per endpoint topology  was effectively removed in favor of the dedicated\\nfields nodeName  and zone .\\nSetting arbitrary topology fields on the endpoint  field of an EndpointSlice  resource has been\\ndeprecated and is not supported in the v1 API. Instead, the v1 API supports setting individual \\nnodeName  and zone  fields. These fields are automatically translated between API versions. For', metadata={'source': './PDFS/Concepts.pdf', 'page': 242}),\n",
       " Document(page_content='nodeName  and zone  fields. These fields are automatically translated between API versions. For\\nexample, the value of the \"topology.kubernetes.io/zone\"  key in the topology  field in the v1beta1\\nAPI is accessible as the zone  field in the v1 API.\\nManagement\\nMost often, the control plane (specifically, the endpoint slice controller ) creates and manages\\nEndpointSlice objects. There are a variety of other use cases for EndpointSlices, such as service\\nmesh implementations, that could result in other entities or controllers managing additional\\nsets of EndpointSlices.\\nTo ensure that multiple entities can manage EndpointSlices without interfering with each other,\\nKubernetes defines the label  endpointslice.kubernetes.io/managed-by , which indicates the\\nentity managing an EndpointSlice. The endpoint slice controller sets endpointslice-\\ncontroller.k8s.io  as the value for this label on all EndpointSlices it manages. Other entities', metadata={'source': './PDFS/Concepts.pdf', 'page': 242}),\n",
       " Document(page_content='controller.k8s.io  as the value for this label on all EndpointSlices it manages. Other entities\\nmanaging EndpointSlices should also set a unique value for this label.\\nOwnership\\nIn most use cases, EndpointSlices are owned by the Service that the endpoint slice object tracks\\nendpoints for. This ownership is indicated by an owner reference on each EndpointSlice as well\\nas a kubernetes.io/service-name  label that enables simple lookups of all EndpointSlices\\nbelonging to a Service.• \\n•', metadata={'source': './PDFS/Concepts.pdf', 'page': 242}),\n",
       " Document(page_content=\"EndpointSlice mirroring\\nIn some cases, applications create custom Endpoints resources. To ensure that these\\napplications do not need to concurrently write to both Endpoints and EndpointSlice resources,\\nthe cluster's control plane mirrors most Endpoints resources to corresponding EndpointSlices.\\nThe control plane mirrors Endpoints resources unless:\\nthe Endpoints resource has a endpointslice.kubernetes.io/skip-mirror  label set to true.\\nthe Endpoints resource has a control-plane.alpha.kubernetes.io/leader  annotation.\\nthe corresponding Service resource does not exist.\\nthe corresponding Service resource has a non-nil selector.\\nIndividual Endpoints resources may translate into multiple EndpointSlices. This will occur if an\\nEndpoints resource has multiple subsets or includes endpoints with multiple IP families (IPv4\\nand IPv6). A maximum of 1000 addresses per subset will be mirrored to EndpointSlices.\\nDistribution of EndpointSlices\", metadata={'source': './PDFS/Concepts.pdf', 'page': 243}),\n",
       " Document(page_content=\"Distribution of EndpointSlices\\nEach EndpointSlice has a set of ports that applies to all endpoints within the resource. When\\nnamed ports are used for a Service, Pods may end up with different target port numbers for the\\nsame named port, requiring different EndpointSlices. This is similar to the logic behind how\\nsubsets are grouped with Endpoints.\\nThe control plane tries to fill EndpointSlices as full as possible, but does not actively rebalance\\nthem. The logic is fairly straightforward:\\nIterate through existing EndpointSlices, remove endpoints that are no longer desired and\\nupdate matching endpoints that have changed.\\nIterate through EndpointSlices that have been modified in the first step and fill them up\\nwith any new endpoints needed.\\nIf there's still new endpoints left to add, try to fit them into a previously unchanged slice\\nand/or create new ones.\\nImportantly, the third step prioritizes limiting EndpointSlice updates over a perfectly full\", metadata={'source': './PDFS/Concepts.pdf', 'page': 243}),\n",
       " Document(page_content='Importantly, the third step prioritizes limiting EndpointSlice updates over a perfectly full\\ndistribution of EndpointSlices. As an example, if there are 10 new endpoints to add and 2\\nEndpointSlices with room for 5 more endpoints each, this approach will create a new\\nEndpointSlice instead of filling up the 2 existing EndpointSlices. In other words, a single\\nEndpointSlice creation is preferrable to multiple EndpointSlice updates.\\nWith kube-proxy running on each Node and watching EndpointSlices, every change to an\\nEndpointSlice becomes relatively expensive since it will be transmitted to every Node in the\\ncluster. This approach is intended to limit the number of changes that need to be sent to every\\nNode, even if it may result with multiple EndpointSlices that are not full.\\nIn practice, this less than ideal distribution should be rare. Most changes processed by the\\nEndpointSlice controller will be small enough to fit in an existing EndpointSlice, and if not, a', metadata={'source': './PDFS/Concepts.pdf', 'page': 243}),\n",
       " Document(page_content='EndpointSlice controller will be small enough to fit in an existing EndpointSlice, and if not, a\\nnew EndpointSlice is likely going to be necessary soon anyway. Rolling updates of\\nDeployments also provide a natural repacking of EndpointSlices with all Pods and their\\ncorresponding endpoints getting replaced.• \\n• \\n• \\n• \\n1. \\n2. \\n3.', metadata={'source': './PDFS/Concepts.pdf', 'page': 243}),\n",
       " Document(page_content='Duplicate endpoints\\nDue to the nature of EndpointSlice changes, endpoints may be represented in more than one\\nEndpointSlice at the same time. This naturally occurs as changes to different EndpointSlice\\nobjects can arrive at the Kubernetes client watch / cache at different times.\\nNote:\\nClients of the EndpointSlice API must iterate through all the existing EndpointSlices associated\\nto a Service and build a complete list of unique network endpoints. It is important to mention\\nthat endpoints may be duplicated in different EndpointSlices.\\nYou can find a reference implementation for how to perform this endpoint aggregation and\\ndeduplication as part of the EndpointSliceCache  code within kube-proxy .\\nComparison with Endpoints\\nThe original Endpoints API provided a simple and straightforward way of tracking network\\nendpoints in Kubernetes. As Kubernetes clusters and Services  grew to handle more traffic and', metadata={'source': './PDFS/Concepts.pdf', 'page': 244}),\n",
       " Document(page_content=\"endpoints in Kubernetes. As Kubernetes clusters and Services  grew to handle more traffic and\\nto send more traffic to more backend Pods, the limitations of that original API became more\\nvisible. Most notably, those included challenges with scaling to larger numbers of network\\nendpoints.\\nSince all network endpoints for a Service were stored in a single Endpoints object, those\\nEndpoints objects could get quite large. For Services that stayed stable (the same set of\\nendpoints over a long period of time) the impact was less noticeable; even then, some use cases\\nof Kubernetes weren't well served.\\nWhen a Service had a lot of backend endpoints and the workload was either scaling frequently,\\nor rolling out new changes frequently, each update to the single Endpoints object for that\\nService meant a lot of traffic between Kubernetes cluster components (within the control plane,\\nand also between nodes and the API server). This extra traffic also had a cost in terms of CPU\\nuse.\", metadata={'source': './PDFS/Concepts.pdf', 'page': 244}),\n",
       " Document(page_content=\"and also between nodes and the API server). This extra traffic also had a cost in terms of CPU\\nuse.\\nWith EndpointSlices, adding or removing a single Pod triggers the same number  of updates to\\nclients that are watching for changes, but the size of those update message is much smaller at\\nlarge scale.\\nEndpointSlices also enabled innovation around new features such dual-stack networking and\\ntopology-aware routing.\\nWhat's next\\nFollow the Connecting Applications with Services  tutorial\\nRead the API reference  for the EndpointSlice API\\nRead the API reference  for the Endpoints API\\nNetwork Policies\\nIf you want to control traffic flow at the IP address or port level (OSI layer 3 or 4),\\nNetworkPolicies allow you to specify rules for traffic flow within your cluster, and also between• \\n• \\n•\", metadata={'source': './PDFS/Concepts.pdf', 'page': 244}),\n",
       " Document(page_content='Pods and the outside world. Your cluster must use a network plugin that supports\\nNetworkPolicy enforcement.\\nIf you want to control traffic flow at the IP address or port level for TCP, UDP, and SCTP\\nprotocols, then you might consider using Kubernetes NetworkPolicies for particular\\napplications in your cluster. NetworkPolicies are an application-centric construct which allow\\nyou to specify how a pod is allowed to communicate with various network \"entities\" (we use\\nthe word \"entity\" here to avoid overloading the more common terms such as \"endpoints\" and\\n\"services\", which have specific Kubernetes connotations) over the network. NetworkPolicies\\napply to a connection with a pod on one or both ends, and are not relevant to other\\nconnections.\\nThe entities that a Pod can communicate with are identified through a combination of the\\nfollowing 3 identifiers:\\nOther pods that are allowed (exception: a pod cannot block access to itself)\\nNamespaces that are allowed', metadata={'source': './PDFS/Concepts.pdf', 'page': 245}),\n",
       " Document(page_content='Namespaces that are allowed\\nIP blocks (exception: traffic to and from the node where a Pod is running is always\\nallowed, regardless of the IP address of the Pod or the node)\\nWhen defining a pod- or namespace- based NetworkPolicy, you use a selector  to specify what\\ntraffic is allowed to and from the Pod(s) that match the selector.\\nMeanwhile, when IP based NetworkPolicies are created, we define policies based on IP blocks\\n(CIDR ranges).\\nPrerequisites\\nNetwork policies are implemented by the network plugin . To use network policies, you must be\\nusing a networking solution which supports NetworkPolicy. Creating a NetworkPolicy resource\\nwithout a controller that implements it will have no effect.\\nThe Two Sorts of Pod Isolation\\nThere are two sorts of isolation for a pod: isolation for egress, and isolation for ingress. They\\nconcern what connections may be established. \"Isolation\" here is not absolute, rather it means', metadata={'source': './PDFS/Concepts.pdf', 'page': 245}),\n",
       " Document(page_content='concern what connections may be established. \"Isolation\" here is not absolute, rather it means\\n\"some restrictions apply\". The alternative, \"non-isolated for $direction\", means that no\\nrestrictions apply in the stated direction. The two sorts of isolation (or not) are declared\\nindependently, and are both relevant for a connection from one pod to another.\\nBy default, a pod is non-isolated for egress; all outbound connections are allowed. A pod is\\nisolated for egress if there is any NetworkPolicy that both selects the pod and has \"Egress\" in its\\npolicyTypes ; we say that such a policy applies to the pod for egress. When a pod is isolated for\\negress, the only allowed connections from the pod are those allowed by the egress  list of some\\nNetworkPolicy that applies to the pod for egress. The effects of those egress  lists combine\\nadditively.\\nBy default, a pod is non-isolated for ingress; all inbound connections are allowed. A pod is', metadata={'source': './PDFS/Concepts.pdf', 'page': 245}),\n",
       " Document(page_content='By default, a pod is non-isolated for ingress; all inbound connections are allowed. A pod is\\nisolated for ingress if there is any NetworkPolicy that both selects the pod and has \"Ingress\" in\\nits policyTypes ; we say that such a policy applies to the pod for ingress. When a pod is isolated\\nfor ingress, the only allowed connections into the pod are those from the pod\\'s node and those1. \\n2. \\n3.', metadata={'source': './PDFS/Concepts.pdf', 'page': 245}),\n",
       " Document(page_content='allowed by the ingress  list of some NetworkPolicy that applies to the pod for ingress. The\\neffects of those ingress  lists combine additively.\\nNetwork policies do not conflict; they are additive. If any policy or policies apply to a given pod\\nfor a given direction, the connections allowed in that direction from that pod is the union of\\nwhat the applicable policies allow. Thus, order of evaluation does not affect the policy result.\\nFor a connection from a source pod to a destination pod to be allowed, both the egress policy\\non the source pod and the ingress policy on the destination pod need to allow the connection. If\\neither side does not allow the connection, it will not happen.\\nThe NetworkPolicy resource\\nSee the NetworkPolicy  reference for a full definition of the resource.\\nAn example NetworkPolicy might look like this:\\nservice/networking/networkpolicy.yaml  \\napiVersion : networking.k8s.io/v1\\nkind: NetworkPolicy\\nmetadata :\\n  name : test-network-policy\\n  namespace : default\\nspec:', metadata={'source': './PDFS/Concepts.pdf', 'page': 246}),\n",
       " Document(page_content='kind: NetworkPolicy\\nmetadata :\\n  name : test-network-policy\\n  namespace : default\\nspec:\\n  podSelector :\\n    matchLabels :\\n      role: db\\n  policyTypes :\\n    - Ingress\\n    - Egress\\n  ingress :\\n    - from :\\n        - ipBlock :\\n            cidr: 172.17.0.0 /16\\n            except :\\n              - 172.17.1.0 /24\\n        - namespaceSelector :\\n            matchLabels :\\n              project : myproject\\n        - podSelector :\\n            matchLabels :\\n              role: frontend\\n      ports :\\n        - protocol : TCP\\n          port: 6379\\n  egress :\\n    - to:\\n        - ipBlock :\\n            cidr: 10.0.0.0 /24\\n      ports :\\n        - protocol : TCP', metadata={'source': './PDFS/Concepts.pdf', 'page': 246}),\n",
       " Document(page_content='port: 5978\\nNote:  POSTing this to the API server for your cluster will have no effect unless your chosen\\nnetworking solution supports network policy.\\nMandatory Fields : As with all other Kubernetes config, a NetworkPolicy needs apiVersion , \\nkind, and metadata  fields. For general information about working with config files, see \\nConfigure a Pod to Use a ConfigMap , and Object Management .\\nspec : NetworkPolicy spec has all the information needed to define a particular network policy\\nin the given namespace.\\npodSelector : Each NetworkPolicy includes a podSelector  which selects the grouping of pods to\\nwhich the policy applies. The example policy selects pods with the label \"role=db\". An empty \\npodSelector  selects all pods in the namespace.\\npolicyTypes : Each NetworkPolicy includes a policyTypes  list which may include either \\nIngress , Egress , or both. The policyTypes  field indicates whether or not the given policy applies', metadata={'source': './PDFS/Concepts.pdf', 'page': 247}),\n",
       " Document(page_content='Ingress , Egress , or both. The policyTypes  field indicates whether or not the given policy applies\\nto ingress traffic to selected pod, egress traffic from selected pods, or both. If no policyTypes  are\\nspecified on a NetworkPolicy then by default Ingress  will always be set and Egress  will be set if\\nthe NetworkPolicy has any egress rules.\\ningress : Each NetworkPolicy may include a list of allowed ingress  rules. Each rule allows traffic\\nwhich matches both the from  and ports  sections. The example policy contains a single rule,\\nwhich matches traffic on a single port, from one of three sources, the first specified via an \\nipBlock , the second via a namespaceSelector  and the third via a podSelector .\\negress : Each NetworkPolicy may include a list of allowed egress  rules. Each rule allows traffic\\nwhich matches both the to and ports  sections. The example policy contains a single rule, which\\nmatches traffic on a single port to any destination in 10.0.0.0/24 .', metadata={'source': './PDFS/Concepts.pdf', 'page': 247}),\n",
       " Document(page_content=\"matches traffic on a single port to any destination in 10.0.0.0/24 .\\nSo, the example NetworkPolicy:\\nisolates role=db  pods in the default  namespace for both ingress and egress traffic (if they\\nweren't already isolated)\\n(Ingress rules) allows connections to all pods in the default  namespace with the label \\nrole=db  on TCP port 6379 from:\\nany pod in the default  namespace with the label role=frontend\\nany pod in a namespace with the label project=myproject\\nIP addresses in the ranges 172.17.0.0 –172.17.0.255  and 172.17.2.0 –172.17.255.255  (ie,\\nall of 172.17.0.0/16  except 172.17.1.0/24 )\\n(Egress rules) allows connections from any pod in the default  namespace with the label \\nrole=db  to CIDR 10.0.0.0/24  on TCP port 5978\\nSee the Declare Network Policy  walkthrough for further examples.\\nBehavior of to and from  selectors\\nThere are four kinds of selectors that can be specified in an ingress  from  section or egress  to\\nsection:1. \\n2. \\n◦ \\n◦ \\n◦ \\n3.\", metadata={'source': './PDFS/Concepts.pdf', 'page': 247}),\n",
       " Document(page_content='podSelector : This selects particular Pods in the same namespace as the NetworkPolicy which\\nshould be allowed as ingress sources or egress destinations.\\nnamespaceSelector : This selects particular namespaces for which all Pods should be allowed\\nas ingress sources or egress destinations.\\nnamespaceSelector  and podSelector : A single to/from  entry that specifies both \\nnamespaceSelector  and podSelector  selects particular Pods within particular namespaces. Be\\ncareful to use correct YAML syntax. For example:\\n  ...\\n  ingress :\\n  - from :\\n    - namespaceSelector :\\n        matchLabels :\\n          user: alice\\n      podSelector :\\n        matchLabels :\\n          role: client\\n  ...\\nThis policy contains a single from  element allowing connections from Pods with the label \\nrole=client  in namespaces with the label user=alice . But the following policy is different:\\n  ...\\n  ingress :\\n  - from :\\n    - namespaceSelector :\\n        matchLabels :\\n          user: alice\\n    - podSelector :', metadata={'source': './PDFS/Concepts.pdf', 'page': 248}),\n",
       " Document(page_content='- from :\\n    - namespaceSelector :\\n        matchLabels :\\n          user: alice\\n    - podSelector :\\n        matchLabels :\\n          role: client\\n  ...\\nIt contains two elements in the from  array, and allows connections from Pods in the local\\nNamespace with the label role=client , or from any Pod in any namespace with the label \\nuser=alice .\\nWhen in doubt, use kubectl describe  to see how Kubernetes has interpreted the policy.\\n ipBlock : This selects particular IP CIDR ranges to allow as ingress sources or egress\\ndestinations. These should be cluster-external IPs, since Pod IPs are ephemeral and\\nunpredictable.\\nCluster ingress and egress mechanisms often require rewriting the source or destination IP of\\npackets. In cases where this happens, it is not defined whether this happens before or after\\nNetworkPolicy processing, and the behavior may be different for different combinations of\\nnetwork plugin, cloud provider, Service  implementation, etc.', metadata={'source': './PDFS/Concepts.pdf', 'page': 248}),\n",
       " Document(page_content='network plugin, cloud provider, Service  implementation, etc.\\nIn the case of ingress, this means that in some cases you may be able to filter incoming packets\\nbased on the actual original source IP, while in other cases, the \"source IP\" that the\\nNetworkPolicy acts on may be the IP of a LoadBalancer  or of the Pod\\'s node, etc.', metadata={'source': './PDFS/Concepts.pdf', 'page': 248}),\n",
       " Document(page_content='For egress, this means that connections from pods to Service  IPs that get rewritten to cluster-\\nexternal IPs may or may not be subject to ipBlock -based policies.\\nDefault policies\\nBy default, if no policies exist in a namespace, then all ingress and egress traffic is allowed to\\nand from pods in that namespace. The following examples let you change the default behavior\\nin that namespace.\\nDefault deny all ingress traffic\\nYou can create a \"default\" ingress isolation policy for a namespace by creating a NetworkPolicy\\nthat selects all pods but does not allow any ingress traffic to those pods.\\nservice/networking/network-policy-default-deny-ingress.yaml  \\n---\\napiVersion : networking.k8s.io/v1\\nkind: NetworkPolicy\\nmetadata :\\n  name : default-deny-ingress\\nspec:\\n  podSelector : {}\\n  policyTypes :\\n  - Ingress\\nThis ensures that even pods that aren\\'t selected by any other NetworkPolicy will still be isolated\\nfor ingress. This policy does not affect isolation for egress from any pod.', metadata={'source': './PDFS/Concepts.pdf', 'page': 249}),\n",
       " Document(page_content='for ingress. This policy does not affect isolation for egress from any pod.\\nAllow all ingress traffic\\nIf you want to allow all incoming connections to all pods in a namespace, you can create a\\npolicy that explicitly allows that.\\nservice/networking/network-policy-allow-all-ingress.yaml  \\n---\\napiVersion : networking.k8s.io/v1\\nkind: NetworkPolicy\\nmetadata :\\n  name : allow-all-ingress\\nspec:\\n  podSelector : {}\\n  ingress :\\n  - {}\\n  policyTypes :\\n  - Ingress\\nWith this policy in place, no additional policy or policies can cause any incoming connection to\\nthose pods to be denied. This policy has no effect on isolation for egress from any pod.', metadata={'source': './PDFS/Concepts.pdf', 'page': 249}),\n",
       " Document(page_content='Default deny all egress traffic\\nYou can create a \"default\" egress isolation policy for a namespace by creating a NetworkPolicy\\nthat selects all pods but does not allow any egress traffic from those pods.\\nservice/networking/network-policy-default-deny-egress.yaml  \\n---\\napiVersion : networking.k8s.io/v1\\nkind: NetworkPolicy\\nmetadata :\\n  name : default-deny-egress\\nspec:\\n  podSelector : {}\\n  policyTypes :\\n  - Egress\\nThis ensures that even pods that aren\\'t selected by any other NetworkPolicy will not be allowed\\negress traffic. This policy does not change the ingress isolation behavior of any pod.\\nAllow all egress traffic\\nIf you want to allow all connections from all pods in a namespace, you can create a policy that\\nexplicitly allows all outgoing connections from pods in that namespace.\\nservice/networking/network-policy-allow-all-egress.yaml  \\n---\\napiVersion : networking.k8s.io/v1\\nkind: NetworkPolicy\\nmetadata :\\n  name : allow-all-egress\\nspec:\\n  podSelector : {}\\n  egress :\\n  - {}', metadata={'source': './PDFS/Concepts.pdf', 'page': 250}),\n",
       " Document(page_content='kind: NetworkPolicy\\nmetadata :\\n  name : allow-all-egress\\nspec:\\n  podSelector : {}\\n  egress :\\n  - {}\\n  policyTypes :\\n  - Egress\\nWith this policy in place, no additional policy or policies can cause any outgoing connection\\nfrom those pods to be denied. This policy has no effect on isolation for ingress to any pod.\\nDefault deny all ingress and all egress traffic\\nYou can create a \"default\" policy for a namespace which prevents all ingress AND egress traffic\\nby creating the following NetworkPolicy in that namespace.\\nservice/networking/network-policy-default-deny-all.yaml  \\n---\\napiVersion : networking.k8s.io/v1\\nkind: NetworkPolicy\\nmetadata :', metadata={'source': './PDFS/Concepts.pdf', 'page': 250}),\n",
       " Document(page_content=\"name : default-deny-all\\nspec:\\n  podSelector : {}\\n  policyTypes :\\n  - Ingress\\n  - Egress\\nThis ensures that even pods that aren't selected by any other NetworkPolicy will not be allowed\\ningress or egress traffic.\\nNetwork traffic filtering\\nNetworkPolicy is defined for layer 4  connections (TCP, UDP, and optionally SCTP). For all the\\nother protocols, the behaviour may vary across network plugins.\\nNote:  You must be using a CNI plugin that supports SCTP protocol NetworkPolicies.\\nWhen a deny all  network policy is defined, it is only guaranteed to deny TCP, UDP and SCTP\\nconnections. For other protocols, such as ARP or ICMP, the behaviour is undefined. The same\\napplies to allow rules: when a specific pod is allowed as ingress source or egress destination, it\\nis undefined what happens with (for example) ICMP packets. Protocols such as ICMP may be\\nallowed by some network plugins and denied by others.\\nTargeting a range of ports\\nFEATURE STATE:  Kubernetes v1.25 [stable]\", metadata={'source': './PDFS/Concepts.pdf', 'page': 251}),\n",
       " Document(page_content='Targeting a range of ports\\nFEATURE STATE:  Kubernetes v1.25 [stable]\\nWhen writing a NetworkPolicy, you can target a range of ports instead of a single port.\\nThis is achievable with the usage of the endPort  field, as the following example:\\nservice/networking/networkpolicy-multiport-egress.yaml  \\napiVersion : networking.k8s.io/v1\\nkind: NetworkPolicy\\nmetadata :\\n  name : multi-port-egress\\n  namespace : default\\nspec:\\n  podSelector :\\n    matchLabels :\\n      role: db\\n  policyTypes :\\n    - Egress\\n  egress :\\n    - to:\\n        - ipBlock :\\n            cidr: 10.0.0.0 /24\\n      ports :\\n        - protocol : TCP\\n          port: 32000', metadata={'source': './PDFS/Concepts.pdf', 'page': 251}),\n",
       " Document(page_content='endPort : 32768\\nThe above rule allows any Pod with label role=db  on the namespace default  to communicate\\nwith any IP within the range 10.0.0.0/24  over TCP, provided that the target port is between the\\nrange 32000 and 32768.\\nThe following restrictions apply when using this field:\\nThe endPort  field must be equal to or greater than the port field.\\nendPort  can only be defined if port is also defined.\\nBoth ports must be numeric.\\nNote:  Your cluster must be using a CNI plugin that supports the endPort  field in NetworkPolicy\\nspecifications. If your network plugin  does not support the endPort  field and you specify a\\nNetworkPolicy with that, the policy will be applied only for the single port field.\\nTargeting multiple namespaces by label\\nIn this scenario, your Egress  NetworkPolicy targets more than one namespace using their label\\nnames. For this to work, you need to label the target namespaces. For example:\\n kubectl label namespace frontend namespace =frontend', metadata={'source': './PDFS/Concepts.pdf', 'page': 252}),\n",
       " Document(page_content='kubectl label namespace frontend namespace =frontend\\n kubectl label namespace backend namespace =backend\\nAdd the labels under namespaceSelector  in your NetworkPolicy document. For example:\\napiVersion : networking.k8s.io/v1\\nkind: NetworkPolicy\\nmetadata :\\n  name : egress-namespaces\\nspec:\\n  podSelector :\\n    matchLabels :\\n      app: myapp\\n  policyTypes :\\n  - Egress\\n  egress :\\n  - to:\\n    - namespaceSelector :\\n        matchExpressions :\\n        - key: namespace\\n          operator : In\\n          values : [\"frontend\" , \"backend\" ]\\nNote:  It is not possible to directly specify the name of the namespaces in a NetworkPolicy. You\\nmust use a namespaceSelector  with matchLabels  or matchExpressions  to select the namespaces\\nbased on their labels.• \\n• \\n•', metadata={'source': './PDFS/Concepts.pdf', 'page': 252}),\n",
       " Document(page_content='Targeting a Namespace by its name\\nThe Kubernetes control plane sets an immutable label kubernetes.io/metadata.name  on all\\nnamespaces, the value of the label is the namespace name.\\nWhile NetworkPolicy cannot target a namespace by its name with some object field, you can\\nuse the standardized label to target a specific namespace.\\nPod lifecycle\\nNote:  The following applies to clusters with a conformant networking plugin and a conformant\\nimplementation of NetworkPolicy.\\nWhen a new NetworkPolicy object is created, it may take some time for a network plugin to\\nhandle the new object. If a pod that is affected by a NetworkPolicy is created before the\\nnetwork plugin has completed NetworkPolicy handling, that pod may be started unprotected,\\nand isolation rules will be applied when the NetworkPolicy handling is completed.\\nOnce the NetworkPolicy is handled by a network plugin,\\nAll newly created pods affected by a given NetworkPolicy will be isolated before they are', metadata={'source': './PDFS/Concepts.pdf', 'page': 253}),\n",
       " Document(page_content='All newly created pods affected by a given NetworkPolicy will be isolated before they are\\nstarted. Implementations of NetworkPolicy must ensure that filtering is effective\\nthroughout the Pod lifecycle, even from the very first instant that any container in that\\nPod is started. Because they are applied at Pod level, NetworkPolicies apply equally to\\ninit containers, sidecar containers, and regular containers.\\nAllow rules will be applied eventually after the isolation rules (or may be applied at the\\nsame time). In the worst case, a newly created pod may have no network connectivity at\\nall when it is first started, if isolation rules were already applied, but no allow rules were\\napplied yet.\\nEvery created NetworkPolicy will be handled by a network plugin eventually, but there is no\\nway to tell from the Kubernetes API when exactly that happens.\\nTherefore, pods must be resilient against being started up with different network connectivity', metadata={'source': './PDFS/Concepts.pdf', 'page': 253}),\n",
       " Document(page_content='Therefore, pods must be resilient against being started up with different network connectivity\\nthan expected. If you need to make sure the pod can reach certain destinations before being\\nstarted, you can use an init container  to wait for those destinations to be reachable before\\nkubelet starts the app containers.\\nEvery NetworkPolicy will be applied to all selected pods eventually. Because the network\\nplugin may implement NetworkPolicy in a distributed manner, it is possible that pods may see a\\nslightly inconsistent view of network policies when the pod is first created, or when pods or\\npolicies change. For example, a newly-created pod that is supposed to be able to reach both Pod\\nA on Node 1 and Pod B on Node 2 may find that it can reach Pod A immediately, but cannot\\nreach Pod B until a few seconds later.1. \\n2.', metadata={'source': './PDFS/Concepts.pdf', 'page': 253}),\n",
       " Document(page_content='NetworkPolicy and hostNetwork  pods\\nNetworkPolicy behaviour for hostNetwork  pods is undefined, but it should be limited to 2\\npossibilities:\\nThe network plugin can distinguish hostNetwork  pod traffic from all other traffic\\n(including being able to distinguish traffic from different hostNetwork  pods on the same\\nnode), and will apply NetworkPolicy to hostNetwork  pods just like it does to pod-\\nnetwork pods.\\nThe network plugin cannot properly distinguish hostNetwork  pod traffic, and so it\\nignores hostNetwork  pods when matching podSelector  and namespaceSelector . Traffic to/\\nfrom hostNetwork  pods is treated the same as all other traffic to/from the node IP. (This is\\nthe most common implementation.)\\nThis applies when\\na hostNetwork  pod is selected by spec.podSelector .\\n  ...\\n  spec:\\n    podSelector :\\n      matchLabels :\\n        role: client\\n  ...\\na hostNetwork  pod is selected by a podSelector  or namespaceSelector  in an ingress  or \\negress  rule.\\n  ...\\n  ingress :\\n    - from :', metadata={'source': './PDFS/Concepts.pdf', 'page': 254}),\n",
       " Document(page_content=\"egress  rule.\\n  ...\\n  ingress :\\n    - from :\\n      - podSelector :\\n          matchLabels :\\n            role: client\\n  ...\\nAt the same time, since hostNetwork  pods have the same IP addresses as the nodes they reside\\non, their connections will be treated as node connections. For example, you can allow traffic\\nfrom a hostNetwork  Pod using an ipBlock  rule.\\nWhat you can't do with network policies (at least, not yet)\\nAs of Kubernetes 1.28, the following functionality does not exist in the NetworkPolicy API, but\\nyou might be able to implement workarounds using Operating System components (such as\\nSELinux, OpenVSwitch, IPTables, and so on) or Layer 7 technologies (Ingress controllers,\\nService Mesh implementations) or admission controllers. In case you are new to network\\nsecurity in Kubernetes, its worth noting that the following User Stories cannot (yet) be\\nimplemented using the NetworkPolicy API.\\nForcing internal cluster traffic to go through a common gateway (this might be best\", metadata={'source': './PDFS/Concepts.pdf', 'page': 254}),\n",
       " Document(page_content='Forcing internal cluster traffic to go through a common gateway (this might be best\\nserved with a service mesh or other proxy).\\nAnything TLS related (use a service mesh or ingress controller for this).• \\n• \\n1. \\n2. \\n• \\n•', metadata={'source': './PDFS/Concepts.pdf', 'page': 254}),\n",
       " Document(page_content='Node specific policies (you can use CIDR notation for these, but you cannot target nodes\\nby their Kubernetes identities specifically).\\nTargeting of services by name (you can, however, target pods or namespaces by their \\nlabels , which is often a viable workaround).\\nCreation or management of \"Policy requests\" that are fulfilled by a third party.\\nDefault policies which are applied to all namespaces or pods (there are some third party\\nKubernetes distributions and projects which can do this).\\nAdvanced policy querying and reachability tooling.\\nThe ability to log network security events (for example connections that are blocked or\\naccepted).\\nThe ability to explicitly deny policies (currently the model for NetworkPolicies are deny\\nby default, with only the ability to add allow rules).\\nThe ability to prevent loopback or incoming host traffic (Pods cannot currently block\\nlocalhost access, nor do they have the ability to block access from their resident node).\\nWhat\\'s next', metadata={'source': './PDFS/Concepts.pdf', 'page': 255}),\n",
       " Document(page_content=\"What's next\\nSee the Declare Network Policy  walkthrough for further examples.\\nSee more recipes  for common scenarios enabled by the NetworkPolicy resource.\\nDNS for Services and Pods\\nYour workload can discover Services within your cluster using DNS; this page explains how\\nthat works.\\nKubernetes creates DNS records for Services and Pods. You can contact Services with consistent\\nDNS names instead of IP addresses.\\nKubernetes publishes information about Pods and Services which is used to program DNS.\\nKubelet configures Pods' DNS so that running containers can lookup Services by name rather\\nthan IP.\\nServices defined in the cluster are assigned DNS names. By default, a client Pod's DNS search\\nlist includes the Pod's own namespace and the cluster's default domain.\\nNamespaces of Services\\nA DNS query may return different results based on the namespace of the Pod making it. DNS\\nqueries that don't specify a namespace are limited to the Pod's namespace. Access Services in\", metadata={'source': './PDFS/Concepts.pdf', 'page': 255}),\n",
       " Document(page_content=\"queries that don't specify a namespace are limited to the Pod's namespace. Access Services in\\nother namespaces by specifying it in the DNS query.\\nFor example, consider a Pod in a test namespace. A data Service is in the prod  namespace.\\nA query for data returns no results, because it uses the Pod's test namespace.\\nA query for data.prod  returns the intended result, because it specifies the namespace.\\nDNS queries may be expanded using the Pod's /etc/resolv.conf . Kubelet configures this file for\\neach Pod. For example, a query for just data may be expanded to data.test.svc.cluster.local . The\\nvalues of the search  option are used to expand queries. To learn more about DNS queries, see \\nthe resolv.conf  manual page.• \\n• \\n• \\n• \\n• \\n• \\n• \\n• \\n• \\n•\", metadata={'source': './PDFS/Concepts.pdf', 'page': 255}),\n",
       " Document(page_content='nameserver 10.32.0.10\\nsearch <namespace>.svc.cluster.local svc.cluster.local cluster.local\\noptions ndots:5\\nIn summary, a Pod in the test namespace can successfully resolve either data.prod  or \\ndata.prod.svc.cluster.local .\\nDNS Records\\nWhat objects get DNS records?\\nServices\\nPods\\nThe following sections detail the supported DNS record types and layout that is supported. Any\\nother layout or names or queries that happen to work are considered implementation details\\nand are subject to change without warning. For more up-to-date specification, see Kubernetes\\nDNS-Based Service Discovery .\\nServices\\nA/AAAA records\\n\"Normal\" (not headless) Services are assigned DNS A and/or AAAA records, depending on the\\nIP family or families of the Service, with a name of the form my-svc.my-namespace.svc.cluster-\\ndomain.example . This resolves to the cluster IP of the Service.\\nHeadless Services  (without a cluster IP) Services are also assigned DNS A and/or AAAA', metadata={'source': './PDFS/Concepts.pdf', 'page': 256}),\n",
       " Document(page_content='Headless Services  (without a cluster IP) Services are also assigned DNS A and/or AAAA\\nrecords, with a name of the form my-svc.my-namespace.svc.cluster-domain.example . Unlike\\nnormal Services, this resolves to the set of IPs of all of the Pods selected by the Service. Clients\\nare expected to consume the set or else use standard round-robin selection from the set.\\nSRV records\\nSRV Records are created for named ports that are part of normal or headless services. For each\\nnamed port, the SRV record has the form _port-name._port-protocol.my-svc.my-\\nnamespace.svc.cluster-domain.example . For a regular Service, this resolves to the port number\\nand the domain name: my-svc.my-namespace.svc.cluster-domain.example . For a headless\\nService, this resolves to multiple answers, one for each Pod that is backing the Service, and\\ncontains the port number and the domain name of the Pod of the form hostname.my-svc.my-\\nnamespace.svc.cluster-domain.example .\\nPods\\nA/AAAA records', metadata={'source': './PDFS/Concepts.pdf', 'page': 256}),\n",
       " Document(page_content='namespace.svc.cluster-domain.example .\\nPods\\nA/AAAA records\\nIn general a Pod has the following DNS resolution:\\npod-ip-address.my-namespace.pod.cluster-domain.example .1. \\n2.', metadata={'source': './PDFS/Concepts.pdf', 'page': 256}),\n",
       " Document(page_content='For example, if a Pod in the default  namespace has the IP address 172.17.0.3, and the domain\\nname for your cluster is cluster.local , then the Pod has a DNS name:\\n172-17-0-3.default.pod.cluster.local .\\nAny Pods exposed by a Service have the following DNS resolution available:\\npod-ip-address.service-name.my-namespace.svc.cluster-domain.example .\\nPod\\'s hostname and subdomain fields\\nCurrently when a Pod is created, its hostname (as observed from within the Pod) is the Pod\\'s \\nmetadata.name  value.\\nThe Pod spec has an optional hostname  field, which can be used to specify a different hostname.\\nWhen specified, it takes precedence over the Pod\\'s name to be the hostname of the Pod (again,\\nas observed from within the Pod). For example, given a Pod with spec.hostname  set to \"my-\\nhost\" , the Pod will have its hostname set to \"my-host\" .\\nThe Pod spec also has an optional subdomain  field which can be used to indicate that the pod is', metadata={'source': './PDFS/Concepts.pdf', 'page': 257}),\n",
       " Document(page_content='The Pod spec also has an optional subdomain  field which can be used to indicate that the pod is\\npart of sub-group of the namespace. For example, a Pod with spec.hostname  set to \"foo\" , and \\nspec.subdomain  set to \"bar\" , in namespace \"my-namespace\" , will have its hostname set to \"foo\"\\nand its fully qualified domain name (FQDN) set to \"foo.bar.my-namespace.svc.cluster.local\"\\n(once more, as observed from within the Pod).\\nIf there exists a headless Service in the same namespace as the Pod, with the same name as the\\nsubdomain, the cluster\\'s DNS Server also returns A and/or AAAA records for the Pod\\'s fully\\nqualified hostname.\\nExample:\\napiVersion : v1\\nkind: Service\\nmetadata :\\n  name : busybox-subdomain\\nspec:\\n  selector :\\n    name : busybox\\n  clusterIP : None\\n  ports :\\n  - name : foo # name is not required for single-port Services\\n    port: 1234\\n---\\napiVersion : v1\\nkind: Pod\\nmetadata :\\n  name : busybox1\\n  labels :\\n    name : busybox\\nspec:\\n  hostname : busybox-1', metadata={'source': './PDFS/Concepts.pdf', 'page': 257}),\n",
       " Document(page_content='kind: Pod\\nmetadata :\\n  name : busybox1\\n  labels :\\n    name : busybox\\nspec:\\n  hostname : busybox-1\\n  subdomain : busybox-subdomain\\n  containers :\\n  - image : busybox:1.28', metadata={'source': './PDFS/Concepts.pdf', 'page': 257}),\n",
       " Document(page_content='command :\\n      - sleep\\n      - \"3600\"\\n    name : busybox\\n---\\napiVersion : v1\\nkind: Pod\\nmetadata :\\n  name : busybox2\\n  labels :\\n    name : busybox\\nspec:\\n  hostname : busybox-2\\n  subdomain : busybox-subdomain\\n  containers :\\n  - image : busybox:1.28\\n    command :\\n      - sleep\\n      - \"3600\"\\n    name : busybox\\nGiven the above Service \"busybox-subdomain\"  and the Pods which set spec.subdomain  to \\n\"busybox-subdomain\" , the first Pod will see its own FQDN as \"busybox-1.busybox-\\nsubdomain.my-namespace.svc.cluster-domain.example\" . DNS serves A and/or AAAA records at\\nthat name, pointing to the Pod\\'s IP. Both Pods \" busybox1 \" and \" busybox2 \" will have their own\\naddress records.\\nAn EndpointSlice  can specify the DNS hostname for any endpoint addresses, along with its IP.\\nNote:  Because A and AAAA records are not created for Pod names, hostname  is required for\\nthe Pod\\'s A or AAAA record to be created. A Pod with no hostname  but with subdomain  will', metadata={'source': './PDFS/Concepts.pdf', 'page': 258}),\n",
       " Document(page_content=\"the Pod's A or AAAA record to be created. A Pod with no hostname  but with subdomain  will\\nonly create the A or AAAA record for the headless Service ( busybox-subdomain.my-\\nnamespace.svc.cluster-domain.example ), pointing to the Pods' IP addresses. Also, the Pod needs\\nto be ready in order to have a record unless publishNotReadyAddresses=True  is set on the\\nService.\\nPod's setHostnameAsFQDN field\\nFEATURE STATE:  Kubernetes v1.22 [stable]\\nWhen a Pod is configured to have fully qualified domain name (FQDN), its hostname is the\\nshort hostname. For example, if you have a Pod with the fully qualified domain name \\nbusybox-1.busybox-subdomain.my-namespace.svc.cluster-domain.example , then by default the \\nhostname  command inside that Pod returns busybox-1  and the hostname --fqdn  command\\nreturns the FQDN.\\nWhen you set setHostnameAsFQDN: true  in the Pod spec, the kubelet writes the Pod's FQDN\\ninto the hostname for that Pod's namespace. In this case, both hostname  and hostname --fqdn\", metadata={'source': './PDFS/Concepts.pdf', 'page': 258}),\n",
       " Document(page_content=\"into the hostname for that Pod's namespace. In this case, both hostname  and hostname --fqdn\\nreturn the Pod's FQDN.\\nNote:\", metadata={'source': './PDFS/Concepts.pdf', 'page': 258}),\n",
       " Document(page_content='In Linux, the hostname field of the kernel (the nodename  field of struct utsname ) is limited to\\n64 characters.\\nIf a Pod enables this feature and its FQDN is longer than 64 character, it will fail to start. The\\nPod will remain in Pending  status ( ContainerCreating  as seen by kubectl ) generating error\\nevents, such as Failed to construct FQDN from Pod hostname and cluster domain, FQDN long-\\nFQDN  is too long (64 characters is the max, 70 characters requested). One way of improving\\nuser experience for this scenario is to create an admission webhook controller  to control FQDN\\nsize when users create top level objects, for example, Deployment.\\nPod\\'s DNS Policy\\nDNS policies can be set on a per-Pod basis. Currently Kubernetes supports the following Pod-\\nspecific DNS policies. These policies are specified in the dnsPolicy  field of a Pod Spec.\\n\"Default \": The Pod inherits the name resolution configuration from the node that the Pods\\nrun on. See related discussion  for more details.', metadata={'source': './PDFS/Concepts.pdf', 'page': 259}),\n",
       " Document(page_content='run on. See related discussion  for more details.\\n\"ClusterFirst \": Any DNS query that does not match the configured cluster domain suffix,\\nsuch as \" www.kubernetes.io \", is forwarded to an upstream nameserver by the DNS server.\\nCluster administrators may have extra stub-domain and upstream DNS servers\\nconfigured. See related discussion  for details on how DNS queries are handled in those\\ncases.\\n\"ClusterFirstWithHostNet \": For Pods running with hostNetwork, you should explicitly set\\nits DNS policy to \" ClusterFirstWithHostNet \". Otherwise, Pods running with hostNetwork\\nand \"ClusterFirst\"  will fallback to the behavior of the \"Default\"  policy.\\nNote: This is not supported on Windows. See below  for details\\n\"None \": It allows a Pod to ignore DNS settings from the Kubernetes environment. All\\nDNS settings are supposed to be provided using the dnsConfig  field in the Pod Spec. See \\nPod\\'s DNS config  subsection below.', metadata={'source': './PDFS/Concepts.pdf', 'page': 259}),\n",
       " Document(page_content='Pod\\'s DNS config  subsection below.\\nNote:  \"Default\" is not the default DNS policy. If dnsPolicy  is not explicitly specified, then\\n\"ClusterFirst\" is used.\\nThe example below shows a Pod with its DNS policy set to \" ClusterFirstWithHostNet \" because\\nit has hostNetwork  set to true.\\napiVersion : v1\\nkind: Pod\\nmetadata :\\n  name : busybox\\n  namespace : default\\nspec:\\n  containers :\\n  - image : busybox:1.28\\n    command :\\n      - sleep\\n      - \"3600\"\\n    imagePullPolicy : IfNotPresent\\n    name : busybox\\n  restartPolicy : Always\\n  hostNetwork : true\\n  dnsPolicy : ClusterFirstWithHostNet• \\n• \\n• \\n◦ \\n•', metadata={'source': './PDFS/Concepts.pdf', 'page': 259}),\n",
       " Document(page_content='Pod\\'s DNS Config\\nFEATURE STATE:  Kubernetes v1.14 [stable]\\nPod\\'s DNS Config allows users more control on the DNS settings for a Pod.\\nThe dnsConfig  field is optional and it can work with any dnsPolicy  settings. However, when a\\nPod\\'s dnsPolicy  is set to \" None \", the dnsConfig  field has to be specified.\\nBelow are the properties a user can specify in the dnsConfig  field:\\nnameservers : a list of IP addresses that will be used as DNS servers for the Pod. There can\\nbe at most 3 IP addresses specified. When the Pod\\'s dnsPolicy  is set to \" None \", the list\\nmust contain at least one IP address, otherwise this property is optional. The servers\\nlisted will be combined to the base nameservers generated from the specified DNS policy\\nwith duplicate addresses removed.\\nsearches : a list of DNS search domains for hostname lookup in the Pod. This property is\\noptional. When specified, the provided list will be merged into the base search domain', metadata={'source': './PDFS/Concepts.pdf', 'page': 260}),\n",
       " Document(page_content='optional. When specified, the provided list will be merged into the base search domain\\nnames generated from the chosen DNS policy. Duplicate domain names are removed.\\nKubernetes allows up to 32 search domains.\\noptions : an optional list of objects where each object may have a name  property\\n(required) and a value  property (optional). The contents in this property will be merged to\\nthe options generated from the specified DNS policy. Duplicate entries are removed.\\nThe following is an example Pod with custom DNS settings:\\nservice/networking/custom-dns.yaml  \\napiVersion : v1\\nkind: Pod\\nmetadata :\\n  namespace : default\\n  name : dns-example\\nspec:\\n  containers :\\n    - name : test\\n      image : nginx\\n  dnsPolicy : \"None\"\\n  dnsConfig :\\n    nameservers :\\n      - 192.0.2.1  # this is an example\\n    searches :\\n      - ns1.svc.cluster-domain.example\\n      - my.dns.search.suffix\\n    options :\\n      - name : ndots\\n        value : \"2\"\\n      - name : edns0', metadata={'source': './PDFS/Concepts.pdf', 'page': 260}),\n",
       " Document(page_content='options :\\n      - name : ndots\\n        value : \"2\"\\n      - name : edns0\\nWhen the Pod above is created, the container test gets the following contents in its /etc/\\nresolv.conf  file:• \\n• \\n•', metadata={'source': './PDFS/Concepts.pdf', 'page': 260}),\n",
       " Document(page_content=\"nameserver 192.0.2.1\\nsearch ns1.svc.cluster-domain.example my.dns.search.suffix\\noptions ndots:2 edns0\\nFor IPv6 setup, search path and name server should be set up like this:\\nkubectl exec -it dns-example -- cat /etc/resolv.conf\\nThe output is similar to this:\\nnameserver 2001:db8:30::a\\nsearch default.svc.cluster-domain.example svc.cluster-domain.example cluster-domain.example\\noptions ndots:5\\nDNS search domain list limits\\nFEATURE STATE:  Kubernetes 1.28 [stable]\\nKubernetes itself does not limit the DNS Config until the length of the search domain list\\nexceeds 32 or the total length of all search domains exceeds 2048. This limit applies to the\\nnode's resolver configuration file, the Pod's DNS Config, and the merged DNS Config\\nrespectively.\\nNote:\\nSome container runtimes of earlier versions may have their own restrictions on the number of\\nDNS search domains. Depending on the container runtime environment, the pods with a large\\nnumber of DNS search domains may get stuck in the pending state.\", metadata={'source': './PDFS/Concepts.pdf', 'page': 261}),\n",
       " Document(page_content=\"number of DNS search domains may get stuck in the pending state.\\nIt is known that containerd v1.5.5 or earlier and CRI-O v1.21 or earlier have this problem.\\nDNS resolution on Windows nodes\\nClusterFirstWithHostNet is not supported for Pods that run on Windows nodes.\\nWindows treats all names with a . as a FQDN and skips FQDN resolution.\\nOn Windows, there are multiple DNS resolvers that can be used. As these come with\\nslightly different behaviors, using the Resolve-DNSName  powershell cmdlet for name\\nquery resolutions is recommended.\\nOn Linux, you have a DNS suffix list, which is used after resolution of a name as fully\\nqualified has failed. On Windows, you can only have 1 DNS suffix, which is the DNS\\nsuffix associated with that Pod's namespace (example: mydns.svc.cluster.local ). Windows\\ncan resolve FQDNs, Services, or network name which can be resolved with this single\\nsuffix. For example, a Pod spawned in the default  namespace, will have the DNS suffix\", metadata={'source': './PDFS/Concepts.pdf', 'page': 261}),\n",
       " Document(page_content=\"suffix. For example, a Pod spawned in the default  namespace, will have the DNS suffix \\ndefault.svc.cluster.local . Inside a Windows Pod, you can resolve both \\nkubernetes.default.svc.cluster.local  and kubernetes , but not the partially qualified names\\n(kubernetes.default  or kubernetes.default.svc ).\\nWhat's next\\nFor guidance on administering DNS configurations, check Configure DNS Service• \\n• \\n•\", metadata={'source': './PDFS/Concepts.pdf', 'page': 261}),\n",
       " Document(page_content='IPv4/IPv6 dual-stack\\nKubernetes lets you configure single-stack IPv4 networking, single-stack IPv6 networking, or\\ndual stack networking with both network families active. This page explains how.\\nFEATURE STATE:  Kubernetes v1.23 [stable]\\nIPv4/IPv6 dual-stack networking enables the allocation of both IPv4 and IPv6 addresses to Pods\\nand Services .\\nIPv4/IPv6 dual-stack networking is enabled by default for your Kubernetes cluster starting in\\n1.21, allowing the simultaneous assignment of both IPv4 and IPv6 addresses.\\nSupported Features\\nIPv4/IPv6 dual-stack on your Kubernetes cluster provides the following features:\\nDual-stack Pod networking (a single IPv4 and IPv6 address assignment per Pod)\\nIPv4 and IPv6 enabled Services\\nPod off-cluster egress routing (eg. the Internet) via both IPv4 and IPv6 interfaces\\nPrerequisites\\nThe following prerequisites are needed in order to utilize IPv4/IPv6 dual-stack Kubernetes\\nclusters:\\nKubernetes 1.20 or later', metadata={'source': './PDFS/Concepts.pdf', 'page': 262}),\n",
       " Document(page_content='clusters:\\nKubernetes 1.20 or later\\nFor information about using dual-stack services with earlier Kubernetes versions, refer to\\nthe documentation for that version of Kubernetes.\\nProvider support for dual-stack networking (Cloud provider or otherwise must be able to\\nprovide Kubernetes nodes with routable IPv4/IPv6 network interfaces)\\nA network plugin  that supports dual-stack networking.\\nConfigure IPv4/IPv6 dual-stack\\nTo configure IPv4/IPv6 dual-stack, set dual-stack cluster network assignments:\\nkube-apiserver:\\n--service-cluster-ip-range=<IPv4 CIDR>,<IPv6 CIDR>\\nkube-controller-manager:\\n--cluster-cidr=<IPv4 CIDR>,<IPv6 CIDR>\\n--service-cluster-ip-range=<IPv4 CIDR>,<IPv6 CIDR>\\n--node-cidr-mask-size-ipv4|--node-cidr-mask-size-ipv6  defaults to /24 for IPv4 and /\\n64 for IPv6\\nkube-proxy:\\n--cluster-cidr=<IPv4 CIDR>,<IPv6 CIDR>\\nkubelet:\\nwhen there is no --cloud-provider  the administrator can pass a comma-separated', metadata={'source': './PDFS/Concepts.pdf', 'page': 262}),\n",
       " Document(page_content='kubelet:\\nwhen there is no --cloud-provider  the administrator can pass a comma-separated\\npair of IP addresses via --node-ip  to manually configure dual-stack .status.addresses• \\n• \\n• \\n• \\n• \\n• \\n• \\n◦ \\n• \\n◦ \\n◦ \\n◦ \\n• \\n◦ \\n• \\n◦', metadata={'source': './PDFS/Concepts.pdf', 'page': 262}),\n",
       " Document(page_content='for that Node. If a Pod runs on that node in HostNetwork mode, the Pod reports\\nthese IP addresses in its .status.podIPs  field. All podIPs  in a node match the IP\\nfamily preference defined by the .status.addresses  field for that Node.\\nNote:\\nAn example of an IPv4 CIDR: 10.244.0.0/16  (though you would supply your own address range)\\nAn example of an IPv6 CIDR: fdXY:IJKL:MNOP:15::/64  (this shows the format but is not a valid\\naddress - see RFC 4193 )\\nFEATURE STATE:  Kubernetes v1.27 [alpha]\\nWhen using an external cloud provider, you can pass a dual-stack --node-ip  value to kubelet if\\nyou enable the CloudDualStackNodeIPs  feature gate in both kubelet and the external cloud\\nprovider. This is only supported for cloud providers that support dual stack clusters.\\nServices\\nYou can create Services  which can use IPv4, IPv6, or both.\\nThe address family of a Service defaults to the address family of the first service cluster IP range', metadata={'source': './PDFS/Concepts.pdf', 'page': 263}),\n",
       " Document(page_content='The address family of a Service defaults to the address family of the first service cluster IP range\\n(configured via the --service-cluster-ip-range  flag to the kube-apiserver).\\nWhen you define a Service you can optionally configure it as dual stack. To specify the\\nbehavior you want, you set the .spec.ipFamilyPolicy  field to one of the following values:\\nSingleStack : Single-stack service. The control plane allocates a cluster IP for the Service,\\nusing the first configured service cluster IP range.\\nPreferDualStack :\\nAllocates IPv4 and IPv6 cluster IPs for the Service.\\nRequireDualStack : Allocates Service .spec.ClusterIPs  from both IPv4 and IPv6 address\\nranges.\\nSelects the .spec.ClusterIP  from the list of .spec.ClusterIPs  based on the address\\nfamily of the first element in the .spec.ipFamilies  array.\\nIf you would like to define which IP family to use for single stack or define the order of IP\\nfamilies for dual-stack, you can choose the address families by setting an optional field,', metadata={'source': './PDFS/Concepts.pdf', 'page': 263}),\n",
       " Document(page_content='families for dual-stack, you can choose the address families by setting an optional field,\\n.spec.ipFamilies , on the Service.\\nNote:  The .spec.ipFamilies  field is conditionally mutable: you can add or remove a secondary IP\\naddress family, but you cannot change the primary IP address family of an existing Service.\\nYou can set .spec.ipFamilies  to any of the following array values:\\n[\"IPv4\"]\\n[\"IPv6\"]\\n[\"IPv4\",\"IPv6\"]  (dual stack)\\n[\"IPv6\",\"IPv4\"]  (dual stack)\\nThe first family you list is used for the legacy .spec.ClusterIP  field.• \\n• \\n◦ \\n• \\n◦ \\n• \\n• \\n• \\n•', metadata={'source': './PDFS/Concepts.pdf', 'page': 263}),\n",
       " Document(page_content='Dual-stack Service configuration scenarios\\nThese examples demonstrate the behavior of various dual-stack Service configuration scenarios.\\nDual-stack options on new Services\\nThis Service specification does not explicitly define .spec.ipFamilyPolicy . When you\\ncreate this Service, Kubernetes assigns a cluster IP for the Service from the first\\nconfigured service-cluster-ip-range  and sets the .spec.ipFamilyPolicy  to SingleStack .\\n(Services without selectors  and headless Services  with selectors will behave in this same\\nway.)\\nservice/networking/dual-stack-default-svc.yaml  \\napiVersion : v1\\nkind: Service\\nmetadata :\\n  name : my-service\\n  labels :\\n    app.kubernetes.io/name : MyApp\\nspec:\\n  selector :\\n    app.kubernetes.io/name : MyApp\\n  ports :\\n    - protocol : TCP\\n      port: 80\\nThis Service specification explicitly defines PreferDualStack  in .spec.ipFamilyPolicy .\\nWhen you create this Service on a dual-stack cluster, Kubernetes assigns both IPv4 and', metadata={'source': './PDFS/Concepts.pdf', 'page': 264}),\n",
       " Document(page_content='When you create this Service on a dual-stack cluster, Kubernetes assigns both IPv4 and\\nIPv6 addresses for the service. The control plane updates the .spec  for the Service to\\nrecord the IP address assignments. The field .spec.ClusterIPs  is the primary field, and\\ncontains both assigned IP addresses; .spec.ClusterIP  is a secondary field with its value\\ncalculated from .spec.ClusterIPs .\\nFor the .spec.ClusterIP  field, the control plane records the IP address that is from\\nthe same address family as the first service cluster IP range.\\nOn a single-stack cluster, the .spec.ClusterIPs  and .spec.ClusterIP  fields both only\\nlist one address.\\nOn a cluster with dual-stack enabled, specifying RequireDualStack\\nin .spec.ipFamilyPolicy  behaves the same as PreferDualStack .\\nservice/networking/dual-stack-preferred-svc.yaml  \\napiVersion : v1\\nkind: Service\\nmetadata :\\n  name : my-service\\n  labels :\\n    app.kubernetes.io/name : MyApp\\nspec:\\n  ipFamilyPolicy : PreferDualStack\\n  selector :', metadata={'source': './PDFS/Concepts.pdf', 'page': 264}),\n",
       " Document(page_content='labels :\\n    app.kubernetes.io/name : MyApp\\nspec:\\n  ipFamilyPolicy : PreferDualStack\\n  selector :\\n    app.kubernetes.io/name : MyApp\\n  ports :1. \\n2. \\n◦ \\n◦ \\n◦', metadata={'source': './PDFS/Concepts.pdf', 'page': 264}),\n",
       " Document(page_content='- protocol : TCP\\n      port: 80\\nThis Service specification explicitly defines IPv6 and IPv4 in .spec.ipFamilies  as well as\\ndefining PreferDualStack  in .spec.ipFamilyPolicy . When Kubernetes assigns an IPv6 and\\nIPv4 address in .spec.ClusterIPs , .spec.ClusterIP  is set to the IPv6 address because that is\\nthe first element in the .spec.ClusterIPs  array, overriding the default.\\nservice/networking/dual-stack-preferred-ipfamilies-svc.yaml  \\napiVersion : v1\\nkind: Service\\nmetadata :\\n  name : my-service\\n  labels :\\n    app.kubernetes.io/name : MyApp\\nspec:\\n  ipFamilyPolicy : PreferDualStack\\n  ipFamilies :\\n  - IPv6\\n  - IPv4\\n  selector :\\n    app.kubernetes.io/name : MyApp\\n  ports :\\n    - protocol : TCP\\n      port: 80\\nDual-stack defaults on existing Services\\nThese examples demonstrate the default behavior when dual-stack is newly enabled on a cluster\\nwhere Services already exist. (Upgrading an existing cluster to 1.21 or beyond will enable dual-\\nstack.)', metadata={'source': './PDFS/Concepts.pdf', 'page': 265}),\n",
       " Document(page_content='stack.)\\nWhen dual-stack is enabled on a cluster, existing Services (whether IPv4 or IPv6) are\\nconfigured by the control plane to set .spec.ipFamilyPolicy  to SingleStack  and\\nset .spec.ipFamilies  to the address family of the existing Service. The existing Service\\ncluster IP will be stored in .spec.ClusterIPs .\\nservice/networking/dual-stack-default-svc.yaml  \\napiVersion : v1\\nkind: Service\\nmetadata :\\n  name : my-service\\n  labels :\\n    app.kubernetes.io/name : MyApp\\nspec:\\n  selector :\\n    app.kubernetes.io/name : MyApp\\n  ports :\\n    - protocol : TCP\\n      port: 803. \\n1.', metadata={'source': './PDFS/Concepts.pdf', 'page': 265}),\n",
       " Document(page_content='You can validate this behavior by using kubectl to inspect an existing service.\\nkubectl get svc my-service -o yaml\\napiVersion : v1\\nkind: Service\\nmetadata :\\n  labels :\\n    app.kubernetes.io/name : MyApp\\n  name : my-service\\nspec:\\n  clusterIP : 10.0.197.123\\n  clusterIPs :\\n  - 10.0.197.123\\n  ipFamilies :\\n  - IPv4\\n  ipFamilyPolicy : SingleStack\\n  ports :\\n  - port: 80\\n    protocol : TCP\\n    targetPort : 80\\n  selector :\\n    app.kubernetes.io/name : MyApp\\n  type: ClusterIP\\nstatus :\\n  loadBalancer : {}\\nWhen dual-stack is enabled on a cluster, existing headless Services  with selectors are\\nconfigured by the control plane to set .spec.ipFamilyPolicy  to SingleStack  and\\nset .spec.ipFamilies  to the address family of the first service cluster IP range (configured\\nvia the --service-cluster-ip-range  flag to the kube-apiserver) even though .spec.ClusterIP\\nis set to None .\\nservice/networking/dual-stack-default-svc.yaml  \\napiVersion : v1\\nkind: Service\\nmetadata :\\n  name : my-service\\n  labels :', metadata={'source': './PDFS/Concepts.pdf', 'page': 266}),\n",
       " Document(page_content='apiVersion : v1\\nkind: Service\\nmetadata :\\n  name : my-service\\n  labels :\\n    app.kubernetes.io/name : MyApp\\nspec:\\n  selector :\\n    app.kubernetes.io/name : MyApp\\n  ports :\\n    - protocol : TCP\\n      port: 80\\nYou can validate this behavior by using kubectl to inspect an existing headless service\\nwith selectors.\\nkubectl get svc my-service -o yaml2.', metadata={'source': './PDFS/Concepts.pdf', 'page': 266}),\n",
       " Document(page_content='apiVersion : v1\\nkind: Service\\nmetadata :\\n  labels :\\n    app.kubernetes.io/name : MyApp\\n  name : my-service\\nspec:\\n  clusterIP : None\\n  clusterIPs :\\n  - None\\n  ipFamilies :\\n  - IPv4\\n  ipFamilyPolicy : SingleStack\\n  ports :\\n  - port: 80\\n    protocol : TCP\\n    targetPort : 80\\n  selector :\\n    app.kubernetes.io/name : MyApp\\nSwitching Services between single-stack and dual-stack\\nServices can be changed from single-stack to dual-stack and from dual-stack to single-stack.\\nTo change a Service from single-stack to dual-stack, change .spec.ipFamilyPolicy  from \\nSingleStack  to PreferDualStack  or RequireDualStack  as desired. When you change this\\nService from single-stack to dual-stack, Kubernetes assigns the missing address family so\\nthat the Service now has IPv4 and IPv6 addresses.\\nEdit the Service specification updating the .spec.ipFamilyPolicy  from SingleStack  to \\nPreferDualStack .\\nBefore:\\nspec:\\n  ipFamilyPolicy : SingleStack\\nAfter:\\nspec:\\n  ipFamilyPolicy : PreferDualStack', metadata={'source': './PDFS/Concepts.pdf', 'page': 267}),\n",
       " Document(page_content='Before:\\nspec:\\n  ipFamilyPolicy : SingleStack\\nAfter:\\nspec:\\n  ipFamilyPolicy : PreferDualStack\\nTo change a Service from dual-stack to single-stack, change .spec.ipFamilyPolicy  from \\nPreferDualStack  or RequireDualStack  to SingleStack . When you change this Service from\\ndual-stack to single-stack, Kubernetes retains only the first element in the\\n.spec.ClusterIPs  array, and sets .spec.ClusterIP  to that IP address and sets .spec.ipFamilies\\nto the address family of .spec.ClusterIPs .\\nHeadless Services without selector\\nFor Headless Services without selectors  and without .spec.ipFamilyPolicy  explicitly set,\\nthe .spec.ipFamilyPolicy  field defaults to RequireDualStack .1. \\n2.', metadata={'source': './PDFS/Concepts.pdf', 'page': 267}),\n",
       " Document(page_content='Service type LoadBalancer\\nTo provision a dual-stack load balancer for your Service:\\nSet the .spec.type  field to LoadBalancer\\nSet .spec.ipFamilyPolicy  field to PreferDualStack  or RequireDualStack\\nNote:  To use a dual-stack LoadBalancer  type Service, your cloud provider must support IPv4\\nand IPv6 load balancers.\\nEgress traffic\\nIf you want to enable egress traffic in order to reach off-cluster destinations (eg. the public\\nInternet) from a Pod that uses non-publicly routable IPv6 addresses, you need to enable the Pod\\nto use a publicly routed IPv6 address via a mechanism such as transparent proxying or IP\\nmasquerading. The ip-masq-agent  project supports IP masquerading on dual-stack clusters.\\nNote:  Ensure your CNI provider supports IPv6.\\nWindows support\\nKubernetes on Windows does not support single-stack \"IPv6-only\" networking. However, dual-\\nstack IPv4/IPv6 networking for pods and nodes with single-family services is supported.', metadata={'source': './PDFS/Concepts.pdf', 'page': 268}),\n",
       " Document(page_content=\"stack IPv4/IPv6 networking for pods and nodes with single-family services is supported.\\nYou can use IPv4/IPv6 dual-stack networking with l2bridge  networks.\\nNote:  Overlay (VXLAN) networks on Windows do not  support dual-stack networking.\\nYou can read more about the different network modes for Windows within the Networking on\\nWindows  topic.\\nWhat's next\\nValidate IPv4/IPv6 dual-stack  networking\\nEnable dual-stack networking using kubeadm\\nTopology Aware Routing\\nTopology Aware Routing  provides a mechanism to help keep network traffic within the zone\\nwhere it originated. Preferring same-zone traffic between Pods in your cluster can help with\\nreliability, performance (network latency and throughput), or cost.\\nFEATURE STATE:  Kubernetes v1.23 [beta]\\nNote:  Prior to Kubernetes 1.27, this feature was known as Topology Aware Hints .\\nTopology Aware Routing  adjusts routing behavior to prefer keeping traffic in the zone it\", metadata={'source': './PDFS/Concepts.pdf', 'page': 268}),\n",
       " Document(page_content='Topology Aware Routing  adjusts routing behavior to prefer keeping traffic in the zone it\\noriginated from. In some cases this can help reduce costs or improve network performance.• \\n• \\n• \\n•', metadata={'source': './PDFS/Concepts.pdf', 'page': 268}),\n",
       " Document(page_content='Motivation\\nKubernetes clusters are increasingly deployed in multi-zone environments. Topology Aware\\nRouting  provides a mechanism to help keep traffic within the zone it originated from. When\\ncalculating the endpoints for a Service , the EndpointSlice controller considers the topology\\n(region and zone) of each endpoint and populates the hints field to allocate it to a zone. Cluster\\ncomponents such as kube-proxy  can then consume those hints, and use them to influence how\\nthe traffic is routed (favoring topologically closer endpoints).\\nEnabling Topology Aware Routing\\nNote:  Prior to Kubernetes 1.27, this behavior was controlled using the service.kubernetes.io/\\ntopology-aware-hints  annotation.\\nYou can enable Topology Aware Routing for a Service by setting the service.kubernetes.io/\\ntopology-mode  annotation to Auto . When there are enough endpoints available in each zone,\\nTopology Hints will be populated on EndpointSlices to allocate individual endpoints to specific', metadata={'source': './PDFS/Concepts.pdf', 'page': 269}),\n",
       " Document(page_content='Topology Hints will be populated on EndpointSlices to allocate individual endpoints to specific\\nzones, resulting in traffic being routed closer to where it originated from.\\nWhen it works best\\nThis feature works best when:\\n1. Incoming traffic is evenly distributed\\nIf a large proportion of traffic is originating from a single zone, that traffic could overload the\\nsubset of endpoints that have been allocated to that zone. This feature is not recommended\\nwhen incoming traffic is expected to originate from a single zone.\\n2. The Service has 3 or more endpoints per zone\\nIn a three zone cluster, this means 9 or more endpoints. If there are fewer than 3 endpoints per\\nzone, there is a high (≈50%) probability that the EndpointSlice controller will not be able to\\nallocate endpoints evenly and instead will fall back to the default cluster-wide routing\\napproach.\\nHow It Works\\nThe \"Auto\" heuristic attempts to proportionally allocate a number of endpoints to each zone.', metadata={'source': './PDFS/Concepts.pdf', 'page': 269}),\n",
       " Document(page_content='The \"Auto\" heuristic attempts to proportionally allocate a number of endpoints to each zone.\\nNote that this heuristic works best for Services that have a significant number of endpoints.\\nEndpointSlice controller\\nThe EndpointSlice controller is responsible for setting hints on EndpointSlices when this\\nheuristic is enabled. The controller allocates a proportional amount of endpoints to each zone.\\nThis proportion is based on the allocatable  CPU cores for nodes running in that zone. For\\nexample, if one zone had 2 CPU cores and another zone only had 1 CPU core, the controller\\nwould allocate twice as many endpoints to the zone with 2 CPU cores.', metadata={'source': './PDFS/Concepts.pdf', 'page': 269}),\n",
       " Document(page_content='The following example shows what an EndpointSlice looks like when hints have been\\npopulated:\\napiVersion : discovery.k8s.io/v1\\nkind: EndpointSlice\\nmetadata :\\n  name : example-hints\\n  labels :\\n    kubernetes.io/service-name : example-svc\\naddressType : IPv4\\nports :\\n  - name : http\\n    protocol : TCP\\n    port: 80\\nendpoints :\\n  - addresses :\\n      - \"10.1.2.3\"\\n    conditions :\\n      ready : true\\n    hostname : pod-1\\n    zone : zone-a\\n    hints :\\n      forZones :\\n        - name : \"zone-a\"\\nkube-proxy\\nThe kube-proxy component filters the endpoints it routes to based on the hints set by the\\nEndpointSlice controller. In most cases, this means that the kube-proxy is able to route traffic to\\nendpoints in the same zone. Sometimes the controller allocates endpoints from a different zone\\nto ensure more even distribution of endpoints between zones. This would result in some traffic\\nbeing routed to other zones.\\nSafeguards', metadata={'source': './PDFS/Concepts.pdf', 'page': 270}),\n",
       " Document(page_content='being routed to other zones.\\nSafeguards\\nThe Kubernetes control plane and the kube-proxy on each node apply some safeguard rules\\nbefore using Topology Aware Hints. If these don\\'t check out, the kube-proxy selects endpoints\\nfrom anywhere in your cluster, regardless of the zone.\\nInsufficient number of endpoints:  If there are less endpoints than zones in a cluster,\\nthe controller will not assign any hints.\\nImpossible to achieve balanced allocation:  In some cases, it will be impossible to\\nachieve a balanced allocation of endpoints among zones. For example, if zone-a is twice\\nas large as zone-b, but there are only 2 endpoints, an endpoint allocated to zone-a may\\nreceive twice as much traffic as zone-b. The controller does not assign hints if it can\\'t get\\nthis \"expected overload\" value below an acceptable threshold for each zone. Importantly\\nthis is not based on real-time feedback. It is still possible for individual endpoints to\\nbecome overloaded.', metadata={'source': './PDFS/Concepts.pdf', 'page': 270}),\n",
       " Document(page_content='become overloaded.\\nOne or more Nodes has insufficient information:  If any node does not have a \\ntopology.kubernetes.io/zone  label or is not reporting a value for allocatable CPU, the1. \\n2. \\n3.', metadata={'source': './PDFS/Concepts.pdf', 'page': 270}),\n",
       " Document(page_content='control plane does not set any topology-aware endpoint hints and so kube-proxy does\\nnot filter endpoints by zone.\\nOne or more endpoints does not have a zone hint:  When this happens, the kube-\\nproxy assumes that a transition from or to Topology Aware Hints is underway. Filtering\\nendpoints for a Service in this state would be dangerous so the kube-proxy falls back to\\nusing all endpoints.\\nA zone is not represented in hints:  If the kube-proxy is unable to find at least one\\nendpoint with a hint targeting the zone it is running in, it falls back to using endpoints\\nfrom all zones. This is most likely to happen as you add a new zone into your existing\\ncluster.\\nConstraints\\nTopology Aware Hints are not used when internalTrafficPolicy  is set to Local  on a\\nService. It is possible to use both features in the same cluster on different Services, just\\nnot on the same Service.\\nThis approach will not work well for Services that have a large proportion of traffic', metadata={'source': './PDFS/Concepts.pdf', 'page': 271}),\n",
       " Document(page_content='This approach will not work well for Services that have a large proportion of traffic\\noriginating from a subset of zones. Instead this assumes that incoming traffic will be\\nroughly proportional to the capacity of the Nodes in each zone.\\nThe EndpointSlice controller ignores unready nodes as it calculates the proportions of\\neach zone. This could have unintended consequences if a large portion of nodes are\\nunready.\\nThe EndpointSlice controller ignores nodes with the node-role.kubernetes.io/control-\\nplane  or node-role.kubernetes.io/master  label set. This could be problematic if workloads\\nare also running on those nodes.\\nThe EndpointSlice controller does not take into account tolerations  when deploying or\\ncalculating the proportions of each zone. If the Pods backing a Service are limited to a\\nsubset of Nodes in the cluster, this will not be taken into account.\\nThis may not work well with autoscaling. For example, if a lot of traffic is originating', metadata={'source': './PDFS/Concepts.pdf', 'page': 271}),\n",
       " Document(page_content=\"This may not work well with autoscaling. For example, if a lot of traffic is originating\\nfrom a single zone, only the endpoints allocated to that zone will be handling that traffic.\\nThat could result in Horizontal Pod Autoscaler  either not picking up on this event, or\\nnewly added pods starting in a different zone.\\nCustom heuristics\\nKubernetes is deployed in many different ways, there is no single heuristic for allocating\\nendpoints to zones will work for every use case. A key goal of this feature is to enable custom\\nheuristics to be developed if the built in heuristic does not work for your use case. The first\\nsteps to enable custom heuristics were included in the 1.27 release. This is a limited\\nimplementation that may not yet cover some relevant and plausible situations.\\nWhat's next\\nFollow the Connecting Applications with Services  tutorial4. \\n5. \\n• \\n• \\n• \\n• \\n• \\n• \\n•\", metadata={'source': './PDFS/Concepts.pdf', 'page': 271}),\n",
       " Document(page_content='Networking on Windows\\nKubernetes supports running nodes on either Linux or Windows. You can mix both kinds of\\nnode within a single cluster. This page provides an overview to networking specific to the\\nWindows operating system.\\nContainer networking on Windows\\nNetworking for Windows containers is exposed through CNI plugins . Windows containers\\nfunction similarly to virtual machines in regards to networking. Each container has a virtual\\nnetwork adapter (vNIC) which is connected to a Hyper-V virtual switch (vSwitch). The Host\\nNetworking Service (HNS) and the Host Compute Service (HCS) work together to create\\ncontainers and attach container vNICs to networks. HCS is responsible for the management of\\ncontainers whereas HNS is responsible for the management of networking resources such as:\\nVirtual networks (including creation of vSwitches)\\nEndpoints / vNICs\\nNamespaces\\nPolicies including packet encapsulations, load-balancing rules, ACLs, and NAT rules.', metadata={'source': './PDFS/Concepts.pdf', 'page': 272}),\n",
       " Document(page_content=\"Namespaces\\nPolicies including packet encapsulations, load-balancing rules, ACLs, and NAT rules.\\nThe Windows HNS and vSwitch implement namespacing and can create virtual NICs as needed\\nfor a pod or container. However, many configurations such as DNS, routes, and metrics are\\nstored in the Windows registry database rather than as files inside /etc, which is how Linux\\nstores those configurations. The Windows registry for the container is separate from that of the\\nhost, so concepts like mapping /etc/resolv.conf  from the host into a container don't have the\\nsame effect they would on Linux. These must be configured using Windows APIs run in the\\ncontext of that container. Therefore CNI implementations need to call the HNS instead of\\nrelying on file mappings to pass network details into the pod or container.\\nNetwork modes\\nWindows supports five different networking drivers/modes: L2bridge, L2tunnel, Overlay (Beta),\", metadata={'source': './PDFS/Concepts.pdf', 'page': 272}),\n",
       " Document(page_content=\"Windows supports five different networking drivers/modes: L2bridge, L2tunnel, Overlay (Beta),\\nTransparent, and NAT. In a heterogeneous cluster with Windows and Linux worker nodes, you\\nneed to select a networking solution that is compatible on both Windows and Linux. The\\nfollowing table lists the out-of-tree plugins are supported on Windows, with recommendations\\non when to use each CNI:\\nNetwork\\nDriverDescriptionContainer\\nPacket\\nModificationsNetwork\\nPluginsNetwork Plugin\\nCharacteristics\\nL2bridgeContainers are\\nattached to an\\nexternal vSwitch.\\nContainers are\\nattached to the\\nunderlay network,\\nalthough the\\nphysical network\\ndoesn't need to\\nlearn the containerMAC is rewritten\\nto host MAC, IP\\nmay be rewritten\\nto host IP using\\nHNS\\nOutboundNAT\\npolicy.win-bridge , \\nAzure-CNI ,\\nFlannel host-\\ngateway uses\\nwin-bridgewin-bridge uses L2bridge\\nnetwork mode, connects\\ncontainers to the underlay\\nof hosts, offering best\\nperformance. Requires\\nuser-defined routes (UDR)\\nfor inter-node\\nconnectivity.• \\n• \\n• \\n•\", metadata={'source': './PDFS/Concepts.pdf', 'page': 272}),\n",
       " Document(page_content='Network\\nDriverDescriptionContainer\\nPacket\\nModificationsNetwork\\nPluginsNetwork Plugin\\nCharacteristics\\nMACs because they\\nare rewritten on\\ningress/egress.\\nL2TunnelThis is a special case\\nof l2bridge, but only\\nused on Azure. All\\npackets are sent to\\nthe virtualization\\nhost where SDN\\npolicy is applied.MAC rewritten, IP\\nvisible on the\\nunderlay networkAzure-CNIAzure-CNI allows\\nintegration of containers\\nwith Azure vNET, and\\nallows them to leverage\\nthe set of capabilities that \\nAzure Virtual Network\\nprovides . For example,\\nsecurely connect to Azure\\nservices or use Azure\\nNSGs. See azure-cni for\\nsome examples\\nOverlayContainers are\\ngiven a vNIC\\nconnected to an\\nexternal vSwitch.\\nEach overlay\\nnetwork gets its\\nown IP subnet,\\ndefined by a custom\\nIP prefix.The\\noverlay network\\ndriver uses VXLAN\\nencapsulation.Encapsulated\\nwith an outer\\nheader.win-overlay ,\\nFlannel\\nVXLAN (uses\\nwin-overlay)win-overlay should be\\nused when virtual\\ncontainer networks are\\ndesired to be isolated from\\nunderlay of hosts (e.g. for', metadata={'source': './PDFS/Concepts.pdf', 'page': 273}),\n",
       " Document(page_content='used when virtual\\ncontainer networks are\\ndesired to be isolated from\\nunderlay of hosts (e.g. for\\nsecurity reasons). Allows\\nfor IPs to be re-used for\\ndifferent overlay networks\\n(which have different\\nVNID tags) if you are\\nrestricted on IPs in your\\ndatacenter. This option\\nrequires KB4489899  on\\nWindows Server 2019.\\nTransparent\\n(special use\\ncase for ovn-\\nkubernetes )Requires an\\nexternal vSwitch.\\nContainers are\\nattached to an\\nexternal vSwitch\\nwhich enables intra-\\npod communication\\nvia logical networks\\n(logical switches\\nand routers).Packet is\\nencapsulated\\neither via \\nGENEVE  or STT\\ntunneling to reach\\npods which are\\nnot on the same\\nhost.\\nPackets are\\nforwarded or\\ndropped via the\\ntunnel metadata\\ninformation\\nsupplied by the\\novn network\\ncontroller.\\nNAT is done for\\nnorth-south\\ncommunication.ovn-\\nkubernetesDeploy via ansible .\\nDistributed ACLs can be\\napplied via Kubernetes\\npolicies. IPAM support.\\nLoad-balancing can be\\nachieved without kube-\\nproxy. NATing is done\\nwithout using iptables/\\nnetsh.', metadata={'source': './PDFS/Concepts.pdf', 'page': 273}),\n",
       " Document(page_content='Load-balancing can be\\nachieved without kube-\\nproxy. NATing is done\\nwithout using iptables/\\nnetsh.\\nContainers are\\ngiven a vNICnatIncluded here for\\ncompleteness', metadata={'source': './PDFS/Concepts.pdf', 'page': 273}),\n",
       " Document(page_content='Network\\nDriverDescriptionContainer\\nPacket\\nModificationsNetwork\\nPluginsNetwork Plugin\\nCharacteristics\\nNAT ( not\\nused in\\nKubernetes )connected to an\\ninternal vSwitch.\\nDNS/DHCP is\\nprovided using an\\ninternal component\\ncalled WinNATMAC and IP is\\nrewritten to host\\nMAC/IP.\\nAs outlined above, the Flannel  CNI plugin  is also supported  on Windows via the VXLAN\\nnetwork backend  (Beta support  ; delegates to win-overlay) and host-gateway network\\nbackend  (stable support; delegates to win-bridge).\\nThis plugin supports delegating to one of the reference CNI plugins (win-overlay, win-bridge),\\nto work in conjunction with Flannel daemon on Windows (Flanneld) for automatic node subnet\\nlease assignment and HNS network creation. This plugin reads in its own configuration file\\n(cni.conf), and aggregates it with the environment variables from the FlannelD generated\\nsubnet.env file. It then delegates to one of the reference CNI plugins for network plumbing, and', metadata={'source': './PDFS/Concepts.pdf', 'page': 274}),\n",
       " Document(page_content='subnet.env file. It then delegates to one of the reference CNI plugins for network plumbing, and\\nsends the correct configuration containing the node-assigned subnet to the IPAM plugin (for\\nexample: host-local ).\\nFor Node, Pod, and Service objects, the following network flows are supported for TCP/UDP\\ntraffic:\\nPod → Pod (IP)\\nPod → Pod (Name)\\nPod → Service (Cluster IP)\\nPod → Service (PQDN, but only if there are no \".\")\\nPod → Service (FQDN)\\nPod → external (IP)\\nPod → external (DNS)\\nNode → Pod\\nPod → Node\\nIP address management (IPAM)\\nThe following IPAM options are supported on Windows:\\nhost-local\\nazure-vnet-ipam  (for azure-cni only)\\nWindows Server IPAM  (fallback option if no IPAM is set)\\nLoad balancing and Services\\nA Kubernetes Service  is an abstraction that defines a logical set of Pods and a means to access\\nthem over a network. In a cluster that includes Windows nodes, you can use the following\\ntypes of Service:\\nNodePort\\nClusterIP\\nLoadBalancer• \\n• \\n• \\n• \\n• \\n• \\n• \\n• \\n• \\n• \\n• \\n• \\n• \\n• \\n•', metadata={'source': './PDFS/Concepts.pdf', 'page': 274}),\n",
       " Document(page_content='ExternalName\\nWindows container networking differs in some important ways from Linux networking. The \\nMicrosoft documentation for Windows Container Networking  provides additional details and\\nbackground.\\nOn Windows, you can use the following settings to configure Services and load balancing\\nbehavior:\\nWindows Service Settings\\nFeature DescriptionMinimum\\nSupported\\nWindows\\nOS buildHow to enable\\nSession\\naffinityEnsures that connections\\nfrom a particular client are\\npassed to the same Pod\\neach time.Windows\\nServer 2022Set service.spec.sessionAffinity  to\\n\"ClientIP\"\\nDirect Server\\nReturn (DSR)Load balancing mode\\nwhere the IP address\\nfixups and the LBNAT\\noccurs at the container\\nvSwitch port directly;\\nservice traffic arrives with\\nthe source IP set as the\\noriginating pod IP.Windows\\nServer 2019Set the following flags in kube-proxy: \\n--feature-gates=\"WinDSR=true\" --\\nenable-dsr=true\\nPreserve-\\nDestinationSkips DNAT of service\\ntraffic, thereby preserving\\nthe virtual IP of the target', metadata={'source': './PDFS/Concepts.pdf', 'page': 275}),\n",
       " Document(page_content='Preserve-\\nDestinationSkips DNAT of service\\ntraffic, thereby preserving\\nthe virtual IP of the target\\nservice in packets reaching\\nthe backend Pod. Also\\ndisables node-node\\nforwarding.Windows\\nServer,\\nversion 1903Set \"preserve-destination\": \"true\"  in\\nservice annotations and enable DSR in\\nkube-proxy.\\nIPv4/IPv6\\ndual-stack\\nnetworkingNative IPv4-to-IPv4 in\\nparallel with IPv6-to-IPv6\\ncommunications to, from,\\nand within a clusterWindows\\nServer 2019See IPv4/IPv6 dual-stack\\nClient IP\\npreservationEnsures that source IP of\\nincoming ingress traffic\\ngets preserved. Also\\ndisables node-node\\nforwarding.Windows\\nServer 2019Set service.spec.externalTrafficPolicy  to\\n\"Local\" and enable DSR in kube-proxy\\nWarning:\\nThere are known issue with NodePort Services on overlay networking, if the destination node is\\nrunning Windows Server 2022. To avoid the issue entirely, you can configure the service with \\nexternalTrafficPolicy: Local .', metadata={'source': './PDFS/Concepts.pdf', 'page': 275}),\n",
       " Document(page_content='externalTrafficPolicy: Local .\\nThere are known issues with Pod to Pod connectivity on l2bridge network on Windows Server\\n2022 with KB5005619 or higher installed. To workaround the issue and restore Pod to Pod\\nconnectivity, you can disable the WinDSR feature in kube-proxy.•', metadata={'source': './PDFS/Concepts.pdf', 'page': 275}),\n",
       " Document(page_content=\"These issues require OS fixes. Please follow https://github.com/microsoft/Windows-Containers/\\nissues/204  for updates.\\nLimitations\\nThe following networking functionality is not supported on Windows nodes:\\nHost networking mode\\nLocal NodePort access from the node itself (works for other nodes or external clients)\\nMore than 64 backend pods (or unique destination addresses) for a single Service\\nIPv6 communication between Windows pods connected to overlay networks\\nLocal Traffic Policy in non-DSR mode\\nOutbound communication using the ICMP protocol via the win-overlay , win-bridge , or\\nusing the Azure-CNI plugin.\\nSpecifically, the Windows data plane ( VFP) doesn't support ICMP packet transpositions,\\nand this means:\\nICMP packets directed to destinations within the same network (such as pod to pod\\ncommunication via ping) work as expected;\\nTCP/UDP packets work as expected;\\nICMP packets directed to pass through a remote network (e.g. pod to external\", metadata={'source': './PDFS/Concepts.pdf', 'page': 276}),\n",
       " Document(page_content='ICMP packets directed to pass through a remote network (e.g. pod to external\\ninternet communication via ping) cannot be transposed and thus will not be routed\\nback to their source;\\nSince TCP/UDP packets can still be transposed, you can substitute ping \\n<destination>  with curl <destination>  when debugging connectivity with the\\noutside world.\\nOther limitations:\\nWindows reference network plugins win-bridge and win-overlay do not implement CNI\\nspec v0.4.0, due to a missing CHECK  implementation.\\nThe Flannel VXLAN CNI plugin has the following limitations on Windows:\\nNode-pod connectivity is only possible for local pods with Flannel v0.12.0 (or\\nhigher).\\nFlannel is restricted to using VNI 4096 and UDP port 4789. See the official Flannel\\nVXLAN  backend docs for more details on these parameters.\\nService ClusterIP allocation\\nIn Kubernetes, Services  are an abstract way to expose an application running on a set of Pods.', metadata={'source': './PDFS/Concepts.pdf', 'page': 276}),\n",
       " Document(page_content=\"In Kubernetes, Services  are an abstract way to expose an application running on a set of Pods.\\nServices can have a cluster-scoped virtual IP address (using a Service of type: ClusterIP ). Clients\\ncan connect using that virtual IP address, and Kubernetes then load-balances traffic to that\\nService across the different backing Pods.\\nHow Service ClusterIPs are allocated?\\nWhen Kubernetes needs to assign a virtual IP address for a Service, that assignment happens\\none of two ways:\\ndynamically\\nthe cluster's control plane automatically picks a free IP address from within the\\nconfigured IP range for type: ClusterIP  Services.• \\n• \\n• \\n• \\n• \\n• \\n◦ \\n◦ \\n◦ \\n◦ \\n• \\n• \\n◦ \\n◦\", metadata={'source': './PDFS/Concepts.pdf', 'page': 276}),\n",
       " Document(page_content='statically\\nyou specify an IP address of your choice, from within the configured IP range for\\nServices.\\nAcross your whole cluster, every Service ClusterIP  must be unique. Trying to create a Service\\nwith a specific ClusterIP  that has already been allocated will return an error.\\nWhy do you need to reserve Service Cluster IPs?\\nSometimes you may want to have Services running in well-known IP addresses, so other\\ncomponents and users in the cluster can use them.\\nThe best example is the DNS Service for the cluster. As a soft convention, some Kubernetes\\ninstallers assign the 10th IP address from the Service IP range to the DNS service. Assuming\\nyou configured your cluster with Service IP range 10.96.0.0/16 and you want your DNS Service\\nIP to be 10.96.0.10, you\\'d have to create a Service like this:\\napiVersion : v1\\nkind: Service\\nmetadata :\\n  labels :\\n    k8s-app : kube-dns\\n    kubernetes.io/cluster-service : \"true\"\\n    kubernetes.io/name : CoreDNS\\n  name : kube-dns\\n  namespace : kube-system', metadata={'source': './PDFS/Concepts.pdf', 'page': 277}),\n",
       " Document(page_content='kubernetes.io/name : CoreDNS\\n  name : kube-dns\\n  namespace : kube-system\\nspec:\\n  clusterIP : 10.96.0.10\\n  ports :\\n  - name : dns\\n    port: 53\\n    protocol : UDP\\n    targetPort : 53\\n  - name : dns-tcp\\n    port: 53\\n    protocol : TCP\\n    targetPort : 53\\n  selector :\\n    k8s-app : kube-dns\\n  type: ClusterIP\\nbut as it was explained before, the IP address 10.96.0.10 has not been reserved; if other Services\\nare created before or in parallel with dynamic allocation, there is a chance they can allocate this\\nIP, hence, you will not be able to create the DNS Service because it will fail with a conflict error.\\nHow can you avoid Service ClusterIP conflicts?\\nThe allocation strategy implemented in Kubernetes to allocate ClusterIPs to Services reduces\\nthe risk of collision.\\nThe ClusterIP  range is divided, based on the formula min(max(16, cidrSize / 16), 256) , described\\nas never less than 16 or more than 256 with a graduated step between them .', metadata={'source': './PDFS/Concepts.pdf', 'page': 277}),\n",
       " Document(page_content='Dynamic IP assignment uses the upper band by default, once this has been exhausted it will use\\nthe lower range. This will allow users to use static allocations on the lower band with a low risk\\nof collision.\\nExamples\\nExample 1\\nThis example uses the IP address range: 10.96.0.0/24 (CIDR notation) for the IP addresses of\\nServices.\\nRange Size: 28 - 2 = 254\\nBand Offset: min(max(16, 256/16), 256)  = min(16, 256)  = 16\\nStatic band start: 10.96.0.1\\nStatic band end: 10.96.0.16\\nRange end: 10.96.0.254\\npie showData title 10.96.0.0/24 \"Static\" : 16 \"Dynamic\" : 238\\nJavaScript must be enabled  to view this content\\nExample 2\\nThis example uses the IP address range: 10.96.0.0/20 (CIDR notation) for the IP addresses of\\nServices.\\nRange Size: 212 - 2 = 4094\\nBand Offset: min(max(16, 4096/16), 256)  = min(256, 256)  = 256\\nStatic band start: 10.96.0.1\\nStatic band end: 10.96.1.0\\nRange end: 10.96.15.254\\npie showData title 10.96.0.0/20 \"Static\" : 256 \"Dynamic\" : 3838', metadata={'source': './PDFS/Concepts.pdf', 'page': 278}),\n",
       " Document(page_content='Range end: 10.96.15.254\\npie showData title 10.96.0.0/20 \"Static\" : 256 \"Dynamic\" : 3838\\nJavaScript must be enabled  to view this content\\nExample 3\\nThis example uses the IP address range: 10.96.0.0/16 (CIDR notation) for the IP addresses of\\nServices.\\nRange Size: 216 - 2 = 65534\\nBand Offset: min(max(16, 65536/16), 256)  = min(4096, 256)  = 256\\nStatic band start: 10.96.0.1\\nStatic band ends: 10.96.1.0\\nRange end: 10.96.255.254\\npie showData title 10.96.0.0/16 \"Static\" : 256 \"Dynamic\" : 65278\\nJavaScript must be enabled  to view this content', metadata={'source': './PDFS/Concepts.pdf', 'page': 278}),\n",
       " Document(page_content='What\\'s next\\nRead about Service External Traffic Policy\\nRead about Connecting Applications with Services\\nRead about Services\\nService Internal Traffic Policy\\nIf two Pods in your cluster want to communicate, and both Pods are actually running on the\\nsame node, use Service Internal Traffic Policy  to keep network traffic within that node. Avoiding\\na round trip via the cluster network can help with reliability, performance (network latency and\\nthroughput), or cost.\\nFEATURE STATE:  Kubernetes v1.26 [stable]\\nService Internal Traffic Policy  enables internal traffic restrictions to only route internal traffic to\\nendpoints within the node the traffic originated from. The \"internal\" traffic here refers to traffic\\noriginated from Pods in the current cluster. This can help to reduce costs and improve\\nperformance.\\nUsing Service Internal Traffic Policy\\nYou can enable the internal-only traffic policy for a Service , by setting', metadata={'source': './PDFS/Concepts.pdf', 'page': 279}),\n",
       " Document(page_content=\"You can enable the internal-only traffic policy for a Service , by setting\\nits .spec.internalTrafficPolicy  to Local . This tells kube-proxy to only use node local endpoints\\nfor cluster internal traffic.\\nNote:  For pods on nodes with no endpoints for a given Service, the Service behaves as if it has\\nzero endpoints (for Pods on this node) even if the service does have endpoints on other nodes.\\nThe following example shows what a Service looks like when you set .spec.internalTrafficPolicy\\nto Local :\\napiVersion : v1\\nkind: Service\\nmetadata :\\n  name : my-service\\nspec:\\n  selector :\\n    app.kubernetes.io/name : MyApp\\n  ports :\\n    - protocol : TCP\\n      port: 80\\n      targetPort : 9376\\n  internalTrafficPolicy : Local\\nHow it works\\nThe kube-proxy filters the endpoints it routes to based on the spec.internalTrafficPolicy  setting.\\nWhen it's set to Local , only node local endpoints are considered. When it's Cluster  (the default),\\nor is not set, Kubernetes considers all endpoints.• \\n• \\n•\", metadata={'source': './PDFS/Concepts.pdf', 'page': 279}),\n",
       " Document(page_content=\"What's next\\nRead about Topology Aware Routing\\nRead about Service External Traffic Policy\\nFollow the Connecting Applications with Services  tutorial\\nStorage\\nWays to provide both long-term and temporary storage to Pods in your cluster.\\nVolumes\\nPersistent Volumes\\nProjected Volumes\\nEphemeral Volumes\\nStorage Classes\\nDynamic Volume Provisioning\\nVolume Snapshots\\nVolume Snapshot Classes\\nCSI Volume Cloning\\nStorage Capacity\\nNode-specific Volume Limits\\nVolume Health Monitoring\\nWindows Storage\\nVolumes\\nOn-disk files in a container are ephemeral, which presents some problems for non-trivial\\napplications when running in containers. One problem occurs when a container crashes or is\\nstopped. Container state is not saved so all of the files that were created or modified during the\\nlifetime of the container are lost. During a crash, kubelet restarts the container with a clean\\nstate. Another problem occurs when multiple containers are running in a Pod and need to share\", metadata={'source': './PDFS/Concepts.pdf', 'page': 280}),\n",
       " Document(page_content='state. Another problem occurs when multiple containers are running in a Pod and need to share\\nfiles. It can be challenging to setup and access a shared filesystem across all of the containers.\\nThe Kubernetes volume  abstraction solves both of these problems. Familiarity with Pods  is\\nsuggested.• \\n• \\n•', metadata={'source': './PDFS/Concepts.pdf', 'page': 280}),\n",
       " Document(page_content='Background\\nKubernetes supports many types of volumes. A Pod can use any number of volume types\\nsimultaneously. Ephemeral volume  types have a lifetime of a pod, but persistent volumes  exist\\nbeyond the lifetime of a pod. When a pod ceases to exist, Kubernetes destroys ephemeral\\nvolumes; however, Kubernetes does not destroy persistent volumes. For any kind of volume in a\\ngiven pod, data is preserved across container restarts.\\nAt its core, a volume is a directory, possibly with some data in it, which is accessible to the\\ncontainers in a pod. How that directory comes to be, the medium that backs it, and the contents\\nof it are determined by the particular volume type used.\\nTo use a volume, specify the volumes to provide for the Pod in .spec.volumes  and declare where\\nto mount those volumes into containers in .spec.containers[*].volumeMounts . A process in a\\ncontainer sees a filesystem view composed from the initial contents of the container image , plus', metadata={'source': './PDFS/Concepts.pdf', 'page': 281}),\n",
       " Document(page_content='container sees a filesystem view composed from the initial contents of the container image , plus\\nvolumes (if defined) mounted inside the container. The process sees a root filesystem that\\ninitially matches the contents of the container image. Any writes to within that filesystem\\nhierarchy, if allowed, affect what that process views when it performs a subsequent filesystem\\naccess. Volumes mount at the specified paths  within the image. For each container defined\\nwithin a Pod, you must independently specify where to mount each volume that the container\\nuses.\\nVolumes cannot mount within other volumes (but see Using subPath  for a related mechanism).\\nAlso, a volume cannot contain a hard link to anything in a different volume.\\nTypes of volumes\\nKubernetes supports several types of volumes.\\nawsElasticBlockStore (removed)\\nKubernetes 1.28 does not include a awsElasticBlockStore  volume type.\\nThe AWSElasticBlockStore in-tree storage driver was deprecated in the Kubernetes v1.19', metadata={'source': './PDFS/Concepts.pdf', 'page': 281}),\n",
       " Document(page_content='The AWSElasticBlockStore in-tree storage driver was deprecated in the Kubernetes v1.19\\nrelease and then removed entirely in the v1.27 release.\\nThe Kubernetes project suggests that you use the AWS EBS  third party storage driver instead.\\nazureDisk (removed)\\nKubernetes 1.28 does not include a azureDisk  volume type.\\nThe AzureDisk in-tree storage driver was deprecated in the Kubernetes v1.19 release and then\\nremoved entirely in the v1.27 release.\\nThe Kubernetes project suggests that you use the Azure Disk  third party storage driver instead.\\nazureFile (deprecated)\\nFEATURE STATE:  Kubernetes v1.21 [deprecated]\\nThe azureFile  volume type mounts a Microsoft Azure File volume (SMB 2.1 and 3.0) into a pod.', metadata={'source': './PDFS/Concepts.pdf', 'page': 281}),\n",
       " Document(page_content=\"For more details, see the azureFile  volume plugin .\\nazureFile CSI migration\\nFEATURE STATE:  Kubernetes v1.26 [stable]\\nThe CSIMigration  feature for azureFile , when enabled, redirects all plugin operations from the\\nexisting in-tree plugin to the file.csi.azure.com  Container Storage Interface (CSI) Driver. In\\norder to use this feature, the Azure File CSI Driver  must be installed on the cluster and the \\nCSIMigrationAzureFile  feature gates  must be enabled.\\nAzure File CSI driver does not support using same volume with different fsgroups. If \\nCSIMigrationAzureFile  is enabled, using same volume with different fsgroups won't be\\nsupported at all.\\nazureFile CSI migration complete\\nFEATURE STATE:  Kubernetes v1.21 [alpha]\\nTo disable the azureFile  storage plugin from being loaded by the controller manager and the\\nkubelet, set the InTreePluginAzureFileUnregister  flag to true.\\ncephfs\\nFEATURE STATE:  Kubernetes v1.28 [deprecated]\", metadata={'source': './PDFS/Concepts.pdf', 'page': 282}),\n",
       " Document(page_content='cephfs\\nFEATURE STATE:  Kubernetes v1.28 [deprecated]\\nNote:  The Kubernetes project suggests that you use the CephFS CSI  third party storage driver\\ninstead.\\nA cephfs  volume allows an existing CephFS volume to be mounted into your Pod. Unlike \\nemptyDir , which is erased when a pod is removed, the contents of a cephfs  volume are\\npreserved and the volume is merely unmounted. This means that a cephfs  volume can be pre-\\npopulated with data, and that data can be shared between pods. The cephfs  volume can be\\nmounted by multiple writers simultaneously.\\nNote:  You must have your own Ceph server running with the share exported before you can\\nuse it.\\nSee the CephFS example  for more details.\\ncinder (removed)\\nKubernetes 1.28 does not include a cinder  volume type.\\nThe OpenStack Cinder in-tree storage driver was deprecated in the Kubernetes v1.11 release\\nand then removed entirely in the v1.26 release.\\nThe Kubernetes project suggests that you use the OpenStack Cinder  third party storage driver', metadata={'source': './PDFS/Concepts.pdf', 'page': 282}),\n",
       " Document(page_content='The Kubernetes project suggests that you use the OpenStack Cinder  third party storage driver\\ninstead.', metadata={'source': './PDFS/Concepts.pdf', 'page': 282}),\n",
       " Document(page_content='configMap\\nA ConfigMap  provides a way to inject configuration data into pods. The data stored in a\\nConfigMap can be referenced in a volume of type configMap  and then consumed by\\ncontainerized applications running in a pod.\\nWhen referencing a ConfigMap, you provide the name of the ConfigMap in the volume. You\\ncan customize the path to use for a specific entry in the ConfigMap. The following\\nconfiguration shows how to mount the log-config  ConfigMap onto a Pod called configmap-pod :\\napiVersion : v1\\nkind: Pod\\nmetadata :\\n  name : configmap-pod\\nspec:\\n  containers :\\n    - name : test\\n      image : busybox:1.28\\n      command : [\\'sh\\', \\'-c\\', \\'echo \"The app is running!\" && tail -f /dev/null\\' ]\\n      volumeMounts :\\n        - name : config-vol\\n          mountPath : /etc/config\\n  volumes :\\n    - name : config-vol\\n      configMap :\\n        name : log-config\\n        items :\\n          - key: log_level\\n            path: log_level', metadata={'source': './PDFS/Concepts.pdf', 'page': 283}),\n",
       " Document(page_content=\"name : log-config\\n        items :\\n          - key: log_level\\n            path: log_level\\nThe log-config  ConfigMap is mounted as a volume, and all contents stored in its log_level  entry\\nare mounted into the Pod at path /etc/config/log_level . Note that this path is derived from the\\nvolume's mountPath  and the path keyed with log_level .\\nNote:\\nYou must create a ConfigMap  before you can use it.\\nA ConfigMap is always mounted as readOnly .\\nA container using a ConfigMap as a subPath  volume mount will not receive ConfigMap\\nupdates.\\nText data is exposed as files using the UTF-8 character encoding. For other character\\nencodings, use binaryData .\\ndownwardAPI\\nA downwardAPI  volume makes downward API  data available to applications. Within the\\nvolume, you can find the exposed data as read-only files in plain text format.\\nNote:  A container using the downward API as a subPath  volume mount does not receive\\nupdates when field values change.• \\n• \\n• \\n•\", metadata={'source': './PDFS/Concepts.pdf', 'page': 283}),\n",
       " Document(page_content='See Expose Pod Information to Containers Through Files  to learn more.\\nemptyDir\\nFor a Pod that defines an emptyDir  volume, the volume is created when the Pod is assigned to a\\nnode. As the name says, the emptyDir  volume is initially empty. All containers in the Pod can\\nread and write the same files in the emptyDir  volume, though that volume can be mounted at\\nthe same or different paths in each container. When a Pod is removed from a node for any\\nreason, the data in the emptyDir  is deleted permanently.\\nNote:  A container crashing does not remove a Pod from a node. The data in an emptyDir\\nvolume is safe across container crashes.\\nSome uses for an emptyDir  are:\\nscratch space, such as for a disk-based merge sort\\ncheckpointing a long computation for recovery from crashes\\nholding files that a content-manager container fetches while a webserver container serves\\nthe data\\nThe emptyDir.medium  field controls where emptyDir  volumes are stored. By default emptyDir', metadata={'source': './PDFS/Concepts.pdf', 'page': 284}),\n",
       " Document(page_content='the data\\nThe emptyDir.medium  field controls where emptyDir  volumes are stored. By default emptyDir\\nvolumes are stored on whatever medium that backs the node such as disk, SSD, or network\\nstorage, depending on your environment. If you set the emptyDir.medium  field to \"Memory\" ,\\nKubernetes mounts a tmpfs (RAM-backed filesystem) for you instead. While tmpfs is very fast\\nbe aware that, unlike disks, files you write count against the memory limit of the container that\\nwrote them.\\nA size limit can be specified for the default medium, which limits the capacity of the emptyDir\\nvolume. The storage is allocated from node ephemeral storage . If that is filled up from another\\nsource (for example, log files or image overlays), the emptyDir  may run out of capacity before\\nthis limit.\\nNote:  If the SizeMemoryBackedVolumes  feature gate  is enabled, you can specify a size for\\nmemory backed volumes. If no size is specified, memory backed volumes are sized to node\\nallocatable memory.', metadata={'source': './PDFS/Concepts.pdf', 'page': 284}),\n",
       " Document(page_content='allocatable memory.\\nemptyDir configuration example\\napiVersion : v1\\nkind: Pod\\nmetadata :\\n  name : test-pd\\nspec:\\n  containers :\\n  - image : registry.k8s.io/test-webserver\\n    name : test-container\\n    volumeMounts :\\n    - mountPath : /cache\\n      name : cache-volume\\n  volumes :\\n  - name : cache-volume\\n    emptyDir :\\n      sizeLimit : 500Mi• \\n• \\n•', metadata={'source': './PDFS/Concepts.pdf', 'page': 284}),\n",
       " Document(page_content='fc (fibre channel)\\nAn fc volume type allows an existing fibre channel block storage volume to mount in a Pod.\\nYou can specify single or multiple target world wide names (WWNs) using the parameter \\ntargetWWNs  in your Volume configuration. If multiple WWNs are specified, targetWWNs\\nexpect that those WWNs are from multi-path connections.\\nNote:  You must configure FC SAN Zoning to allocate and mask those LUNs (volumes) to the\\ntarget WWNs beforehand so that Kubernetes hosts can access them.\\nSee the fibre channel example  for more details.\\ngcePersistentDisk (deprecated)\\nFEATURE STATE:  Kubernetes v1.17 [deprecated]\\nA gcePersistentDisk  volume mounts a Google Compute Engine (GCE) persistent disk  (PD) into\\nyour Pod. Unlike emptyDir , which is erased when a pod is removed, the contents of a PD are\\npreserved and the volume is merely unmounted. This means that a PD can be pre-populated\\nwith data, and that data can be shared between pods.', metadata={'source': './PDFS/Concepts.pdf', 'page': 285}),\n",
       " Document(page_content='with data, and that data can be shared between pods.\\nNote:  You must create a PD using gcloud  or the GCE API or UI before you can use it.\\nThere are some restrictions when using a gcePersistentDisk :\\nthe nodes on which Pods are running must be GCE VMs\\nthose VMs need to be in the same GCE project and zone as the persistent disk\\nOne feature of GCE persistent disk is concurrent read-only access to a persistent disk. A \\ngcePersistentDisk  volume permits multiple consumers to simultaneously mount a persistent\\ndisk as read-only. This means that you can pre-populate a PD with your dataset and then serve\\nit in parallel from as many Pods as you need. Unfortunately, PDs can only be mounted by a\\nsingle consumer in read-write mode. Simultaneous writers are not allowed.\\nUsing a GCE persistent disk with a Pod controlled by a ReplicaSet will fail unless the PD is\\nread-only or the replica count is 0 or 1.\\nCreating a GCE persistent disk', metadata={'source': './PDFS/Concepts.pdf', 'page': 285}),\n",
       " Document(page_content='read-only or the replica count is 0 or 1.\\nCreating a GCE persistent disk\\nBefore you can use a GCE persistent disk with a Pod, you need to create it.\\ngcloud compute disks create --size =500GB --zone =us-central1-a my-data-disk\\nGCE persistent disk configuration example\\napiVersion : v1\\nkind: Pod\\nmetadata :\\n  name : test-pd\\nspec:\\n  containers :\\n  - image : registry.k8s.io/test-webserver\\n    name : test-container• \\n•', metadata={'source': './PDFS/Concepts.pdf', 'page': 285}),\n",
       " Document(page_content='volumeMounts :\\n    - mountPath : /test-pd\\n      name : test-volume\\n  volumes :\\n  - name : test-volume\\n    # This GCE PD must already exist.\\n    gcePersistentDisk :\\n      pdName : my-data-disk\\n      fsType : ext4\\nRegional persistent disks\\nThe Regional persistent disks  feature allows the creation of persistent disks that are available in\\ntwo zones within the same region. In order to use this feature, the volume must be provisioned\\nas a PersistentVolume; referencing the volume directly from a pod is not supported.\\nManually provisioning a Regional PD PersistentVolume\\nDynamic provisioning is possible using a StorageClass for GCE PD . Before creating a\\nPersistentVolume, you must create the persistent disk:\\ngcloud compute disks create --size =500GB my-data-disk\\n  --region us-central1\\n  --replica-zones us-central1-a,us-central1-b\\nRegional persistent disk configuration example\\napiVersion : v1\\nkind: PersistentVolume\\nmetadata :\\n  name : test-volume\\nspec:\\n  capacity :\\n    storage : 400Gi', metadata={'source': './PDFS/Concepts.pdf', 'page': 286}),\n",
       " Document(page_content='kind: PersistentVolume\\nmetadata :\\n  name : test-volume\\nspec:\\n  capacity :\\n    storage : 400Gi\\n  accessModes :\\n  - ReadWriteOnce\\n  gcePersistentDisk :\\n    pdName : my-data-disk\\n    fsType : ext4\\n  nodeAffinity :\\n    required :\\n      nodeSelectorTerms :\\n      - matchExpressions :\\n        # failure-domain.beta.kubernetes.io/zone should be used prior to 1.21\\n        - key: topology.kubernetes.io/zone\\n          operator : In\\n          values :\\n          - us-central1-a\\n          - us-central1-b', metadata={'source': './PDFS/Concepts.pdf', 'page': 286}),\n",
       " Document(page_content=\"GCE CSI migration\\nFEATURE STATE:  Kubernetes v1.25 [stable]\\nThe CSIMigration  feature for GCE PD, when enabled, redirects all plugin operations from the\\nexisting in-tree plugin to the pd.csi.storage.gke.io  Container Storage Interface (CSI) Driver. In\\norder to use this feature, the GCE PD CSI Driver  must be installed on the cluster.\\nGCE CSI migration complete\\nFEATURE STATE:  Kubernetes v1.21 [alpha]\\nTo disable the gcePersistentDisk  storage plugin from being loaded by the controller manager\\nand the kubelet, set the InTreePluginGCEUnregister  flag to true.\\ngitRepo (deprecated)\\nWarning:  The gitRepo  volume type is deprecated. To provision a container with a git repo,\\nmount an EmptyDir  into an InitContainer that clones the repo using git, then mount the \\nEmptyDir  into the Pod's container.\\nA gitRepo  volume is an example of a volume plugin. This plugin mounts an empty directory\\nand clones a git repository into this directory for your Pod to use.\\nHere is an example of a gitRepo  volume:\", metadata={'source': './PDFS/Concepts.pdf', 'page': 287}),\n",
       " Document(page_content='Here is an example of a gitRepo  volume:\\napiVersion : v1\\nkind: Pod\\nmetadata :\\n  name : server\\nspec:\\n  containers :\\n  - image : nginx\\n    name : nginx\\n    volumeMounts :\\n    - mountPath : /mypath\\n      name : git-volume\\n  volumes :\\n  - name : git-volume\\n    gitRepo :\\n      repository : \"git@somewhere:me/my-git-repository.git\"\\n      revision : \"22f1d8406d464b0c0874075539c1f2e96c253775\"\\nglusterfs (removed)\\nKubernetes 1.28 does not include a glusterfs  volume type.\\nThe GlusterFS in-tree storage driver was deprecated in the Kubernetes v1.25 release and then\\nremoved entirely in the v1.26 release.', metadata={'source': './PDFS/Concepts.pdf', 'page': 287}),\n",
       " Document(page_content=\"hostPath\\nWarning:\\nHostPath volumes present many security risks, and it is a best practice to avoid the use of\\nHostPaths when possible. When a HostPath volume must be used, it should be scoped to only\\nthe required file or directory, and mounted as ReadOnly.\\nIf restricting HostPath access to specific directories through AdmissionPolicy, volumeMounts\\nMUST be required to use readOnly  mounts for the policy to be effective.\\nA hostPath  volume mounts a file or directory from the host node's filesystem into your Pod.\\nThis is not something that most Pods will need, but it offers a powerful escape hatch for some\\napplications.\\nFor example, some uses for a hostPath  are:\\nrunning a container that needs access to Docker internals; use a hostPath  of /var/lib/\\ndocker\\nrunning cAdvisor in a container; use a hostPath  of /sys\\nallowing a Pod to specify whether a given hostPath  should exist prior to the Pod running,\\nwhether it should be created, and what it should exist as\", metadata={'source': './PDFS/Concepts.pdf', 'page': 288}),\n",
       " Document(page_content='whether it should be created, and what it should exist as\\nIn addition to the required path property, you can optionally specify a type for a hostPath\\nvolume.\\nThe supported values for field type are:\\nValue Behavior\\nEmpty string (default) is for backward compatibility, which means that no\\nchecks will be performed before mounting the hostPath volume.\\nDirectoryOrCreateIf nothing exists at the given path, an empty directory will be created there\\nas needed with permission set to 0755, having the same group and\\nownership with Kubelet.\\nDirectory A directory must exist at the given path\\nFileOrCreateIf nothing exists at the given path, an empty file will be created there as\\nneeded with permission set to 0644, having the same group and ownership\\nwith Kubelet.\\nFile A file must exist at the given path\\nSocket A UNIX socket must exist at the given path\\nCharDevice A character device must exist at the given path\\nBlockDevice A block device must exist at the given path', metadata={'source': './PDFS/Concepts.pdf', 'page': 288}),\n",
       " Document(page_content='BlockDevice A block device must exist at the given path\\nWatch out when using this type of volume, because:\\nHostPaths can expose privileged system credentials (such as for the Kubelet) or privileged\\nAPIs (such as container runtime socket), which can be used for container escape or to\\nattack other parts of the cluster.\\nPods with identical configuration (such as created from a PodTemplate) may behave\\ndifferently on different nodes due to different files on the nodes• \\n• \\n• \\n• \\n•', metadata={'source': './PDFS/Concepts.pdf', 'page': 288}),\n",
       " Document(page_content='The files or directories created on the underlying hosts are only writable by root. You\\neither need to run your process as root in a privileged Container  or modify the file\\npermissions on the host to be able to write to a hostPath  volume\\nhostPath configuration example\\napiVersion : v1\\nkind: Pod\\nmetadata :\\n  name : test-pd\\nspec:\\n  containers :\\n  - image : registry.k8s.io/test-webserver\\n    name : test-container\\n    volumeMounts :\\n    - mountPath : /test-pd\\n      name : test-volume\\n  volumes :\\n  - name : test-volume\\n    hostPath :\\n      # directory location on host\\n      path: /data\\n      # this field is optional\\n      type: Directory\\nCaution:  The FileOrCreate  mode does not create the parent directory of the file. If the parent\\ndirectory of the mounted file does not exist, the pod fails to start. To ensure that this mode\\nworks, you can try to mount directories and files separately, as shown in the \\nFileOrCreate configuration .\\nhostPath FileOrCreate configuration example', metadata={'source': './PDFS/Concepts.pdf', 'page': 289}),\n",
       " Document(page_content='FileOrCreate configuration .\\nhostPath FileOrCreate configuration example\\napiVersion : v1\\nkind: Pod\\nmetadata :\\n  name : test-webserver\\nspec:\\n  containers :\\n  - name : test-webserver\\n    image : registry.k8s.io/test-webserver:latest\\n    volumeMounts :\\n    - mountPath : /var/local/aaa\\n      name : mydir\\n    - mountPath : /var/local/aaa/1.txt\\n      name : myfile\\n  volumes :\\n  - name : mydir\\n    hostPath :\\n      # Ensure the file directory is created.\\n      path: /var/local/aaa\\n      type: DirectoryOrCreate\\n  - name : myfile•', metadata={'source': './PDFS/Concepts.pdf', 'page': 289}),\n",
       " Document(page_content='hostPath :\\n      path: /var/local/aaa/1.txt\\n      type: FileOrCreate\\niscsi\\nAn iscsi volume allows an existing iSCSI (SCSI over IP) volume to be mounted into your Pod.\\nUnlike emptyDir , which is erased when a Pod is removed, the contents of an iscsi volume are\\npreserved and the volume is merely unmounted. This means that an iscsi volume can be pre-\\npopulated with data, and that data can be shared between pods.\\nNote:  You must have your own iSCSI server running with the volume created before you can\\nuse it.\\nA feature of iSCSI is that it can be mounted as read-only by multiple consumers simultaneously.\\nThis means that you can pre-populate a volume with your dataset and then serve it in parallel\\nfrom as many Pods as you need. Unfortunately, iSCSI volumes can only be mounted by a single\\nconsumer in read-write mode. Simultaneous writers are not allowed.\\nSee the iSCSI example  for more details.\\nlocal', metadata={'source': './PDFS/Concepts.pdf', 'page': 290}),\n",
       " Document(page_content=\"See the iSCSI example  for more details.\\nlocal\\nA local  volume represents a mounted local storage device such as a disk, partition or directory.\\nLocal volumes can only be used as a statically created PersistentVolume. Dynamic provisioning\\nis not supported.\\nCompared to hostPath  volumes, local  volumes are used in a durable and portable manner\\nwithout manually scheduling pods to nodes. The system is aware of the volume's node\\nconstraints by looking at the node affinity on the PersistentVolume.\\nHowever, local  volumes are subject to the availability of the underlying node and are not\\nsuitable for all applications. If a node becomes unhealthy, then the local  volume becomes\\ninaccessible by the pod. The pod using this volume is unable to run. Applications using local\\nvolumes must be able to tolerate this reduced availability, as well as potential data loss,\\ndepending on the durability characteristics of the underlying disk.\", metadata={'source': './PDFS/Concepts.pdf', 'page': 290}),\n",
       " Document(page_content='depending on the durability characteristics of the underlying disk.\\nThe following example shows a PersistentVolume using a local  volume and nodeAffinity :\\napiVersion : v1\\nkind: PersistentVolume\\nmetadata :\\n  name : example-pv\\nspec:\\n  capacity :\\n    storage : 100Gi\\n  volumeMode : Filesystem\\n  accessModes :\\n  - ReadWriteOnce\\n  persistentVolumeReclaimPolicy : Delete\\n  storageClassName : local-storage\\n  local :', metadata={'source': './PDFS/Concepts.pdf', 'page': 290}),\n",
       " Document(page_content='path: /mnt/disks/ssd1\\n  nodeAffinity :\\n    required :\\n      nodeSelectorTerms :\\n      - matchExpressions :\\n        - key: kubernetes.io/hostname\\n          operator : In\\n          values :\\n          - example-node\\nYou must set a PersistentVolume nodeAffinity  when using local  volumes. The Kubernetes\\nscheduler uses the PersistentVolume nodeAffinity  to schedule these Pods to the correct node.\\nPersistentVolume volumeMode  can be set to \"Block\" (instead of the default value \"Filesystem\")\\nto expose the local volume as a raw block device.\\nWhen using local volumes, it is recommended to create a StorageClass with \\nvolumeBindingMode  set to WaitForFirstConsumer . For more details, see the local StorageClass\\nexample. Delaying volume binding ensures that the PersistentVolumeClaim binding decision\\nwill also be evaluated with any other node constraints the Pod may have, such as node resource\\nrequirements, node selectors, Pod affinity, and Pod anti-affinity.', metadata={'source': './PDFS/Concepts.pdf', 'page': 291}),\n",
       " Document(page_content='requirements, node selectors, Pod affinity, and Pod anti-affinity.\\nAn external static provisioner can be run separately for improved management of the local\\nvolume lifecycle. Note that this provisioner does not support dynamic provisioning yet. For an\\nexample on how to run an external local provisioner, see the local volume provisioner user\\nguide .\\nNote:  The local PersistentVolume requires manual cleanup and deletion by the user if the\\nexternal static provisioner is not used to manage the volume lifecycle.\\nnfs\\nAn nfs volume allows an existing NFS (Network File System) share to be mounted into a Pod.\\nUnlike emptyDir , which is erased when a Pod is removed, the contents of an nfs volume are\\npreserved and the volume is merely unmounted. This means that an NFS volume can be pre-\\npopulated with data, and that data can be shared between pods. NFS can be mounted by\\nmultiple writers simultaneously.\\napiVersion : v1\\nkind: Pod\\nmetadata :\\n  name : test-pd\\nspec:\\n  containers :', metadata={'source': './PDFS/Concepts.pdf', 'page': 291}),\n",
       " Document(page_content='apiVersion : v1\\nkind: Pod\\nmetadata :\\n  name : test-pd\\nspec:\\n  containers :\\n  - image : registry.k8s.io/test-webserver\\n    name : test-container\\n    volumeMounts :\\n    - mountPath : /my-nfs-data\\n      name : test-volume\\n  volumes :\\n  - name : test-volume\\n    nfs:\\n      server : my-nfs-server.example.com', metadata={'source': './PDFS/Concepts.pdf', 'page': 291}),\n",
       " Document(page_content='path: /my-nfs-volume\\n      readOnly : true\\nNote:\\nYou must have your own NFS server running with the share exported before you can use it.\\nAlso note that you can\\'t specify NFS mount options in a Pod spec. You can either set mount\\noptions server-side or use /etc/nfsmount.conf . You can also mount NFS volumes via\\nPersistentVolumes which do allow you to set mount options.\\nSee the NFS example  for an example of mounting NFS volumes with PersistentVolumes.\\npersistentVolumeClaim\\nA persistentVolumeClaim  volume is used to mount a PersistentVolume  into a Pod.\\nPersistentVolumeClaims are a way for users to \"claim\" durable storage (such as a GCE\\nPersistentDisk or an iSCSI volume) without knowing the details of the particular cloud\\nenvironment.\\nSee the information about PersistentVolumes  for more details.\\nportworxVolume (deprecated)\\nFEATURE STATE:  Kubernetes v1.25 [deprecated]\\nA portworxVolume  is an elastic block storage layer that runs hyperconverged with Kubernetes.', metadata={'source': './PDFS/Concepts.pdf', 'page': 292}),\n",
       " Document(page_content='A portworxVolume  is an elastic block storage layer that runs hyperconverged with Kubernetes. \\nPortworx  fingerprints storage in a server, tiers based on capabilities, and aggregates capacity\\nacross multiple servers. Portworx runs in-guest in virtual machines or on bare metal Linux\\nnodes.\\nA portworxVolume  can be dynamically created through Kubernetes or it can also be pre-\\nprovisioned and referenced inside a Pod. Here is an example Pod referencing a pre-provisioned\\nPortworx volume:\\napiVersion : v1\\nkind: Pod\\nmetadata :\\n  name : test-portworx-volume-pod\\nspec:\\n  containers :\\n  - image : registry.k8s.io/test-webserver\\n    name : test-container\\n    volumeMounts :\\n    - mountPath : /mnt\\n      name : pxvol\\n  volumes :\\n  - name : pxvol\\n    # This Portworx volume must already exist.\\n    portworxVolume :\\n      volumeID : \"pxvol\"\\n      fsType : \"<fs-type>\"', metadata={'source': './PDFS/Concepts.pdf', 'page': 292}),\n",
       " Document(page_content=\"Note:  Make sure you have an existing PortworxVolume with name pxvol  before using it in the\\nPod.\\nFor more details, see the Portworx volume  examples.\\nPortworx CSI migration\\nFEATURE STATE:  Kubernetes v1.25 [beta]\\nThe CSIMigration  feature for Portworx has been added but disabled by default in Kubernetes\\n1.23 since it's in alpha state. It has been beta now since v1.25 but it is still turned off by default.\\nIt redirects all plugin operations from the existing in-tree plugin to the pxd.portworx.com\\nContainer Storage Interface (CSI) Driver. Portworx CSI Driver  must be installed on the cluster.\\nTo enable the feature, set CSIMigrationPortworx=true  in kube-controller-manager and kubelet.\\nprojected\\nA projected volume maps several existing volume sources into the same directory. For more\\ndetails, see projected volumes .\\nrbd\\nFEATURE STATE:  Kubernetes v1.28 [deprecated]\\nNote:  The Kubernetes project suggests that you use the Ceph CSI  third party storage driver\\ninstead, in RBD mode.\", metadata={'source': './PDFS/Concepts.pdf', 'page': 293}),\n",
       " Document(page_content='instead, in RBD mode.\\nAn rbd volume allows a Rados Block Device  (RBD) volume to mount into your Pod. Unlike \\nemptyDir , which is erased when a pod is removed, the contents of an rbd volume are preserved\\nand the volume is unmounted. This means that a RBD volume can be pre-populated with data,\\nand that data can be shared between pods.\\nNote:  You must have a Ceph installation running before you can use RBD.\\nA feature of RBD is that it can be mounted as read-only by multiple consumers simultaneously.\\nThis means that you can pre-populate a volume with your dataset and then serve it in parallel\\nfrom as many pods as you need. Unfortunately, RBD volumes can only be mounted by a single\\nconsumer in read-write mode. Simultaneous writers are not allowed.\\nSee the RBD example  for more details.\\nRBD CSI migration\\nFEATURE STATE:  Kubernetes v1.28 [deprecated]\\nThe CSIMigration  feature for RBD , when enabled, redirects all plugin operations from the', metadata={'source': './PDFS/Concepts.pdf', 'page': 293}),\n",
       " Document(page_content='The CSIMigration  feature for RBD , when enabled, redirects all plugin operations from the\\nexisting in-tree plugin to the rbd.csi.ceph.com  CSI driver. In order to use this feature, the Ceph\\nCSI driver  must be installed on the cluster and the CSIMigrationRBD  feature gate  must be\\nenabled. (Note that the csiMigrationRBD  flag has been removed and replaced with \\nCSIMigrationRBD  in release v1.24)\\nNote:', metadata={'source': './PDFS/Concepts.pdf', 'page': 293}),\n",
       " Document(page_content=\"As a Kubernetes cluster operator that administers storage, here are the prerequisites that you\\nmust complete before you attempt migration to the RBD CSI driver:\\nYou must install the Ceph CSI driver ( rbd.csi.ceph.com ), v3.5.0 or above, into your\\nKubernetes cluster.\\nconsidering the clusterID  field is a required parameter for CSI driver for its operations,\\nbut in-tree StorageClass has monitors  field as a required parameter, a Kubernetes storage\\nadmin has to create a clusterID based on the monitors hash ( ex: #echo -n \\n'<monitors_string>' | md5sum ) in the CSI config map and keep the monitors under this\\nclusterID configuration.\\nAlso, if the value of adminId  in the in-tree Storageclass is different from admin , the \\nadminSecretName  mentioned in the in-tree Storageclass has to be patched with the\\nbase64 value of the adminId  parameter value, otherwise this step can be skipped.\\nsecret\\nA secret  volume is used to pass sensitive information, such as passwords, to Pods. You can store\", metadata={'source': './PDFS/Concepts.pdf', 'page': 294}),\n",
       " Document(page_content='A secret  volume is used to pass sensitive information, such as passwords, to Pods. You can store\\nsecrets in the Kubernetes API and mount them as files for use by pods without coupling to\\nKubernetes directly. secret  volumes are backed by tmpfs (a RAM-backed filesystem) so they are\\nnever written to non-volatile storage.\\nNote:\\nYou must create a Secret in the Kubernetes API before you can use it.\\nA Secret is always mounted as readOnly .\\nA container using a Secret as a subPath  volume mount will not receive Secret updates.\\nFor more details, see Configuring Secrets .\\nvsphereVolume (deprecated)\\nNote:  The Kubernetes project recommends using the vSphere CSI  out-of-tree storage driver\\ninstead.\\nA vsphereVolume  is used to mount a vSphere VMDK volume into your Pod. The contents of a\\nvolume are preserved when it is unmounted. It supports both VMFS and VSAN datastore.\\nFor more information, see the vSphere volume  examples.\\nvSphere CSI migration\\nFEATURE STATE:  Kubernetes v1.26 [stable]', metadata={'source': './PDFS/Concepts.pdf', 'page': 294}),\n",
       " Document(page_content=\"vSphere CSI migration\\nFEATURE STATE:  Kubernetes v1.26 [stable]\\nIn Kubernetes 1.28, all operations for the in-tree vsphereVolume  type are redirected to the \\ncsi.vsphere.vmware.com  CSI driver.\\nvSphere CSI driver  must be installed on the cluster. You can find additional advice on how to\\nmigrate in-tree vsphereVolume  in VMware's documentation page Migrating In-Tree vSphere\\nVolumes to vSphere Container Storage lug-in . If vSphere CSI Driver is not installed volume\\noperations can not be performed on the PV created with the in-tree vsphereVolume  type.• \\n• \\n• \\n• \\n• \\n•\", metadata={'source': './PDFS/Concepts.pdf', 'page': 294}),\n",
       " Document(page_content='You must run vSphere 7.0u2 or later in order to migrate to the vSphere CSI driver.\\nIf you are running a version of Kubernetes other than v1.28, consult the documentation for that\\nversion of Kubernetes.\\nNote:\\nThe following StorageClass parameters from the built-in vsphereVolume  plugin are not\\nsupported by the vSphere CSI driver:\\ndiskformat\\nhostfailurestotolerate\\nforceprovisioning\\ncachereservation\\ndiskstripes\\nobjectspacereservation\\niopslimit\\nExisting volumes created using these parameters will be migrated to the vSphere CSI driver, but\\nnew volumes created by the vSphere CSI driver will not be honoring these parameters.\\nvSphere CSI migration complete\\nFEATURE STATE:  Kubernetes v1.19 [beta]\\nTo turn off the vsphereVolume  plugin from being loaded by the controller manager and the\\nkubelet, you need to set InTreePluginvSphereUnregister  feature flag to true. You must install a \\ncsi.vsphere.vmware.com  CSI driver on all worker nodes.\\nUsing subPath', metadata={'source': './PDFS/Concepts.pdf', 'page': 295}),\n",
       " Document(page_content='csi.vsphere.vmware.com  CSI driver on all worker nodes.\\nUsing subPath\\nSometimes, it is useful to share one volume for multiple uses in a single pod. The \\nvolumeMounts.subPath  property specifies a sub-path inside the referenced volume instead of its\\nroot.\\nThe following example shows how to configure a Pod with a LAMP stack (Linux Apache\\nMySQL PHP) using a single, shared volume. This sample subPath  configuration is not\\nrecommended for production use.\\nThe PHP application\\'s code and assets map to the volume\\'s html  folder and the MySQL database\\nis stored in the volume\\'s mysql  folder. For example:\\napiVersion : v1\\nkind: Pod\\nmetadata :\\n  name : my-lamp-site\\nspec:\\n    containers :\\n    - name : mysql\\n      image : mysql\\n      env:\\n      - name : MYSQL_ROOT_PASSWORD\\n        value : \"rootpasswd\"• \\n• \\n• \\n• \\n• \\n• \\n•', metadata={'source': './PDFS/Concepts.pdf', 'page': 295}),\n",
       " Document(page_content='volumeMounts :\\n      - mountPath : /var/lib/mysql\\n        name : site-data\\n        subPath : mysql\\n    - name : php\\n      image : php:7.0-apache\\n      volumeMounts :\\n      - mountPath : /var/www/html\\n        name : site-data\\n        subPath : html\\n    volumes :\\n    - name : site-data\\n      persistentVolumeClaim :\\n        claimName : my-lamp-site-data\\nUsing subPath with expanded environment variables\\nFEATURE STATE:  Kubernetes v1.17 [stable]\\nUse the subPathExpr  field to construct subPath  directory names from downward API\\nenvironment variables. The subPath  and subPathExpr  properties are mutually exclusive.\\nIn this example, a Pod uses subPathExpr  to create a directory pod1  within the hostPath\\nvolume /var/log/pods . The hostPath  volume takes the Pod name from the downwardAPI . The\\nhost directory /var/log/pods/pod1  is mounted at /logs  in the container.\\napiVersion : v1\\nkind: Pod\\nmetadata :\\n  name : pod1\\nspec:\\n  containers :\\n  - name : container1\\n    env:\\n    - name : POD_NAME', metadata={'source': './PDFS/Concepts.pdf', 'page': 296}),\n",
       " Document(page_content='metadata :\\n  name : pod1\\nspec:\\n  containers :\\n  - name : container1\\n    env:\\n    - name : POD_NAME\\n      valueFrom :\\n        fieldRef :\\n          apiVersion : v1\\n          fieldPath : metadata.name\\n    image : busybox:1.28\\n    command : [ \"sh\", \"-c\", \"while [ true ]; do echo \\'Hello\\'; sleep 10; done | tee -a /logs/hello.txt\"  ]\\n    volumeMounts :\\n    - name : workdir1\\n      mountPath : /logs\\n      # The variable expansion uses round brackets (not curly brackets).\\n      subPathExpr : $(POD_NAME)\\n  restartPolicy : Never\\n  volumes :\\n  - name : workdir1\\n    hostPath :\\n      path: /var/log/pods', metadata={'source': './PDFS/Concepts.pdf', 'page': 296}),\n",
       " Document(page_content='Resources\\nThe storage media (such as Disk or SSD) of an emptyDir  volume is determined by the medium\\nof the filesystem holding the kubelet root dir (typically /var/lib/kubelet ). There is no limit on\\nhow much space an emptyDir  or hostPath  volume can consume, and no isolation between\\ncontainers or between pods.\\nTo learn about requesting space using a resource specification, see how to manage resources .\\nOut-of-tree volume plugins\\nThe out-of-tree volume plugins include Container Storage Interface  (CSI), and also FlexVolume\\n(which is deprecated). These plugins enable storage vendors to create custom storage plugins\\nwithout adding their plugin source code to the Kubernetes repository.\\nPreviously, all volume plugins were \"in-tree\". The \"in-tree\" plugins were built, linked, compiled,\\nand shipped with the core Kubernetes binaries. This meant that adding a new storage system to\\nKubernetes (a volume plugin) required checking code into the core Kubernetes code repository.', metadata={'source': './PDFS/Concepts.pdf', 'page': 297}),\n",
       " Document(page_content=\"Kubernetes (a volume plugin) required checking code into the core Kubernetes code repository.\\nBoth CSI and FlexVolume allow volume plugins to be developed independent of the Kubernetes\\ncode base, and deployed (installed) on Kubernetes clusters as extensions.\\nFor storage vendors looking to create an out-of-tree volume plugin, please refer to the volume\\nplugin FAQ .\\ncsi\\nContainer Storage Interface  (CSI) defines a standard interface for container orchestration\\nsystems (like Kubernetes) to expose arbitrary storage systems to their container workloads.\\nPlease read the CSI design proposal  for more information.\\nNote:  Support for CSI spec versions 0.2 and 0.3 are deprecated in Kubernetes v1.13 and will be\\nremoved in a future release.\\nNote:  CSI drivers may not be compatible across all Kubernetes releases. Please check the\\nspecific CSI driver's documentation for supported deployments steps for each Kubernetes\\nrelease and a compatibility matrix.\", metadata={'source': './PDFS/Concepts.pdf', 'page': 297}),\n",
       " Document(page_content='release and a compatibility matrix.\\nOnce a CSI compatible volume driver is deployed on a Kubernetes cluster, users may use the csi\\nvolume type to attach or mount the volumes exposed by the CSI driver.\\nA csi volume can be used in a Pod in three different ways:\\nthrough a reference to a PersistentVolumeClaim\\nwith a generic ephemeral volume\\nwith a CSI ephemeral volume  if the driver supports that\\nThe following fields are available to storage administrators to configure a CSI persistent\\nvolume:\\ndriver : A string value that specifies the name of the volume driver to use. This value must\\ncorrespond to the value returned in the GetPluginInfoResponse  by the CSI driver as• \\n• \\n• \\n•', metadata={'source': './PDFS/Concepts.pdf', 'page': 297}),\n",
       " Document(page_content='defined in the CSI spec . It is used by Kubernetes to identify which CSI driver to call out\\nto, and by CSI driver components to identify which PV objects belong to the CSI driver.\\nvolumeHandle : A string value that uniquely identifies the volume. This value must\\ncorrespond to the value returned in the volume.id  field of the CreateVolumeResponse  by\\nthe CSI driver as defined in the CSI spec . The value is passed as volume_id  on all calls to\\nthe CSI volume driver when referencing the volume.\\nreadOnly : An optional boolean value indicating whether the volume is to be\\n\"ControllerPublished\" (attached) as read only. Default is false. This value is passed to the\\nCSI driver via the readonly  field in the ControllerPublishVolumeRequest .\\nfsType : If the PV\\'s VolumeMode  is Filesystem  then this field may be used to specify the\\nfilesystem that should be used to mount the volume. If the volume has not been formatted', metadata={'source': './PDFS/Concepts.pdf', 'page': 298}),\n",
       " Document(page_content='filesystem that should be used to mount the volume. If the volume has not been formatted\\nand formatting is supported, this value will be used to format the volume. This value is\\npassed to the CSI driver via the VolumeCapability  field of \\nControllerPublishVolumeRequest , NodeStageVolumeRequest , and \\nNodePublishVolumeRequest .\\nvolumeAttributes : A map of string to string that specifies static properties of a volume.\\nThis map must correspond to the map returned in the volume.attributes  field of the \\nCreateVolumeResponse  by the CSI driver as defined in the CSI spec . The map is passed to\\nthe CSI driver via the volume_context  field in the ControllerPublishVolumeRequest , \\nNodeStageVolumeRequest , and NodePublishVolumeRequest .\\ncontrollerPublishSecretRef : A reference to the secret object containing sensitive\\ninformation to pass to the CSI driver to complete the CSI ControllerPublishVolume  and \\nControllerUnpublishVolume  calls. This field is optional, and may be empty if no secret is', metadata={'source': './PDFS/Concepts.pdf', 'page': 298}),\n",
       " Document(page_content='ControllerUnpublishVolume  calls. This field is optional, and may be empty if no secret is\\nrequired. If the Secret contains more than one secret, all secrets are passed.\\nnodeExpandSecretRef : A reference to the secret containing sensitive information to pass\\nto the CSI driver to complete the CSI NodeExpandVolume  call. This field is optional, and\\nmay be empty if no secret is required. If the object contains more than one secret, all\\nsecrets are passed. When you have configured secret data for node-initiated volume\\nexpansion, the kubelet passes that data via the NodeExpandVolume()  call to the CSI\\ndriver. In order to use the nodeExpandSecretRef  field, your cluster should be running\\nKubernetes version 1.25 or later.\\nIf you are running Kubernetes Version 1.25 or 1.26, you must enable the feature gate\\nnamed CSINodeExpandSecret  for each kube-apiserver and for the kubelet on every node.\\nIn Kubernetes version 1.27 this feature has been enabled by default and no explicit', metadata={'source': './PDFS/Concepts.pdf', 'page': 298}),\n",
       " Document(page_content='In Kubernetes version 1.27 this feature has been enabled by default and no explicit\\nenablement of the feature gate is required. You must also be using a CSI driver that\\nsupports or requires secret data during node-initiated storage resize operations.\\nnodePublishSecretRef : A reference to the secret object containing sensitive information to\\npass to the CSI driver to complete the CSI NodePublishVolume  call. This field is optional,\\nand may be empty if no secret is required. If the secret object contains more than one\\nsecret, all secrets are passed.\\nnodeStageSecretRef : A reference to the secret object containing sensitive information to\\npass to the CSI driver to complete the CSI NodeStageVolume  call. This field is optional,\\nand may be empty if no secret is required. If the Secret contains more than one secret, all\\nsecrets are passed.\\nCSI raw block volume support\\nFEATURE STATE:  Kubernetes v1.18 [stable]', metadata={'source': './PDFS/Concepts.pdf', 'page': 298}),\n",
       " Document(page_content='secrets are passed.\\nCSI raw block volume support\\nFEATURE STATE:  Kubernetes v1.18 [stable]\\nVendors with external CSI drivers can implement raw block volume support in Kubernetes\\nworkloads.• \\n• \\n• \\n• \\n• \\n• \\n• \\n• \\n•', metadata={'source': './PDFS/Concepts.pdf', 'page': 298}),\n",
       " Document(page_content='You can set up your PersistentVolume/PersistentVolumeClaim with raw block volume support\\nas usual, without any CSI specific changes.\\nCSI ephemeral volumes\\nFEATURE STATE:  Kubernetes v1.25 [stable]\\nYou can directly configure CSI volumes within the Pod specification. Volumes specified in this\\nway are ephemeral and do not persist across pod restarts. See Ephemeral Volumes  for more\\ninformation.\\nFor more information on how to develop a CSI driver, refer to the kubernetes-csi\\ndocumentation\\nWindows CSI proxy\\nFEATURE STATE:  Kubernetes v1.22 [stable]\\nCSI node plugins need to perform various privileged operations like scanning of disk devices\\nand mounting of file systems. These operations differ for each host operating system. For Linux\\nworker nodes, containerized CSI node plugins are typically deployed as privileged containers.\\nFor Windows worker nodes, privileged operations for containerized CSI node plugins is', metadata={'source': './PDFS/Concepts.pdf', 'page': 299}),\n",
       " Document(page_content='For Windows worker nodes, privileged operations for containerized CSI node plugins is\\nsupported using csi-proxy , a community-managed, stand-alone binary that needs to be pre-\\ninstalled on each Windows node.\\nFor more details, refer to the deployment guide of the CSI plugin you wish to deploy.\\nMigrating to CSI drivers from in-tree plugins\\nFEATURE STATE:  Kubernetes v1.25 [stable]\\nThe CSIMigration  feature directs operations against existing in-tree plugins to corresponding\\nCSI plugins (which are expected to be installed and configured). As a result, operators do not\\nhave to make any configuration changes to existing Storage Classes, PersistentVolumes or\\nPersistentVolumeClaims (referring to in-tree plugins) when transitioning to a CSI driver that\\nsupersedes an in-tree plugin.\\nThe operations and features that are supported include: provisioning/delete, attach/detach,\\nmount/unmount and resizing of volumes.', metadata={'source': './PDFS/Concepts.pdf', 'page': 299}),\n",
       " Document(page_content='mount/unmount and resizing of volumes.\\nIn-tree plugins that support CSIMigration  and have a corresponding CSI driver implemented\\nare listed in Types of Volumes .\\nThe following in-tree plugins support persistent storage on Windows nodes:\\nazureFile\\ngcePersistentDisk\\nvsphereVolume\\nflexVolume (deprecated)\\nFEATURE STATE:  Kubernetes v1.23 [deprecated]• \\n• \\n•', metadata={'source': './PDFS/Concepts.pdf', 'page': 299}),\n",
       " Document(page_content='FlexVolume is an out-of-tree plugin interface that uses an exec-based model to interface with\\nstorage drivers. The FlexVolume driver binaries must be installed in a pre-defined volume\\nplugin path on each node and in some cases the control plane nodes as well.\\nPods interact with FlexVolume drivers through the flexVolume  in-tree volume plugin. For more\\ndetails, see the FlexVolume README  document.\\nThe following FlexVolume plugins , deployed as PowerShell scripts on the host, support\\nWindows nodes:\\nSMB\\niSCSI\\nNote:\\nFlexVolume is deprecated. Using an out-of-tree CSI driver is the recommended way to integrate\\nexternal storage with Kubernetes.\\nMaintainers of FlexVolume driver should implement a CSI Driver and help to migrate users of\\nFlexVolume drivers to CSI. Users of FlexVolume should move their workloads to use the\\nequivalent CSI Driver.\\nMount propagation\\nMount propagation allows for sharing volumes mounted by a container to other containers in', metadata={'source': './PDFS/Concepts.pdf', 'page': 300}),\n",
       " Document(page_content=\"Mount propagation allows for sharing volumes mounted by a container to other containers in\\nthe same pod, or even to other pods on the same node.\\nMount propagation of a volume is controlled by the mountPropagation  field in \\nContainer.volumeMounts . Its values are:\\nNone  - This volume mount will not receive any subsequent mounts that are mounted to\\nthis volume or any of its subdirectories by the host. In similar fashion, no mounts created\\nby the container will be visible on the host. This is the default mode.\\nThis mode is equal to rprivate  mount propagation as described in mount(8)\\nHowever, the CRI runtime may choose rslave  mount propagation (i.e., HostToContainer )\\ninstead, when rprivate  propagation is not applicable. cri-dockerd (Docker) is known to\\nchoose rslave  mount propagation when the mount source contains the Docker daemon's\\nroot directory ( /var/lib/docker ).\\nHostToContainer  - This volume mount will receive all subsequent mounts that are\", metadata={'source': './PDFS/Concepts.pdf', 'page': 300}),\n",
       " Document(page_content='HostToContainer  - This volume mount will receive all subsequent mounts that are\\nmounted to this volume or any of its subdirectories.\\nIn other words, if the host mounts anything inside the volume mount, the container will\\nsee it mounted there.\\nSimilarly, if any Pod with Bidirectional  mount propagation to the same volume mounts\\nanything there, the container with HostToContainer  mount propagation will see it.\\nThis mode is equal to rslave  mount propagation as described in the mount(8)• \\n• \\n• \\n•', metadata={'source': './PDFS/Concepts.pdf', 'page': 300}),\n",
       " Document(page_content=\"Bidirectional  - This volume mount behaves the same the HostToContainer  mount. In\\naddition, all volume mounts created by the container will be propagated back to the host\\nand to all containers of all pods that use the same volume.\\nA typical use case for this mode is a Pod with a FlexVolume or CSI driver or a Pod that\\nneeds to mount something on the host using a hostPath  volume.\\nThis mode is equal to rshared  mount propagation as described in the mount(8)\\nWarning:  Bidirectional  mount propagation can be dangerous. It can damage the host\\noperating system and therefore it is allowed only in privileged containers. Familiarity\\nwith Linux kernel behavior is strongly recommended. In addition, any volume mounts\\ncreated by containers in pods must be destroyed (unmounted) by the containers on\\ntermination.\\nWhat's next\\nFollow an example of deploying WordPress and MySQL with Persistent Volumes .\\nPersistent Volumes\", metadata={'source': './PDFS/Concepts.pdf', 'page': 301}),\n",
       " Document(page_content='Follow an example of deploying WordPress and MySQL with Persistent Volumes .\\nPersistent Volumes\\nThis document describes persistent volumes  in Kubernetes. Familiarity with volumes  is\\nsuggested.\\nIntroduction\\nManaging storage is a distinct problem from managing compute instances. The\\nPersistentVolume subsystem provides an API for users and administrators that abstracts details\\nof how storage is provided from how it is consumed. To do this, we introduce two new API\\nresources: PersistentVolume and PersistentVolumeClaim.\\nA PersistentVolume  (PV) is a piece of storage in the cluster that has been provisioned by an\\nadministrator or dynamically provisioned using Storage Classes . It is a resource in the cluster\\njust like a node is a cluster resource. PVs are volume plugins like Volumes, but have a lifecycle\\nindependent of any individual Pod that uses the PV. This API object captures the details of the', metadata={'source': './PDFS/Concepts.pdf', 'page': 301}),\n",
       " Document(page_content='independent of any individual Pod that uses the PV. This API object captures the details of the\\nimplementation of the storage, be that NFS, iSCSI, or a cloud-provider-specific storage system.\\nA PersistentVolumeClaim  (PVC) is a request for storage by a user. It is similar to a Pod. Pods\\nconsume node resources and PVCs consume PV resources. Pods can request specific levels of\\nresources (CPU and Memory). Claims can request specific size and access modes (e.g., they can\\nbe mounted ReadWriteOnce, ReadOnlyMany or ReadWriteMany, see AccessModes ).\\nWhile PersistentVolumeClaims allow a user to consume abstract storage resources, it is\\ncommon that users need PersistentVolumes with varying properties, such as performance, for\\ndifferent problems. Cluster administrators need to be able to offer a variety of\\nPersistentVolumes that differ in more ways than size and access modes, without exposing users\\nto the details of how those volumes are implemented. For these needs, there is the StorageClass', metadata={'source': './PDFS/Concepts.pdf', 'page': 301}),\n",
       " Document(page_content='to the details of how those volumes are implemented. For these needs, there is the StorageClass\\nresource.\\nSee the detailed walkthrough with working examples .•', metadata={'source': './PDFS/Concepts.pdf', 'page': 301}),\n",
       " Document(page_content='Lifecycle of a volume and claim\\nPVs are resources in the cluster. PVCs are requests for those resources and also act as claim\\nchecks to the resource. The interaction between PVs and PVCs follows this lifecycle:\\nProvisioning\\nThere are two ways PVs may be provisioned: statically or dynamically.\\nStatic\\nA cluster administrator creates a number of PVs. They carry the details of the real storage,\\nwhich is available for use by cluster users. They exist in the Kubernetes API and are available\\nfor consumption.\\nDynamic\\nWhen none of the static PVs the administrator created match a user\\'s PersistentVolumeClaim,\\nthe cluster may try to dynamically provision a volume specially for the PVC. This provisioning\\nis based on StorageClasses: the PVC must request a storage class  and the administrator must\\nhave created and configured that class for dynamic provisioning to occur. Claims that request\\nthe class \"\" effectively disable dynamic provisioning for themselves.', metadata={'source': './PDFS/Concepts.pdf', 'page': 302}),\n",
       " Document(page_content='the class \"\" effectively disable dynamic provisioning for themselves.\\nTo enable dynamic storage provisioning based on storage class, the cluster administrator needs\\nto enable the DefaultStorageClass  admission controller  on the API server. This can be done, for\\nexample, by ensuring that DefaultStorageClass  is among the comma-delimited, ordered list of\\nvalues for the --enable-admission-plugins  flag of the API server component. For more\\ninformation on API server command-line flags, check kube-apiserver  documentation.\\nBinding\\nA user creates, or in the case of dynamic provisioning, has already created, a\\nPersistentVolumeClaim with a specific amount of storage requested and with certain access\\nmodes. A control loop in the control plane watches for new PVCs, finds a matching PV (if\\npossible), and binds them together. If a PV was dynamically provisioned for a new PVC, the\\nloop will always bind that PV to the PVC. Otherwise, the user will always get at least what they', metadata={'source': './PDFS/Concepts.pdf', 'page': 302}),\n",
       " Document(page_content='loop will always bind that PV to the PVC. Otherwise, the user will always get at least what they\\nasked for, but the volume may be in excess of what was requested. Once bound,\\nPersistentVolumeClaim binds are exclusive, regardless of how they were bound. A PVC to PV\\nbinding is a one-to-one mapping, using a ClaimRef which is a bi-directional binding between\\nthe PersistentVolume and the PersistentVolumeClaim.\\nClaims will remain unbound indefinitely if a matching volume does not exist. Claims will be\\nbound as matching volumes become available. For example, a cluster provisioned with many\\n50Gi PVs would not match a PVC requesting 100Gi. The PVC can be bound when a 100Gi PV is\\nadded to the cluster.\\nUsing\\nPods use claims as volumes. The cluster inspects the claim to find the bound volume and\\nmounts that volume for a Pod. For volumes that support multiple access modes, the user\\nspecifies which mode is desired when using their claim as a volume in a Pod.', metadata={'source': './PDFS/Concepts.pdf', 'page': 302}),\n",
       " Document(page_content=\"Once a user has a claim and that claim is bound, the bound PV belongs to the user for as long as\\nthey need it. Users schedule Pods and access their claimed PVs by including a \\npersistentVolumeClaim  section in a Pod's volumes  block. See Claims As Volumes  for more\\ndetails on this.\\nStorage Object in Use Protection\\nThe purpose of the Storage Object in Use Protection feature is to ensure that\\nPersistentVolumeClaims (PVCs) in active use by a Pod and PersistentVolume (PVs) that are\\nbound to PVCs are not removed from the system, as this may result in data loss.\\nNote:  PVC is in active use by a Pod when a Pod object exists that is using the PVC.\\nIf a user deletes a PVC in active use by a Pod, the PVC is not removed immediately. PVC\\nremoval is postponed until the PVC is no longer actively used by any Pods. Also, if an admin\\ndeletes a PV that is bound to a PVC, the PV is not removed immediately. PV removal is\\npostponed until the PV is no longer bound to a PVC.\", metadata={'source': './PDFS/Concepts.pdf', 'page': 303}),\n",
       " Document(page_content=\"postponed until the PV is no longer bound to a PVC.\\nYou can see that a PVC is protected when the PVC's status is Terminating  and the Finalizers  list\\nincludes kubernetes.io/pvc-protection :\\nkubectl describe pvc hostpath\\nName:          hostpath\\nNamespace:     default\\nStorageClass:  example-hostpath\\nStatus:        Terminating\\nVolume:\\nLabels:        <none>\\nAnnotations:   volume.beta.kubernetes.io/storage-class =example-hostpath\\n               volume.beta.kubernetes.io/storage-provisioner =example.com/hostpath\\nFinalizers:    [kubernetes.io/pvc-protection ]\\n...\\nYou can see that a PV is protected when the PV's status is Terminating  and the Finalizers  list\\nincludes kubernetes.io/pv-protection  too:\\nkubectl describe pv task-pv-volume\\nName:            task-pv-volume\\nLabels:          type=local\\nAnnotations:     <none>\\nFinalizers:      [kubernetes.io/pv-protection ]\\nStorageClass:    standard\\nStatus:          Terminating\\nClaim:\\nReclaim Policy:  Delete\\nAccess Modes:    RWO\\nCapacity:        1Gi\", metadata={'source': './PDFS/Concepts.pdf', 'page': 303}),\n",
       " Document(page_content='Claim:\\nReclaim Policy:  Delete\\nAccess Modes:    RWO\\nCapacity:        1Gi\\nMessage:\\nSource:\\n    Type:          HostPath (bare host directory volume )\\n    Path:          /tmp/data\\n    HostPathType:\\nEvents:            <none>', metadata={'source': './PDFS/Concepts.pdf', 'page': 303}),\n",
       " Document(page_content='Reclaiming\\nWhen a user is done with their volume, they can delete the PVC objects from the API that\\nallows reclamation of the resource. The reclaim policy for a PersistentVolume tells the cluster\\nwhat to do with the volume after it has been released of its claim. Currently, volumes can either\\nbe Retained, Recycled, or Deleted.\\nRetain\\nThe Retain  reclaim policy allows for manual reclamation of the resource. When the\\nPersistentVolumeClaim is deleted, the PersistentVolume still exists and the volume is\\nconsidered \"released\". But it is not yet available for another claim because the previous\\nclaimant\\'s data remains on the volume. An administrator can manually reclaim the volume with\\nthe following steps.\\nDelete the PersistentVolume. The associated storage asset in external infrastructure (such\\nas an AWS EBS or GCE PD volume) still exists after the PV is deleted.\\nManually clean up the data on the associated storage asset accordingly.\\nManually delete the associated storage asset.', metadata={'source': './PDFS/Concepts.pdf', 'page': 304}),\n",
       " Document(page_content=\"Manually delete the associated storage asset.\\nIf you want to reuse the same storage asset, create a new PersistentVolume with the same\\nstorage asset definition.\\nDelete\\nFor volume plugins that support the Delete  reclaim policy, deletion removes both the\\nPersistentVolume object from Kubernetes, as well as the associated storage asset in the external\\ninfrastructure, such as an AWS EBS or GCE PD volume. Volumes that were dynamically\\nprovisioned inherit the reclaim policy of their StorageClass , which defaults to Delete . The\\nadministrator should configure the StorageClass according to users' expectations; otherwise,\\nthe PV must be edited or patched after it is created. See Change the Reclaim Policy of a\\nPersistentVolume .\\nRecycle\\nWarning:  The Recycle  reclaim policy is deprecated. Instead, the recommended approach is to\\nuse dynamic provisioning.\\nIf supported by the underlying volume plugin, the Recycle  reclaim policy performs a basic\", metadata={'source': './PDFS/Concepts.pdf', 'page': 304}),\n",
       " Document(page_content='If supported by the underlying volume plugin, the Recycle  reclaim policy performs a basic\\nscrub ( rm -rf /thevolume/* ) on the volume and makes it available again for a new claim.\\nHowever, an administrator can configure a custom recycler Pod template using the Kubernetes\\ncontroller manager command line arguments as described in the reference . The custom recycler\\nPod template must contain a volumes  specification, as shown in the example below:\\napiVersion : v1\\nkind: Pod\\nmetadata :\\n  name : pv-recycler\\n  namespace : default\\nspec:\\n  restartPolicy : Never1. \\n2. \\n3.', metadata={'source': './PDFS/Concepts.pdf', 'page': 304}),\n",
       " Document(page_content='volumes :\\n  - name : vol\\n    hostPath :\\n      path: /any/path/it/will/be/replaced\\n  containers :\\n  - name : pv-recycler\\n    image : \"registry.k8s.io/busybox\"\\n    command : [\"/bin/sh\" , \"-c\", \\n\"test -e /scrub && rm -rf /scrub/..?* /scrub/.[!.]* /scrub/*  && test -z \\\\\"$(ls -A /scrub)\\\\\" || exit 1\" ]\\n    volumeMounts :\\n    - name : vol\\n      mountPath : /scrub\\nHowever, the particular path specified in the custom recycler Pod template in the volumes  part\\nis replaced with the particular path of the volume that is being recycled.\\nPersistentVolume deletion protection finalizer\\nFEATURE STATE:  Kubernetes v1.23 [alpha]\\nFinalizers can be added on a PersistentVolume to ensure that PersistentVolumes having Delete\\nreclaim policy are deleted only after the backing storage are deleted.\\nThe newly introduced finalizers kubernetes.io/pv-controller  and external-\\nprovisioner.volume.kubernetes.io/finalizer  are only added to dynamically provisioned volumes.', metadata={'source': './PDFS/Concepts.pdf', 'page': 305}),\n",
       " Document(page_content='provisioner.volume.kubernetes.io/finalizer  are only added to dynamically provisioned volumes.\\nThe finalizer kubernetes.io/pv-controller  is added to in-tree plugin volumes. The following is an\\nexample\\nkubectl describe pv pvc-74a498d6-3929-47e8-8c02-078c1ece4d78\\nName:            pvc-74a498d6-3929-47e8-8c02-078c1ece4d78\\nLabels:          <none>\\nAnnotations:     kubernetes.io/createdby: vsphere-volume-dynamic-provisioner\\n                 pv.kubernetes.io/bound-by-controller: yes\\n                 pv.kubernetes.io/provisioned-by: kubernetes.io/vsphere-volume\\nFinalizers:      [kubernetes.io/pv-protection kubernetes.io/pv-controller ]\\nStorageClass:    vcp-sc\\nStatus:          Bound\\nClaim:           default/vcp-pvc-1\\nReclaim Policy:  Delete\\nAccess Modes:    RWO\\nVolumeMode:      Filesystem\\nCapacity:        1Gi\\nNode Affinity:   <none>\\nMessage:         \\nSource:\\n    Type:               vSphereVolume (a Persistent Disk resource in vSphere )', metadata={'source': './PDFS/Concepts.pdf', 'page': 305}),\n",
       " Document(page_content='Source:\\n    Type:               vSphereVolume (a Persistent Disk resource in vSphere )\\n    VolumePath:         [vsanDatastore ] d49c4a62-166f-ce12-c464-020077ba5d46/kubernetes-\\ndynamic-pvc-74a498d6-3929-47e8-8c02-078c1ece4d78.vmdk\\n    FSType:             ext4\\n    StoragePolicyName:  vSAN Default Storage Policy\\nEvents:                 <none>', metadata={'source': './PDFS/Concepts.pdf', 'page': 305}),\n",
       " Document(page_content='The finalizer external-provisioner.volume.kubernetes.io/finalizer  is added for CSI volumes. The\\nfollowing is an example:\\nName:            pvc-2f0bab97-85a8-4552-8044-eb8be45cf48d\\nLabels:          <none>\\nAnnotations:     pv.kubernetes.io/provisioned-by: csi.vsphere.vmware.com\\nFinalizers:      [kubernetes.io/pv-protection external-provisioner.volume.kubernetes.io/finalizer ]\\nStorageClass:    fast\\nStatus:          Bound\\nClaim:           demo-app/nginx-logs\\nReclaim Policy:  Delete\\nAccess Modes:    RWO\\nVolumeMode:      Filesystem\\nCapacity:        200Mi\\nNode Affinity:   <none>\\nMessage:         \\nSource:\\n    Type:              CSI (a Container Storage Interface (CSI) volume source )\\n    Driver:            csi.vsphere.vmware.com\\n    FSType:            ext4\\n    VolumeHandle:      44830fa8-79b4-406b-8b58-621ba25353fd\\n    ReadOnly:          false\\n    VolumeAttributes:      storage.kubernetes.io/csiProvisionerIdentity =1648442357185-8081-\\ncsi.vsphere.vmware.com', metadata={'source': './PDFS/Concepts.pdf', 'page': 306}),\n",
       " Document(page_content='csi.vsphere.vmware.com\\n                           type=vSphere CNS Block Volume\\nEvents:                <none>\\nWhen the CSIMigration{provider}  feature flag is enabled for a specific in-tree volume plugin,\\nthe kubernetes.io/pv-controller  finalizer is replaced by the external-\\nprovisioner.volume.kubernetes.io/finalizer  finalizer.\\nReserving a PersistentVolume\\nThe control plane can bind PersistentVolumeClaims to matching PersistentVolumes  in the\\ncluster. However, if you want a PVC to bind to a specific PV, you need to pre-bind them.\\nBy specifying a PersistentVolume in a PersistentVolumeClaim, you declare a binding between\\nthat specific PV and PVC. If the PersistentVolume exists and has not reserved\\nPersistentVolumeClaims through its claimRef  field, then the PersistentVolume and\\nPersistentVolumeClaim will be bound.\\nThe binding happens regardless of some volume matching criteria, including node affinity. The', metadata={'source': './PDFS/Concepts.pdf', 'page': 306}),\n",
       " Document(page_content='The binding happens regardless of some volume matching criteria, including node affinity. The\\ncontrol plane still checks that storage class , access modes, and requested storage size are valid.\\napiVersion : v1\\nkind: PersistentVolumeClaim\\nmetadata :\\n  name : foo-pvc\\n  namespace : foo\\nspec:\\n  storageClassName : \"\" \\n# Empty string must be explicitly set otherwise default StorageClass will be set', metadata={'source': './PDFS/Concepts.pdf', 'page': 306}),\n",
       " Document(page_content='volumeName : foo-pv\\n  ...\\nThis method does not guarantee any binding privileges to the PersistentVolume. If other\\nPersistentVolumeClaims could use the PV that you specify, you first need to reserve that\\nstorage volume. Specify the relevant PersistentVolumeClaim in the claimRef  field of the PV so\\nthat other PVCs can not bind to it.\\napiVersion : v1\\nkind: PersistentVolume\\nmetadata :\\n  name : foo-pv\\nspec:\\n  storageClassName : \"\"\\n  claimRef :\\n    name : foo-pvc\\n    namespace : foo\\n  ...\\nThis is useful if you want to consume PersistentVolumes that have their claimPolicy  set to \\nRetain , including cases where you are reusing an existing PV.\\nExpanding Persistent Volumes Claims\\nFEATURE STATE:  Kubernetes v1.24 [stable]\\nSupport for expanding PersistentVolumeClaims (PVCs) is enabled by default. You can expand\\nthe following types of volumes:\\nazureFile (deprecated)\\ncsi\\nflexVolume (deprecated)\\ngcePersistentDisk (deprecated)\\nrbd (deprecated)\\nportworxVolume (deprecated)', metadata={'source': './PDFS/Concepts.pdf', 'page': 307}),\n",
       " Document(page_content='flexVolume (deprecated)\\ngcePersistentDisk (deprecated)\\nrbd (deprecated)\\nportworxVolume (deprecated)\\nYou can only expand a PVC if its storage class\\'s allowVolumeExpansion  field is set to true.\\napiVersion : storage.k8s.io/v1\\nkind: StorageClass\\nmetadata :\\n  name : example-vol-default\\nprovisioner : vendor-name.example/magicstorage\\nparameters :\\n  resturl : \"http://192.168.10.100:8080\"\\n  restuser : \"\"\\n  secretNamespace : \"\"\\n  secretName : \"\"\\nallowVolumeExpansion : true\\nTo request a larger volume for a PVC, edit the PVC object and specify a larger size. This triggers\\nexpansion of the volume that backs the underlying PersistentVolume. A new PersistentVolume\\nis never created to satisfy the claim. Instead, an existing volume is resized.• \\n• \\n• \\n• \\n• \\n•', metadata={'source': './PDFS/Concepts.pdf', 'page': 307}),\n",
       " Document(page_content='Warning:  Directly editing the size of a PersistentVolume can prevent an automatic resize of\\nthat volume. If you edit the capacity of a PersistentVolume, and then edit the .spec  of a\\nmatching PersistentVolumeClaim to make the size of the PersistentVolumeClaim match the\\nPersistentVolume, then no storage resize happens. The Kubernetes control plane will see that\\nthe desired state of both resources matches, conclude that the backing volume size has been\\nmanually increased and that no resize is necessary.\\nCSI Volume expansion\\nFEATURE STATE:  Kubernetes v1.24 [stable]\\nSupport for expanding CSI volumes is enabled by default but it also requires a specific CSI\\ndriver to support volume expansion. Refer to documentation of the specific CSI driver for more\\ninformation.\\nResizing a volume containing a file system\\nYou can only resize volumes containing a file system if the file system is XFS, Ext3, or Ext4.', metadata={'source': './PDFS/Concepts.pdf', 'page': 308}),\n",
       " Document(page_content=\"You can only resize volumes containing a file system if the file system is XFS, Ext3, or Ext4.\\nWhen a volume contains a file system, the file system is only resized when a new Pod is using\\nthe PersistentVolumeClaim in ReadWrite  mode. File system expansion is either done when a\\nPod is starting up or when a Pod is running and the underlying file system supports online\\nexpansion.\\nFlexVolumes (deprecated since Kubernetes v1.23) allow resize if the driver is configured with\\nthe RequiresFSResize  capability to true. The FlexVolume can be resized on Pod restart.\\nResizing an in-use PersistentVolumeClaim\\nFEATURE STATE:  Kubernetes v1.24 [stable]\\nIn this case, you don't need to delete and recreate a Pod or deployment that is using an existing\\nPVC. Any in-use PVC automatically becomes available to its Pod as soon as its file system has\\nbeen expanded. This feature has no effect on PVCs that are not in use by a Pod or deployment.\", metadata={'source': './PDFS/Concepts.pdf', 'page': 308}),\n",
       " Document(page_content='been expanded. This feature has no effect on PVCs that are not in use by a Pod or deployment.\\nYou must create a Pod that uses the PVC before the expansion can complete.\\nSimilar to other volume types - FlexVolume volumes can also be expanded when in-use by a\\nPod.\\nNote:  FlexVolume resize is possible only when the underlying driver supports resize.\\nNote:  Expanding EBS volumes is a time-consuming operation. Also, there is a per-volume\\nquota of one modification every 6 hours.\\nRecovering from Failure when Expanding Volumes\\nIf a user specifies a new size that is too big to be satisfied by underlying storage system,\\nexpansion of PVC will be continuously retried until user or cluster administrator takes some\\naction. This can be undesirable and hence Kubernetes provides following methods of recovering\\nfrom such failures.\\nManually with Cluster Administrator access\\nBy requesting expansion to smaller size• \\n•', metadata={'source': './PDFS/Concepts.pdf', 'page': 308}),\n",
       " Document(page_content=\"If expanding underlying storage fails, the cluster administrator can manually recover the\\nPersistent Volume Claim (PVC) state and cancel the resize requests. Otherwise, the resize\\nrequests are continuously retried by the controller without administrator intervention.\\nMark the PersistentVolume(PV) that is bound to the PersistentVolumeClaim(PVC) with \\nRetain  reclaim policy.\\nDelete the PVC. Since PV has Retain  reclaim policy - we will not lose any data when we\\nrecreate the PVC.\\nDelete the claimRef  entry from PV specs, so as new PVC can bind to it. This should make\\nthe PV Available .\\nRe-create the PVC with smaller size than PV and set volumeName  field of the PVC to the\\nname of the PV. This should bind new PVC to existing PV.\\nDon't forget to restore the reclaim policy of the PV.\\nFEATURE STATE:  Kubernetes v1.23 [alpha]\\nNote:  Recovery from failing PVC expansion by users is available as an alpha feature since\", metadata={'source': './PDFS/Concepts.pdf', 'page': 309}),\n",
       " Document(page_content='Note:  Recovery from failing PVC expansion by users is available as an alpha feature since\\nKubernetes 1.23. The RecoverVolumeExpansionFailure  feature must be enabled for this feature\\nto work. Refer to the feature gate  documentation for more information.\\nIf the feature gates RecoverVolumeExpansionFailure  is enabled in your cluster, and expansion\\nhas failed for a PVC, you can retry expansion with a smaller size than the previously requested\\nvalue. To request a new expansion attempt with a smaller proposed size, edit .spec.resources  for\\nthat PVC and choose a value that is less than the value you previously tried. This is useful if\\nexpansion to a higher value did not succeed because of capacity constraint. If that has\\nhappened, or you suspect that it might have, you can retry expansion by specifying a size that\\nis within the capacity limits of underlying storage provider. You can monitor status of resize\\noperation by watching .status.allocatedResourceStatuses  and events on the PVC.', metadata={'source': './PDFS/Concepts.pdf', 'page': 309}),\n",
       " Document(page_content='operation by watching .status.allocatedResourceStatuses  and events on the PVC.\\nNote that, although you can specify a lower amount of storage than what was requested\\npreviously, the new value must still be higher than .status.capacity . Kubernetes does not\\nsupport shrinking a PVC to less than its current size.\\nTypes of Persistent Volumes\\nPersistentVolume types are implemented as plugins. Kubernetes currently supports the\\nfollowing plugins:\\ncsi - Container Storage Interface (CSI)\\nfc - Fibre Channel (FC) storage\\nhostPath  - HostPath volume (for single node testing only; WILL NOT WORK in a multi-\\nnode cluster; consider using local  volume instead)\\niscsi - iSCSI (SCSI over IP) storage\\nlocal  - local storage devices mounted on nodes.\\nnfs - Network File System (NFS) storage\\nThe following types of PersistentVolume are deprecated. This means that support is still\\navailable but will be removed in a future Kubernetes release.\\nazureFile  - Azure File ( deprecated  in v1.21)', metadata={'source': './PDFS/Concepts.pdf', 'page': 309}),\n",
       " Document(page_content='azureFile  - Azure File ( deprecated  in v1.21)\\nflexVolume  - FlexVolume ( deprecated  in v1.23)\\ngcePersistentDisk  - GCE Persistent Disk ( deprecated  in v1.17)\\nportworxVolume  - Portworx volume ( deprecated  in v1.25)\\nvsphereVolume  - vSphere VMDK volume ( deprecated  in v1.19)1. \\n2. \\n3. \\n4. \\n5. \\n• \\n• \\n• \\n• \\n• \\n• \\n• \\n• \\n• \\n• \\n•', metadata={'source': './PDFS/Concepts.pdf', 'page': 309}),\n",
       " Document(page_content='cephfs  - CephFS volume ( deprecated  in v1.28)\\nrbd - Rados Block Device (RBD) volume ( deprecated  in v1.28)\\nOlder versions of Kubernetes also supported the following in-tree PersistentVolume types:\\nawsElasticBlockStore  - AWS Elastic Block Store (EBS) ( not available  in v1.27)\\nazureDisk  - Azure Disk ( not available  in v1.27)\\ncinder  - Cinder (OpenStack block storage) ( not available  in v1.26)\\nphotonPersistentDisk  - Photon controller persistent disk. ( not available  starting v1.15)\\nscaleIO  - ScaleIO volume. ( not available  starting v1.21)\\nflocker  - Flocker storage. ( not available  starting v1.25)\\nquobyte  - Quobyte volume. ( not available  starting v1.25)\\nstorageos  - StorageOS volume. ( not available  starting v1.25)\\nPersistent Volumes\\nEach PV contains a spec and status, which is the specification and status of the volume. The\\nname of a PersistentVolume object must be a valid DNS subdomain name .\\napiVersion : v1\\nkind: PersistentVolume\\nmetadata :\\n  name : pv0003\\nspec:', metadata={'source': './PDFS/Concepts.pdf', 'page': 310}),\n",
       " Document(page_content=\"apiVersion : v1\\nkind: PersistentVolume\\nmetadata :\\n  name : pv0003\\nspec:\\n  capacity :\\n    storage : 5Gi\\n  volumeMode : Filesystem\\n  accessModes :\\n    - ReadWriteOnce\\n  persistentVolumeReclaimPolicy : Recycle\\n  storageClassName : slow\\n  mountOptions :\\n    - hard\\n    - nfsvers=4.1\\n  nfs:\\n    path: /tmp\\n    server : 172.17.0.2\\nNote:  Helper programs relating to the volume type may be required for consumption of a\\nPersistentVolume within a cluster. In this example, the PersistentVolume is of type NFS and the\\nhelper program /sbin/mount.nfs is required to support the mounting of NFS filesystems.\\nCapacity\\nGenerally, a PV will have a specific storage capacity. This is set using the PV's capacity\\nattribute. Read the glossary term Quantity  to understand the units expected by capacity .\\nCurrently, storage size is the only resource that can be set or requested. Future attributes may\\ninclude IOPS, throughput, etc.• \\n• \\n• \\n• \\n• \\n• \\n• \\n• \\n• \\n•\", metadata={'source': './PDFS/Concepts.pdf', 'page': 310}),\n",
       " Document(page_content='Volume Mode\\nFEATURE STATE:  Kubernetes v1.18 [stable]\\nKubernetes supports two volumeModes  of PersistentVolumes: Filesystem  and Block .\\nvolumeMode  is an optional API parameter. Filesystem  is the default mode used when \\nvolumeMode  parameter is omitted.\\nA volume with volumeMode: Filesystem  is mounted  into Pods into a directory. If the volume is\\nbacked by a block device and the device is empty, Kubernetes creates a filesystem on the device\\nbefore mounting it for the first time.\\nYou can set the value of volumeMode  to Block  to use a volume as a raw block device. Such\\nvolume is presented into a Pod as a block device, without any filesystem on it. This mode is\\nuseful to provide a Pod the fastest possible way to access a volume, without any filesystem\\nlayer between the Pod and the volume. On the other hand, the application running in the Pod\\nmust know how to handle a raw block device. See Raw Block Volume Support  for an example\\non how to use a volume with volumeMode: Block  in a Pod.', metadata={'source': './PDFS/Concepts.pdf', 'page': 311}),\n",
       " Document(page_content=\"on how to use a volume with volumeMode: Block  in a Pod.\\nAccess Modes\\nA PersistentVolume can be mounted on a host in any way supported by the resource provider.\\nAs shown in the table below, providers will have different capabilities and each PV's access\\nmodes are set to the specific modes supported by that particular volume. For example, NFS can\\nsupport multiple read/write clients, but a specific NFS PV might be exported on the server as\\nread-only. Each PV gets its own set of access modes describing that specific PV's capabilities.\\nThe access modes are:\\nReadWriteOnce\\nthe volume can be mounted as read-write by a single node. ReadWriteOnce access mode\\nstill can allow multiple pods to access the volume when the pods are running on the same\\nnode.\\nReadOnlyMany\\nthe volume can be mounted as read-only by many nodes.\\nReadWriteMany\\nthe volume can be mounted as read-write by many nodes.\\nReadWriteOncePod\\nFEATURE STATE:  Kubernetes v1.27 [beta]\", metadata={'source': './PDFS/Concepts.pdf', 'page': 311}),\n",
       " Document(page_content='ReadWriteOncePod\\nFEATURE STATE:  Kubernetes v1.27 [beta]\\nthe volume can be mounted as read-write by a single Pod. Use ReadWriteOncePod access\\nmode if you want to ensure that only one pod across the whole cluster can read that PVC\\nor write to it. This is only supported for CSI volumes and Kubernetes version 1.22+.\\nThe blog article Introducing Single Pod Access Mode for PersistentVolumes  covers this in more\\ndetail.\\nIn the CLI, the access modes are abbreviated to:\\nRWO - ReadWriteOnce\\nROX - ReadOnlyMany\\nRWX - ReadWriteMany\\nRWOP - ReadWriteOncePod• \\n• \\n• \\n•', metadata={'source': './PDFS/Concepts.pdf', 'page': 311}),\n",
       " Document(page_content=\"Note:  Kubernetes uses volume access modes to match PersistentVolumeClaims and\\nPersistentVolumes. In some cases, the volume access modes also constrain where the\\nPersistentVolume can be mounted. Volume access modes do not enforce write protection once\\nthe storage has been mounted. Even if the access modes are specified as ReadWriteOnce,\\nReadOnlyMany, or ReadWriteMany, they don't set any constraints on the volume. For example,\\neven if a PersistentVolume is created as ReadOnlyMany, it is no guarantee that it will be read-\\nonly. If the access modes are specified as ReadWriteOncePod, the volume is constrained and can\\nbe mounted on only a single Pod.\\nImportant!  A volume can only be mounted using one access mode at a time, even\\nif it supports many. For example, a GCEPersistentDisk can be mounted as\\nReadWriteOnce by a single node or ReadOnlyMany by many nodes, but not at the\\nsame time.\\nVolume Plugin ReadWriteOnce ReadOnlyMany ReadWriteMany ReadWriteOncePod\\nAzureFile -\\nCephFS -\", metadata={'source': './PDFS/Concepts.pdf', 'page': 312}),\n",
       " Document(page_content='Volume Plugin ReadWriteOnce ReadOnlyMany ReadWriteMany ReadWriteOncePod\\nAzureFile -\\nCephFS -\\nCSIdepends on the\\ndriverdepends on the\\ndriverdepends on the\\ndriverdepends on the\\ndriver\\nFC - -\\nFlexVolume depends on the\\ndriver-\\nGCEPersistentDisk - -\\nGlusterfs -\\nHostPath - - -\\niSCSI - -\\nNFS -\\nRBD - -\\nVsphereVolume -- (works when\\nPods are\\ncollocated)-\\nPortworxVolume - -\\nClass\\nA PV can have a class, which is specified by setting the storageClassName  attribute to the name\\nof a StorageClass . A PV of a particular class can only be bound to PVCs requesting that class. A\\nPV with no storageClassName  has no class and can only be bound to PVCs that request no\\nparticular class.\\nIn the past, the annotation volume.beta.kubernetes.io/storage-class  was used instead of the \\nstorageClassName  attribute. This annotation is still working; however, it will become fully\\ndeprecated in a future Kubernetes release.\\nReclaim Policy\\nCurrent reclaim policies are:\\nRetain -- manual reclamation', metadata={'source': './PDFS/Concepts.pdf', 'page': 312}),\n",
       " Document(page_content='Reclaim Policy\\nCurrent reclaim policies are:\\nRetain -- manual reclamation\\nRecycle -- basic scrub ( rm -rf /thevolume/* )\\nDelete -- associated storage asset such as AWS EBS or GCE PD volume is deleted• \\n• \\n•', metadata={'source': './PDFS/Concepts.pdf', 'page': 312}),\n",
       " Document(page_content='Currently, only NFS and HostPath support recycling. AWS EBS and GCE PD volumes support\\ndeletion.\\nMount Options\\nA Kubernetes administrator can specify additional mount options for when a Persistent Volume\\nis mounted on a node.\\nNote:  Not all Persistent Volume types support mount options.\\nThe following volume types support mount options:\\nazureFile\\ncephfs  (deprecated  in v1.28)\\ncinder  (deprecated  in v1.18)\\ngcePersistentDisk  (deprecated  in v1.28)\\niscsi\\nnfs\\nrbd (deprecated  in v1.28)\\nvsphereVolume\\nMount options are not validated. If a mount option is invalid, the mount fails.\\nIn the past, the annotation volume.beta.kubernetes.io/mount-options  was used instead of the \\nmountOptions  attribute. This annotation is still working; however, it will become fully\\ndeprecated in a future Kubernetes release.\\nNode Affinity\\nNote:  For most volume types, you do not need to set this field. It is automatically populated for', metadata={'source': './PDFS/Concepts.pdf', 'page': 313}),\n",
       " Document(page_content='Note:  For most volume types, you do not need to set this field. It is automatically populated for \\nGCE PD  volume block types. You need to explicitly set this for local  volumes.\\nA PV can specify node affinity to define constraints that limit what nodes this volume can be\\naccessed from. Pods that use a PV will only be scheduled to nodes that are selected by the node\\naffinity. To specify node affinity, set nodeAffinity  in the .spec  of a PV. The PersistentVolume  API\\nreference has more details on this field.\\nPhase\\nA PersistentVolume will be in one of the following phases:\\nAvailable\\na free resource that is not yet bound to a claim\\nBound\\nthe volume is bound to a claim\\nReleased\\nthe claim has been deleted, but the associated storage resource is not yet reclaimed by the\\ncluster\\nFailed\\nthe volume has failed its (automated) reclamation\\nYou can see the name of the PVC bound to the PV using kubectl describe persistentvolume \\n<name> .• \\n• \\n• \\n• \\n• \\n• \\n• \\n•', metadata={'source': './PDFS/Concepts.pdf', 'page': 313}),\n",
       " Document(page_content='Phase transition timestamp\\nFEATURE STATE:  Kubernetes v1.28 [alpha]\\nThe .status  field for a PersistentVolume can include an alpha lastPhaseTransitionTime  field. This\\nfield records the timestamp of when the volume last transitioned its phase. For newly created\\nvolumes the phase is set to Pending  and lastPhaseTransitionTime  is set to the current time.\\nNote:  You need to enable the PersistentVolumeLastPhaseTransitionTime  feature gate  to use or\\nsee the lastPhaseTransitionTime  field.\\nPersistentVolumeClaims\\nEach PVC contains a spec and status, which is the specification and status of the claim. The\\nname of a PersistentVolumeClaim object must be a valid DNS subdomain name .\\napiVersion : v1\\nkind: PersistentVolumeClaim\\nmetadata :\\n  name : myclaim\\nspec:\\n  accessModes :\\n    - ReadWriteOnce\\n  volumeMode : Filesystem\\n  resources :\\n    requests :\\n      storage : 8Gi\\n  storageClassName : slow\\n  selector :\\n    matchLabels :\\n      release : \"stable\"\\n    matchExpressions :', metadata={'source': './PDFS/Concepts.pdf', 'page': 314}),\n",
       " Document(page_content='selector :\\n    matchLabels :\\n      release : \"stable\"\\n    matchExpressions :\\n      - {key: environment, operator: In, values : [dev]}\\nAccess Modes\\nClaims use the same conventions as volumes  when requesting storage with specific access\\nmodes.\\nVolume Modes\\nClaims use the same convention as volumes  to indicate the consumption of the volume as either\\na filesystem or block device.\\nResources\\nClaims, like Pods, can request specific quantities of a resource. In this case, the request is for\\nstorage. The same resource model  applies to both volumes and claims.', metadata={'source': './PDFS/Concepts.pdf', 'page': 314}),\n",
       " Document(page_content='Selector\\nClaims can specify a label selector  to further filter the set of volumes. Only the volumes whose\\nlabels match the selector can be bound to the claim. The selector can consist of two fields:\\nmatchLabels  - the volume must have a label with this value\\nmatchExpressions  - a list of requirements made by specifying key, list of values, and\\noperator that relates the key and values. Valid operators include In, NotIn, Exists, and\\nDoesNotExist.\\nAll of the requirements, from both matchLabels  and matchExpressions , are ANDed together –\\nthey must all be satisfied in order to match.\\nClass\\nA claim can request a particular class by specifying the name of a StorageClass  using the\\nattribute storageClassName . Only PVs of the requested class, ones with the same \\nstorageClassName  as the PVC, can be bound to the PVC.\\nPVCs don\\'t necessarily have to request a class. A PVC with its storageClassName  set equal to \"\"', metadata={'source': './PDFS/Concepts.pdf', 'page': 315}),\n",
       " Document(page_content='PVCs don\\'t necessarily have to request a class. A PVC with its storageClassName  set equal to \"\"\\nis always interpreted to be requesting a PV with no class, so it can only be bound to PVs with\\nno class (no annotation or one set equal to \"\"). A PVC with no storageClassName  is not quite\\nthe same and is treated differently by the cluster, depending on whether the \\nDefaultStorageClass  admission plugin  is turned on.\\nIf the admission plugin is turned on, the administrator may specify a default\\nStorageClass. All PVCs that have no storageClassName  can be bound only to PVs of that\\ndefault. Specifying a default StorageClass is done by setting the annotation \\nstorageclass.kubernetes.io/is-default-class  equal to true in a StorageClass object. If the\\nadministrator does not specify a default, the cluster responds to PVC creation as if the\\nadmission plugin were turned off. If more than one default StorageClass is specified, the\\nnewest default is used when the PVC is dynamically provisioned.', metadata={'source': './PDFS/Concepts.pdf', 'page': 315}),\n",
       " Document(page_content='newest default is used when the PVC is dynamically provisioned.\\nIf the admission plugin is turned off, there is no notion of a default StorageClass. All\\nPVCs that have storageClassName  set to \"\" can be bound only to PVs that have \\nstorageClassName  also set to \"\". However, PVCs with missing storageClassName  can be\\nupdated later once default StorageClass becomes available. If the PVC gets updated it will\\nno longer bind to PVs that have storageClassName  also set to \"\".\\nSee retroactive default StorageClass assignment  for more details.\\nDepending on installation method, a default StorageClass may be deployed to a Kubernetes\\ncluster by addon manager during installation.\\nWhen a PVC specifies a selector  in addition to requesting a StorageClass, the requirements are\\nANDed together: only a PV of the requested class and with the requested labels may be bound\\nto the PVC.\\nNote:  Currently, a PVC with a non-empty selector  can\\'t have a PV dynamically provisioned for\\nit.', metadata={'source': './PDFS/Concepts.pdf', 'page': 315}),\n",
       " Document(page_content=\"Note:  Currently, a PVC with a non-empty selector  can't have a PV dynamically provisioned for\\nit.\\nIn the past, the annotation volume.beta.kubernetes.io/storage-class  was used instead of \\nstorageClassName  attribute. This annotation is still working; however, it won't be supported in\\na future Kubernetes release.• \\n• \\n• \\n•\", metadata={'source': './PDFS/Concepts.pdf', 'page': 315}),\n",
       " Document(page_content='Retroactive default StorageClass assignment\\nFEATURE STATE:  Kubernetes v1.28 [stable]\\nYou can create a PersistentVolumeClaim without specifying a storageClassName  for the new\\nPVC, and you can do so even when no default StorageClass exists in your cluster. In this case,\\nthe new PVC creates as you defined it, and the storageClassName  of that PVC remains unset\\nuntil default becomes available.\\nWhen a default StorageClass becomes available, the control plane identifies any existing PVCs\\nwithout storageClassName . For the PVCs that either have an empty value for \\nstorageClassName  or do not have this key, the control plane then updates those PVCs to set \\nstorageClassName  to match the new default StorageClass. If you have an existing PVC where\\nthe storageClassName  is \"\", and you configure a default StorageClass, then this PVC will not get\\nupdated.\\nIn order to keep binding to PVs with storageClassName  set to \"\" (while a default StorageClass is', metadata={'source': './PDFS/Concepts.pdf', 'page': 316}),\n",
       " Document(page_content='In order to keep binding to PVs with storageClassName  set to \"\" (while a default StorageClass is\\npresent), you need to set the storageClassName  of the associated PVC to \"\".\\nThis behavior helps administrators change default StorageClass by removing the old one first\\nand then creating or setting another one. This brief window while there is no default causes\\nPVCs without storageClassName  created at that time to not have any default, but due to the\\nretroactive default StorageClass assignment this way of changing defaults is safe.\\nClaims As Volumes\\nPods access storage by using the claim as a volume. Claims must exist in the same namespace\\nas the Pod using the claim. The cluster finds the claim in the Pod\\'s namespace and uses it to get\\nthe PersistentVolume backing the claim. The volume is then mounted to the host and into the\\nPod.\\napiVersion : v1\\nkind: Pod\\nmetadata :\\n  name : mypod\\nspec:\\n  containers :\\n    - name : myfrontend\\n      image : nginx\\n      volumeMounts :', metadata={'source': './PDFS/Concepts.pdf', 'page': 316}),\n",
       " Document(page_content='name : mypod\\nspec:\\n  containers :\\n    - name : myfrontend\\n      image : nginx\\n      volumeMounts :\\n      - mountPath : \"/var/www/html\"\\n        name : mypd\\n  volumes :\\n    - name : mypd\\n      persistentVolumeClaim :\\n        claimName : myclaim\\nA Note on Namespaces\\nPersistentVolumes binds are exclusive, and since PersistentVolumeClaims are namespaced\\nobjects, mounting claims with \"Many\" modes ( ROX , RWX ) is only possible within one\\nnamespace.', metadata={'source': './PDFS/Concepts.pdf', 'page': 316}),\n",
       " Document(page_content='PersistentVolumes typed hostPath\\nA hostPath  PersistentVolume uses a file or directory on the Node to emulate network-attached\\nstorage. See an example of hostPath  typed volume .\\nRaw Block Volume Support\\nFEATURE STATE:  Kubernetes v1.18 [stable]\\nThe following volume plugins support raw block volumes, including dynamic provisioning\\nwhere applicable:\\nCSI\\nFC (Fibre Channel)\\nGCEPersistentDisk (deprecated)\\niSCSI\\nLocal volume\\nOpenStack Cinder\\nRBD (deprecated)\\nRBD (Ceph Block Device; deprecated)\\nVsphereVolume\\nPersistentVolume using a Raw Block Volume\\napiVersion : v1\\nkind: PersistentVolume\\nmetadata :\\n  name : block-pv\\nspec:\\n  capacity :\\n    storage : 10Gi\\n  accessModes :\\n    - ReadWriteOnce\\n  volumeMode : Block\\n  persistentVolumeReclaimPolicy : Retain\\n  fc:\\n    targetWWNs : [\"50060e801049cfd1\" ]\\n    lun: 0\\n    readOnly : false\\nPersistentVolumeClaim requesting a Raw Block Volume\\napiVersion : v1\\nkind: PersistentVolumeClaim\\nmetadata :\\n  name : block-pvc\\nspec:\\n  accessModes :\\n    - ReadWriteOnce', metadata={'source': './PDFS/Concepts.pdf', 'page': 317}),\n",
       " Document(page_content='kind: PersistentVolumeClaim\\nmetadata :\\n  name : block-pvc\\nspec:\\n  accessModes :\\n    - ReadWriteOnce\\n  volumeMode : Block\\n  resources :• \\n• \\n• \\n• \\n• \\n• \\n• \\n• \\n•', metadata={'source': './PDFS/Concepts.pdf', 'page': 317}),\n",
       " Document(page_content='requests :\\n      storage : 10Gi\\nPod specification adding Raw Block Device path in container\\napiVersion : v1\\nkind: Pod\\nmetadata :\\n  name : pod-with-block-volume\\nspec:\\n  containers :\\n    - name : fc-container\\n      image : fedora:26\\n      command : [\"/bin/sh\" , \"-c\"]\\n      args: [ \"tail -f /dev/null\"  ]\\n      volumeDevices :\\n        - name : data\\n          devicePath : /dev/xvda\\n  volumes :\\n    - name : data\\n      persistentVolumeClaim :\\n        claimName : block-pvc\\nNote:  When adding a raw block device for a Pod, you specify the device path in the container\\ninstead of a mount path.\\nBinding Block Volumes\\nIf a user requests a raw block volume by indicating this using the volumeMode  field in the\\nPersistentVolumeClaim spec, the binding rules differ slightly from previous releases that didn\\'t\\nconsider this mode as part of the spec. Listed is a table of possible combinations the user and\\nadmin might specify for requesting a raw block device. The table indicates if the volume will be', metadata={'source': './PDFS/Concepts.pdf', 'page': 318}),\n",
       " Document(page_content='admin might specify for requesting a raw block device. The table indicates if the volume will be\\nbound or not given the combinations: Volume binding matrix for statically provisioned\\nvolumes:\\nPV volumeMode PVC volumeMode Result\\nunspecified unspecified BIND\\nunspecified Block NO BIND\\nunspecified Filesystem BIND\\nBlock unspecified NO BIND\\nBlock Block BIND\\nBlock Filesystem NO BIND\\nFilesystem Filesystem BIND\\nFilesystem Block NO BIND\\nFilesystem unspecified BIND\\nNote:  Only statically provisioned volumes are supported for alpha release. Administrators\\nshould take care to consider these values when working with raw block devices.', metadata={'source': './PDFS/Concepts.pdf', 'page': 318}),\n",
       " Document(page_content='Volume Snapshot and Restore Volume from Snapshot\\nSupport\\nFEATURE STATE:  Kubernetes v1.20 [stable]\\nVolume snapshots only support the out-of-tree CSI volume plugins. For details, see Volume\\nSnapshots . In-tree volume plugins are deprecated. You can read about the deprecated volume\\nplugins in the Volume Plugin FAQ .\\nCreate a PersistentVolumeClaim from a Volume Snapshot\\napiVersion : v1\\nkind: PersistentVolumeClaim\\nmetadata :\\n  name : restore-pvc\\nspec:\\n  storageClassName : csi-hostpath-sc\\n  dataSource :\\n    name : new-snapshot-test\\n    kind: VolumeSnapshot\\n    apiGroup : snapshot.storage.k8s.io\\n  accessModes :\\n    - ReadWriteOnce\\n  resources :\\n    requests :\\n      storage : 10Gi\\nVolume Cloning\\nVolume Cloning  only available for CSI volume plugins.\\nCreate PersistentVolumeClaim from an existing PVC\\napiVersion : v1\\nkind: PersistentVolumeClaim\\nmetadata :\\n  name : cloned-pvc\\nspec:\\n  storageClassName : my-csi-plugin\\n  dataSource :\\n    name : existing-src-pvc-name\\n    kind: PersistentVolumeClaim', metadata={'source': './PDFS/Concepts.pdf', 'page': 319}),\n",
       " Document(page_content='dataSource :\\n    name : existing-src-pvc-name\\n    kind: PersistentVolumeClaim\\n  accessModes :\\n    - ReadWriteOnce\\n  resources :\\n    requests :\\n      storage : 10Gi', metadata={'source': './PDFS/Concepts.pdf', 'page': 319}),\n",
       " Document(page_content='Volume populators and data sources\\nFEATURE STATE:  Kubernetes v1.24 [beta]\\nKubernetes supports custom volume populators. To use custom volume populators, you must\\nenable the AnyVolumeDataSource  feature gate  for the kube-apiserver and kube-controller-\\nmanager.\\nVolume populators take advantage of a PVC spec field called dataSourceRef . Unlike the \\ndataSource  field, which can only contain either a reference to another PersistentVolumeClaim\\nor to a VolumeSnapshot, the dataSourceRef  field can contain a reference to any object in the\\nsame namespace, except for core objects other than PVCs. For clusters that have the feature\\ngate enabled, use of the dataSourceRef  is preferred over dataSource .\\nCross namespace data sources\\nFEATURE STATE:  Kubernetes v1.26 [alpha]\\nKubernetes supports cross namespace volume data sources. To use cross namespace volume\\ndata sources, you must enable the AnyVolumeDataSource  and', metadata={'source': './PDFS/Concepts.pdf', 'page': 320}),\n",
       " Document(page_content='data sources, you must enable the AnyVolumeDataSource  and \\nCrossNamespaceVolumeDataSource  feature gates  for the kube-apiserver and kube-controller-\\nmanager. Also, you must enable the CrossNamespaceVolumeDataSource  feature gate for the csi-\\nprovisioner.\\nEnabling the CrossNamespaceVolumeDataSource  feature gate allows you to specify a\\nnamespace in the dataSourceRef field.\\nNote:  When you specify a namespace for a volume data source, Kubernetes checks for a\\nReferenceGrant in the other namespace before accepting the reference. ReferenceGrant is part\\nof the gateway.networking.k8s.io  extension APIs. See ReferenceGrant  in the Gateway API\\ndocumentation for details. This means that you must extend your Kubernetes cluster with at\\nleast ReferenceGrant from the Gateway API before you can use this mechanism.\\nData source references\\nThe dataSourceRef  field behaves almost the same as the dataSource  field. If one is specified', metadata={'source': './PDFS/Concepts.pdf', 'page': 320}),\n",
       " Document(page_content='The dataSourceRef  field behaves almost the same as the dataSource  field. If one is specified\\nwhile the other is not, the API server will give both fields the same value. Neither field can be\\nchanged after creation, and attempting to specify different values for the two fields will result in\\na validation error. Therefore the two fields will always have the same contents.\\nThere are two differences between the dataSourceRef  field and the dataSource  field that users\\nshould be aware of:\\nThe dataSource  field ignores invalid values (as if the field was blank) while the \\ndataSourceRef  field never ignores values and will cause an error if an invalid value is\\nused. Invalid values are any core object (objects with no apiGroup) except for PVCs.\\nThe dataSourceRef  field may contain different types of objects, while the dataSource  field\\nonly allows PVCs and VolumeSnapshots.• \\n•', metadata={'source': './PDFS/Concepts.pdf', 'page': 320}),\n",
       " Document(page_content='When the CrossNamespaceVolumeDataSource  feature is enabled, there are additional\\ndifferences:\\nThe dataSource  field only allows local objects, while the dataSourceRef  field allows\\nobjects in any namespaces.\\nWhen namespace is specified, dataSource  and dataSourceRef  are not synced.\\nUsers should always use dataSourceRef  on clusters that have the feature gate enabled, and fall\\nback to dataSource  on clusters that do not. It is not necessary to look at both fields under any\\ncircumstance. The duplicated values with slightly different semantics exist only for backwards\\ncompatibility. In particular, a mixture of older and newer controllers are able to interoperate\\nbecause the fields are the same.\\nUsing volume populators\\nVolume populators are controllers  that can create non-empty volumes, where the contents of\\nthe volume are determined by a Custom Resource. Users create a populated volume by referring\\nto a Custom Resource using the dataSourceRef  field:\\napiVersion : v1', metadata={'source': './PDFS/Concepts.pdf', 'page': 321}),\n",
       " Document(page_content=\"to a Custom Resource using the dataSourceRef  field:\\napiVersion : v1\\nkind: PersistentVolumeClaim\\nmetadata :\\n  name : populated-pvc\\nspec:\\n  dataSourceRef :\\n    name : example-name\\n    kind: ExampleDataSource\\n    apiGroup : example.storage.k8s.io\\n  accessModes :\\n    - ReadWriteOnce\\n  resources :\\n    requests :\\n      storage : 10Gi\\nBecause volume populators are external components, attempts to create a PVC that uses one\\ncan fail if not all the correct components are installed. External controllers should generate\\nevents on the PVC to provide feedback on the status of the creation, including warnings if the\\nPVC cannot be created due to some missing component.\\nYou can install the alpha volume data source validator  controller into your cluster. That\\ncontroller generates warning Events on a PVC in the case that no populator is registered to\\nhandle that kind of data source. When a suitable populator is installed for a PVC, it's the\", metadata={'source': './PDFS/Concepts.pdf', 'page': 321}),\n",
       " Document(page_content=\"handle that kind of data source. When a suitable populator is installed for a PVC, it's the\\nresponsibility of that populator controller to report Events that relate to volume creation and\\nissues during the process.\\nUsing a cross-namespace volume data source\\nFEATURE STATE:  Kubernetes v1.26 [alpha]\\nCreate a ReferenceGrant to allow the namespace owner to accept the reference. You define a\\npopulated volume by specifying a cross namespace volume data source using the dataSourceRef\\nfield. You must already have a valid ReferenceGrant in the source namespace:• \\n•\", metadata={'source': './PDFS/Concepts.pdf', 'page': 321}),\n",
       " Document(page_content='apiVersion : gateway.networking.k8s.io/v1beta1\\nkind: ReferenceGrant\\nmetadata :\\n  name : allow-ns1-pvc\\n  namespace : default\\nspec:\\n  from :\\n  - group : \"\"\\n    kind: PersistentVolumeClaim\\n    namespace : ns1\\n  to:\\n  - group : snapshot.storage.k8s.io\\n    kind: VolumeSnapshot\\n    name : new-snapshot-demo\\napiVersion : v1\\nkind: PersistentVolumeClaim\\nmetadata :\\n  name : foo-pvc\\n  namespace : ns1\\nspec:\\n  storageClassName : example\\n  accessModes :\\n  - ReadWriteOnce\\n  resources :\\n    requests :\\n      storage : 1Gi\\n  dataSourceRef :\\n    apiGroup : snapshot.storage.k8s.io\\n    kind: VolumeSnapshot\\n    name : new-snapshot-demo\\n    namespace : default\\n  volumeMode : Filesystem\\nWriting Portable Configuration\\nIf you\\'re writing configuration templates or examples that run on a wide range of clusters and\\nneed persistent storage, it is recommended that you use the following pattern:\\nInclude PersistentVolumeClaim objects in your bundle of config (alongside Deployments,\\nConfigMaps, etc).', metadata={'source': './PDFS/Concepts.pdf', 'page': 322}),\n",
       " Document(page_content='ConfigMaps, etc).\\nDo not include PersistentVolume objects in the config, since the user instantiating the\\nconfig may not have permission to create PersistentVolumes.\\nGive the user the option of providing a storage class name when instantiating the\\ntemplate.\\nIf the user provides a storage class name, put that value into the \\npersistentVolumeClaim.storageClassName  field. This will cause the PVC to match\\nthe right storage class if the cluster has StorageClasses enabled by the admin.\\nIf the user does not provide a storage class name, leave the \\npersistentVolumeClaim.storageClassName  field as nil. This will cause a PV to be\\nautomatically provisioned for the user with the default StorageClass in the cluster.• \\n• \\n• \\n◦ \\n◦', metadata={'source': './PDFS/Concepts.pdf', 'page': 322}),\n",
       " Document(page_content=\"Many cluster environments have a default StorageClass installed, or administrators\\ncan create their own default StorageClass.\\nIn your tooling, watch for PVCs that are not getting bound after some time and surface\\nthis to the user, as this may indicate that the cluster has no dynamic storage support (in\\nwhich case the user should create a matching PV) or the cluster has no storage system (in\\nwhich case the user cannot deploy config requiring PVCs).\\nWhat's next\\nLearn more about Creating a PersistentVolume .\\nLearn more about Creating a PersistentVolumeClaim .\\nRead the Persistent Storage design document .\\nAPI references\\nRead about the APIs described in this page:\\nPersistentVolume\\nPersistentVolumeClaim\\nProjected Volumes\\nThis document describes projected volumes  in Kubernetes. Familiarity with volumes  is\\nsuggested.\\nIntroduction\\nA projected  volume maps several existing volume sources into the same directory.\\nCurrently, the following types of volume sources can be projected:\\nsecret\", metadata={'source': './PDFS/Concepts.pdf', 'page': 323}),\n",
       " Document(page_content='Currently, the following types of volume sources can be projected:\\nsecret\\ndownwardAPI\\nconfigMap\\nserviceAccountToken\\nAll sources are required to be in the same namespace as the Pod. For more details, see the all-\\nin-one volume  design document.\\nExample configuration with a secret, a downwardAPI, and a configMap\\npods/storage/projected-secret-downwardapi-configmap.yaml  \\napiVersion : v1\\nkind: Pod\\nmetadata :\\n  name : volume-test\\nspec:\\n  containers :\\n  - name : container-test• \\n• \\n• \\n• \\n• \\n• \\n• \\n• \\n• \\n•', metadata={'source': './PDFS/Concepts.pdf', 'page': 323}),\n",
       " Document(page_content='image : busybox:1.28\\n    command : [\"sleep\" , \"3600\" ]\\n    volumeMounts :\\n    - name : all-in-one\\n      mountPath : \"/projected-volume\"\\n      readOnly : true\\n  volumes :\\n  - name : all-in-one\\n    projected :\\n      sources :\\n      - secret :\\n          name : mysecret\\n          items :\\n            - key: username\\n              path: my-group/my-username\\n      - downwardAPI :\\n          items :\\n            - path: \"labels\"\\n              fieldRef :\\n                fieldPath : metadata.labels\\n            - path: \"cpu_limit\"\\n              resourceFieldRef :\\n                containerName : container-test\\n                resource : limits.cpu\\n      - configMap :\\n          name : myconfigmap\\n          items :\\n            - key: config\\n              path: my-group/my-config\\nExample configuration: secrets with a non-default permission mode set\\npods/storage/projected-secrets-nondefault-permission-mode.yaml  \\napiVersion : v1\\nkind: Pod\\nmetadata :\\n  name : volume-test\\nspec:\\n  containers :', metadata={'source': './PDFS/Concepts.pdf', 'page': 324}),\n",
       " Document(page_content='apiVersion : v1\\nkind: Pod\\nmetadata :\\n  name : volume-test\\nspec:\\n  containers :\\n  - name : container-test\\n    image : busybox:1.28\\n    command : [\"sleep\" , \"3600\" ]\\n    volumeMounts :\\n    - name : all-in-one\\n      mountPath : \"/projected-volume\"\\n      readOnly : true\\n  volumes :\\n  - name : all-in-one\\n    projected :\\n      sources :\\n      - secret :\\n          name : mysecret', metadata={'source': './PDFS/Concepts.pdf', 'page': 324}),\n",
       " Document(page_content='items :\\n            - key: username\\n              path: my-group/my-username\\n      - secret :\\n          name : mysecret2\\n          items :\\n            - key: password\\n              path: my-group/my-password\\n              mode : 511\\nEach projected volume source is listed in the spec under sources . The parameters are nearly the\\nsame with two exceptions:\\nFor secrets, the secretName  field has been changed to name  to be consistent with\\nConfigMap naming.\\nThe defaultMode  can only be specified at the projected level and not for each volume\\nsource. However, as illustrated above, you can explicitly set the mode  for each individual\\nprojection.\\nserviceAccountToken projected volumes\\nYou can inject the token for the current service account  into a Pod at a specified path. For\\nexample:\\npods/storage/projected-service-account-token.yaml  \\napiVersion : v1\\nkind: Pod\\nmetadata :\\n  name : sa-token-test\\nspec:\\n  containers :\\n  - name : container-test\\n    image : busybox:1.28', metadata={'source': './PDFS/Concepts.pdf', 'page': 325}),\n",
       " Document(page_content='name : sa-token-test\\nspec:\\n  containers :\\n  - name : container-test\\n    image : busybox:1.28\\n    command : [\"sleep\" , \"3600\" ]\\n    volumeMounts :\\n    - name : token-vol\\n      mountPath : \"/service-account\"\\n      readOnly : true\\n  serviceAccountName : default\\n  volumes :\\n  - name : token-vol\\n    projected :\\n      sources :\\n      - serviceAccountToken :\\n          audience : api\\n          expirationSeconds : 3600\\n          path: token\\nThe example Pod has a projected volume containing the injected service account token.\\nContainers in this Pod can use that token to access the Kubernetes API server, authenticating\\nwith the identity of the pod\\'s ServiceAccount . The audience  field contains the intended\\naudience of the token. A recipient of the token must identify itself with an identifier specified in• \\n•', metadata={'source': './PDFS/Concepts.pdf', 'page': 325}),\n",
       " Document(page_content='the audience of the token, and otherwise should reject the token. This field is optional and it\\ndefaults to the identifier of the API server.\\nThe expirationSeconds  is the expected duration of validity of the service account token. It\\ndefaults to 1 hour and must be at least 10 minutes (600 seconds). An administrator can also limit\\nits maximum value by specifying the --service-account-max-token-expiration  option for the\\nAPI server. The path field specifies a relative path to the mount point of the projected volume.\\nNote:  A container using a projected volume source as a subPath  volume mount will not receive\\nupdates for those volume sources.\\nSecurityContext interactions\\nThe proposal  for file permission handling in projected service account volume enhancement\\nintroduced the projected files having the correct owner permissions set.\\nLinux\\nIn Linux pods that have a projected volume and RunAsUser  set in the Pod SecurityContext , the', metadata={'source': './PDFS/Concepts.pdf', 'page': 326}),\n",
       " Document(page_content=\"Linux\\nIn Linux pods that have a projected volume and RunAsUser  set in the Pod SecurityContext , the\\nprojected files have the correct ownership set including container user ownership.\\nWhen all containers in a pod have the same runAsUser  set in their PodSecurityContext  or\\ncontainer SecurityContext , then the kubelet ensures that the contents of the \\nserviceAccountToken  volume are owned by that user, and the token file has its permission\\nmode set to 0600.\\nNote:\\nEphemeral containers  added to a Pod after it is created do not change volume permissions that\\nwere set when the pod was created.\\nIf a Pod's serviceAccountToken  volume permissions were set to 0600 because all other\\ncontainers in the Pod have the same runAsUser , ephemeral containers must use the same \\nrunAsUser  to be able to read the token.\\nWindows\\nIn Windows pods that have a projected volume and RunAsUsername  set in the Pod \\nSecurityContext , the ownership is not enforced due to the way user accounts are managed in\", metadata={'source': './PDFS/Concepts.pdf', 'page': 326}),\n",
       " Document(page_content='SecurityContext , the ownership is not enforced due to the way user accounts are managed in\\nWindows. Windows stores and manages local user and group accounts in a database file called\\nSecurity Account Manager (SAM). Each container maintains its own instance of the SAM\\ndatabase, to which the host has no visibility into while the container is running. Windows\\ncontainers are designed to run the user mode portion of the OS in isolation from the host,\\nhence the maintenance of a virtual SAM database. As a result, the kubelet running on the host\\ndoes not have the ability to dynamically configure host file ownership for virtualized container\\naccounts. It is recommended that if files on the host machine are to be shared with the\\ncontainer then they should be placed into their own volume mount outside of C:\\\\.\\nBy default, the projected files will have the following ownership as shown for an example\\nprojected volume file:', metadata={'source': './PDFS/Concepts.pdf', 'page': 326}),\n",
       " Document(page_content='projected volume file:\\nPS C:\\\\> Get-Acl  C:\\\\var\\\\run\\\\secrets\\\\kubernetes.io\\\\serviceaccount\\\\..2021_08_31_22_22_18. 3182300\\n61\\\\ca.crt | Format-List', metadata={'source': './PDFS/Concepts.pdf', 'page': 326}),\n",
       " Document(page_content=\"Path   : Microsoft.PowerShell.Core\\\\FileSystem::C:\\n\\\\var\\\\run\\\\secrets\\\\kubernetes.io\\\\serviceaccount\\\\..2021_08_31_22_22_18. 318230061 \\\\ca.crt\\nOwner  : BUILTIN\\\\Administrators\\nGroup  : NT AUTHORITY\\\\SYSTEM\\nAccess : NT AUTHORITY\\\\SYSTEM Allow  FullControl\\n         BUILTIN\\\\Administrators Allow  FullControl\\n         BUILTIN\\\\Users Allow  ReadAndExecute, Synchronize\\nAudit  :\\nSddl   : O:BAG :SYD:AI(A;ID;FA;;;SY)(A;ID;FA;;;BA)(A;ID;0x1200a9;;;BU)\\nThis implies all administrator users like ContainerAdministrator  will have read, write and\\nexecute access while, non-administrator users will have read and execute access.\\nNote:\\nIn general, granting the container access to the host is discouraged as it can open the door for\\npotential security exploits.\\nCreating a Windows Pod with RunAsUser  in it's SecurityContext  will result in the Pod being\\nstuck at ContainerCreating  forever. So it is advised to not use the Linux only RunAsUser  option\\nwith Windows Pods.\\nEphemeral Volumes\", metadata={'source': './PDFS/Concepts.pdf', 'page': 327}),\n",
       " Document(page_content=\"with Windows Pods.\\nEphemeral Volumes\\nThis document describes ephemeral volumes  in Kubernetes. Familiarity with volumes  is\\nsuggested, in particular PersistentVolumeClaim and PersistentVolume.\\nSome application need additional storage but don't care whether that data is stored persistently\\nacross restarts. For example, caching services are often limited by memory size and can move\\ninfrequently used data into storage that is slower than memory with little impact on overall\\nperformance.\\nOther applications expect some read-only input data to be present in files, like configuration\\ndata or secret keys.\\nEphemeral volumes  are designed for these use cases. Because volumes follow the Pod's lifetime\\nand get created and deleted along with the Pod, Pods can be stopped and restarted without\\nbeing limited to where some persistent volume is available.\\nEphemeral volumes are specified inline  in the Pod spec, which simplifies application\\ndeployment and management.\\nTypes of ephemeral volumes\", metadata={'source': './PDFS/Concepts.pdf', 'page': 327}),\n",
       " Document(page_content='deployment and management.\\nTypes of ephemeral volumes\\nKubernetes supports several different kinds of ephemeral volumes for different purposes:\\nemptyDir : empty at Pod startup, with storage coming locally from the kubelet base\\ndirectory (usually the root disk) or RAM\\nconfigMap , downwardAPI , secret : inject different kinds of Kubernetes data into a Pod\\nCSI ephemeral volumes : similar to the previous volume kinds, but provided by special CSI\\ndrivers which specifically support this feature• \\n• \\n•', metadata={'source': './PDFS/Concepts.pdf', 'page': 327}),\n",
       " Document(page_content='generic ephemeral volumes , which can be provided by all storage drivers that also\\nsupport persistent volumes\\nemptyDir , configMap , downwardAPI , secret  are provided as local ephemeral storage . They are\\nmanaged by kubelet on each node.\\nCSI ephemeral volumes must  be provided by third-party CSI storage drivers.\\nGeneric ephemeral volumes can be provided by third-party CSI storage drivers, but also by any\\nother storage driver that supports dynamic provisioning. Some CSI drivers are written\\nspecifically for CSI ephemeral volumes and do not support dynamic provisioning: those then\\ncannot be used for generic ephemeral volumes.\\nThe advantage of using third-party drivers is that they can offer functionality that Kubernetes\\nitself does not support, for example storage with different performance characteristics than the\\ndisk that is managed by kubelet, or injecting different data.\\nCSI ephemeral volumes\\nFEATURE STATE:  Kubernetes v1.25 [stable]', metadata={'source': './PDFS/Concepts.pdf', 'page': 328}),\n",
       " Document(page_content=\"CSI ephemeral volumes\\nFEATURE STATE:  Kubernetes v1.25 [stable]\\nNote:  CSI ephemeral volumes are only supported by a subset of CSI drivers. The Kubernetes\\nCSI Drivers list  shows which drivers support ephemeral volumes.\\nConceptually, CSI ephemeral volumes are similar to configMap , downwardAPI  and secret\\nvolume types: the storage is managed locally on each node and is created together with other\\nlocal resources after a Pod has been scheduled onto a node. Kubernetes has no concept of\\nrescheduling Pods anymore at this stage. Volume creation has to be unlikely to fail, otherwise\\nPod startup gets stuck. In particular, storage capacity aware Pod scheduling  is not supported for\\nthese volumes. They are currently also not covered by the storage resource usage limits of a\\nPod, because that is something that kubelet can only enforce for storage that it manages itself.\\nHere's an example manifest for a Pod that uses CSI ephemeral storage:\\nkind: Pod\\napiVersion : v1\\nmetadata :\\n  name : my-csi-app\", metadata={'source': './PDFS/Concepts.pdf', 'page': 328}),\n",
       " Document(page_content='kind: Pod\\napiVersion : v1\\nmetadata :\\n  name : my-csi-app\\nspec:\\n  containers :\\n    - name : my-frontend\\n      image : busybox:1.28\\n      volumeMounts :\\n      - mountPath : \"/data\"\\n        name : my-csi-inline-vol\\n      command : [ \"sleep\" , \"1000000\"  ]\\n  volumes :\\n    - name : my-csi-inline-vol\\n      csi:\\n        driver : inline.storage.kubernetes.io\\n        volumeAttributes :\\n          foo: bar•', metadata={'source': './PDFS/Concepts.pdf', 'page': 328}),\n",
       " Document(page_content='The volumeAttributes  determine what volume is prepared by the driver. These attributes are\\nspecific to each driver and not standardized. See the documentation of each CSI driver for\\nfurther instructions.\\nCSI driver restrictions\\nCSI ephemeral volumes allow users to provide volumeAttributes  directly to the CSI driver as\\npart of the Pod spec. A CSI driver allowing volumeAttributes  that are typically restricted to\\nadministrators is NOT suitable for use in an inline ephemeral volume. For example, parameters\\nthat are normally defined in the StorageClass should not be exposed to users through the use of\\ninline ephemeral volumes.\\nCluster administrators who need to restrict the CSI drivers that are allowed to be used as inline\\nvolumes within a Pod spec may do so by:\\nRemoving Ephemeral  from volumeLifecycleModes  in the CSIDriver spec, which prevents\\nthe driver from being used as an inline ephemeral volume.\\nUsing an admission webhook  to restrict how this driver is used.', metadata={'source': './PDFS/Concepts.pdf', 'page': 329}),\n",
       " Document(page_content='Using an admission webhook  to restrict how this driver is used.\\nGeneric ephemeral volumes\\nFEATURE STATE:  Kubernetes v1.23 [stable]\\nGeneric ephemeral volumes are similar to emptyDir  volumes in the sense that they provide a\\nper-pod directory for scratch data that is usually empty after provisioning. But they may also\\nhave additional features:\\nStorage can be local or network-attached.\\nVolumes can have a fixed size that Pods are not able to exceed.\\nVolumes may have some initial data, depending on the driver and parameters.\\nTypical operations on volumes are supported assuming that the driver supports them,\\nincluding snapshotting , cloning , resizing , and storage capacity tracking .\\nExample:\\nkind: Pod\\napiVersion : v1\\nmetadata :\\n  name : my-app\\nspec:\\n  containers :\\n    - name : my-frontend\\n      image : busybox:1.28\\n      volumeMounts :\\n      - mountPath : \"/scratch\"\\n        name : scratch-volume\\n      command : [ \"sleep\" , \"1000000\"  ]\\n  volumes :\\n    - name : scratch-volume', metadata={'source': './PDFS/Concepts.pdf', 'page': 329}),\n",
       " Document(page_content='command : [ \"sleep\" , \"1000000\"  ]\\n  volumes :\\n    - name : scratch-volume\\n      ephemeral :\\n        volumeClaimTemplate :\\n          metadata :\\n            labels :• \\n• \\n• \\n• \\n• \\n•', metadata={'source': './PDFS/Concepts.pdf', 'page': 329}),\n",
       " Document(page_content='type: my-frontend-volume\\n          spec:\\n            accessModes : [ \"ReadWriteOnce\"  ]\\n            storageClassName : \"scratch-storage-class\"\\n            resources :\\n              requests :\\n                storage : 1Gi\\nLifecycle and PersistentVolumeClaim\\nThe key design idea is that the parameters for a volume claim  are allowed inside a volume\\nsource of the Pod. Labels, annotations and the whole set of fields for a PersistentVolumeClaim\\nare supported. When such a Pod gets created, the ephemeral volume controller then creates an\\nactual PersistentVolumeClaim object in the same namespace as the Pod and ensures that the\\nPersistentVolumeClaim gets deleted when the Pod gets deleted.\\nThat triggers volume binding and/or provisioning, either immediately if the StorageClass  uses\\nimmediate volume binding or when the Pod is tentatively scheduled onto a node\\n(WaitForFirstConsumer  volume binding mode). The latter is recommended for generic', metadata={'source': './PDFS/Concepts.pdf', 'page': 330}),\n",
       " Document(page_content='(WaitForFirstConsumer  volume binding mode). The latter is recommended for generic\\nephemeral volumes because then the scheduler is free to choose a suitable node for the Pod.\\nWith immediate binding, the scheduler is forced to select a node that has access to the volume\\nonce it is available.\\nIn terms of resource ownership , a Pod that has generic ephemeral storage is the owner of the\\nPersistentVolumeClaim(s) that provide that ephemeral storage. When the Pod is deleted, the\\nKubernetes garbage collector deletes the PVC, which then usually triggers deletion of the\\nvolume because the default reclaim policy of storage classes is to delete volumes. You can create\\nquasi-ephemeral local storage using a StorageClass with a reclaim policy of retain : the storage\\noutlives the Pod, and in this case you need to ensure that volume clean up happens separately.\\nWhile these PVCs exist, they can be used like any other PVC. In particular, they can be', metadata={'source': './PDFS/Concepts.pdf', 'page': 330}),\n",
       " Document(page_content='While these PVCs exist, they can be used like any other PVC. In particular, they can be\\nreferenced as data source in volume cloning or snapshotting. The PVC object also holds the\\ncurrent status of the volume.\\nPersistentVolumeClaim naming\\nNaming of the automatically created PVCs is deterministic: the name is a combination of the\\nPod name and volume name, with a hyphen ( -) in the middle. In the example above, the PVC\\nname will be my-app-scratch-volume . This deterministic naming makes it easier to interact\\nwith the PVC because one does not have to search for it once the Pod name and volume name\\nare known.\\nThe deterministic naming also introduces a potential conflict between different Pods (a Pod\\n\"pod-a\" with volume \"scratch\" and another Pod with name \"pod\" and volume \"a-scratch\" both\\nend up with the same PVC name \"pod-a-scratch\") and between Pods and manually created\\nPVCs.\\nSuch conflicts are detected: a PVC is only used for an ephemeral volume if it was created for the', metadata={'source': './PDFS/Concepts.pdf', 'page': 330}),\n",
       " Document(page_content='Such conflicts are detected: a PVC is only used for an ephemeral volume if it was created for the\\nPod. This check is based on the ownership relationship. An existing PVC is not overwritten or\\nmodified. But this does not resolve the conflict because without the right PVC, the Pod cannot\\nstart.', metadata={'source': './PDFS/Concepts.pdf', 'page': 330}),\n",
       " Document(page_content=\"Caution:  Take care when naming Pods and volumes inside the same namespace, so that these\\nconflicts can't occur.\\nSecurity\\nUsing generic ephemeral volumes allows users to create PVCs indirectly if they can create Pods,\\neven if they do not have permission to create PVCs directly. Cluster administrators must be\\naware of this. If this does not fit their security model, they should use an admission webhook\\nthat rejects objects like Pods that have a generic ephemeral volume.\\nThe normal namespace quota for PVCs  still applies, so even if users are allowed to use this new\\nmechanism, they cannot use it to circumvent other policies.\\nWhat's next\\nEphemeral volumes managed by kubelet\\nSee local ephemeral storage .\\nCSI ephemeral volumes\\nFor more information on the design, see the Ephemeral Inline CSI volumes KEP .\\nFor more information on further development of this feature, see the enhancement\\ntracking issue #596 .\\nGeneric ephemeral volumes\", metadata={'source': './PDFS/Concepts.pdf', 'page': 331}),\n",
       " Document(page_content='tracking issue #596 .\\nGeneric ephemeral volumes\\nFor more information on the design, see the Generic ephemeral inline volumes KEP .\\nStorage Classes\\nThis document describes the concept of a StorageClass in Kubernetes. Familiarity with volumes\\nand persistent volumes  is suggested.\\nIntroduction\\nA StorageClass provides a way for administrators to describe the \"classes\" of storage they offer.\\nDifferent classes might map to quality-of-service levels, or to backup policies, or to arbitrary\\npolicies determined by the cluster administrators. Kubernetes itself is unopinionated about\\nwhat classes represent. This concept is sometimes called \"profiles\" in other storage systems.\\nThe StorageClass Resource\\nEach StorageClass contains the fields provisioner , parameters , and reclaimPolicy , which are\\nused when a PersistentVolume belonging to the class needs to be dynamically provisioned.\\nThe name of a StorageClass object is significant, and is how users can request a particular class.', metadata={'source': './PDFS/Concepts.pdf', 'page': 331}),\n",
       " Document(page_content='The name of a StorageClass object is significant, and is how users can request a particular class.\\nAdministrators set the name and other parameters of a class when first creating StorageClass\\nobjects.• \\n• \\n•', metadata={'source': './PDFS/Concepts.pdf', 'page': 331}),\n",
       " Document(page_content=\"Administrators can specify a default StorageClass only for PVCs that don't request any\\nparticular class to bind to: see the PersistentVolumeClaim section  for details.\\napiVersion : storage.k8s.io/v1\\nkind: StorageClass\\nmetadata :\\n  name : standard\\nprovisioner : kubernetes.io/aws-ebs\\nparameters :\\n  type: gp2\\nreclaimPolicy : Retain\\nallowVolumeExpansion : true\\nmountOptions :\\n  - debug\\nvolumeBindingMode : Immediate\\nDefault StorageClass\\nWhen a PVC does not specify a storageClassName , the default StorageClass is used. The cluster\\ncan only have one default StorageClass. If more than one default StorageClass is accidentally\\nset, the newest default is used when the PVC is dynamically provisioned.\\nFor instructions on setting the default StorageClass, see Change the default StorageClass . Note\\nthat certain cloud providers may already define a default StorageClass.\\nProvisioner\\nEach StorageClass has a provisioner that determines what volume plugin is used for\", metadata={'source': './PDFS/Concepts.pdf', 'page': 332}),\n",
       " Document(page_content='Provisioner\\nEach StorageClass has a provisioner that determines what volume plugin is used for\\nprovisioning PVs. This field must be specified.\\nVolume Plugin Internal Provisioner Config Example\\nAzureFile Azure File\\nCephFS - -\\nFC - -\\nFlexVolume - -\\nGCEPersistentDisk GCE PD\\niSCSI - -\\nNFS - NFS\\nRBD Ceph RBD\\nVsphereVolume vSphere\\nPortworxVolume Portworx Volume\\nLocal - Local\\nYou are not restricted to specifying the \"internal\" provisioners listed here (whose names are\\nprefixed with \"kubernetes.io\" and shipped alongside Kubernetes). You can also run and specify\\nexternal provisioners, which are independent programs that follow a specification  defined by\\nKubernetes. Authors of external provisioners have full discretion over where their code lives,\\nhow the provisioner is shipped, how it needs to be run, what volume plugin it uses (including\\nFlex), etc. The repository kubernetes-sigs/sig-storage-lib-external-provisioner  houses a library', metadata={'source': './PDFS/Concepts.pdf', 'page': 332}),\n",
       " Document(page_content='Flex), etc. The repository kubernetes-sigs/sig-storage-lib-external-provisioner  houses a library\\nfor writing external provisioners that implements the bulk of the specification. Some external', metadata={'source': './PDFS/Concepts.pdf', 'page': 332}),\n",
       " Document(page_content=\"provisioners are listed under the repository kubernetes-sigs/sig-storage-lib-external-\\nprovisioner .\\nFor example, NFS doesn't provide an internal provisioner, but an external provisioner can be\\nused. There are also cases when 3rd party storage vendors provide their own external\\nprovisioner.\\nReclaim Policy\\nPersistentVolumes that are dynamically created by a StorageClass will have the reclaim policy\\nspecified in the reclaimPolicy  field of the class, which can be either Delete  or Retain . If no \\nreclaimPolicy  is specified when a StorageClass object is created, it will default to Delete .\\nPersistentVolumes that are created manually and managed via a StorageClass will have\\nwhatever reclaim policy they were assigned at creation.\\nAllow Volume Expansion\\nPersistentVolumes can be configured to be expandable. This feature when set to true, allows the\\nusers to resize the volume by editing the corresponding PVC object.\", metadata={'source': './PDFS/Concepts.pdf', 'page': 333}),\n",
       " Document(page_content='users to resize the volume by editing the corresponding PVC object.\\nThe following types of volumes support volume expansion, when the underlying StorageClass\\nhas the field allowVolumeExpansion  set to true.\\nTable of Volume types and the version of\\nKubernetes they require\\nVolume type Required Kubernetes version\\ngcePersistentDisk 1.11\\nrbd 1.11\\nAzure File 1.11\\nPortworx 1.11\\nFlexVolume 1.13\\nCSI 1.14 (alpha), 1.16 (beta)\\nNote:  You can only use the volume expansion feature to grow a Volume, not to shrink it.\\nMount Options\\nPersistentVolumes that are dynamically created by a StorageClass will have the mount options\\nspecified in the mountOptions  field of the class.\\nIf the volume plugin does not support mount options but mount options are specified,\\nprovisioning will fail. Mount options are not validated on either the class or PV. If a mount\\noption is invalid, the PV mount fails.\\nVolume Binding Mode\\nThe volumeBindingMode  field controls when volume binding and dynamic provisioning  should', metadata={'source': './PDFS/Concepts.pdf', 'page': 333}),\n",
       " Document(page_content='The volumeBindingMode  field controls when volume binding and dynamic provisioning  should\\noccur. When unset, \"Immediate\" mode is used by default.\\nThe Immediate  mode indicates that volume binding and dynamic provisioning occurs once the\\nPersistentVolumeClaim is created. For storage backends that are topology-constrained and not', metadata={'source': './PDFS/Concepts.pdf', 'page': 333}),\n",
       " Document(page_content=\"globally accessible from all Nodes in the cluster, PersistentVolumes will be bound or\\nprovisioned without knowledge of the Pod's scheduling requirements. This may result in\\nunschedulable Pods.\\nA cluster administrator can address this issue by specifying the WaitForFirstConsumer  mode\\nwhich will delay the binding and provisioning of a PersistentVolume until a Pod using the\\nPersistentVolumeClaim is created. PersistentVolumes will be selected or provisioned\\nconforming to the topology that is specified by the Pod's scheduling constraints. These include,\\nbut are not limited to, resource requirements , node selectors , pod affinity and anti-affinity , and \\ntaints and tolerations .\\nThe following plugins support WaitForFirstConsumer  with dynamic provisioning:\\nGCEPersistentDisk\\nThe following plugins support WaitForFirstConsumer  with pre-created PersistentVolume\\nbinding:\\nAll of the above\\nLocal\\nCSI volumes  are also supported with dynamic provisioning and pre-created PVs, but you'll need\", metadata={'source': './PDFS/Concepts.pdf', 'page': 334}),\n",
       " Document(page_content='Local\\nCSI volumes  are also supported with dynamic provisioning and pre-created PVs, but you\\'ll need\\nto look at the documentation for a specific CSI driver to see its supported topology keys and\\nexamples.\\nNote:\\nIf you choose to use WaitForFirstConsumer , do not use nodeName  in the Pod spec to specify\\nnode affinity. If nodeName  is used in this case, the scheduler will be bypassed and PVC will\\nremain in pending  state.\\nInstead, you can use node selector for hostname in this case as shown below.\\napiVersion : v1\\nkind: Pod\\nmetadata :\\n  name : task-pv-pod\\nspec:\\n  nodeSelector :\\n    kubernetes.io/hostname : kube-01\\n  volumes :\\n    - name : task-pv-storage\\n      persistentVolumeClaim :\\n        claimName : task-pv-claim\\n  containers :\\n    - name : task-pv-container\\n      image : nginx\\n      ports :\\n        - containerPort : 80\\n          name : \"http-server\"\\n      volumeMounts :\\n        - mountPath : \"/usr/share/nginx/html\"\\n          name : task-pv-storage• \\n• \\n•', metadata={'source': './PDFS/Concepts.pdf', 'page': 334}),\n",
       " Document(page_content='Allowed Topologies\\nWhen a cluster operator specifies the WaitForFirstConsumer  volume binding mode, it is no\\nlonger necessary to restrict provisioning to specific topologies in most situations. However, if\\nstill required, allowedTopologies  can be specified.\\nThis example demonstrates how to restrict the topology of provisioned volumes to specific\\nzones and should be used as a replacement for the zone  and zones  parameters for the supported\\nplugins.\\napiVersion : storage.k8s.io/v1\\nkind: StorageClass\\nmetadata :\\n  name : standard\\nprovisioner : kubernetes.io/gce-pd\\nparameters :\\n  type: pd-standard\\nvolumeBindingMode : WaitForFirstConsumer\\nallowedTopologies :\\n- matchLabelExpressions :\\n  - key: topology.kubernetes.io/zone\\n    values :\\n    - us-central-1a\\n    - us-central-1b\\nParameters\\nStorage Classes have parameters that describe volumes belonging to the storage class. Different\\nparameters may be accepted depending on the provisioner . For example, the value io1, for the', metadata={'source': './PDFS/Concepts.pdf', 'page': 335}),\n",
       " Document(page_content='parameters may be accepted depending on the provisioner . For example, the value io1, for the\\nparameter type, and the parameter iopsPerGB  are specific to EBS. When a parameter is omitted,\\nsome default is used.\\nThere can be at most 512 parameters defined for a StorageClass. The total length of the\\nparameters object including its keys and values cannot exceed 256 KiB.\\nAWS EBS\\napiVersion : storage.k8s.io/v1\\nkind: StorageClass\\nmetadata :\\n  name : slow\\nprovisioner : kubernetes.io/aws-ebs\\nparameters :\\n  type: io1\\n  iopsPerGB : \"10\"\\n  fsType : ext4\\ntype: io1, gp2, sc1, st1. See AWS docs  for details. Default: gp2.\\nzone  (Deprecated): AWS zone. If neither zone  nor zones  is specified, volumes are\\ngenerally round-robin-ed across all active zones where Kubernetes cluster has a node. \\nzone  and zones  parameters must not be used at the same time.• \\n•', metadata={'source': './PDFS/Concepts.pdf', 'page': 335}),\n",
       " Document(page_content='zones  (Deprecated): A comma separated list of AWS zone(s). If neither zone  nor zones  is\\nspecified, volumes are generally round-robin-ed across all active zones where Kubernetes\\ncluster has a node. zone  and zones  parameters must not be used at the same time.\\niopsPerGB : only for io1 volumes. I/O operations per second per GiB. AWS volume plugin\\nmultiplies this with size of requested volume to compute IOPS of the volume and caps it\\nat 20 000 IOPS (maximum supported by AWS, see AWS docs ). A string is expected here,\\ni.e. \"10\", not 10.\\nfsType : fsType that is supported by kubernetes. Default: \"ext4\" .\\nencrypted : denotes whether the EBS volume should be encrypted or not. Valid values are \\n\"true\"  or \"false\" . A string is expected here, i.e. \"true\" , not true.\\nkmsKeyId : optional. The full Amazon Resource Name of the key to use when encrypting\\nthe volume. If none is supplied but encrypted  is true, a key is generated by AWS. See\\nAWS docs for valid ARN value.', metadata={'source': './PDFS/Concepts.pdf', 'page': 336}),\n",
       " Document(page_content='AWS docs for valid ARN value.\\nNote:  zone  and zones  parameters are deprecated and replaced with allowedTopologies\\nGCE PD\\napiVersion : storage.k8s.io/v1\\nkind: StorageClass\\nmetadata :\\n  name : slow\\nprovisioner : kubernetes.io/gce-pd\\nparameters :\\n  type: pd-standard\\n  fstype : ext4\\n  replication-type : none\\ntype: pd-standard  or pd-ssd . Default: pd-standard\\nzone  (Deprecated): GCE zone. If neither zone  nor zones  is specified, volumes are\\ngenerally round-robin-ed across all active zones where Kubernetes cluster has a node. \\nzone  and zones  parameters must not be used at the same time.\\nzones  (Deprecated): A comma separated list of GCE zone(s). If neither zone  nor zones  is\\nspecified, volumes are generally round-robin-ed across all active zones where Kubernetes\\ncluster has a node. zone  and zones  parameters must not be used at the same time.\\nfstype : ext4 or xfs. Default: ext4. The defined filesystem type must be supported by the\\nhost operating system.', metadata={'source': './PDFS/Concepts.pdf', 'page': 336}),\n",
       " Document(page_content=\"host operating system.\\nreplication-type : none  or regional-pd . Default: none .\\nIf replication-type  is set to none , a regular (zonal) PD will be provisioned.\\nIf replication-type  is set to regional-pd , a Regional Persistent Disk  will be provisioned. It's\\nhighly recommended to have volumeBindingMode: WaitForFirstConsumer  set, in which case\\nwhen you create a Pod that consumes a PersistentVolumeClaim which uses this StorageClass, a\\nRegional Persistent Disk is provisioned with two zones. One zone is the same as the zone that\\nthe Pod is scheduled in. The other zone is randomly picked from the zones available to the\\ncluster. Disk zones can be further constrained using allowedTopologies .\\nNote:  zone  and zones  parameters are deprecated and replaced with allowedTopologies . When \\nGCE CSI Migration  is enabled, a GCE PD volume can be provisioned in a topology that does not• \\n• \\n• \\n• \\n• \\n• \\n• \\n• \\n• \\n•\", metadata={'source': './PDFS/Concepts.pdf', 'page': 336}),\n",
       " Document(page_content='match any nodes, but any pod trying to use that volume will fail to schedule. With legacy pre-\\nmigration GCE PD, in this case an error will be produced instead at provisioning time. GCE CSI\\nMigration is enabled by default beginning from the Kubernetes 1.23 release.\\nNFS\\napiVersion : storage.k8s.io/v1\\nkind: StorageClass\\nmetadata :\\n  name : example-nfs\\nprovisioner : example.com/external-nfs\\nparameters :\\n  server : nfs-server.example.com\\n  path: /share\\n  readOnly : \"false\"\\nserver : Server is the hostname or IP address of the NFS server.\\npath: Path that is exported by the NFS server.\\nreadOnly : A flag indicating whether the storage will be mounted as read only (default\\nfalse).\\nKubernetes doesn\\'t include an internal NFS provisioner. You need to use an external provisioner\\nto create a StorageClass for NFS. Here are some examples:\\nNFS Ganesha server and external provisioner\\nNFS subdir external provisioner\\nvSphere\\nThere are two types of provisioners for vSphere storage classes:', metadata={'source': './PDFS/Concepts.pdf', 'page': 337}),\n",
       " Document(page_content='vSphere\\nThere are two types of provisioners for vSphere storage classes:\\nCSI provisioner : csi.vsphere.vmware.com\\nvCP provisioner : kubernetes.io/vsphere-volume\\nIn-tree provisioners are deprecated . For more information on the CSI provisioner, see \\nKubernetes vSphere CSI Driver  and vSphereVolume CSI migration .\\nCSI Provisioner\\nThe vSphere CSI StorageClass provisioner works with Tanzu Kubernetes clusters. For an\\nexample, refer to the vSphere CSI repository .\\nvCP Provisioner\\nThe following examples use the VMware Cloud Provider (vCP) StorageClass provisioner.\\nCreate a StorageClass with a user specified disk format.\\napiVersion : storage.k8s.io/v1\\nkind: StorageClass\\nmetadata :\\n  name : fast\\nprovisioner : kubernetes.io/vsphere-volume• \\n• \\n• \\n• \\n• \\n• \\n• \\n1.', metadata={'source': './PDFS/Concepts.pdf', 'page': 337}),\n",
       " Document(page_content='parameters :\\n  diskformat : zeroedthick\\ndiskformat : thin, zeroedthick  and eagerzeroedthick . Default: \"thin\" .\\nCreate a StorageClass with a disk format on a user specified datastore.\\napiVersion : storage.k8s.io/v1\\nkind: StorageClass\\nmetadata :\\n  name : fast\\nprovisioner : kubernetes.io/vsphere-volume\\nparameters :\\n  diskformat : zeroedthick\\n  datastore : VSANDatastore\\ndatastore : The user can also specify the datastore in the StorageClass. The volume will be\\ncreated on the datastore specified in the StorageClass, which in this case is \\nVSANDatastore . This field is optional. If the datastore is not specified, then the volume\\nwill be created on the datastore specified in the vSphere config file used to initialize the\\nvSphere Cloud Provider.\\nStorage Policy Management inside kubernetes\\nUsing existing vCenter SPBM policy\\nOne of the most important features of vSphere for Storage Management is policy\\nbased Management. Storage Policy Based Management (SPBM) is a storage policy', metadata={'source': './PDFS/Concepts.pdf', 'page': 338}),\n",
       " Document(page_content='based Management. Storage Policy Based Management (SPBM) is a storage policy\\nframework that provides a single unified control plane across a broad range of data\\nservices and storage solutions. SPBM enables vSphere administrators to overcome\\nupfront storage provisioning challenges, such as capacity planning, differentiated\\nservice levels and managing capacity headroom.\\nThe SPBM policies can be specified in the StorageClass using the \\nstoragePolicyName  parameter.\\nVirtual SAN policy support inside Kubernetes\\nVsphere Infrastructure (VI) Admins will have the ability to specify custom Virtual\\nSAN Storage Capabilities during dynamic volume provisioning. You can now define\\nstorage requirements, such as performance and availability, in the form of storage\\ncapabilities during dynamic volume provisioning. The storage capability\\nrequirements are converted into a Virtual SAN policy which are then pushed down\\nto the Virtual SAN layer when a persistent volume (virtual disk) is being created.', metadata={'source': './PDFS/Concepts.pdf', 'page': 338}),\n",
       " Document(page_content='to the Virtual SAN layer when a persistent volume (virtual disk) is being created.\\nThe virtual disk is distributed across the Virtual SAN datastore to meet the\\nrequirements.\\nYou can see Storage Policy Based Management for dynamic provisioning of\\nvolumes  for more details on how to use storage policies for persistent volumes\\nmanagement.\\nThere are few vSphere examples  which you try out for persistent volume management inside\\nKubernetes for vSphere.2. \\n3. \\n◦ \\n◦', metadata={'source': './PDFS/Concepts.pdf', 'page': 338}),\n",
       " Document(page_content='Ceph RBD\\nNote:\\nFEATURE STATE:  Kubernetes v1.28 [deprecated]\\nThis internal provisioner of Ceph RBD is deprecated. Please use CephFS RBD CSI driver .\\napiVersion : storage.k8s.io/v1\\nkind: StorageClass\\nmetadata :\\n  name : fast\\nprovisioner : kubernetes.io/rbd\\nparameters :\\n  monitors : 10.16.153.105 :6789\\n  adminId : kube\\n  adminSecretName : ceph-secret\\n  adminSecretNamespace : kube-system\\n  pool: kube\\n  userId : kube\\n  userSecretName : ceph-secret-user\\n  userSecretNamespace : default\\n  fsType : ext4\\n  imageFormat : \"2\"\\n  imageFeatures : \"layering\"\\nmonitors : Ceph monitors, comma delimited. This parameter is required.\\nadminId : Ceph client ID that is capable of creating images in the pool. Default is \"admin\".\\nadminSecretName : Secret Name for adminId . This parameter is required. The provided\\nsecret must have type \"kubernetes.io/rbd\".\\nadminSecretNamespace : The namespace for adminSecretName . Default is \"default\".\\npool: Ceph RBD pool. Default is \"rbd\".', metadata={'source': './PDFS/Concepts.pdf', 'page': 339}),\n",
       " Document(page_content='pool: Ceph RBD pool. Default is \"rbd\".\\nuserId : Ceph client ID that is used to map the RBD image. Default is the same as adminId .\\nuserSecretName : The name of Ceph Secret for userId  to map RBD image. It must exist in\\nthe same namespace as PVCs. This parameter is required. The provided secret must have\\ntype \"kubernetes.io/rbd\", for example created in this way:\\nkubectl create secret generic ceph-secret --type =\"kubernetes.io/rbd\"  \\\\\\n  --from-literal =key=\\'QVFEQ1pMdFhPUnQrSmhBQUFYaERWNHJsZ3BsMmNjcDR6RFZS\\nT0E9PQ==\\'  \\\\\\n  --namespace =kube-system\\nuserSecretNamespace : The namespace for userSecretName .\\nfsType : fsType that is supported by kubernetes. Default: \"ext4\" .\\nimageFormat : Ceph RBD image format, \"1\" or \"2\". Default is \"2\".• \\n• \\n• \\n• \\n• \\n• \\n• \\n• \\n• \\n•', metadata={'source': './PDFS/Concepts.pdf', 'page': 339}),\n",
       " Document(page_content='imageFeatures : This parameter is optional and should only be used if you set \\nimageFormat  to \"2\". Currently supported features are layering  only. Default is \"\", and no\\nfeatures are turned on.\\nAzure Disk\\nAzure Unmanaged Disk storage class\\napiVersion : storage.k8s.io/v1\\nkind: StorageClass\\nmetadata :\\n  name : slow\\nprovisioner : kubernetes.io/azure-disk\\nparameters :\\n  skuName : Standard_LRS\\n  location : eastus\\n  storageAccount : azure_storage_account_name\\nskuName : Azure storage account Sku tier. Default is empty.\\nlocation : Azure storage account location. Default is empty.\\nstorageAccount : Azure storage account name. If a storage account is provided, it must\\nreside in the same resource group as the cluster, and location  is ignored. If a storage\\naccount is not provided, a new storage account will be created in the same resource\\ngroup as the cluster.\\nAzure Disk storage class (starting from v1.7.2)\\napiVersion : storage.k8s.io/v1\\nkind: StorageClass\\nmetadata :\\n  name : slow', metadata={'source': './PDFS/Concepts.pdf', 'page': 340}),\n",
       " Document(page_content='apiVersion : storage.k8s.io/v1\\nkind: StorageClass\\nmetadata :\\n  name : slow\\nprovisioner : kubernetes.io/azure-disk\\nparameters :\\n  storageaccounttype : Standard_LRS\\n  kind: managed\\nstorageaccounttype : Azure storage account Sku tier. Default is empty.\\nkind: Possible values are shared , dedicated , and managed  (default). When kind is shared ,\\nall unmanaged disks are created in a few shared storage accounts in the same resource\\ngroup as the cluster. When kind is dedicated , a new dedicated storage account will be\\ncreated for the new unmanaged disk in the same resource group as the cluster. When \\nkind is managed , all managed disks are created in the same resource group as the cluster.\\nresourceGroup : Specify the resource group in which the Azure disk will be created. It\\nmust be an existing resource group name. If it is unspecified, the disk will be placed in the\\nsame resource group as the current Kubernetes cluster.', metadata={'source': './PDFS/Concepts.pdf', 'page': 340}),\n",
       " Document(page_content='same resource group as the current Kubernetes cluster.\\nPremium VM can attach both Standard_LRS and Premium_LRS disks, while Standard VM\\ncan only attach Standard_LRS disks.\\nManaged VM can only attach managed disks and unmanaged VM can only attach\\nunmanaged disks.• \\n• \\n• \\n• \\n• \\n• \\n• \\n• \\n•', metadata={'source': './PDFS/Concepts.pdf', 'page': 340}),\n",
       " Document(page_content='Azure File\\napiVersion : storage.k8s.io/v1\\nkind: StorageClass\\nmetadata :\\n  name : azurefile\\nprovisioner : kubernetes.io/azure-file\\nparameters :\\n  skuName : Standard_LRS\\n  location : eastus\\n  storageAccount : azure_storage_account_name\\nskuName : Azure storage account Sku tier. Default is empty.\\nlocation : Azure storage account location. Default is empty.\\nstorageAccount : Azure storage account name. Default is empty. If a storage account is not\\nprovided, all storage accounts associated with the resource group are searched to find one\\nthat matches skuName  and location . If a storage account is provided, it must reside in the\\nsame resource group as the cluster, and skuName  and location  are ignored.\\nsecretNamespace : the namespace of the secret that contains the Azure Storage Account\\nName and Key. Default is the same as the Pod.\\nsecretName : the name of the secret that contains the Azure Storage Account Name and\\nKey. Default is azure-storage-account-<accountName>-secret', metadata={'source': './PDFS/Concepts.pdf', 'page': 341}),\n",
       " Document(page_content='Key. Default is azure-storage-account-<accountName>-secret\\nreadOnly : a flag indicating whether the storage will be mounted as read only. Defaults to\\nfalse which means a read/write mount. This setting will impact the ReadOnly  setting in\\nVolumeMounts as well.\\nDuring storage provisioning, a secret named by secretName  is created for the mounting\\ncredentials. If the cluster has enabled both RBAC  and Controller Roles , add the create\\npermission of resource secret  for clusterrole system:controller:persistent-volume-binder .\\nIn a multi-tenancy context, it is strongly recommended to set the value for secretNamespace\\nexplicitly, otherwise the storage account credentials may be read by other users.\\nPortworx Volume\\napiVersion : storage.k8s.io/v1\\nkind: StorageClass\\nmetadata :\\n  name : portworx-io-priority-high\\nprovisioner : kubernetes.io/portworx-volume\\nparameters :\\n  repl: \"1\"\\n  snap_interval : \"70\"\\n  priority_io : \"high\"\\nfs: filesystem to be laid out: none/xfs/ext4  (default: ext4).', metadata={'source': './PDFS/Concepts.pdf', 'page': 341}),\n",
       " Document(page_content='priority_io : \"high\"\\nfs: filesystem to be laid out: none/xfs/ext4  (default: ext4).\\nblock_size : block size in Kbytes (default: 32).\\nrepl: number of synchronous replicas to be provided in the form of replication factor 1..3\\n(default: 1) A string is expected here i.e. \"1\" and not 1.\\npriority_io : determines whether the volume will be created from higher performance or a\\nlower priority storage high/medium/low  (default: low).• \\n• \\n• \\n• \\n• \\n• \\n• \\n• \\n• \\n•', metadata={'source': './PDFS/Concepts.pdf', 'page': 341}),\n",
       " Document(page_content='snap_interval : clock/time interval in minutes for when to trigger snapshots. Snapshots\\nare incremental based on difference with the prior snapshot, 0 disables snaps (default: 0).\\nA string is expected here i.e. \"70\" and not 70.\\naggregation_level : specifies the number of chunks the volume would be distributed into, 0\\nindicates a non-aggregated volume (default: 0). A string is expected here i.e. \"0\" and not 0\\nephemeral : specifies whether the volume should be cleaned-up after unmount or should\\nbe persistent. emptyDir  use case can set this value to true and persistent volumes  use\\ncase such as for databases like Cassandra should set to false, true/false  (default false). A\\nstring is expected here i.e. \"true\"  and not true.\\nLocal\\napiVersion : storage.k8s.io/v1\\nkind: StorageClass\\nmetadata :\\n  name : local-storage\\nprovisioner : kubernetes.io/no-provisioner\\nvolumeBindingMode : WaitForFirstConsumer\\nLocal volumes do not currently support dynamic provisioning, however a StorageClass should', metadata={'source': './PDFS/Concepts.pdf', 'page': 342}),\n",
       " Document(page_content=\"Local volumes do not currently support dynamic provisioning, however a StorageClass should\\nstill be created to delay volume binding until Pod scheduling. This is specified by the \\nWaitForFirstConsumer  volume binding mode.\\nDelaying volume binding allows the scheduler to consider all of a Pod's scheduling constraints\\nwhen choosing an appropriate PersistentVolume for a PersistentVolumeClaim.\\nDynamic Volume Provisioning\\nDynamic volume provisioning allows storage volumes to be created on-demand. Without\\ndynamic provisioning, cluster administrators have to manually make calls to their cloud or\\nstorage provider to create new storage volumes, and then create PersistentVolume  objects  to\\nrepresent them in Kubernetes. The dynamic provisioning feature eliminates the need for cluster\\nadministrators to pre-provision storage. Instead, it automatically provisions storage when it is\\nrequested by users.\\nBackground\", metadata={'source': './PDFS/Concepts.pdf', 'page': 342}),\n",
       " Document(page_content=\"requested by users.\\nBackground\\nThe implementation of dynamic volume provisioning is based on the API object StorageClass\\nfrom the API group storage.k8s.io . A cluster administrator can define as many StorageClass\\nobjects as needed, each specifying a volume plugin  (aka provisioner ) that provisions a volume\\nand the set of parameters to pass to that provisioner when provisioning. A cluster administrator\\ncan define and expose multiple flavors of storage (from the same or different storage systems)\\nwithin a cluster, each with a custom set of parameters. This design also ensures that end users\\ndon't have to worry about the complexity and nuances of how storage is provisioned, but still\\nhave the ability to select from multiple storage options.\\nMore information on storage classes can be found here.• \\n• \\n•\", metadata={'source': './PDFS/Concepts.pdf', 'page': 342}),\n",
       " Document(page_content='Enabling Dynamic Provisioning\\nTo enable dynamic provisioning, a cluster administrator needs to pre-create one or more\\nStorageClass objects for users. StorageClass objects define which provisioner should be used\\nand what parameters should be passed to that provisioner when dynamic provisioning is\\ninvoked. The name of a StorageClass object must be a valid DNS subdomain name .\\nThe following manifest creates a storage class \"slow\" which provisions standard disk-like\\npersistent disks.\\napiVersion : storage.k8s.io/v1\\nkind: StorageClass\\nmetadata :\\n  name : slow\\nprovisioner : kubernetes.io/gce-pd\\nparameters :\\n  type: pd-standard\\nThe following manifest creates a storage class \"fast\" which provisions SSD-like persistent disks.\\napiVersion : storage.k8s.io/v1\\nkind: StorageClass\\nmetadata :\\n  name : fast\\nprovisioner : kubernetes.io/gce-pd\\nparameters :\\n  type: pd-ssd\\nUsing Dynamic Provisioning\\nUsers request dynamically provisioned storage by including a storage class in their', metadata={'source': './PDFS/Concepts.pdf', 'page': 343}),\n",
       " Document(page_content='Users request dynamically provisioned storage by including a storage class in their \\nPersistentVolumeClaim . Before Kubernetes v1.6, this was done via the \\nvolume.beta.kubernetes.io/storage-class  annotation. However, this annotation is deprecated\\nsince v1.9. Users now can and should instead use the storageClassName  field of the \\nPersistentVolumeClaim  object. The value of this field must match the name of a StorageClass\\nconfigured by the administrator (see below ).\\nTo select the \"fast\" storage class, for example, a user would create the following\\nPersistentVolumeClaim:\\napiVersion : v1\\nkind: PersistentVolumeClaim\\nmetadata :\\n  name : claim1\\nspec:\\n  accessModes :\\n    - ReadWriteOnce\\n  storageClassName : fast\\n  resources :\\n    requests :\\n      storage : 30Gi', metadata={'source': './PDFS/Concepts.pdf', 'page': 343}),\n",
       " Document(page_content='This claim results in an SSD-like Persistent Disk being automatically provisioned. When the\\nclaim is deleted, the volume is destroyed.\\nDefaulting Behavior\\nDynamic provisioning can be enabled on a cluster such that all claims are dynamically\\nprovisioned if no storage class is specified. A cluster administrator can enable this behavior by:\\nMarking one StorageClass  object as default ;\\nMaking sure that the DefaultStorageClass  admission controller  is enabled on the API\\nserver.\\nAn administrator can mark a specific StorageClass  as default by adding the \\nstorageclass.kubernetes.io/is-default-class  annotation  to it. When a default StorageClass  exists\\nin a cluster and a user creates a PersistentVolumeClaim  with storageClassName  unspecified, the \\nDefaultStorageClass  admission controller automatically adds the storageClassName  field\\npointing to the default storage class.\\nNote that there can be at most one default  storage class on a cluster, or a', metadata={'source': './PDFS/Concepts.pdf', 'page': 344}),\n",
       " Document(page_content='Note that there can be at most one default  storage class on a cluster, or a \\nPersistentVolumeClaim  without storageClassName  explicitly specified cannot be created.\\nTopology Awareness\\nIn Multi-Zone  clusters, Pods can be spread across Zones in a Region. Single-Zone storage\\nbackends should be provisioned in the Zones where Pods are scheduled. This can be\\naccomplished by setting the Volume Binding Mode .\\nVolume Snapshots\\nIn Kubernetes, a VolumeSnapshot  represents a snapshot of a volume on a storage system. This\\ndocument assumes that you are already familiar with Kubernetes persistent volumes .\\nIntroduction\\nSimilar to how API resources PersistentVolume  and PersistentVolumeClaim  are used to\\nprovision volumes for users and administrators, VolumeSnapshotContent  and VolumeSnapshot\\nAPI resources are provided to create volume snapshots for users and administrators.\\nA VolumeSnapshotContent  is a snapshot taken from a volume in the cluster that has been', metadata={'source': './PDFS/Concepts.pdf', 'page': 344}),\n",
       " Document(page_content='A VolumeSnapshotContent  is a snapshot taken from a volume in the cluster that has been\\nprovisioned by an administrator. It is a resource in the cluster just like a PersistentVolume is a\\ncluster resource.\\nA VolumeSnapshot  is a request for snapshot of a volume by a user. It is similar to a\\nPersistentVolumeClaim.\\nVolumeSnapshotClass  allows you to specify different attributes belonging to a VolumeSnapshot .\\nThese attributes may differ among snapshots taken from the same volume on the storage\\nsystem and therefore cannot be expressed by using the same StorageClass  of a \\nPersistentVolumeClaim .• \\n•', metadata={'source': './PDFS/Concepts.pdf', 'page': 344}),\n",
       " Document(page_content=\"Volume snapshots provide Kubernetes users with a standardized way to copy a volume's\\ncontents at a particular point in time without creating an entirely new volume. This\\nfunctionality enables, for example, database administrators to backup databases before\\nperforming edit or delete modifications.\\nUsers need to be aware of the following when using this feature:\\nAPI Objects VolumeSnapshot , VolumeSnapshotContent , and VolumeSnapshotClass  are \\nCRDs , not part of the core API.\\nVolumeSnapshot  support is only available for CSI drivers.\\nAs part of the deployment process of VolumeSnapshot , the Kubernetes team provides a\\nsnapshot controller to be deployed into the control plane, and a sidecar helper container\\ncalled csi-snapshotter to be deployed together with the CSI driver. The snapshot\\ncontroller watches VolumeSnapshot  and VolumeSnapshotContent  objects and is\\nresponsible for the creation and deletion of VolumeSnapshotContent  object. The sidecar\", metadata={'source': './PDFS/Concepts.pdf', 'page': 345}),\n",
       " Document(page_content='responsible for the creation and deletion of VolumeSnapshotContent  object. The sidecar\\ncsi-snapshotter watches VolumeSnapshotContent  objects and triggers CreateSnapshot\\nand DeleteSnapshot  operations against a CSI endpoint.\\nThere is also a validating webhook server which provides tightened validation on\\nsnapshot objects. This should be installed by the Kubernetes distros along with the\\nsnapshot controller and CRDs, not CSI drivers. It should be installed in all Kubernetes\\nclusters that has the snapshot feature enabled.\\nCSI drivers may or may not have implemented the volume snapshot functionality. The\\nCSI drivers that have provided support for volume snapshot will likely use the csi-\\nsnapshotter. See CSI Driver documentation  for details.\\nThe CRDs and snapshot controller installations are the responsibility of the Kubernetes\\ndistribution.\\nLifecycle of a volume snapshot and volume snapshot\\ncontent', metadata={'source': './PDFS/Concepts.pdf', 'page': 345}),\n",
       " Document(page_content='distribution.\\nLifecycle of a volume snapshot and volume snapshot\\ncontent\\nVolumeSnapshotContents  are resources in the cluster. VolumeSnapshots  are requests for those\\nresources. The interaction between VolumeSnapshotContents  and VolumeSnapshots  follow this\\nlifecycle:\\nProvisioning Volume Snapshot\\nThere are two ways snapshots may be provisioned: pre-provisioned or dynamically provisioned.\\nPre-provisioned\\nA cluster administrator creates a number of VolumeSnapshotContents . They carry the details of\\nthe real volume snapshot on the storage system which is available for use by cluster users. They\\nexist in the Kubernetes API and are available for consumption.\\nDynamic\\nInstead of using a pre-existing snapshot, you can request that a snapshot to be dynamically\\ntaken from a PersistentVolumeClaim. The VolumeSnapshotClass  specifies storage provider-\\nspecific parameters to use when taking a snapshot.• \\n• \\n• \\n• \\n• \\n•', metadata={'source': './PDFS/Concepts.pdf', 'page': 345}),\n",
       " Document(page_content='Binding\\nThe snapshot controller handles the binding of a VolumeSnapshot  object with an appropriate \\nVolumeSnapshotContent  object, in both pre-provisioned and dynamically provisioned\\nscenarios. The binding is a one-to-one mapping.\\nIn the case of pre-provisioned binding, the VolumeSnapshot will remain unbound until the\\nrequested VolumeSnapshotContent object is created.\\nPersistent Volume Claim as Snapshot Source Protection\\nThe purpose of this protection is to ensure that in-use PersistentVolumeClaim  API objects are\\nnot removed from the system while a snapshot is being taken from it (as this may result in data\\nloss).\\nWhile a snapshot is being taken of a PersistentVolumeClaim, that PersistentVolumeClaim is in-\\nuse. If you delete a PersistentVolumeClaim API object in active use as a snapshot source, the\\nPersistentVolumeClaim object is not removed immediately. Instead, removal of the\\nPersistentVolumeClaim object is postponed until the snapshot is readyToUse or aborted.\\nDelete', metadata={'source': './PDFS/Concepts.pdf', 'page': 346}),\n",
       " Document(page_content='PersistentVolumeClaim object is postponed until the snapshot is readyToUse or aborted.\\nDelete\\nDeletion is triggered by deleting the VolumeSnapshot  object, and the DeletionPolicy  will be\\nfollowed. If the DeletionPolicy  is Delete , then the underlying storage snapshot will be deleted\\nalong with the VolumeSnapshotContent  object. If the DeletionPolicy  is Retain , then both the\\nunderlying snapshot and VolumeSnapshotContent  remain.\\nVolumeSnapshots\\nEach VolumeSnapshot contains a spec and a status.\\napiVersion : snapshot.storage.k8s.io/v1\\nkind: VolumeSnapshot\\nmetadata :\\n  name : new-snapshot-test\\nspec:\\n  volumeSnapshotClassName : csi-hostpath-snapclass\\n  source :\\n    persistentVolumeClaimName : pvc-test\\npersistentVolumeClaimName  is the name of the PersistentVolumeClaim data source for the\\nsnapshot. This field is required for dynamically provisioning a snapshot.\\nA volume snapshot can request a particular class by specifying the name of a', metadata={'source': './PDFS/Concepts.pdf', 'page': 346}),\n",
       " Document(page_content='A volume snapshot can request a particular class by specifying the name of a \\nVolumeSnapshotClass  using the attribute volumeSnapshotClassName . If nothing is set, then the\\ndefault class is used if available.\\nFor pre-provisioned snapshots, you need to specify a volumeSnapshotContentName  as the\\nsource for the snapshot as shown in the following example. The volumeSnapshotContentName\\nsource field is required for pre-provisioned snapshots.', metadata={'source': './PDFS/Concepts.pdf', 'page': 346}),\n",
       " Document(page_content='apiVersion : snapshot.storage.k8s.io/v1\\nkind: VolumeSnapshot\\nmetadata :\\n  name : test-snapshot\\nspec:\\n  source :\\n    volumeSnapshotContentName : test-content\\nVolume Snapshot Contents\\nEach VolumeSnapshotContent contains a spec and status. In dynamic provisioning, the\\nsnapshot common controller creates VolumeSnapshotContent  objects. Here is an example:\\napiVersion : snapshot.storage.k8s.io/v1\\nkind: VolumeSnapshotContent\\nmetadata :\\n  name : snapcontent-72d9a349-aacd-42d2-a240-d775650d2455\\nspec:\\n  deletionPolicy : Delete\\n  driver : hostpath.csi.k8s.io\\n  source :\\n    volumeHandle : ee0cfb94-f8d4-11e9-b2d8-0242ac110002\\n  sourceVolumeMode : Filesystem\\n  volumeSnapshotClassName : csi-hostpath-snapclass\\n  volumeSnapshotRef :\\n    name : new-snapshot-test\\n    namespace : default\\n    uid: 72d9a349-aacd-42d2-a240-d775650d2455\\nvolumeHandle  is the unique identifier of the volume created on the storage backend and', metadata={'source': './PDFS/Concepts.pdf', 'page': 347}),\n",
       " Document(page_content='volumeHandle  is the unique identifier of the volume created on the storage backend and\\nreturned by the CSI driver during the volume creation. This field is required for dynamically\\nprovisioning a snapshot. It specifies the volume source of the snapshot.\\nFor pre-provisioned snapshots, you (as cluster administrator) are responsible for creating the \\nVolumeSnapshotContent  object as follows.\\napiVersion : snapshot.storage.k8s.io/v1\\nkind: VolumeSnapshotContent\\nmetadata :\\n  name : new-snapshot-content-test\\nspec:\\n  deletionPolicy : Delete\\n  driver : hostpath.csi.k8s.io\\n  source :\\n    snapshotHandle : 7bdd0de3-aaeb-11e8-9aae-0242ac110002\\n  sourceVolumeMode : Filesystem\\n  volumeSnapshotRef :\\n    name : new-snapshot-test\\n    namespace : default\\nsnapshotHandle  is the unique identifier of the volume snapshot created on the storage backend.\\nThis field is required for the pre-provisioned snapshots. It specifies the CSI snapshot id on the\\nstorage system that this VolumeSnapshotContent  represents.', metadata={'source': './PDFS/Concepts.pdf', 'page': 347}),\n",
       " Document(page_content=\"sourceVolumeMode  is the mode of the volume whose snapshot is taken. The value of the \\nsourceVolumeMode  field can be either Filesystem  or Block . If the source volume mode is not\\nspecified, Kubernetes treats the snapshot as if the source volume's mode is unknown.\\nvolumeSnapshotRef  is the reference of the corresponding VolumeSnapshot . Note that when the \\nVolumeSnapshotContent  is being created as a pre-provisioned snapshot, the VolumeSnapshot\\nreferenced in volumeSnapshotRef  might not exist yet.\\nConverting the volume mode of a Snapshot\\nIf the VolumeSnapshots  API installed on your cluster supports the sourceVolumeMode  field,\\nthen the API has the capability to prevent unauthorized users from converting the mode of a\\nvolume.\\nTo check if your cluster has capability for this feature, run the following command:\\n$ kubectl get crd volumesnapshotcontent -o yaml\\nIf you want to allow users to create a PersistentVolumeClaim  from an existing VolumeSnapshot ,\", metadata={'source': './PDFS/Concepts.pdf', 'page': 348}),\n",
       " ...]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "texts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embeddings loaded\n"
     ]
    }
   ],
   "source": [
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "\n",
    "model_name = \"BAAI/bge-small-en\"\n",
    "model_kwargs = {'device': 'cpu'}\n",
    "encode_kwargs = {'normalize_embeddings': True}\n",
    "embeddings = HuggingFaceEmbeddings(\n",
    "    model_name=model_name,\n",
    "    model_kwargs=model_kwargs,\n",
    "    encode_kwargs=encode_kwargs\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save embeddings to vectorDB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.vectorstores import FAISS\n",
    "db = FAISS.from_documents(texts, embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Use Chatbot from verctorDB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.llms import LlamaCpp\n",
    "model = LlamaCpp(\n",
    "  model_path=\"../llama-2-7b-chat.Q5_K_M.gguf\",\n",
    "  temperature=0.0,\n",
    "  top_p=1,\n",
    "  max_tokens=8192,\n",
    "  verbose=True,\n",
    "  n_ctx=4096,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_template = \"\"\"You're a Kubernetes expert. You understand the question and can generate a good kubernetes manifest.\n",
    "Question: {question}\"\"\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
